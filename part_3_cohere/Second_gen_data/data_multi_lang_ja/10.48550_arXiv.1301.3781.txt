DOI: 10.48550/arXiv.1301.3781
Title: ベクトル空間における単語表現の効率的な推定 Efficient Estimation of Word Representations in Vector Space
Abstract: 我々は、非常に大規模なデータセットから単語の連続ベクトル表現を計算するための 2 つの新しいモデル アーキテクチャを提案します。これらの表現の品質は単語類似性タスクで測定され、その結果は、さまざまな種類のニューラル ネットワークに基づいた、以前に最高のパフォーマンスを発揮した手法と比較されます。はるかに低い計算コストで精度が大幅に向上していることがわかります。つまり、16 億語のデータセットから高品質の単語ベクトルを学習するのに 1 日もかかりません。さらに、これらのベクトルが構文的および意味的な単語の類似性を測定するためのテスト セットで最先端のパフォーマンスを提供することを示します。 We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.