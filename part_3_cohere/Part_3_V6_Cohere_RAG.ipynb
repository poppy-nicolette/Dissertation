{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere RAG with dense retriever and ReRank model\n",
    "- references: https://docs.cohere.com/v2/docs/rag-complete-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import cohere\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import sys\n",
    "from colorama import Fore, Style, Back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mall is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "# load secret from local .env file\n",
    "def get_key():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    _key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "    #verify if it worked\n",
    "    if _key is not None:\n",
    "        print(Fore.GREEN + \"all is good, beautiful!\")\n",
    "        return _key\n",
    "    else:\n",
    "        print(Fore.LIGHTRED_EX + \"API Key is missing\")\n",
    "\n",
    "# initilize client\n",
    "co = cohere.ClientV2(get_key())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of documents: 45\n",
      "Rank: 1\n",
      "Score: 0.6244086\n",
      "Document: DOI: 10.1162/qss_a_00286\n",
      " Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Abstract: Abstract The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\n",
      "\n",
      "\n",
      "Rank: 2\n",
      "Score: 0.334177\n",
      "Document: DOI: 10.31222/osf.io/smxe5\n",
      " Title: Crossref as a source of open bibliographic metadata\n",
      "Abstract: Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\n",
      "\n",
      "\n",
      "Rank: 3\n",
      "Score: 0.2940279\n",
      "Document: DOI: 10.48550/arXiv.2303.17661\n",
      " Title: MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\n",
      "Abstract: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\n",
      "\n",
      "\n",
      "Rank: 4\n",
      "Score: 0.24431385\n",
      "Document: DOI: 10.5281/ZENODO.13960973\n",
      " Title: Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\n",
      "Abstract: The DORA declaration and CoARA call for the use of bibliometric indicators based on open data. However, established scholarly metadata datasets are closed, and the quality of open datasets has not yet been thoroughly examined. In this paper, I present a method to detect errors in a dataset using missing data patterns. As an example, the method is applied to the affiliation metadata of publications associated with ETH Zurich. This allows me to identify a series of incorrectly affiliated papers. The method introduced in this paper is not specifically designed for affiliation data and can also be used to detect errors in other types of data. It could lead to corrections which will hopefully benefit providers as well as users of data.\n",
      "\n",
      "\n",
      "Rank: 5\n",
      "Score: 0.104981124\n",
      "Document: DOI: 10.1002/leap.1411\n",
      " Title: Over‐promotion and caution in abstracts of preprints during the <scp>COVID</scp>‐19 crisis\n",
      "Abstract: Abstract The abstract is known to be a promotional genre where researchers tend to exaggerate the benefit of their research and use a promotional discourse to catch the reader's attention. The COVID‐19 pandemic has prompted intensive research and has changed traditional publishing with the massive adoption of preprints by researchers. Our aim is to investigate whether the crisis and the ensuing scientific and economic competition have changed the lexical content of abstracts. We propose a comparative study of abstracts associated with preprints issued in response to the pandemic relative to abstracts produced during the closest pre‐pandemic period. We show that with the increase (on average and in percentage) of positive words (especially effective ) and the slight decrease of negative words, there is a strong increase in hedge words (the most frequent of which are the modal verbs can and may ). Hedge words counterbalance the excessive use of positive words and thus invite the readers, who go probably beyond the ‘usual’ audience, to be cautious with the obtained results. The abstracts of preprints urgently produced in response to the COVID‐19 crisis stand between uncertainty and over‐promotion, illustrating the balance that authors have to achieve between promoting their results and appealing for caution.\n",
      "\n",
      "\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00286\\n Title: Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: Abstract The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: Crossref as a source of open bibliographic metadata\\nAbstract: Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5281/ZENODO.13960973\\n Title: Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\\nAbstract: The DORA declaration and CoARA call for the use of bibliometric indicators based on open data. However, established scholarly metadata datasets are closed, and the quality of open datasets has not yet been thoroughly examined. In this paper, I present a method to detect errors in a dataset using missing data patterns. As an example, the method is applied to the affiliation metadata of publications associated with ETH Zurich. This allows me to identify a series of incorrectly affiliated papers. The method introduced in this paper is not specifically designed for affiliation data and can also be used to detect errors in other types of data. It could lead to corrections which will hopefully benefit providers as well as users of data.\\n']\n",
      "length of reranked_documents: 4\n",
      "Summary: Two studies examined the abstract in the metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - This study compared the amount of metadata and the completeness degree of research publications in new academic databases.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This study presented an up-to-date overview of the availability of six metadata elements in Crossref, including abstracts.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "\u001b[96m------\n",
      "Reranked documents:\n",
      "DOI: 10.1162/qss_a_00286\n",
      " Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Abstract: Abstract The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5\n",
      " Title: Crossref as a source of open bibliographic metadata\n",
      "Abstract: Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661\n",
      " Title: MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\n",
      "Abstract: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\n",
      "\n",
      "DOI: 10.5281/ZENODO.13960973\n",
      " Title: Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\n",
      "Abstract: The DORA declaration and CoARA call for the use of bibliometric indicators based on open data. However, established scholarly metadata datasets are closed, and the quality of open datasets has not yet been thoroughly examined. In this paper, I present a method to detect errors in a dataset using missing data patterns. As an example, the method is applied to the affiliation metadata of publications associated with ETH Zurich. This allows me to identify a series of incorrectly affiliated papers. The method introduced in this paper is not specifically designed for affiliation data and can also be used to detect errors in other types of data. It could lead to corrections which will hopefully benefit providers as well as users of data.\n",
      "\n",
      "\u001b[93m\n",
      "CITATIONS:\n",
      "source text: Two studies,\n",
      "source: DOI: 10.1162/qss_a_00286\n",
      "------\n",
      "source text: 10.1162/qss_a_00286,\n",
      "source: DOI: 10.1162/qss_a_00286\n",
      "------\n",
      "source text: This study compared the amount of metadata and the completeness degree of research publications in new academic databases.,\n",
      "source: DOI: 10.1162/qss_a_00286\n",
      "------\n",
      "source text: 10.31222/osf.io/smxe5,\n",
      "source: DOI: 10.31222/osf.io/smxe5\n",
      "------\n",
      "source text: This study presented an up-to-date overview of the availability of six metadata elements in Crossref, including abstracts.,\n",
      "source: DOI: 10.31222/osf.io/smxe5\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "    global documents_with_doi\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0]\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append(f\"{doi} {text}\\n\")\n",
    "    return documents_with_doi\n",
    "\n",
    "# initialize empty search query\n",
    "search_queries = []\n",
    "# Embed the documents\n",
    "def document_embed(documents:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the documents from a list provided from read_documents_with_doi()\n",
    "    \"\"\"\n",
    "    doc_emb = co.embed(\n",
    "        model=\"embed-v4.0\",\n",
    "        input_type=\"search_document\",\n",
    "        texts=[doc for doc in documents],\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return doc_emb\n",
    "\n",
    "# Embed the search query\n",
    "def query_embed(search_queries:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the query from a list provided in search_queries variable\n",
    "    \"\"\"\n",
    "    query_emb = co.embed(\n",
    "        model=\"embed-v4.0\",\n",
    "        input_type=\"search_query\",\n",
    "        texts=search_queries,\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return query_emb\n",
    "\n",
    "# retrieve top_k and compute similarity using dot product\n",
    "def retrieve_top_k(top_k, query_embedded, documents_embedded, documents)->List[str]:\n",
    "    \"\"\"\n",
    "    returns the top_k documents based on dot product similarity\n",
    "    \"\"\"\n",
    "\n",
    "    scores = np.dot(query_embedded, np.transpose(documents_embedded))[0]#ordered list!\n",
    "    # takes top scores, and returns sorted list and returns indices sliced by top_k\n",
    "    max_idx = np.argsort(-scores)[:top_k]\n",
    "    # returns documents by index\n",
    "    retrieved_docs = [documents[item] for item in max_idx]\n",
    "    # returns a list of documents\n",
    "    return retrieved_docs\n",
    "\n",
    "def rerank_documents(retrieved_documents,search_queries,threshold,top_k)->List[str]:\n",
    "    \"\"\"\n",
    "    takes retrieved_documents as input along with search_queries and runs them through the \n",
    "    rerank model from cohere for semantic similarity. \n",
    "\n",
    "    top_n = top_k\n",
    "    Limits those returned by a threshold score. this is to reduce those that are irrelevant.\n",
    "    \"\"\"\n",
    "    # Rerank the documents\n",
    "    results = co.rerank(\n",
    "        model=\"rerank-v3.5\",\n",
    "        query=search_queries[0],\n",
    "        documents=[doc for doc in retrieved_documents],\n",
    "        top_n=top_k,\n",
    "        max_tokens_per_doc=4096,# defaults to 4096\n",
    "    )\n",
    "\n",
    "    # Display the reranking results\n",
    "    for idx, result in enumerate(results.results):\n",
    "        print(f\"Rank: {idx+1}\")\n",
    "        print(f\"Score: {result.relevance_score}\")\n",
    "        print(f\"Document: {retrieved_documents[result.index]}\\n\")\n",
    "\n",
    "    #returns only those over threshold\n",
    "    reranked_docs = [\n",
    "        retrieved_documents[result.index] for result in results.results if result.relevance_score >=threshold\n",
    "    ]\n",
    "\n",
    "    print(f\"reranked_documents: {reranked_docs}\")\n",
    "    print(f\"length of reranked_documents: {len(reranked_docs)}\")\n",
    "\n",
    "    return reranked_docs\n",
    "\n",
    "def cohere_rag_pipeline(directory_path,search_queries,top_k,threshold):\n",
    "\n",
    "    # retrieve documents from directory\n",
    "    documents = read_documents_with_doi(directory_path)\n",
    "    print(f\"Length of documents: {len(documents)}\")\n",
    "    # embed the documents\n",
    "    documents_embedded = document_embed(documents)\n",
    "\n",
    "    #embed the query:\n",
    "    query_embedded = query_embed(search_queries)\n",
    "\n",
    "    # retrieve the top_k documents\n",
    "    retrieved_documents = retrieve_top_k(top_k, query_embedded, documents_embedded, documents)\n",
    "\n",
    "    # rerank the documents using the Rerank model from Cohere\n",
    "    reranked_documents = rerank_documents(retrieved_documents,search_queries,threshold,top_k)\n",
    "    # set system instructions\n",
    "    instructions = \"\"\"\n",
    "                    You are an academic research assistant.\n",
    "                    You must include the DOI in your response.\n",
    "                    If there is no content provided, ask for a different question.\n",
    "                    Please structure your response like this:\n",
    "                    Summary: summary statement here. \n",
    "                    DOI: summary of the text associated with this DOI.\n",
    "                    Address me as, 'my lady'.\n",
    "                    \"\"\"\n",
    "    # create messages to model\n",
    "    messages = [{\"role\":\"user\",\n",
    "                \"content\": search_queries[0]},\n",
    "                {\"role\":\"system\",\n",
    "                \"content\":instructions}]\n",
    "\n",
    "    # Generate the response\n",
    "    resp = co.chat(\n",
    "        model=\"command-a-03-2025\",\n",
    "        messages=messages,\n",
    "        documents=reranked_documents,\n",
    "    )\n",
    "\n",
    "    return resp, reranked_documents\n",
    "\n",
    "# ****** Pipeline ********\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.2\n",
    "\n",
    "response, reranked_documents_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "# Display the response\n",
    "print(response.message.content[0].text)\n",
    "print(Fore.LIGHTCYAN_EX + f\"------\\nReranked documents:\")\n",
    "for doc in reranked_documents_end:\n",
    "    print(doc)\n",
    "\n",
    "# Display the citations and source documents\n",
    "if response.message.citations:\n",
    "    print(Fore.LIGHTYELLOW_EX + \"\\nCITATIONS:\")\n",
    "    for citation in response.message.citations:\n",
    "        print(f\"source text: {citation.text},\\nsource: {citation.sources[0].document.get('content').split(\"\\n\")[0]}\\n------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# Analysis\n",
    "Precision, recall, accuracy, F1 scores and faithfulness\n",
    "## Precision, recall, F1 score\n",
    "### references\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "- https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "import openpyxl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query, Precision, Recall, F1-Score, Accuracy, Balanced accuracy, Faithfulness score, Response]\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial dataframe to capture results from each query and results\n",
    "#ONLY DO THIS AT THE BEGINNING OF THE ANALYSIS PROCEDURE, OTHERWISE, IT WILL ERASE THE PREVIOUS RESULTS!!\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Query','Precision','Recall','F1-Score','Accuracy', 'Balanced accuracy', 'Faithfulness score', 'Response'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.31222/osf.io/smxe5']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_dois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.31222/osf.io/smxe5']\n",
      "45\n",
      "For query: which studies examined the abstract in metadata?:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.933\n",
      "Accuracy: 0.933\n",
      "Balanced accuracy: 0.933\n",
      "Faithfulness score: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2801: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Query': 'which studies examined the abstract in metadata?',\n",
       " 'Precision': 0.0,\n",
       " 'Recall': 0.0,\n",
       " 'F1-Score': 0.9333333333333333,\n",
       " 'Accuracy': 0.9333333333333333,\n",
       " 'Balanced accuracy': 0.9333333333333333,\n",
       " 'Faithfulness score': 2,\n",
       " 'Response': 'Summary: Two studies examined the abstract in metadata.\\n\\nDOI: 10.1162/qss_a_00286 - This study compared the amount of metadata and the completeness degree of research publications in new academic databases.\\n\\nDOI: 10.31222/osf.io/smxe5 - This study presented an up-to-date overview of the availability of six metadata elements in Crossref, including abstracts.\\n\\nIs there anything else I can help you with, my lady?'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "\"\"\"\n",
    "change this to read in an excel sheet of queries and ground_truth dois.\n",
    "Then it should be isolated as a function.\n",
    "Run the function to iterature through the list.\n",
    "\"\"\"\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# initiates the variable\n",
    "ground_truth = []\n",
    "\n",
    "def evaluate_retrieval(retrieved_dois, ground_truth):\n",
    "    corpus_doi_list = []\n",
    "    #corpus_list is a global variable in rag_pipeline()\n",
    "    for each in range(len(documents_with_doi)):\n",
    "        a = documents_with_doi[each].split(\"\\n\")[0].strip(\"DOI: \")\n",
    "        corpus_doi_list.append(a)\n",
    "    print(len(corpus_doi_list))\n",
    "\n",
    "    def compare_lists(list1, list2, list3):\n",
    "        for val in list1:\n",
    "            if val in list2:\n",
    "                list3.append(1)\n",
    "            else:\n",
    "                list3.append(0)\n",
    "\n",
    "    #set y_true so that len(y_true)==len(corpus_doi_list)\n",
    "    global y_true,y_pred\n",
    "    y_true = []\n",
    "    compare_lists(corpus_doi_list,ground_truth,y_true)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = []\n",
    "    compare_lists(corpus_doi_list,retrieved_dois,y_pred)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # calculate metrics - could also use sklearn.metrics functions such as precision_score, but this is easier to read\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    faithfulness_score = 0\n",
    "    for each in retrieved_dois:\n",
    "        if each in response.message.content[0].text:\n",
    "            faithfulness_score+=1\n",
    "        else:\n",
    "            faithfulness_score+=0\n",
    "\n",
    "    return {\n",
    "        'Query':f\"{search_queries[0]}\",\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\":accuracy,\n",
    "        \"Balanced accuracy\":balanced_accuracy,\n",
    "        \"Faithfulness score\":faithfulness_score,\n",
    "        \"Response\":response.message.content[0].text\n",
    "    }\n",
    "\n",
    "def print_results()->Dict:\n",
    "    \"\"\"\n",
    "    Prints a nicely ordered set of results from evalaute_retrieval()\n",
    "    \"\"\"\n",
    "    global results\n",
    "    results = evaluate_retrieval(retrieved_dois, ground_truth)\n",
    "    print(f\"For query: {results['Query']}:\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"F1-Score: {results['F1-Score']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    print(f\"Balanced accuracy: {results['Balanced accuracy']:.3f}\")\n",
    "    print(f\"Faithfulness score: {results['Faithfulness score']}\")\n",
    "    return results\n",
    "\n",
    "#for debugging\n",
    "\n",
    "print_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the test from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.31222/osf.io/smxe5', '10.3145/epi.2023.mar.09']\n",
      "Length of documents: 32\n",
      "Rank: 1\n",
      "Score: 0.077608645\n",
      "Document: DOI: 10.1002/curj.136\n",
      " Title: Teachers' sources of information about climate change: A scoping review\n",
      "Abstract: Abstract This paper sheds light on an important and under‐researched issue: The sources of information about climate change that teachers use. Utilising a ‘scoping review’ methodological approach, we analysed over 600 papers to address two main questions: What sources of information about climate change are teachers using? In what ways are teachers using these sources of information? Through our use of inclusive search terms and detailed analysis of papers, we found only 13 studies of relevance, none of which primarily focus on the sources of information teachers use. The 13 studies are all located in the Global North, and within this nearly half are in the USA. Methodologically, all apart from two rely on teachers' reports rather than observation or other methods. Four types of sources of information were frequently mentioned: The Internet; government sources; mass media and professional development courses. The ‘superabundance’ of information now available to teachers (particularly online), the importance of high‐quality information for students' understandings of climate change, and the limited research on the sources of information about climate change that teachers use makes this is a significant blind spot for research to address.\n",
      "\n",
      "\n",
      "Rank: 2\n",
      "Score: 0.04646909\n",
      "Document: DOI: 10.1175/2009BAMS2625.1\n",
      " Title: None\n",
      "Abstract: None\n",
      "\n",
      "\n",
      "Rank: 3\n",
      "Score: 0.045289297\n",
      "Document: DOI: 10.1177/0092055X19862012\n",
      " Title: None\n",
      "Abstract: None\n",
      "\n",
      "\n",
      "Rank: 4\n",
      "Score: 0.0428703\n",
      "Document: DOI: 10.5281/zenodo.4011278\n",
      " Title: None\n",
      "Abstract: None\n",
      "\n",
      "\n",
      "Rank: 5\n",
      "Score: 0.037120476\n",
      "Document: DOI: 10.1175/WCAS-D-23-0143.1\n",
      " Title: None\n",
      "Abstract: None\n",
      "\n",
      "\n",
      "reranked_documents: []\n",
      "length of reranked_documents: 0\n",
      "My lady, it seems there is no specific content provided to examine or analyze. If you have a particular study or set of studies in mind that examined abstracts in metadata, please provide more details or a specific question. I’d be happy to assist you further. \n",
      "\n",
      "If you’re looking for a general approach to finding studies that examined abstracts in metadata, I could suggest searching academic databases like PubMed, IEEE Xplore, or Scopus using keywords such as \"abstract metadata analysis\" or \"metadata in academic abstracts.\" Let me know how you'd like to proceed!\n",
      "32\n",
      "For query: which studies examined the abstract in metadata?:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 1.000\n",
      "Accuracy: 1.000\n",
      "Balanced accuracy: 1.000\n",
      "Faithfulness score: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:534: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>which studies examined the abstract in metadata?</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>2</td>\n",
       "      <td>Summary: Two studies examined the abstract in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>which studies examined the abstract in metadata?</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>2</td>\n",
       "      <td>Summary: Two studies examined the abstract in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>which studies examined the abstract in metadata?</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.906977</td>\n",
       "      <td>0.686842</td>\n",
       "      <td>2</td>\n",
       "      <td>Summary: Four studies examined the abstract in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>which studies examined the abstract in metadata?</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>2</td>\n",
       "      <td>Summary: Three studies examined the abstract i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>which studies examined the abstract in metadata?</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>My lady, it seems there is no specific content...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Query  ...                                           Response\n",
       "3  which studies examined the abstract in metadata?  ...  Summary: Two studies examined the abstract in ...\n",
       "4  which studies examined the abstract in metadata?  ...  Summary: Two studies examined the abstract in ...\n",
       "5  which studies examined the abstract in metadata?  ...  Summary: Four studies examined the abstract in...\n",
       "6  which studies examined the abstract in metadata?  ...  Summary: Three studies examined the abstract i...\n",
       "7  which studies examined the abstract in metadata?  ...  My lady, it seems there is no specific content...\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the test from here\n",
    "\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# Ground truth relevant documents (DOIs) for each query\n",
    "ground_truth = [\"10.1002/leap.1411\",\"10.1007/s11192-020-03632-0\",\"10.1162/qss_a_00286\",\"10.1162/qss_a_00022\",\"10.31222/osf.io/smxe5\"]\n",
    "\n",
    "#***** Begin chat session *****\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.2\n",
    "\n",
    "response, reranked_documents_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "# Display the response\n",
    "print(response.message.content[0].text)\n",
    "\n",
    "new_result = print_results()\n",
    "# add the new result to the df\n",
    "results_df.loc[len(results_df)] = new_result\n",
    "\n",
    "#save the queries and responses to separate dataframe to be manually annontated\n",
    "answer_relevance_df = results_df[['Query','Response']].copy(deep=True)\n",
    "\n",
    "# save out answer_relevance_df\n",
    "filename=\"analysis/dense_answer_relevance_results.xlsx\"\n",
    "answer_relevance_df.to_excel(filename)\n",
    "\n",
    "filename = \"analysis/dense_analysis_results.xlsx\"\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "results_df.to_excel(filename)\n",
    "results_df.tail(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
