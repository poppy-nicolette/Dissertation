Key,Item Type,Publication Year,Author,Title,Publication Title,ISBN,ISSN,DOI,Url,Abstract Note,Date,Date Added,Date Modified,Access Date,Pages,Num Pages,Issue,Volume,Number Of Volumes,Journal Abbreviation,Short Title,Series,Series Number,Series Text,Series Title,Publisher,Place,Language,Rights,Type,Archive,Archive Location,Library Catalog,Call Number,Extra,Notes,File Attachments,Link Attachments,Manual Tags,Automatic Tags,Editor,Series Editor,Translator,Contributor,Attorney Agent,Book Author,Cast Member,Commenter,Composer,Cosponsor,Counsel,Interviewer,Producer,Recipient,Reviewed Author,Scriptwriter,Words By,Guest,Number,Edition,Running Time,Scale,Medium,Artwork Size,Filing Date,Application Number,Assignee,Issuing Authority,Country,Meeting Name,Conference Name,Court,References,Reporter,Legal Status,Priority Numbers,Programming Language,Version,System,Code,Code Number,Section,Session,Committee,History,Legislative Body
TKNGI2YH,conferencePaper,2024,"Braun, Marvin; Greve, Maike; Kegel, Felix; Kolbe, Lutz; Beyer, Philipp Emanuel",Can (A)I Have a Word with You? A Taxonomy on the Design Dimensions of AI Prompts,Proceedings of the 57th Hawaii International Conference on System Sciences,978-0-9981331-7-1,,10.24251/HICSS.2024.068,https://hdl.handle.net/10125/106443,,2024-01-03,2024-07-01 13:13,2025-12-03 15:34,2024-07-01 13:13,,,,,,,Can (A)I Have a Word with You?,,,,,HICSS,"Hawaii, US",eng,,,,,scholarspace.manoa.hawaii.edu,,,,/Users/poppyriddle/Zotero/storage/UZP4PVYJ/Braun et al. - 2024 - Can (A)I Have a Word with You A Taxonomy on the D.pdf,,cited; CORPUS; Human-AI interaction; Information Science; information seeking; Prompt design; prompt taxonomy; promptology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,International Conference on System Sciences,,,,,,,,,,,,,,,
TFN77T3T,preprint,2024,"Olla, Phillip; Elliot, Lauren; Abumeeiz, Mustapha; Pardalis, Elaina",Ask and You Shall Receive: Taxonomy of AI Prompts for Medical Education,,,,10.21203/rs.3.rs-3750487/v1,https://www.researchsquare.com/article/rs-3750487/v1,"This manuscript meticulously explores the approach for interacting with Artificial Intelligence (AI) Large Language Models (LLMs) to elicit optimal outputs. The generation of high-caliber prompts serves as a pivotal element in achieving the sought-after outcomes from these computational models. The discourse herein delineates various categories of prompts, substantiated with exemplars within each domain of application under investigation. This manuscript highlights the categories of prompts related to the particular utility of each application domain, especially accentuating their relevance to educational stakeholders such as students and educators in medical education. The Application of Learning Domains (ALDs) proposed within this article, endeavor to demarcate areas that may find the most utility from AI LLMs, facilitating knowledge dissemination, practice and training, simulated personas, and augmented interactivity across a spectrum of users in the educational milieu and beyond.",2024-01-03,2024-07-01 12:56,2025-12-03 15:42,2024-07-01 12:56,,,,,,,Ask and You Shall Receive,,,,,,,,,,,,Research Square,,ISSN: 2693-5015,,/Users/poppyriddle/Zotero/storage/BZ2MN7WN/Olla et al. - 2024 - Ask and You Shall Receive Taxonomy of AI Prompts .pdf,,6 application domains; cited; CORPUS; excellent; prompt taxonomy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HLMVPEGC,conferencePaper,2023,"Linzbach, Stephan; Tressel, Tim; Kallmeyer, Laura; Dietze, Stefan; Jabeen, Hajira",Decoding Prompt Syntax: Analysing its Impact on Knowledge Retrieval in Large Language Models,Companion Proceedings of the ACM Web Conference 2023,978-1-4503-9419-2,,10.1145/3543873.3587655,https://dl.acm.org/doi/10.1145/3543873.3587655,"Large Language Models (LLMs), with their advanced architectures and training on massive language datasets, contain unexplored knowledge. One method to infer this knowledge is through the use of cloze-style prompts. Typically, these prompts are manually designed because the phrasing of these prompts impacts the knowledge retrieval performance, even if the LLM encodes the desired information. In this paper, we study the impact of prompt syntax on the knowledge retrieval capacity of LLMs. We use a template-based approach to paraphrase simple prompts into prompts with a more complex grammatical structure. We then analyse the LLM performance for these structurally diferent but semantically equivalent prompts. Our study reveals that simple prompts work better than complex forms of sentences. The performance across the syntactical variations for simple relations (1:1) remains best, with a marginal decrease across diferent typologies. These results reinforce that simple prompt structures are more efective for knowledge retrieval in LLMs and motivate future research into the impact of prompt syntax on various tasks.",2023-04-30,2024-05-21 18:15,2025-12-03 15:39,2024-05-21 18:15,1145-1149,,,,,,Decoding Prompt Syntax,,,,,ACM,Austin TX USA,en,,,,,DOI.org (Crossref),,,,/Users/poppyriddle/Zotero/storage/VUX2H93E/Linzbach et al. - 2023 - Decoding Prompt Syntax Analysing its Impact on Kn.pdf,,CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,WWW '23: The ACM Web Conference 2023,,,,,,,,,,,,,,,
XHTXUZNZ,conferencePaper,2019,"Beltagy, Iz; Lo, Kyle; Cohan, Arman",SciBERT: A Pretrained Language Model for Scientific Text,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),,,10.18653/v1/D19-1371,https://aclanthology.org/D19-1371,"Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.",Nov-19,2022-06-30 15:53,2025-12-03 15:34,2022-06-30 15:53,3615–3620,,,,,,SciBERT,,,,,Association for Computational Linguistics,"Hong Kong, China",,,,,,ACLWeb,,,,/Users/poppyriddle/Zotero/storage/SEUM5X78/Beltagy et al. - 2019 - SciBERT A Pretrained Language Model for Scientifi.pdf,,CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,EMNLP-IJCNLP 2019,,,,,,,,,,,,,,,
TL46R9CG,journalArticle,2021,"Visser, Martijn; van Eck, Nees Jan; Waltman, Ludo","Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic",Quantitative Science Studies,,2641-3337,10.1162/qss_a_00112,https://doi.org/10.1162/qss_a_00112,"We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.",2021-04-08,2021-09-09 18:40,2025-12-03 15:30,2021-09-09 18:40,20-41,,1,2,,Quantitative Science Studies,Large-scale comparison of bibliographic data sources,,,,,,,,,,,,Silverchair,,,,"/Users/poppyriddle/Zotero/storage/TTSRFX4W/Visser et al. - 2021 - Large-scale comparison of bibliographic data sourc.pdf; /Users/poppyriddle/Zotero/storage/LJWLV6NN/Visser et al. - 2021 - Large-scale comparison of bibliographic data sources Scopus, Web of Science, Dimensions, Crossref,.pdf; /Users/poppyriddle/Zotero/storage/E3QRRHDP/Large-scale-comparison-of-bibliographic-data.html",,CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9GJH3XHV,journalArticle,1968,"Merton, R. K.",The Matthew Effect in Science: The reward and communication systems of science are considered,Science,,"0036-8075, 1095-9203",10.1126/science.159.3810.56,https://www.sciencemag.org/lookup/doi/10.1126/science.159.3810.56,,1968-01-05,2021-09-09 18:32,2025-12-03 15:40,2021-09-09 18:32,56-63,,3810,159,,Science,The Matthew Effect in Science,,,,,,,en,,,,,DOI.org (Crossref),,,,,,CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6V9UZN89,journalArticle,2023,"Van Noorden, Richard",ChatGPT-like AIs are coming to major science search engines,Nature,,,10.1038/d41586-023-02470-3,https://www.nature.com/articles/d41586-023-02470-3,"Scopus, Dimensions and Web of Science are introducing conversational AI search.",2023-08-02,2023-08-02 17:19,2025-12-03 15:45,2023-08-02 17:19,,,,,,,,,,,,,,en,2023 Springer Nature Limited,,,,www.nature.com,,"Bandiera_abtest: a Cg_type: News Publisher: Nature Publishing Group Subject_term: Machine learning, Databases, Research data",,/Users/poppyriddle/Zotero/storage/8Y6ARGMS/d41586-023-02470-3.html,,CORPUS,Databases; Machine learning; Research data,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
EM2BELP3,journalArticle,2025,"Shi, Julie; Nason, Michael; Tullney, Marco; Alperin, Juan Pablo",Identifying Metadata Quality Issues Across Cultures,College & Research Libraries,,2150-6701,10.5860/crl.86.1.101,https://crl.acrl.org/index.php/crl/article/view/26625,"Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.",2025-01-07,2024-03-13 14:33,2025-12-03 15:29,2024-03-13 14:33,101,,1,86,,,,,,,,,,en-us,"Copyright Julie Shi, Mike Nason, Marco Tullney, Juan Pablo Alperin",,,,crl.acrl.org,,Number: 1,,/Users/poppyriddle/Zotero/storage/78XHFHP6/Shi et al. - 2025 - Identifying Metadata Quality Issues Across Cultures.pdf; /Users/poppyriddle/Zotero/storage/9EXV632Z/6fykh.html; /Users/poppyriddle/Zotero/storage/BXPE58NF/Shi et al. - 2023 - Identifying Metadata Quality Issues Across Culture.pdf,,cited; CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6XKVNSQZ,preprint,2023,"Alshammari, Suad; Basalelah, Lama; Rukbah, Walaa Abu; Alsuhibani, Ali; Wijesinghe, Dayanjan S.",KNIMEZoBot: Enhancing Literature Review with Zotero and KNIME OpenAI Integration using Retrieval-Augmented Generation,,,,10.48550/arXiv.2311.04310,http://arxiv.org/abs/2311.04310,"Academic researchers face challenges keeping up with exponentially growing published findings in their field. Performing comprehensive literature reviews to synthesize knowledge is time-consuming and labor-intensive using manual approaches. Recent advances in artificial intelligence provide promising solutions, yet many require coding expertise, limiting accessibility. KNIMEZoBot represents an innovative integration of Zotero, OpenAI, and the KNIME visual programming platform to automate literature review tasks for users with no coding experience. By leveraging KNIME's intuitive graphical interface, researchers can create workflows to search their Zotero libraries and utilize OpenAI models to extract key information without coding. Users simply provide API keys and configure settings through a user-friendly interface in a locally stored copy of the workflow. KNIMEZoBot then allows asking natural language questions via a chatbot and retrieves relevant passages from papers to generate synthesized answers. This system has significant potential to expedite literature reviews for researchers unfamiliar with coding by automating retrieval and analysis of publications in personal Zotero libraries. KNIMEZoBot demonstrates how thoughtfully designed AI tools can expand accessibility and accelerate knowledge building across diverse research domains.",2023-11-07,2024-04-17 12:50,2025-12-03 15:10,2024-04-17 12:50,,,,,,,KNIMEZoBot,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2311.04310 [cs],,/Users/poppyriddle/Zotero/storage/M7KAZJRM/2311.html; /Users/poppyriddle/Zotero/storage/E3QDPMRL/Alshammari et al. - 2023 - KNIMEZoBot Enhancing Literature Review with Zoter.pdf,,CORPUS,Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,arXiv:2311.04310,,,,,,,,,,,,,,,,,,,,,,,,,,,
5WTQQQD7,journalArticle,1996,"Ingwersen, Peter",Cognitive Perspectives of Information Retrieval Interaction: Elements of a Cognitive IR Theory,Journal of Documentation,,0022-0418,10.1108/eb026960,https://doi.org/10.1108/eb026960,"The objective of the paper is to amalgamate theories of text retrieval from various research traditions into a cognitive theory for information retrieval interaction. Set in a cognitive framework, the paper outlines the concept of polyrepresentation applied to both the user's cognitive space and the information space of IR systems. The concept seeks to represent the current user's information need, problem state, and domain work task or interest in a structure of causality. Further, it implies that we should apply different methods of representation and a variety of IR techniques of different cognitive and functional origin simultaneously to each semantic full‐text entity in the information space. The cognitive differences imply that by applying cognitive overlaps of information objects, originating from different interpretations of such objects through time and by type, the degree of uncertainty inherent in IR is decreased. Polyrepresentation and the use of cognitive overlaps are associated with, but not identical to, data fusion in IR. By explicitly incorporating all the cognitive structures participating in the interactive communication processes during IR, the cognitive theory provides a comprehensive view of these processes. It encompasses the ad hoc theories of text retrieval and IR techniques hitherto developed in mainstream retrieval research. It has elements in common with van Rijsbergen and Lalmas' logical uncertainty theory and may be regarded as compatible with that conception of IR. Epistemologically speaking, the theory views IR interaction as processes of cognition, potentially occurring in all the information processing components of IR, that may be applied, in particular, to the user in a situational context. The theory draws upon basic empirical results from information seeking investigations in the operational online environment, and from mainstream IR research on partial matching techniques and relevance feedback. By viewing users, source systems, intermediary mechanisms and information in a global context, the cognitive perspective attempts a comprehensive understanding of essential IR phenomena and concepts, such as the nature of information needs, cognitive inconsistency and retrieval overlaps, logical uncertainty, the concept of ‘document’, relevance measures and experimental settings. An inescapable consequence of this approach is to rely more on sociological and psychological investigative methods when evaluating systems and to view relevance in IR as situational, relative, partial, differentiated and non‐linear. The lack of consistency among authors, indexers, evaluators or users is of an identical cognitive nature. It is unavoidable, and indeed favourable to IR. In particular, for full‐text retrieval, alternative semantic entities, including Salton et al.'s ‘passage retrieval’, are proposed to replace the traditional document record as the basic retrieval entity. These empirically observed phenomena of inconsistency and of semantic entities and values associated with data interpretation support strongly a cognitive approach to IR and the logical use of polyrepresentation, cognitive overlaps, and both data fusion and data diffusion.",1996-01-01,2023-12-16 12:40,2025-12-03 15:20,2023-12-16 12:40,3-50,,1,52,,,COGNITIVE PERSPECTIVES OF INFORMATION RETRIEVAL INTERACTION,,,,,,,,,,,,Emerald Insight,,Publisher: MCB UP Ltd,,/Users/poppyriddle/Zotero/storage/Q33PUU5U/html.html,,CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
J43RAGL9,journalArticle,1981,"Wilson, T.D.",On User Studies and Information Needs,Journal of Documentation,,0022-0418,10.1108/eb026702,https://doi.org/10.1108/eb026702,Apart from information retrieval there is virtually no other area of information science that has occasioned as much research effort and writing as ‘user studies’. Within user studies the investigation of ‘information needs’ has been the subject of much debate and no little confusion. The aim of this paper is to attempt to reduce this confusion by devoting attention to the definition of some concepts and by proposing the basis for a theory of the motivations for information‐seeking behaviour.,1981-01-01,2023-12-03 0:23,2025-12-03 15:47,2023-12-03 0:23,15-Mar,,1,37,,,,,,,,,,,,,,,Emerald Insight,,Publisher: MCB UP Ltd,,,,CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VCB8CH4D,presentation,2024,"Alonso-Álvarez, Patricia; van Eck, Nees Jan",Coverage and metadata availability of African publications in OpenAlex: A comparative analysis,,,,10.5281/zenodo.14006424,https://zenodo.org/records/14006425,"Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.",2024-10-29,2025-01-09 15:23,2025-12-03 15:10,2025-01-09 15:23,,,,,,,Coverage and metadata availability of African publications in OpenAlex,,,,,,,,,,,,,,Publisher: Zenodo DOI: 10.5281/zenodo.14006425,,/Users/poppyriddle/Zotero/storage/C7AI5QZF/Alonso-Álvarez and van Eck - 2024 - Coverage and metadata availability of African publications in OpenAlex A comparative analysis.pdf,,CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CJIU4R74,presentation,2024,"Krause, Geoff; Marjoram, Rebecca; Mongeon, Philippe",Well-tailored words: Comparing the fit of articles within scholarly journals to their citation rates,,,,10.5281/zenodo.13951886,https://zenodo.org/records/13951887,"The articles published within a scholarly journal reflect the research interests and activities of the community of authors contributing to it; it is not only the underlying ideas that may be shared, but the language used to express them. This work-in-progress uses the text of articles’ abstracts to attempt to further understand the relationship between scholarly articles and the journals in which they are published, and, through these, the communities and disciplines in which research takes place. An indicator of journal fit, leveraging cosine similarity, is used to characterize the positioning of articles within seventy-five journals across three subject areas, and is compared to the articles’ citation impact. A weak but significant correlation is found between the two, and differing distributions of fit across journals is observed.",2024-10-18,2025-01-09 20:14,2025-12-03 15:39,2025-01-09 20:14,,,,,,,Well-tailored words,,,,,,,eng,,,,,,,Publisher: Zenodo DOI: 10.5281/zenodo.13951887,,/Users/poppyriddle/Zotero/storage/C28JFY62/Krause et al. - 2024 - Well-tailored words Comparing the fit of articles within scholarly journals to their citation rates.pdf,,CORPUS,article fit; bibliometrics; scholarly journals; scientometrics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ISJMSHHB,preprint,2023,"Choudhury, Muntabir Hasan; Salsabil, Lamia; Jayanetti, Himarsha R.; Wu, Jian; Ingram, William A.; Fox, Edward A.",MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries,,,,10.48550/arXiv.2303.17661,http://arxiv.org/abs/2303.17661,"Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.",2023-03-30,2025-01-12 17:54,2025-12-03 15:12,2025-01-12 17:54,,,,,,,MetaEnhance,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2303.17661 [cs],,/Users/poppyriddle/Zotero/storage/IGP5QLX7/Choudhury et al. - 2023 - MetaEnhance Metadata Quality Improvement for Electronic Theses and Dissertations of University Libr.pdf; /Users/poppyriddle/Zotero/storage/SBNQMY6R/2303.html,,CORE PAPER; CORPUS,Computer Science - Artificial Intelligence; Computer Science - Digital Libraries; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2303.17661,,,,,,,,,,,,,,,,,,,,,,,,,,,
J2B8WMCE,journalArticle,2011,"Yasser, Chuttur M.",An Analysis of Problems in Metadata Records,Journal of Library Metadata,,1938-6389,10.1080/19386389.2011.570654,https://doi.org/10.1080/19386389.2011.570654,"Metadata plays an important role in digital libraries. But to be useful, metadata records must be problem free. When problems are present in the metadata, resources are not correctly represented and users are not able to reap the benefits of metadata. To minimize, if not eliminate, such problems, it is essential to understand the kinds of problems that can occur in metadata records. In this paper, problems found in metadata records as reported in the literature are compared and analyzed. It is found that five categories of metadata problems can be identified. These are the problems of Incorrect Values, Incorrect Elements, Missing Information, Information Loss, and Inconsistent Value Representation. Given that these problems are detrimental to the services that can be provided by metadata, preventive or corrective measures need to be put in place so as to ensure that the benefits derived from using metadata balance the costs and efforts spent in the creation of metadata records.",2011-04-01,2025-01-12 18:04,2025-12-03 15:31,2025-01-12 18:04,51-62,,2,11,,,,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/19386389.2011.570654,,/Users/poppyriddle/Zotero/storage/T657EFT8/Yasser - 2011 - An Analysis of Problems in Metadata Records.pdf,,cited; CORPUS,digital library; Dublin Core; IEEE LOM; IMLS; metadata problems,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8T3EX42J,journalArticle,2023,"Kang, Seok-Hoon; Kim, Sung-Jin",M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation,한국정보통신학회논문지,,2234-4772,10.6109/jkiice.2023.27.12.1489,https://www.dbpia.co.kr,"본 논문에서는 하나 이상의 문서에 대해 open-domain Question Answering (ODQA) 시스템에서 효과적인 검색 포함 질의 응답을 할 수 있는 Metadata Retrieval-Augmented Generation(M-RAG) 방법을 제안하고 그 성능을 비교한다. 이를 위하여 메타데이터가 포함된 임베딩을 활용하고, 자동화된 응답을 생성하기 위해 gpt-3.5-turbo-16k와 gpt-4와 같은 생성 모델을 활용한다. 본 논문의 방식을 통하여 생성 모델(gpt-3.5, gpt-4)이 문서의 순서와 맥락을 메타데이터를 통해 파악하여 답변 할 수 있게 된다. 그리고 문서의 출처, 원문 요구를 추가하는 프롬프트 엔지니어링을 통해 질의응답(QA)의 출처 표기 기능을 활성화시킬 수 있어서 답변의 정확성을 증대할 수 있다. 실험결과, 본 논문의 방법은 같은 외부 추론 ODQA 시스템과 비교하여 최대 46%의 성능 향상을 보였고, 기존의 RAG 방식보다도 6% 향상된 성능을 보였다.",Dec-23,2025-01-14 11:25,2025-12-03 15:21,2025-01-14 11:25,1489-1500,,12,27,,,M-RAG,,,,,,,ko,,,,,www.dbpia.co.kr,,,,/Users/poppyriddle/Zotero/storage/A2LYMHJG/articleDetail.html,,CORE PAPER; CORPUS,AI; Generative AI; GPT; LLM; RAG; 생성형AI; 인공지능,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XKYTXF73,preprint,2024,"Hayashi, Teruaki; Sakaji, Hiroki; Dai, Jiayi; Goebel, Randy",Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models,,,,10.48550/arXiv.2410.04231,http://arxiv.org/abs/2410.04231,"Developing the capacity to effectively search for requisite datasets is an urgent requirement to assist data users in identifying relevant datasets considering the very limited available metadata. For this challenge, the utilization of third-party data is emerging as a valuable source for improvement. Our research introduces a new architecture for data exploration which employs a form of Retrieval-Augmented Generation (RAG) to enhance metadata-based data discovery. The system integrates large language models (LLMs) with external vector databases to identify semantic relationships among diverse types of datasets. The proposed framework offers a new method for evaluating semantic similarity among heterogeneous data sources and for improving data exploration. Our study includes experimental results on four critical tasks: 1) recommending similar datasets, 2) suggesting combinable datasets, 3) estimating tags, and 4) predicting variables. Our results demonstrate that RAG can enhance the selection of relevant datasets, particularly from different categories, when compared to conventional metadata approaches. However, performance varied across tasks and models, which confirms the significance of selecting appropriate techniques based on specific use cases. The findings suggest that this approach holds promise for addressing challenges in data exploration and discovery, although further refinement is necessary for estimation tasks.",2024-10-05,2025-01-14 11:30,2025-12-03 15:19,2025-01-14 11:30,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2410.04231 [cs],,/Users/poppyriddle/Zotero/storage/2HG7MQJ9/Hayashi et al. - 2024 - Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models.pdf; /Users/poppyriddle/Zotero/storage/IRJ8YMMZ/2410.html,,CORE PAPER; CORPUS,Computer Science - Information Retrieval,,,,,,,,,,,,,,,,,,,arXiv:2410.04231,,,,,,,,,,,,,,,,,,,,,,,,,,,
CUVM2MRX,preprint,2024,"Rorseth, Joel; Godfrey, Parke; Golab, Lukasz; Srivastava, Divesh; Szlichta, Jaroslaw",RAGE Against the Machine: Retrieval-Augmented LLM Explanations,,,,10.48550/arXiv.2405.13000,http://arxiv.org/abs/2405.13000,"This paper demonstrates RAGE, an interactive tool for explaining Large Language Models (LLMs) augmented with retrieval capabilities; i.e., able to query external sources and pull relevant information into their input context. Our explanations are counterfactual in the sense that they identify parts of the input context that, when removed, change the answer to the question posed to the LLM. RAGE includes pruning methods to navigate the vast space of possible explanations, allowing users to view the provenance of the produced answers.",2024-05-11,2025-01-15 15:04,2025-12-03 15:44,2025-01-15 15:04,,,,,,,RAGE Against the Machine,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2405.13000 [cs],,/Users/poppyriddle/Zotero/storage/TJBEJFXH/Rorseth et al. - 2024 - RAGE Against the Machine Retrieval-Augmented LLM Explanations.pdf; /Users/poppyriddle/Zotero/storage/UURZ7H3L/2405.html,,cited; CORPUS,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Information Retrieval,,,,,,,,,,,,,,,,,,,arXiv:2405.13000,,,,,,,,,,,,,,,,,,,,,,,,,,,
FKFLDV57,journalArticle,2024,"Ghasemi, Shima; Shakery, Azadeh",Harnessing the Power of Metadata for Enhanced Question Retrieval in Community Question Answering,IEEE Access,,2169-3536,10.1109/ACCESS.2024.3395449,https://ieeexplore.ieee.org/abstract/document/10525684,"Community Question Answering (CQA) forums such as Yahoo! Answers and Stack Overflow have become popular. The main goal of a CQA is to provide the most suitable answer in the shortest possible time. Since there is a reach archive of answered questions, similar question retrieval has received much attention intending to answer questions immediately after asking. One of the main challenges in this task is the lexical gap between questions, which refers to the discrepancies between the terminologies used by users asking questions. In this paper, we use metadata and two transformer-based techniques to improve the translation-based language model as a traditional technique addressing the lexical gap in retrieval systems. To overcome the lexical gap problem, additional context and information about the questions can help. Metadata is a rich source of information that refers to supplementary data associated with each question. Subject, category, and answer are metadata used in this article. To leverage these metadata, two transformer-based methods are employed. First, to utilize category information, we build category-specific dictionaries to obtain more accurate translation probabilities. A BERT model predicts the categories of the questions. Second, to utilize answer information, we propose a question expansion technique. Expansion is done by a transformer-based model using a retrieval-augmented generation (RAG) model to generate answers and expand new questions with corresponding answers. Finally, candidate questions are ranked according to their similarity to the expanded new question. Our proposed method achieves 51.47 in terms of MAP, outperforming all state-of-the-art approaches in question retrieval.",2024,2025-01-15 16:37,2025-09-01 18:44,2025-01-15 16:37,65768-65779,,,12,,,,,,,,,,,,,,,IEEE Xplore,,Conference Name: IEEE Access,,/Users/poppyriddle/Zotero/storage/524G4PQ3/Ghasemi and Shakery - 2024 - Harnessing the Power of Metadata for Enhanced Question Retrieval in Community Question Answering.pdf; /Users/poppyriddle/Zotero/storage/GE2NZ6M4/10525684.html,,multilingual translation; CORPUS,Information retrieval; Metadata; metadata; Task analysis; Semantics; Community question answering; Dictionaries; Feature extraction; Question answering (information retrieval); question retrieval; Transformers,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VB8FK94B,journalArticle,2024,"Mazumder, Jhantu; Mukhopadhyay, Parthasarathi",Designing Question-Answer Based Search System in Libraries: Application of Open Source Retrieval Augmented Generation (RAG) Pipeline,Journal of Information and Knowledge,,2583-9314,10.17821/srels/2024/v61i5/171583,https://www.srels.org/index.php/sjim/article/view/171583,"This study primarily aims to prepare a prototype and demonstrate that libraries can develop a low-cost conversational search system using open-source software tools and Large Language Models (LLMs) through a Retrieval-Augmented Generation (RAG) framework. LLMs often hallucinate and provide outdated and non-contextualized responses. However, this experiment shows that LLMs can deliver contextualized, relevant responses when augmented with a set of relevant documents. Augmenting LLMs with relevant documents before generating answers is known as retrieval-augmented generation. The methodology involved creating a RAG pipeline using tools like LangChain, vector databases like ChromaDB, and open-source LLMs like Llama3 (a 70-billion parameter-based model). The prototype developed includes a dataset of 250+ relevant documents on the Chandrayaan-3 mission that was collected, processed, and ingested into the pipeline. Finally, the study compared responses from standard LLMs and LLMs with RAG augmentation. Key findings revealed that standard LLMs (without RAG) produced confidently incorrect, hallucinated responses against queries related to Chandrayaan-3, while LLMs with RAG consistently provided accurate, informative, and contextualized answers when supplied with a set of relevant documents before generating the response. The study concluded that open-source RAG-based systems offer a cost-effective solution for libraries to enhance information retrieval and transform libraries into dynamic information services.",2024-10-21,2025-01-15 16:51,2025-12-03 15:23,2025-01-15 16:51,255-260,,,,,,Designing Question-Answer Based Search System in Libraries,,,,,,,en,Copyright (c) 2024 Journal of Information and Knowledge,,,,www.srels.org,,,,,,CORPUS,ChatGPT; Conversational AI; Gemini; Generative AI; LangChain; Large Language Models (LLMs); Llama3; LlamaIndex; Mistral; NLP; Retrieval Augmented Generation (RAG),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
I2YH82M2,preprint,2024,"Yepes, Antonio Jimeno; You, Yao; Milczek, Jan; Laverde, Sebastian; Li, Renyu",Financial Report Chunking for Effective Retrieval Augmented Generation,,,,10.48550/arXiv.2402.05131,http://arxiv.org/abs/2402.05131,"Chunking information is a key step in Retrieval Augmented Generation (RAG). Current research primarily centers on paragraph-level chunking. This approach treats all texts as equal and neglects the information contained in the structure of documents. We propose an expanded approach to chunk documents by moving beyond mere paragraph-level chunking to chunk primary by structural element components of documents. Dissecting documents into these constituent elements creates a new way to chunk documents that yields the best chunk size without tuning. We introduce a novel framework that evaluates how chunking based on element types annotated by document understanding models contributes to the overall context and accuracy of the information retrieved. We also demonstrate how this approach impacts RAG assisted Question & Answer task performance. Our research includes a comprehensive analysis of various element types, their role in effective information retrieval, and the impact they have on the quality of RAG outputs. Findings support that element type based chunking largely improve RAG results on financial reporting. Through this research, we are also able to answer how to uncover highly accurate RAG.",2024-03-16,2025-01-15 16:53,2025-12-03 15:47,2025-01-15 16:53,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2402.05131 [cs],,/Users/poppyriddle/Zotero/storage/KDKKRPYM/Yepes et al. - 2024 - Financial Report Chunking for Effective Retrieval Augmented Generation.pdf; /Users/poppyriddle/Zotero/storage/N39IYBPA/2402.html,,cited; CORPUS,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2402.05131,,,,,,,,,,,,,,,,,,,,,,,,,,,
M77V8JRN,journalArticle,2016,"Schlosser, Melanie",Write up! A Study of Copyright Information on Library-Published Journals,Journal of Librarianship and Scholarly Communication,,2162-3309,10.7710/2162-3309.2110,https://www.iastatedigitalpress.com/jlsc/article/id/12769/,"INTRODUCTION Libraries have a mission to educate users about copyright, and library publishing staff are often involved in that work. This article investigates a concrete point of intersection between the two areas - copyright statements on library-published journals. METHODS Journals published by members of the Library Publishing Coalition were examined for open access status, type and placement of copyright information, copyright ownership, and open licensing. RESULTS Journals in the sample were overwhelmingly (93%) open access. 80% presented copyright information of some kind, but only 30% of those included it at both the journal and the article level. Open licensing was present in 38% of the journals, and the most common ownership scenario was the author retaining copyright while granting a nonexclusive license to the journal or publisher. 9% of the sample journals included two or more conflicting rights statements. DISCUSSION 76% of the journals did not consistently provide accurate, easily-accessible rights information, and numerous problems were found with the use of open licensing, including conflicting licenses, incomplete licenses, and licenses not appearing at the article level. CONCLUSION Recommendations include presenting full copyright and licensing information at both the journal and the article level, careful use of open licenses, and publicly-available author agreements.External Data or Supplements:Schlosser, Melanie, 2016, ""Data from: Write Up! A Study of Copyright Information on Library-Published Journals"", http://dx.doi.org/10.7910/DVN/R36SVZ, Harvard Dataverse.",2016-07-19,2025-01-16 14:45,2025-12-03 15:28,2025-01-16 14:45,,,0,4,,,,,,,,,,eng,,,,,www.iastatedigitalpress.com,,Number: 0 Publisher: Iowa State University Digital Press,,/Users/poppyriddle/Zotero/storage/9XAYCJJA/Schlosser - 2016 - Write up! A Study of Copyright Information on Library-Published Journals.pdf,,CORE PAPER; CORPUS; License information,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SLKZH5RE,preprint,2022,"Eck, Nees Jan van; Waltman, Ludo",Crossref as a source of open bibliographic metadata,,,,10.31222/osf.io/smxe5,https://osf.io/smxe5,"Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.",2022-07-07,2025-01-16 14:51,2025-12-03 15:14,2025-01-16 14:51,,,,,,,,,,,,OSF,,en-us,,,,,OSF Preprints,,,,/Users/poppyriddle/Zotero/storage/7ZP7AIML/Eck and Waltman - 2022 - Crossref as a source of open bibliographic metadata.pdf,,cited; CORE PAPER; CORPUS,Abstract; Author affiliations; Bibliographic metadata; Crossref; Funding information; License information; Open metadata; ORCID; Publisher; References,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IIDAXYA8,journalArticle,2022,"Cioffi, Alessia; Coppini, Sara; Massari, Arcangelo; Moretti, Arianna; Peroni, Silvio; Santini, Cristian; Shahidzadeh Asadi, Nooshin",Identifying and correcting invalid citations due to DOI errors in Crossref data,Scientometrics,,1588-2861,10.1007/s11192-022-04367-w,https://doi.org/10.1007/s11192-022-04367-w,"This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.",2022-06-01,2025-01-16 14:58,2025-12-03 15:12,2025-01-16 14:58,3593-3612,,6,127,,Scientometrics,,,,,,,,en,,,,,Springer Link,,,,/Users/poppyriddle/Zotero/storage/JQ9ANHXH/Cioffi et al. - 2022 - Identifying and correcting invalid citations due to DOI errors in Crossref data.pdf,,CORE PAPER; CORPUS; problems with DOI,Crossref; Incorrect DOI; Invalid citations; Open citations; OpenCitations,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4KPCLWDW,journalArticle,2023,"Santos, Erika Alves dos; Peroni, Silvio; Mucheroni, Marcos Luiz",An analysis of citing and referencing habits across all scholarly disciplines: approaches and trends in bibliographic referencing and citing practices,Journal of Documentation,,0022-0418,10.1108/JD-10-2022-0234,https://www.emerald.com/insight/content/doi/10.1108/jd-10-2022-0234/full/html,"In this study, the authors want to identify current possible causes for citing and referencing errors in scholarly literature to compare if something changed from the snapshot provided by Sweetland in his 1989 paper.,The authors analysed reference elements, i.e. bibliographic references, mentions, quotations and respective in-text reference pointers, from 729 articles published in 147 journals across the 27 subject areas.,The outcomes of the analysis pointed out that bibliographic errors have been perpetuated for decades and that their possible causes have increased, despite the encouraged use of technological facilities, i.e. the reference managers.,As far as the authors know, the study is the best recent available analysis of errors in referencing and citing practices in the literature since Sweetland (1989).",2023-07-21,2025-01-16 15:10,2025-12-03 15:27,2025-01-16 15:10,196-224,,7,79,,,An analysis of citing and referencing habits across all scholarly disciplines,,,,,,,en,,,,world,www.emerald.com,,Publisher: Emerald Publishing Limited,,/Users/poppyriddle/Zotero/storage/2SCU9F8I/Santos et al. - 2023 - An analysis of citing and referencing habits across all scholarly disciplines approaches and trends.pdf; /Users/poppyriddle/Zotero/storage/CLVF6EPW/html.html,,CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WVQUM6EU,preprint,2024,"Alperin, Juan Pablo; Portenoy, Jason; Demes, Kyle; Larivière, Vincent; Haustein, Stefanie",An analysis of the suitability of OpenAlex for bibliometric analyses,,,,10.48550/arXiv.2404.17663,http://arxiv.org/abs/2404.17663,"Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.",2024-04-26,2025-01-20 11:58,2025-12-03 15:10,2025-01-20 11:58,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2404.17663 [cs],,/Users/poppyriddle/Zotero/storage/Y84RNTV5/Alperin et al. - 2024 - An analysis of the suitability of OpenAlex for bibliometric analyses.pdf; /Users/poppyriddle/Zotero/storage/DD94P8KQ/2404.html,,CORE PAPER; CORPUS,Computer Science - Digital Libraries,,,,,,,,,,,,,,,,,,,arXiv:2404.17663,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q3F88WNV,preprint,2024,"Céspedes, Lucía; Kozlowski, Diego; Pradier, Carolina; Sainte-Marie, Maxime Holmberg; Shokida, Natsumi Solange; Benz, Pierre; Poitras, Constance; Ninkov, Anton Boudreau; Ebrahimy, Saeideh; Ayeni, Philips; Filali, Sarra; Li, Bing; Larivière, Vincent",Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness,,,,10.48550/arXiv.2409.10633,http://arxiv.org/abs/2409.10633,"Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.",2024-09-19,2025-01-20 12:01,2025-12-03 15:11,2025-01-20 12:01,,,,,,,Evaluating the Linguistic Coverage of OpenAlex,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2409.10633 [cs],,/Users/poppyriddle/Zotero/storage/XEGU286P/Céspedes et al. - 2024 - Evaluating the Linguistic Coverage of OpenAlex An Assessment of Metadata Accuracy and Completeness.pdf; /Users/poppyriddle/Zotero/storage/Y7739DL5/2409.html,,CORE PAPER; CORPUS,Computer Science - Databases; Computer Science - Digital Libraries,,,,,,,,,,,,,,,,,,,arXiv:2409.10633,,,,,,,,,,,,,,,,,,,,,,,,,,,
4K2CPMZJ,preprint,2024,"Culbert, Jack; Hobert, Anne; Jahn, Najko; Haupka, Nick; Schmidt, Marion; Donner, Paul; Mayr, Philipp",Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus,,,,10.48550/arXiv.2401.16359,http://arxiv.org/abs/2401.16359,"OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.",2024-11-01,2025-01-20 12:01,2025-12-03 15:12,2025-01-20 12:01,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2401.16359 [cs],,/Users/poppyriddle/Zotero/storage/S62JGJ4G/Culbert et al. - 2024 - Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus.pdf; /Users/poppyriddle/Zotero/storage/VDE525UY/2401.html,,CORE PAPER; CORPUS,Computer Science - Digital Libraries,,,,,,,,,,,,,,,,,,,arXiv:2401.16359,,,,,,,,,,,,,,,,,,,,,,,,,,,
7F6PHJS8,journalArticle,2024,"Delgado-Quirós, Lorena; Ortega, José Luis",Completeness degree of publication metadata in eight free-access scholarly databases,Quantitative Science Studies,,2641-3337,10.1162/qss_a_00286,https://doi.org/10.1162/qss_a_00286,"The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.",2024-03-01,2025-01-20 12:01,2025-12-03 15:13,2025-01-20 12:01,31-49,,1,5,,Quantitative Science Studies,,,,,,,,,,,,,Silverchair,,,,/Users/poppyriddle/Zotero/storage/5UI3GKSR/Delgado-Quirós and Ortega - 2024 - Completeness degree of publication metadata in eight free-access scholarly databases.pdf; /Users/poppyriddle/Zotero/storage/JL4GBEYM/Completeness-degree-of-publication-metadata-in.html,,cited; CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8YTZWWE9,journalArticle,2024,"Schares, Eric",Comparing Funder Metadata in OpenAlex and Dimensions,OpenISU,,,10.31274/b8136f97.ccc3dae4,https://openisu.pubpub.org/pub/n7zgjueg/release/1,A test case looking at an open database and a closed database.,2024-04-23,2025-01-20 12:05,2025-12-03 15:28,2025-01-20 12:05,,,,,,,,,,,,,,en,,,,,openisu.pubpub.org,,Publisher: Iowa State University Digital Press,,/Users/poppyriddle/Zotero/storage/GQYW4YNC/Schares - 2024 - Comparing Funder Metadata in OpenAlex and Dimensions.pdf,,CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DGP5MLG4,journalArticle,2023,"Scheidsteger, Thomas; Haunschild, Robin",Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?,Profesional de la información,,1699-2407,10.3145/epi.2023.mar.09,https://revista.profesionaldelainformacion.com/index.php/EPI/article/view/87295,"With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.",2023-03-04,2025-01-20 12:06,2025-12-03 15:28,2025-01-20 12:06,,,2,32,,,,,,,,,,en,Derechos de autor 2023 Profesional de la información,,,,revista.profesionaldelainformacion.com,,Number: 2,,/Users/poppyriddle/Zotero/storage/JAKTU2XZ/Scheidsteger and Haunschild - 2023 - Which of the metadata with relevance for bibliometrics are the same and which are different when swi.pdf,,CORE PAPER; CORPUS,Bibliographic data; Bibliometrics; Citation analysis; Concepts; Document types; Fields of study; MAG; Metadata; Microsoft Academic Graph; OpenAlex; Subject classification,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
L4ZWZ8UX,preprint,2024,"Simard, Marc-Andre; Basson, Isabel; Hare, Madelaine; Lariviere, Vincent; Mongeon, Philippe","The open access coverage of OpenAlex, Scopus and Web of Science",,,,10.48550/arXiv.2404.01985,http://arxiv.org/abs/2404.01985,"Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.",2024-04-02,2025-01-20 12:07,2025-12-03 15:29,2025-01-20 12:07,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2404.01985 [cs],,"/Users/poppyriddle/Zotero/storage/9YUAPI59/Simard et al. - 2024 - The open access coverage of OpenAlex, Scopus and Web of Science.pdf; /Users/poppyriddle/Zotero/storage/LP942MLC/2404.html",,CORE PAPER; CORPUS,Computer Science - Digital Libraries,,,,,,,,,,,,,,,,,,,arXiv:2404.01985,,,,,,,,,,,,,,,,,,,,,,,,,,,
2WXXVTBF,journalArticle,2024,"Zhang, Lin; Cao, Zhe; Shang, Yuanyuan; Sivertsen, Gunnar; Huang, Ying","Missing institutions in OpenAlex: possible reasons, implications, and solutions",Scientometrics,,1588-2861,10.1007/s11192-023-04923-y,https://doi.org/10.1007/s11192-023-04923-y,"The advent of open science calls for open data platforms with high data quality. As a fully open catalog of the global research system launched in January 2022, OpenAlex features two main advantages of easy data accessibility and broad data coverage, which has been widely used in quantitative science studies. Remarkably, OpenAlex is adopted as an important data source for Leiden university ranking. However, there is a severe data quality problem of missing institutions in journal article metadata in OpenAlex. This study investigates the possible reasons for the problem and its consequences and solutions by defining three types of institutional information—full institutional information (FII), partially missing institutional information (PMII) and completely missing institutional information (CMII). Our results show that the problem of missing institutions occurs in more than 60% of the journal articles in OpenAlex. The problem is particularly widespread in metadata from the early years and in the social sciences and humanities. Using sub-samples of the data, we further explore the possible reasons for the problem, the risk it might represent for distorted results, and possible solutions to the problem of missing institutions. The aim is to raise the importance of data quality improvements in open resources, and thus to support the responsible use of open resources in quantitative science studies and also in broader contexts.",2024-10-01,2025-01-20 12:08,2025-12-03 15:32,2025-01-20 12:08,5869-5891,,10,129,,Scientometrics,Missing institutions in OpenAlex,,,,,,,en,,,,,Springer Link,,,,"/Users/poppyriddle/Zotero/storage/XAJ5YB8N/Zhang et al. - 2024 - Missing institutions in OpenAlex possible reasons, implications, and solutions.pdf",,CORE PAPER; CORPUS; metadata quality,Data quality; Missing institutional information; Open science; OpenAlex,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KZNHWT4C,preprint,2024,"Haupka, Nick; Culbert, Jack H.; Schniedermann, Alexander; Jahn, Najko; Mayr, Philipp","Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar",,,,10.48550/arXiv.2406.15154,http://arxiv.org/abs/2406.15154,"This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.",2024-06-21,2025-01-20 19:53,2025-12-03 15:19,2025-01-20 19:53,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2406.15154 [cs],,"/Users/poppyriddle/Zotero/storage/26286LII/Haupka et al. - 2024 - Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Seman.pdf; /Users/poppyriddle/Zotero/storage/2HVKIKEK/2406.html",,CORE PAPER; CORPUS,Computer Science - Digital Libraries,,,,,,,,,,,,,,,,,,,arXiv:2406.15154,,,,,,,,,,,,,,,,,,,,,,,,,,,
KQKLEDDA,journalArticle,2016,"Mongeon, Philippe; Paul-Hus, Adèle",The journal coverage of Web of Science and Scopus: a comparative analysis,Scientometrics,,1588-2861,10.1007/s11192-015-1765-5,https://doi.org/10.1007/s11192-015-1765-5,"Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.",2016-01-01,2025-01-21 16:53,2025-12-03 15:25,2025-01-21 16:53,213-228,,1,106,,Scientometrics,The journal coverage of Web of Science and Scopus,,,,,,,en,,,,,Springer Link,,,,/Users/poppyriddle/Zotero/storage/8ZQFYHTB/Mongeon and Paul-Hus - 2016 - The journal coverage of Web of Science and Scopus a comparative analysis.pdf,,CORE PAPER; CORPUS,Bibliometrics; Citation indexes; Research evaluation; Scopus; Web of Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23257X9G,journalArticle,2022,"Kramer, Bianca; de Jonge, Hans",The availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council,Quantitative Science Studies,,2641-3337,10.1162/qss_a_00210,https://doi.org/10.1162/qss_a_00210,"Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.",2022-11-01,2025-01-22 0:04,2025-12-03 15:22,2025-01-22 0:04,583-599,,3,3,,Quantitative Science Studies,The availability and completeness of open funder metadata,,,,,,,,,,,,Silverchair,,,,/Users/poppyriddle/Zotero/storage/VHWMIQ55/Kramer and de Jonge - 2022 - The availability and completeness of open funder metadata Case study for publications funded by the.pdf; /Users/poppyriddle/Zotero/storage/X67G5V3D/The-availability-and-completeness-of-open-funder.html,,CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UBBKGSUT,journalArticle,2020,"Hendricks, Ginny; Tkaczyk, Dominika; Lin, Jennifer; Feeney, Patricia",Crossref: The sustainable source of community-owned scholarly metadata,Quantitative Science Studies,,2641-3337,10.1162/qss_a_00022,https://doi.org/10.1162/qss_a_00022,"This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.",2020-02-01,2025-01-22 10:16,2025-12-03 15:38,2025-01-22 10:16,414-427,,1,1,,Quantitative Science Studies,Crossref,,,,,,,,,,,,Silverchair,,,,/Users/poppyriddle/Zotero/storage/79LBTM9W/Hendricks et al. - 2020 - Crossref The sustainable source of community-owned scholarly metadata.pdf; /Users/poppyriddle/Zotero/storage/GFTG4M5H/Crossref-The-sustainable-source-of-community-owned.html,,CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q4TPLJY2,journalArticle,2022,"Mugabushaka, Alexis-Michel; van Eck, Nees Jan; Waltman, Ludo",Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures,Quantitative Science Studies,,2641-3337,10.1162/qss_a_00212,https://doi.org/10.1162/qss_a_00212,"To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.",2022-11-01,2025-01-24 11:01,2025-12-03 15:25,2025-01-24 11:01,560-582,,3,3,,Quantitative Science Studies,Funding COVID-19 research,,,,,,,,,,,,Silverchair,,,,/Users/poppyriddle/Zotero/storage/D353H6RH/Mugabushaka et al. - 2022 - Funding COVID-19 research Insights from an exploratory analysis using open data infrastructures.pdf; /Users/poppyriddle/Zotero/storage/JMNNXT8E/Funding-COVID-19-research-Insights-from-an.html,,cited; CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
XJYU9KRQ,conferencePaper,2024,"Shaik, Kareem; Wang, Dali; Zheng, Weijian; Cao, Qinglei; Fan, Heng; Schwartz, Peter; Feng, Yunhe","S3LLM: Large-Scale Scientific Software Understanding with LLMs Using Source, Metadata, and Document",Computational Science – ICCS 2024,978-3-031-63759-9,,10.1007/978-3-031-63759-9_27,,"The understanding of large-scale scientific software is a significant challenge due to its diverse codebase, extensive code length, and target computing architectures. The emergence of generative AI, specifically large language models (LLMs), provides novel pathways for understanding such complex scientific codes. This paper presents S3LLM, an LLM-based framework designed to enable the examination of source code, code metadata, and summarized information in conjunction with textual technical reports in an interactive, conversational manner through a user-friendly interface. S3LLM leverages open-source LLaMA-2 models to enhance code analysis through the automatic transformation of natural language queries into domain-specific language (DSL) queries. In addition, S3LLM is equipped to handle diverse metadata types, including DOT, SQL, and customized formats. Furthermore, S3LLM incorporates retrieval-augmented generation (RAG) and LangChain technologies to directly query extensive documents. S3LLM demonstrates the potential of using locally deployed open-source LLMs for the rapid understanding of large-scale scientific computing software, eliminating the need for extensive coding expertise and thereby making the process more efficient and effective. S3LLM is available at https://github.com/ResponsibleAILab/s3llm.",2024,2025-01-25 12:10,2025-12-03 15:28,,222-230,,,,,,S3LLM,,,,,Springer Nature Switzerland,Cham,en,,,,,Springer Link,,,,"/Users/poppyriddle/Zotero/storage/FR9LKR5R/Shaik et al. - 2024 - S3LLM Large-Scale Scientific Software Understanding with LLMs Using Source, Metadata, and Document.pdf",,CORE PAPER; CORPUS,ChatGPT; E3SM Land Model; Large-Scale Scientific Software; LLaMA; LLM; Research Software Analysis; Retrieval Augmented Generation (RAG),"Franco, Leonardo; de Mulatier, Clélia; Paszynski, Maciej; Krzhizhanovskaya, Valeria V.; Dongarra, Jack J.; Sloot, Peter M. A.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
X9PB4QSH,preprint,2024,"Poliakov, Mykhailo; Shvai, Nadiya",Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata,,,,10.48550/arXiv.2406.13213,http://arxiv.org/abs/2406.13213,"The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.",2024-08-19,2025-01-25 12:13,2025-12-03 15:26,2025-01-25 12:13,,,,,,,Multi-Meta-RAG,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2406.13213 [cs],,/Users/poppyriddle/Zotero/storage/EJWFKQMA/Poliakov and Shvai - 2024 - Multi-Meta-RAG Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Meta.pdf; /Users/poppyriddle/Zotero/storage/22RHH92Z/2406.html,,CORE PAPER; CORPUS,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Databases,,,,,,,,,,,,,,,,,,,arXiv:2406.13213,,,,,,,,,,,,,,,,,,,,,,,,,,,
CDICXQU8,journalArticle,2024,"Li, Yiming; Zhao, Jeff; Li, Manqi; Dang, Yifang; Yu, Evan; Li, Jianfu; Sun, Zenan; Hussein, Usama; Wen, Jianguo; Abdelhameed, Ahmed M; Mai, Junhua; Li, Shenduo; Yu, Yue; Hu, Xinyue; Yang, Daowei; Feng, Jingna; Li, Zehan; He, Jianping; Tao, Wei; Duan, Tiehang; Lou, Yanyan; Li, Fang; Tao, Cui",RefAI: a GPT-powered retrieval-augmented generative tool for biomedical literature recommendation and summarization,Journal of the American Medical Informatics Association,,1527-974X,10.1093/jamia/ocae129,https://doi.org/10.1093/jamia/ocae129,"Precise literature recommendation and summarization are crucial for biomedical professionals. While the latest iteration of generative pretrained transformer (GPT) incorporates 2 distinct modes—real-time search and pretrained model utilization—it encounters challenges in dealing with these tasks. Specifically, the real-time search can pinpoint some relevant articles but occasionally provides fabricated papers, whereas the pretrained model excels in generating well-structured summaries but struggles to cite specific sources. In response, this study introduces RefAI, an innovative retrieval-augmented generative tool designed to synergize the strengths of large language models (LLMs) while overcoming their limitations.RefAI utilized PubMed for systematic literature retrieval, employed a novel multivariable algorithm for article recommendation, and leveraged GPT-4 turbo for summarization. Ten queries under 2 prevalent topics (“cancer immunotherapy and target therapy” and “LLMs in medicine”) were chosen as use cases and 3 established counterparts (ChatGPT-4, ScholarAI, and Gemini) as our baselines. The evaluation was conducted by 10 domain experts through standard statistical analyses for performance comparison.The overall performance of RefAI surpassed that of the baselines across 5 evaluated dimensions—relevance and quality for literature recommendation, accuracy, comprehensiveness, and reference integration for summarization, with the majority exhibiting statistically significant improvements (P-values &lt;.05).RefAI demonstrated substantial improvements in literature recommendation and summarization over existing tools, addressing issues like fabricated papers, metadata inaccuracies, restricted recommendations, and poor reference integration.By augmenting LLM with external resources and a novel ranking algorithm, RefAI is uniquely capable of recommending high-quality literature and generating well-structured summaries, holding the potential to meet the critical needs of biomedical professionals in navigating and synthesizing vast amounts of scientific literature.",2024-09-01,2025-01-25 12:18,2025-12-03 15:22,2025-01-25 12:18,2030-2039,,9,31,,Journal of the American Medical Informatics Association,RefAI,,,,,,,,,,,,Silverchair,,,,/Users/poppyriddle/Zotero/storage/59QHX6K2/Li et al. - 2024 - RefAI a GPT-powered retrieval-augmented generative tool for biomedical literature recommendation an.pdf; /Users/poppyriddle/Zotero/storage/7XPVVLN4/7690757.html,,CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MSW4IWXT,preprint,2024,"Agarwal, Shubham; Laradji, Issam H.; Charlin, Laurent; Pal, Christopher",LitLLM: A Toolkit for Scientific Literature Review,,,,10.48550/arXiv.2402.01788,http://arxiv.org/abs/2402.01788,"Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on the user-provided abstract. Finally, the related work section is generated based on the re-ranked results and the abstract. There is a substantial reduction in time and effort for literature review compared to traditional methods, establishing our toolkit as an efficient alternative. Our open-source toolkit is accessible at https://github.com/shubhamagarwal92/LitLLM and Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM) with the video demo at https://youtu.be/E2ggOZBAFw0.",2024-02-02,2025-01-25 12:33,2025-12-03 15:09,2025-01-25 12:33,,,,,,,LitLLM,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2402.01788 [cs],,/Users/poppyriddle/Zotero/storage/9ERDDX76/Agarwal et al. - 2024 - LitLLM A Toolkit for Scientific Literature Review.pdf; /Users/poppyriddle/Zotero/storage/QZC2W9H8/2402.html,,abstract in retriever; CORE PAPER; CORPUS; metadata,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Information Retrieval,,,,,,,,,,,,,,,,,,,arXiv:2402.01788,,,,,,,,,,,,,,,,,,,,,,,,,,,
5G4NEL9P,conferencePaper,2024,"Xu, Haowen; Li, Xueping; Tupayachi, Jose; Lian, Jianming Jamie; Omitaomu, Olufemi A.",Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG): A Pilot Study in Semantic and Contextual Search for Customized Literature Characterization for High-Impact Urban Research,Proceedings of the 2nd ACM SIGSPATIAL International Workshop on Advances in Urban-AI,979-8-4007-1156-5,,10.1145/3681780.3697252,https://doi.org/10.1145/3681780.3697252,"Bibliometric analysis is essential for understanding research trends, scope, and impact in urban science, especially in high-impact journals, such Nature Portfolios. However, traditional methods, relying on keyword searches and basic NLP techniques, often fail to uncover valuable insights not explicitly stated in article titles or keywords. These approaches are unable to perform semantic searches and contextual understanding, limiting their effectiveness in classifying topics and characterizing studies. In this paper, we address these limitations by leveraging Generative AI models, specifically transformers and Retrieval-Augmented Generation (RAG), to automate and enhance bibliometric analysis. We developed a technical workflow that integrates a vector database, Sentence Transformers, a Gaussian Mixture Model (GMM), Retrieval Agent, and Large Language Models (LLMs) to enable contextual search, topic ranking, and characterization of research using customized prompt templates. A pilot study analyzing 223 urban science-related articles published in Nature Communications over the past decade highlights the effectiveness of our approach in generating insightful summary statistics on the quality, scope, and characteristics of papers in high-impact journals. This study introduces a new paradigm for enhancing bibliometric analysis and knowledge retrieval in urban research, positioning an AI agent as a powerful tool for advancing research evaluation and understanding.",2024-11-04,2025-01-27 19:50,2025-12-03 15:47,2025-01-27,43–49,,,,,,Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG),UrbanAI '24,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,,,/Users/poppyriddle/Zotero/storage/JCNKMRBS/Xu et al. - 2024 - Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG).pdf; /Users/poppyriddle/Zotero/storage/H4AJF52R/Xu et al. - 2024 - Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG).pdf,,CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S9Q8DDVG,journalArticle,2014,"Royster, Paul",Foxes Propose New Guidelines for Henhouse Design: Comments on NISO’s Proposed Open Access Metadata Standards,Journal of Librarianship and Scholarly Communication,,2162-3309,10.7710/2162-3309.1170,https://www.iastatedigitalpress.com/jlsc/article/id/12714/,"This commentary is in response to: NISO RP-22-201x, Open Access Metadata and Indicators (draft for comment), which is available at: http://www.niso.org/apps/group_public/document.php?document_id=12047",2014-08-01,2025-02-03 19:28,2025-12-03 15:27,2025-02-03 19:28,,,3,2,,,Foxes Propose New Guidelines for Henhouse Design,,,,,,,en,,,,,www.iastatedigitalpress.com,,Number: 3 Publisher: Iowa State University Digital Press,,/Users/poppyriddle/Zotero/storage/ULJ7FZ37/Royster - 2014 - Foxes Propose New Guidelines for Henhouse Design Comments on NISO’s Proposed Open Access Metadata S.pdf,,CORPUS; to review,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WQVFLWN7,journalArticle,2021,"Bordignon, Frederique; Ermakova, Liana; Noel, Marianne",Over-promotion and caution in abstracts of preprints during the COVID-19 crisis,Learned Publishing,,1741-4857,10.1002/leap.1411,https://onlinelibrary.wiley.com/doi/abs/10.1002/leap.1411,"The abstract is known to be a promotional genre where researchers tend to exaggerate the benefit of their research and use a promotional discourse to catch the reader's attention. The COVID-19 pandemic has prompted intensive research and has changed traditional publishing with the massive adoption of preprints by researchers. Our aim is to investigate whether the crisis and the ensuing scientific and economic competition have changed the lexical content of abstracts. We propose a comparative study of abstracts associated with preprints issued in response to the pandemic relative to abstracts produced during the closest pre-pandemic period. We show that with the increase (on average and in percentage) of positive words (especially effective) and the slight decrease of negative words, there is a strong increase in hedge words (the most frequent of which are the modal verbs can and may). Hedge words counterbalance the excessive use of positive words and thus invite the readers, who go probably beyond the ‘usual’ audience, to be cautious with the obtained results. The abstracts of preprints urgently produced in response to the COVID-19 crisis stand between uncertainty and over-promotion, illustrating the balance that authors have to achieve between promoting their results and appealing for caution.",2021,2025-02-03 19:38,2025-12-03 15:34,2025-02-03 19:38,622-636,,4,34,,,,,,,,,,en,© 2021 The Authors. Learned Publishing © 2021 ALPSP,,,,Wiley Online Library,,_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/leap.1411,,/Users/poppyriddle/Zotero/storage/XJDQKSKC/Bordignon et al. - 2021 - Over-promotion and caution in abstracts of preprints during the COVID-19 crisis.pdf; /Users/poppyriddle/Zotero/storage/B5G6YI5Z/leap.html,,CORE PAPER; CORPUS,abstract; academic writing; COVID-19,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YMC9NCV6,journalArticle,2020,"Di Girolamo, Nicola; Meursinge Reynders, Reint",Characteristics of scientific articles on COVID-19 published during the initial 3 months of the pandemic,Scientometrics,,1588-2861,10.1007/s11192-020-03632-0,https://doi.org/10.1007/s11192-020-03632-0,"The COVID-19 pandemic has been characterized by an unprecedented amount of published scientific articles. The aim of this study is to assess the type of articles published during the first 3 months of the COVID-19 pandemic and to compare them with articles published during 2009 H1N1 swine influenza pandemic. Two operators independently extracted and assessed all articles on COVID-19 and on H1N1 swine influenza that had an abstract and were indexed in PubMed during the first 3 months of these pandemics. Of the 2482 articles retrieved on COVID-19, 1165 were included. Over half of them were secondary articles (590, 50.6%). Common primary articles were: human medical research (340, 59.1%), in silico studies (182, 31.7%) and in vitro studies (26, 4.5%). Of the human medical research, the vast majority were observational studies and cases series, followed by single case reports and one randomized controlled trial. Secondary articles were mainly reviews, viewpoints and editorials (373, 63.2%). Limitations were reported in 42 out of 1165 abstracts (3.6%), with 10 abstracts reporting actual methodological limitations. In a similar timeframe, there were 223 articles published on the H1N1 pandemic in 2009. During the COVID-19 pandemic there was a higher prevalence of reviews and guidance articles and a lower prevalence of in vitro and animal research studies compared with the H1N1 pandemic. In conclusions, compared to the H1N1 pandemic, the majority of early publications on COVID-19 does not provide new information, possibly diluting the original data published on this disease and consequently slowing down the development of a valid knowledge base on this disease. Also, only a negligible number of published articles reports limitations in the abstracts, hindering a rapid interpretation of their shortcomings. Researchers, peer reviewers, and editors should take action to flatten the curve of secondary articles.",2020-10-01,2025-02-03 19:44,2025-12-03 15:36,2025-02-03 19:44,795-812,,1,125,,Scientometrics,,,,,,,,en,,,,,Springer Link,,,,/Users/poppyriddle/Zotero/storage/P9I2YZGN/Di Girolamo and Meursinge Reynders - 2020 - Characteristics of scientific articles on COVID-19 published during the initial 3 months of the pand.pdf,,CORE PAPER; CORPUS,Coronavirus; Covid-19; Healthcare policy; Research quality; SARS-nCoV-2; Study design,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DCG6WYJU,journalArticle,2023,"Velez-Estevez, A.; Perez, I.J.; García-Sánchez, P.; Moral-Munoz, J.A.; Cobo, M.J.",New trends in bibliometric APIs: A comparative analysis,Information Processing & Management,,3064573,10.1016/j.ipm.2023.103385,https://linkinghub.elsevier.com/retrieve/pii/S030645732300122X,,Jul-23,2025-02-09 14:14,2025-12-03 15:30,2025-02-09 14:14,103385,,4,60,,Information Processing & Management,New trends in bibliometric APIs,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/poppyriddle/Zotero/storage/HN44IKTB/Velez-Estevez et al. - 2023 - New trends in bibliometric APIs A comparative analysis.pdf,,CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
S4S9XP7U,journalArticle,2024,"Han, Binglan; Susnjak, Teo; Mathrani, Anuradha",Automating Systematic Literature Reviews with Retrieval-Augmented Generation: A Comprehensive Overview,Applied Sciences,,2076-3417,10.3390/app14199103,https://www.mdpi.com/2076-3417/14/19/9103,"This study examines Retrieval-Augmented Generation (RAG) in large language models (LLMs) and their significant application for undertaking systematic literature reviews (SLRs). RAG-based LLMs can potentially automate tasks like data extraction, summarization, and trend identification. However, while LLMs are exceptionally proficient in generating human-like text and interpreting complex linguistic nuances, their dependence on static, pre-trained knowledge can result in inaccuracies and hallucinations. RAG mitigates these limitations by integrating LLMs’ generative capabilities with the precision of real-time information retrieval. We review in detail the three key processes of the RAG framework—retrieval, augmentation, and generation. We then discuss applications of RAG-based LLMs to SLR automation and highlight future research topics, including integration of domain-specific LLMs, multimodal data processing and generation, and utilization of multiple retrieval sources. We propose a framework of RAG-based LLMs for automating SRLs, which covers four stages of SLR process: literature search, literature screening, data extraction, and information synthesis. Future research aims to optimize the interaction between LLM selection, training strategies, RAG techniques, and prompt engineering to implement the proposed framework, with particular emphasis on the retrieval of information from individual scientific papers and the integration of these data to produce outputs addressing various aspects such as current status, existing gaps, and emerging trends.",2024-10-09,2025-02-12 16:35,2025-12-03 15:18,2025-02-12 16:35,9103,,19,14,,Applied Sciences,Automating Systematic Literature Reviews with Retrieval-Augmented Generation,,,,,,,en,https://creativecommons.org/licenses/by/4.0/,,,,DOI.org (Crossref),,,,/Users/poppyriddle/Zotero/storage/9MITBE2T/Han et al. - 2024 - Automating Systematic Literature Reviews with Retrieval-Augmented Generation A Comprehensive Overvi.pdf,,CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
VR7JCSK4,preprint,2025,"Chavarro, Diego; Alperin, Juan Pablo; Willinsky, John",On the Open Road to Universal Indexing: OpenAlex and Open Journal Systems,,,,10.1590/SciELOPreprints.11205,https://preprints.scielo.org/index.php/scielo/preprint/view/11205/version/11814,"This study examines OpenAlex’s indexing of journals using Open Journal Systems (JUOJS), reflecting two open source software initiatives supporting inclusive scholarly participation. By analyzing a dataset of 47,625 active JUOJS, we reveal that 71% of these journals have at least one article indexed in OpenAlex. Our findings underscore the central role of Crossref DOIs in achieving indexing, with 97% of the journals using Crossref DOIs included in OpenAlex. However, this technical dependency reflects broader structural inequities, as resource-limited journals, particularly those from low-income countries (47% of JUOJS) and non-English language journals (55%-64% of JUOJS), remain underrepresented. Our work highlights the theoretical implications of scholarly infrastructure dependencies and their role in perpetuating systemic disparities in global knowledge visibility. We argue that even inclusive bibliographic databases like OpenAlex must actively address financial, infrastructural, and linguistic barriers to foster equitable indexing on a global scale. By conceptualizing the relationship between indexing mechanisms, persistent identifiers, and structural inequities, this study provides a critical lens for rethinking the dynamics of universal indexing and its realization in a global, multilingual scholarly ecosystem.",2025-02-04,2025-02-13 20:25,2025-12-03 15:11,2025-02-13 20:25,,,,,,,On the Open Road to Universal Indexing,,,,,,,,https://creativecommons.org/licenses/by/4.0,,,,DOI.org (Crossref),,,,/Users/poppyriddle/Zotero/storage/9ANZSNIC/Chavarro et al. - 2025 - On the Open Road to Universal Indexing OpenAlex and Open Journal Systems.pdf,,CORE PAPER; CORPUS,open access; preprints; scholarly publishing; scielo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7R6GYZ36,journalArticle,2022,"Gusenbauer, Michael",Search where you will find most: Comparing the disciplinary coverage of 56 bibliographic databases,Scientometrics,,"0138-9130, 1588-2861",10.1007/s11192-022-04289-7,https://link.springer.com/10.1007/s11192-022-04289-7,"Abstract                            This paper introduces a novel scientometrics method and applies it to estimate the subject coverages of many of the popular English-focused bibliographic databases in academia. The method uses query results as a common denominator to compare a wide variety of search engines, repositories, digital libraries, and other bibliographic databases. The method extends existing sampling-based approaches that analyze smaller sets of database coverages. The findings show the relative and absolute subject coverages of 56 databases—information that has often not been available before. Knowing the databases’               absolute               subject coverage allows the selection of the most comprehensive databases for searches requiring high recall/sensitivity, particularly relevant in lookup or exploratory searches. Knowing the databases’               relative               subject coverage allows the selection of specialized databases for searches requiring high precision/specificity, particularly relevant in systematic searches. The findings illustrate not only differences in the disciplinary coverage of Google Scholar, Scopus, or Web of Science, but also of less frequently analyzed databases. For example, researchers might be surprised how Meta (discontinued), Embase, or Europe PMC are found to cover more records than PubMed in Medicine and other health subjects. These findings should encourage researchers to re-evaluate their go-to databases, also against newly introduced options. Searching with more comprehensive databases can improve finding, particularly when selecting the most fitting databases needs particular thought, such as in systematic reviews and meta-analyses. This comparison can also help librarians and other information experts re-evaluate expensive database procurement strategies. Researchers without institutional access learn which open databases are likely most comprehensive in their disciplines.",May-22,2025-02-23 20:06,2025-12-03 15:37,2025-02-23 20:06,2683-2745,,5,127,,Scientometrics,Search where you will find most,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/poppyriddle/Zotero/storage/3GNIMZDQ/Gusenbauer - 2022 - Search where you will find most Comparing the disciplinary coverage of 56 bibliographic databases.pdf,,CORPUS; to review,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7C79795E,journalArticle,2025,"Constantino, Isabel; Kojaku, Sadamori; Fortunato, Santo; Ahn, Yong-Yeol",Representing the disciplinary structure of physics: A comparative evaluation of graph and text embedding methods,Quantitative Science Studies,,2641-3337,10.1162/qss_a_00349,https://doi.org/10.1162/qss_a_00349,"Recent advances in machine learning offer new ways to represent and study scholarly works and the space of knowledge. Graph and text embeddings provide a convenient vector representation of scholarly works based on citations and text. Yet, it is unclear whether their representations are consistent or provide different views of the structure of science. Here, we compare graph and text embedding by testing their ability to capture the hierarchical structure of the Physics and Astronomy Classification Scheme (PACS) of papers published by the American Physical Society (APS). We also provide a qualitative comparison of the overall structure of the graph and text embeddings for reference. We find that neural network-based methods outperform traditional methods, and graph embedding methods node2vec and residual2vec are better than other methods at capturing the PACS structure. Our results call for further investigations into how different contexts of scientific papers are captured by different methods, and how we can combine and leverage such information in an interpretable manner.",2025-02-18,2025-02-24 11:40,2025-12-03 15:36,2025-02-24 11:40,18-Jan,,,,,Quantitative Science Studies,Representing the disciplinary structure of physics,,,,,,,,,,,,Silverchair,,,,/Users/poppyriddle/Zotero/storage/DPQQSDAR/Constantino et al. - 2025 - Representing the disciplinary structure of physics A comparative evaluation of graph and text embed.pdf; /Users/poppyriddle/Zotero/storage/DL62992K/Representing-the-disciplinary-structure-of-physics.html,,CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GKTK4ARW,dataset,2022,"Massari, Arcangelo",Classes of errors in DOI names: evaluation dataset,,,,10.5281/ZENODO.6188748,https://zenodo.org/record/6188748,"This dataset contains the results of the evaluation of the methodology presented in the article <em>Identifying and correcting invalid citations due to DOI errors in Crossref data</em> (https://arxiv.org/abs/2111.11263). The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article (10.5281/zenodo.4892551). They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository <em>open-sci/2020-2021-grasshoppers-code </em>(10.5281/zenodo.4723983).",2022-02-20,2025-03-25 17:43,2025-12-03 15:40,2025-03-25 17:43,,,,,,,Classes of errors in DOI names,,,,,Zenodo,,en,"Creative Commons Zero v1.0 Universal, Open Access",,,,DOI.org (Datacite),,,,,,CORPUS,COCI; evaluation; invalid DOIs; open citations; OpenCitations,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0.0,,,,,,,,
BMU7K78D,journalArticle,2020,"Rogers, Gordon; Szomszor, Martin; Adams, Jonathan",Sample size in bibliometric analysis,Scientometrics,,"0138-9130, 1588-2861",10.1007/s11192-020-03647-7,https://link.springer.com/10.1007/s11192-020-03647-7,"Abstract                            While bibliometric analysis is normally able to rely on complete publication sets this is not universally the case. For example, Australia (in ERA) and the UK (in the RAE/REF) use institutional research assessment that may rely on small or fractional parts of researcher output. Using the Category Normalised Citation Impact (CNCI) for the publications of ten universities with similar output (21,000–28,000 articles and reviews) indexed in the               Web of Science               for 2014–2018, we explore the extent to which a ‘sample’ of institutional data can accurately represent the averages and/or the correct relative status of the population CNCIs. Starting with full institutional data, we find a high variance in average CNCI across 10,000 institutional samples of fewer than 200 papers, which we suggest may be an analytical minimum although smaller samples may be acceptable for qualitative review. When considering the ‘top’ CNCI paper in researcher sets represented by DAIS-ID clusters, we find that samples of 1000 papers provide a good guide to relative (but not absolute) institutional citation performance, which is driven by the abundance of high performing individuals. However, such samples may be perturbed by scarce ‘highly cited’ papers in smaller or less research-intensive units. We draw attention to the significance of this for assessment processes and the further evidence that university rankings are innately unstable and generally unreliable.",Oct-20,2025-05-05 16:00,2025-12-03 15:44,2025-05-05 16:00,777-794,,1,125,,Scientometrics,,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/poppyriddle/Zotero/storage/3GA9LHE6/Rogers et al. - 2020 - Sample size in bibliometric analysis.pdf,,CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DU9TH4EE,journalArticle,2025,"Chen, Guo; Chen, Shuya; Chen, Zhili; Xiao, Lu; Hu, Jiming",How much data is sufficient for reliable bibliometric domain analysis? A multi-scenario experimental approach,Scientometrics,,"0138-9130, 1588-2861",10.1007/s11192-025-05335-w,https://link.springer.com/10.1007/s11192-025-05335-w,,2025-04-27,2025-05-05 16:04,2025-12-03 15:36,2025-05-05 16:04,,,,,,Scientometrics,How much data is sufficient for reliable bibliometric domain analysis?,,,,,,,en,,,,,DOI.org (Crossref),,,,/Users/poppyriddle/Zotero/storage/R5LKA2D6/Chen et al. - 2025 - How much data is sufficient for reliable bibliometric domain analysis A multi-scenario experimental.pdf,,CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
74P8GM9N,conferencePaper,2025,"Tian, Fangzheng; Ganguly, Debasis; Macdonald, Craig",Is Relevance Propagated from Retriever to Generator in RAG?,Advances in Information Retrieval,978-3-031-88708-6,,10.1007/978-3-031-88708-6_3,,"Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.",2025,2025-07-04 13:54,2025-12-03 15:45,,32-48,,,,,,,,,,,Springer Nature Switzerland,Cham,en,,,,,Springer Link,,,,/Users/poppyriddle/Zotero/storage/LXYZK4YE/Tian et al. - 2025 - Is Relevance Propagated from Retriever to Generator in RAG.pdf,,CORE PAPER; CORPUS,Context Utility; RAG Evaluation; Retrieval Augmented Generation (RAG); Topical Relevance,"Hauff, Claudia; Macdonald, Craig; Jannach, Dietmar; Kazai, Gabriella; Nardini, Franco Maria; Pinelli, Fabio; Silvestri, Fabrizio; Tonellotto, Nicola",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
E643G7L7,preprint,2025,"Hagström, Lovisa; Marjanović, Sara Vera; Yu, Haeun; Arora, Arnav; Lioma, Christina; Maistro, Maria; Atanasova, Pepa; Augenstein, Isabelle",A Reality Check on Context Utilisation for Retrieval-Augmented Generation,,,,10.48550/arXiv.2412.17031,http://arxiv.org/abs/2412.17031,"Retrieval-augmented generation (RAG) helps address the limitations of parametric knowledge embedded within a language model (LM). In real world settings, retrieved information can vary in complexity, yet most investigations of LM utilisation of context has been limited to synthetic text. We introduce DRUID (Dataset of Retrieved Unreliable, Insufficient and Difficult-to-understand contexts) with real-world queries and contexts manually annotated for stance. The dataset is based on the prototypical task of automated claim verification, for which automated retrieval of real-world evidence is crucial. We compare DRUID to synthetic datasets (CounterFact, ConflictQA) and find that artificial datasets often fail to represent the complexity and diversity of realistically retrieved context. We show that synthetic datasets exaggerate context characteristics rare in real retrieved data, which leads to inflated context utilisation results, as measured by our novel ACU score. Moreover, while previous work has mainly focused on singleton context characteristics to explain context utilisation, correlations between singleton context properties and ACU on DRUID are surprisingly small compared to other properties related to context source. Overall, our work underscores the need for real-world aligned context utilisation studies to represent and improve performance in real-world RAG settings.",2025-05-29,2025-07-04 16:22,2025-12-03 15:37,2025-07-04 16:22,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2412.17031 [cs],,/Users/poppyriddle/Zotero/storage/K9TB3T37/Hagström et al. - 2025 - A Reality Check on Context Utilisation for Retrieval-Augmented Generation.pdf; /Users/poppyriddle/Zotero/storage/526IDSSV/2412.html,,CORPUS,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2412.17031,,,,,,,,,,,,,,,,,,,,,,,,,,,
GI7BZ328,preprint,2024,"Gao, Yunfan; Xiong, Yun; Gao, Xinyu; Jia, Kangxiang; Pan, Jinliu; Bi, Yuxi; Dai, Yi; Sun, Jiawei; Wang, Meng; Wang, Haofen",Retrieval-Augmented Generation for Large Language Models: A Survey,,,,10.48550/arXiv.2312.10997,http://arxiv.org/abs/2312.10997,"Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.",2024-03-27,2025-07-04 16:23,2025-12-03 15:37,2025-07-04 16:23,,,,,,,Retrieval-Augmented Generation for Large Language Models,,,,,arXiv,,en,,,,,arXiv.org,,arXiv:2312.10997 [cs],,/Users/poppyriddle/Zotero/storage/X95259VI/Gao et al. - 2024 - Retrieval-Augmented Generation for Large Language Models A Survey.pdf,,CORE PAPER; CORPUS,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2312.10997,,,,,,,,,,,,,,,,,,,,,,,,,,,
8W7LR7EG,preprint,2025,"Bruni, Davide; Avvenuti, Marco; Tonellotto, Nicola; Tesconi, Maurizio",AMAQA: A Metadata-based QA Dataset for RAG Systems,,,,10.48550/arXiv.2505.13557,http://arxiv.org/abs/2505.13557,"Retrieval-augmented generation (RAG) systems are widely used in question-answering (QA) tasks, but current benchmarks lack metadata integration, hindering evaluation in scenarios requiring both textual data and external information. To address this, we present AMAQA, a new open-access QA dataset designed to evaluate tasks combining text and metadata. The integration of metadata is especially important in fields that require rapid analysis of large volumes of data, such as cybersecurity and intelligence, where timely access to relevant information is critical. AMAQA includes about 1.1 million English messages collected from 26 public Telegram groups, enriched with metadata such as timestamps, topics, emotional tones, and toxicity indicators, which enable precise and contextualized queries by filtering documents based on specific criteria. It also includes 450 high-quality QA pairs, making it a valuable resource for advancing research on metadata-driven QA and RAG systems. To the best of our knowledge, AMAQA is the first single-hop QA benchmark to incorporate metadata and labels such as topics covered in the messages. We conduct extensive tests on the benchmark, establishing a new standard for future research. We show that leveraging metadata boosts accuracy from 0.12 to 0.61, highlighting the value of structured context. Building on this, we explore several strategies to refine the LLM input by iterating over provided context and enriching it with noisy documents, achieving a further 3-point gain over the best baseline and a 14-point improvement over simple metadata filtering. The dataset is available at https://anonymous.4open.science/r/AMAQA-5D0D/",2025-05-19,2025-07-04 16:46,2025-12-03 15:11,2025-07-04 16:46,,,,,,,AMAQA,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2505.13557 [cs],,/Users/poppyriddle/Zotero/storage/Q3WMRGSP/Bruni et al. - 2025 - AMAQA A Metadata-based QA Dataset for RAG Systems.pdf; /Users/poppyriddle/Zotero/storage/QLXAGC9B/2505.html,,CORPUS,Computer Science - Artificial Intelligence; Computer Science - Information Retrieval,,,,,,,,,,,,,,,,,,,arXiv:2505.13557,,,,,,,,,,,,,,,,,,,,,,,,,,,
NJ7YT9U9,preprint,2025,"Sawarkar, Kunal; Solanki, Shivam R.; Mangal, Abhilasha",MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering,,,,10.48550/arXiv.2505.18247,http://arxiv.org/abs/2505.18247,"Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.",2025-06-04,2025-07-04 16:55,2025-12-03 15:44,2025-07-04 16:55,,,,,,,MetaGen Blended RAG,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2505.18247 [cs],,/Users/poppyriddle/Zotero/storage/CWUE6ED5/Sawarkar et al. - 2025 - MetaGen Blended RAG Unlocking Zero-Shot Precision for Specialized Domain Question-Answering.pdf; /Users/poppyriddle/Zotero/storage/YRUUP26Z/2505.html,,CORE PAPER; CORPUS,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Information Retrieval; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2505.18247,,,,,,,,,,,,,,,,,,,,,,,,,,,
RGQN2D67,journalArticle,2025,"Li, Zongxi; Wang, Zijian; Wang, Weiming; Hung, Kevin; Xie, Haoran; Wang, Fu Lee",Retrieval-augmented generation for educational application: A systematic survey,Computers and Education: Artificial Intelligence,,2666-920X,10.1016/j.caeai.2025.100417,https://www.sciencedirect.com/science/article/pii/S2666920X25000578,"Advancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.",2025-06-01,2025-07-04 17:12,2025-12-03 15:23,2025-07-04 17:12,100417,,,8,,Computers and Education: Artificial Intelligence,Retrieval-augmented generation for educational application,,,,,,,,,,,,ScienceDirect,,,,/Users/poppyriddle/Zotero/storage/E83GPA66/S2666920X25000578.html,,CORPUS,Artificial intelligence in education; Educational applications; Large language models (LLMs); Retrieval-augmented generation (RAG),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
34ERWDEZ,preprint,2025,"Lefton, Lew; Rong, Kexin; Dankhara, Chinar; Ghemri, Lila; Kausar, Firdous; Hamdallahi, A. Hannibal",A Socratic RAG Approach to Connect Natural Language Queries on Research Topics with Knowledge Organization Systems,,,,10.48550/arXiv.2502.15005,http://arxiv.org/abs/2502.15005,"In this paper, we propose a Retrieval Augmented Generation (RAG) agent that maps natural language queries about research topics to precise, machine-interpretable semantic entities. Our approach combines RAG with Socratic dialogue to align a user's intuitive understanding of research topics with established Knowledge Organization Systems (KOSs). The proposed approach will effectively bridge ""little semantics"" (domain-specific KOS structures) with ""big semantics"" (broad bibliometric repositories), making complex academic taxonomies more accessible. Such agents have the potential for broad use. We illustrate with a sample application called CollabNext, which is a person-centric knowledge graph connecting people, organizations, and research topics. We further describe how the application design has an intentional focus on HBCUs and emerging researchers to raise visibility of people historically rendered invisible in the current science system.",2025-02-20,2025-07-04 17:14,2025-12-03 15:22,2025-07-04 17:14,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2502.15005 [cs],,/Users/poppyriddle/Zotero/storage/Z7UXABDP/Lefton et al. - 2025 - A Socratic RAG Approach to Connect Natural Language Queries on Research Topics with Knowledge Organi.pdf; /Users/poppyriddle/Zotero/storage/SPVD5DDT/2502.html,,CORPUS,Computer Science - Artificial Intelligence; Computer Science - Computation and Language; Computer Science - Human-Computer Interaction,,,,,,,,,,,,,,,,,,,arXiv:2502.15005,,,,,,,,,,,,,,,,,,,,,,,,,,,
I7BHBQMR,preprint,2025,"Sainte-Marie, Maxime Holmberg; Kozlowski, Diego; Céspedes, Lucía; Larivière, Vincent",Sorting the Babble in Babel: Assessing the Performance of Language Detection Algorithms on the OpenAlex Database,,,,10.48550/arXiv.2502.03627,http://arxiv.org/abs/2502.03627,"This project aims to compare various language classification procedures, procedures combining various Python language detection algorithms and metadata-based corpora extracted from manually-annotated articles sampled from the OpenAlex database. Following an analysis of precision and recall performance for each algorithm, corpus, and language as well as of processing speeds recorded for each algorithm and corpus type, overall procedure performance at the database level was simulated using probabilistic confusion matrices for each algorithm, corpus, and language as well as a probabilistic model of relative article language frequencies for the whole OpenAlex database. Results show that procedure performance strongly depends on the importance given to each of the measures implemented: for contexts where precision is preferred, using the LangID algorithm on the greedy corpus gives the best results; however, for all cases where recall is considered at least slightly more important than precision or as soon as processing times are given any kind of consideration, the procedure combining the FastSpell algorithm and the Titles corpus outperforms all other alternatives. Given the lack of truly multilingual, large-scale bibliographic databases, it is hoped that these results help confirm and foster the unparalleled potential of the OpenAlex database for cross-linguistic, bibliometric-based research and analysis.",2025-02-18,2025-07-09 0:02,2025-12-03 15:44,2025-07-09 0:02,,,,,,,Sorting the Babble in Babel,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2502.03627 [cs],,/Users/poppyriddle/Zotero/storage/RBNFEYEE/Sainte-Marie et al. - 2025 - Sorting the Babble in Babel Assessing the Performance of Language Detection Algorithms on the OpenA.pdf; /Users/poppyriddle/Zotero/storage/8T7AWNWN/2502.html,,CORE PAPER; CORPUS,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2502.03627,,,,,,,,,,,,,,,,,,,,,,,,,,,
E4UTKMPU,preprint,2023,"Liu, Nelson F.; Lin, Kevin; Hewitt, John; Paranjape, Ashwin; Bevilacqua, Michele; Petroni, Fabio; Liang, Percy",Lost in the Middle: How Language Models Use Long Contexts,,,,10.48550/arXiv.2307.03172,http://arxiv.org/abs/2307.03172,"While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze the performance of language models on two tasks that require identifying relevant information in their input contexts: multi-document question answering and key-value retrieval. We find that performance can degrade significantly when changing the position of relevant information, indicating that current language models do not robustly make use of information in long input contexts. In particular, we observe that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context language models.",2023-11-20,2025-07-11 12:52,2025-12-03 15:40,2025-07-11 12:51,,,,,,,Lost in the Middle,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2307.03172 [cs],,/Users/poppyriddle/Zotero/storage/79B2WX5A/Liu et al. - 2023 - Lost in the Middle How Language Models Use Long Contexts.pdf; /Users/poppyriddle/Zotero/storage/Y2GCXMXS/2307.html,,CORPUS,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2307.03172,,,,,,,,,,,,,,,,,,,,,,,,,,,
LVKKW8XV,preprint,2013,"Mikolov, Tomas; Chen, Kai; Corrado, Greg; Dean, Jeffrey",Efficient Estimation of Word Representations in Vector Space,,,,10.48550/arXiv.1301.3781,http://arxiv.org/abs/1301.3781,"We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.",2013-09-07,2025-07-13 14:44,2025-12-03 15:41,2025-07-13 14:44,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:1301.3781 [cs],,/Users/poppyriddle/Zotero/storage/XW96NPXD/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Vector Space.pdf; /Users/poppyriddle/Zotero/storage/DFS9HRS2/1301.html,,CORPUS,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:1301.3781,,,,,,,,,,,,,,,,,,,,,,,,,,,
MX5LS7KC,conferencePaper,2014,"Pennington, Jeffrey; Socher, Richard; Manning, Christopher",Glove: Global Vectors for Word Representation,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),,,10.3115/v1/d14-1162,http://aclweb.org/anthology/D14-1162,"Recent methods for learning vector space representations of words have succeeded in capturing ﬁne-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efﬁciently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.",2014,2025-07-13 15:08,2025-12-03 15:43,2025-07-13 15:08,,,,,,,Glove,,,,,Association for Computational Linguistics,"Doha, Qatar",en,,,,,Crossref,,,,/Users/poppyriddle/Zotero/storage/IDEYCMPB/Pennington et al. - 2014 - Glove Global Vectors for Word Representation.pdf,,CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),,,,,,,,,,,,,,,
NAQRH2GQ,preprint,2017,"Bojanowski, Piotr; Grave, Edouard; Joulin, Armand; Mikolov, Tomas",Enriching Word Vectors with Subword Information,,,,10.48550/arXiv.1607.04606,http://arxiv.org/abs/1607.04606,"Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character $n$-grams. A vector representation is associated to each character $n$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",2017-06-19,2025-07-13 16:22,2025-12-03 15:34,2025-07-13 16:22,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:1607.04606 [cs],,/Users/poppyriddle/Zotero/storage/XRSPKLHP/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf; /Users/poppyriddle/Zotero/storage/SMPA7F5K/1607.html,,CORPUS,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:1607.04606,,,,,,,,,,,,,,,,,,,,,,,,,,,
2ERYM8YX,preprint,2016,"Joulin, Armand; Grave, Edouard; Bojanowski, Piotr; Mikolov, Tomas",Bag of Tricks for Efficient Text Classification,,,,10.48550/arXiv.1607.01759,http://arxiv.org/abs/1607.01759,"This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.",2016-08-09,2025-07-13 16:23,2025-12-03 15:38,2025-07-13 16:23,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:1607.01759 [cs],,/Users/poppyriddle/Zotero/storage/E9BJPTMA/Joulin et al. - 2016 - Bag of Tricks for Efficient Text Classification.pdf; /Users/poppyriddle/Zotero/storage/WIAQ5ECR/1607.html,,CORPUS,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:1607.01759,,,,,,,,,,,,,,,,,,,,,,,,,,,
ZJKSK9JS,bookSection,2024,"Dakhel, Arghavan Moradi; Nikanjam, Amin; Khomh, Foutse; Desmarais, Michel C.; Washizaki, Hironori",An Overview on Large Language Models,Generative AI for Effective Software Development,978-3-031-55641-8 978-3-031-55642-5,,10.1007/978-3-031-55642-5_1,https://link.springer.com/10.1007/978-3-031-55642-5_1,,2024,2025-07-13 16:35,2025-12-03 15:42,2025-07-13 16:35,21-Mar,,,,,,,,,,,Springer Nature Switzerland,Cham,en,https://www.springernature.com/gp/researchers/text-and-data-mining,,,,Crossref,,DOI: 10.1007/978-3-031-55642-5_1,,,,CORPUS,,"Nguyen-Duc, Anh; Abrahamsson, Pekka; Khomh, Foutse",,,"Dakhel, Arghavan Moradi; Nikanjam, Amin; Khomh, Foutse; Desmarais, Michel C.; Washizaki, Hironori",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
53YPJJEJ,preprint,2022,"Longpre, Shayne; Perisetla, Kartik; Chen, Anthony; Ramesh, Nikhil; DuBois, Chris; Singh, Sameer",Entity-Based Knowledge Conflicts in Question Answering,,,,10.48550/arXiv.2109.05052,http://arxiv.org/abs/2109.05052,"Knowledge-dependent tasks typically use two sources of knowledge: parametric, learned at training time, and contextual, given as a passage at inference time. To understand how models use these sources together, we formalize the problem of knowledge conflicts, where the contextual information contradicts the learned information. Analyzing the behaviour of popular models, we measure their over-reliance on memorized information (the cause of hallucinations), and uncover important factors that exacerbate this behaviour. Lastly, we propose a simple method to mitigate over-reliance on parametric knowledge, which minimizes hallucination, and improves out-of-distribution generalization by 4%-7%. Our findings demonstrate the importance for practitioners to evaluate model tendency to hallucinate rather than read, and show that our mitigation strategy encourages generalization to evolving information (i.e., time-dependent queries). To encourage these practices, we have released our framework for generating knowledge conflicts.",2022-01-12,2025-07-15 14:31,2025-12-03 15:40,2025-07-15 14:31,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2109.05052 [cs],,/Users/poppyriddle/Zotero/storage/XZ5BR9LX/Longpre et al. - 2022 - Entity-Based Knowledge Conflicts in Question Answering.pdf; /Users/poppyriddle/Zotero/storage/BFV5G9IV/2109.html,,CORE PAPER; CORPUS,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2109.05052,,,,,,,,,,,,,,,,,,,,,,,,,,,
372KSSWE,preprint,2024,"Marjanović, Sara Vera; Yu, Haeun; Atanasova, Pepa; Maistro, Maria; Lioma, Christina; Augenstein, Isabelle",DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models,,,,10.48550/arXiv.2407.17023,http://arxiv.org/abs/2407.17023,"Knowledge-intensive language understanding tasks require Language Models (LMs) to integrate relevant context, mitigating their inherent weaknesses, such as incomplete or outdated knowledge. However, conflicting knowledge can be present in the LM's parameters, termed intra-memory conflict, which can affect a model's propensity to accept contextual knowledge. To study the effect of intra-memory conflict on an LM's ability to accept relevant context, we utilize two knowledge conflict measures and a novel dataset containing inherently conflicting data, DynamicQA. This dataset includes facts with a temporal dynamic nature where facts can change over time and disputable dynamic facts, which can change depending on the viewpoint. DynamicQA is the first to include real-world knowledge conflicts and provide context to study the link between the different types of knowledge conflicts. We also evaluate several measures on their ability to reflect the presence of intra-memory conflict: semantic entropy and a novel coherent persuasion score. With our extensive experiments, we verify that LMs exhibit a greater degree of intra-memory conflict with dynamic facts compared to facts that have a single truth value. Furthermore, we reveal that facts with intra-memory conflict are harder to update with context, suggesting that retrieval-augmented generation will struggle with the most commonly adapted facts.",2024-10-07,2025-07-15 14:34,2025-12-03 15:40,2025-07-15 14:34,,,,,,,DYNAMICQA,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2407.17023 [cs],,/Users/poppyriddle/Zotero/storage/B9N8KQDW/Marjanović et al. - 2024 - DYNAMICQA Tracing Internal Knowledge Conflicts in Language Models.pdf; /Users/poppyriddle/Zotero/storage/BDGQI4TR/2407.html,,CORE PAPER; CORPUS,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2407.17023,,,,,,,,,,,,,,,,,,,,,,,,,,,
MFNNZSVS,preprint,2024,"Wan, Alexander; Wallace, Eric; Klein, Dan",What Evidence Do Language Models Find Convincing?,,,,10.48550/arXiv.2402.11782,http://arxiv.org/abs/2402.11782,"Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as ""is aspartame linked to cancer"". To resolve these ambiguous queries, one must search through a large range of websites and consider ""which, if any, of this evidence do I find convincing?"". In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone. Taken together, these results highlight the importance of RAG corpus quality (e.g., the need to filter misinformation), and possibly even a shift in how LLMs are trained to better align with human judgements.",2024-08-09,2025-07-15 14:36,2025-12-03 15:46,2025-07-15 14:36,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2402.11782 [cs],,/Users/poppyriddle/Zotero/storage/NZ4KHXMH/Wan et al. - 2024 - What Evidence Do Language Models Find Convincing.pdf; /Users/poppyriddle/Zotero/storage/ZBX59IIS/2402.html,,CORE PAPER; CORPUS,Computer Science - Computation and Language; Computer Science - Machine Learning,,,,,,,,,,,,,,,,,,,arXiv:2402.11782,,,,,,,,,,,,,,,,,,,,,,,,,,,
KKH8E9YU,preprint,2024,"Cho, Sukmin; Jeong, Soyeong; Seo, Jeongyeon; Hwang, Taeho; Park, Jong C.",Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations,,,,10.48550/arXiv.2404.13948,http://arxiv.org/abs/2404.13948,"The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.",2024-10-22,2025-07-15 19:01,2025-12-03 15:12,2025-07-15 19:01,,,,,,,Typos that Broke the RAG's Back,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2404.13948 [cs],,/Users/poppyriddle/Zotero/storage/ABLBYGNM/Cho et al. - 2024 - Typos that Broke the RAG's Back Genetic Attack on RAG Pipeline by Simulating Documents in the Wild.pdf; /Users/poppyriddle/Zotero/storage/QUXX9GDF/2404.html,,CORE PAPER; CORPUS,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2404.13948,,,,,,,,,,,,,,,,,,,,,,,,,,,
EJ9K9JB2,journalArticle,2024,"Chen, Jiawei; Lin, Hongyu; Han, Xianpei; Sun, Le",Benchmarking Large Language Models in Retrieval-Augmented Generation,Proceedings of the AAAI Conference on Artificial Intelligence,,2374-3468,10.1609/aaai.v38i16.29728,https://ojs.aaai.org/index.php/AAAI/article/view/29728,"Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.",2024-03-24,2025-07-16 13:39,2025-12-03 15:12,2025-07-16 13:38,17754-17762,,16,38,,,,,,,,,,en,Copyright (c) 2024 Association for the Advancement of Artificial Intelligence,,,,ojs.aaai.org,,Number: 16,,/Users/poppyriddle/Zotero/storage/4G3W9IWT/Chen et al. - 2024 - Benchmarking Large Language Models in Retrieval-Augmented Generation.pdf,,CORE PAPER; CORPUS,Analysis; and Evaluation of NLP Models; NLP: Interpretability,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
WRI8W4XA,journalArticle,2025,"Mongeon, Philippe; Hare, Madelaine; Krause, Geoff; Marjoram, Rebecca; Riddle, Poppy; Toupin, Rémi; Wilson, Summer",Investigating Document Type Discrepancies between OpenAlex and the Web of Science,Proceedings of the Annual Conference of CAIS / Actes du congrès annuel de l'ACSI,,2562-7589,10.29173/cais1943,https://journals.library.ualberta.ca/ojs.cais-acsi.ca/index.php/cais-asci/article/view/1943,"Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, launched in 2022, stands out for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinder its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess the metadata quality of works in OpenAlex and WoS, focusing on document type accuracy. We observe that over 4% of the publications indexed in both OpenAlex and WoS appear to be misclassified as research articles or reviews, and that the vast majority (about 97%) of these errors occur in OpenAlex. By addressing discrepancies and misattributions in document types this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.   Enquête sur les divergences de types de documents entre OpenAlex et the Web of Science RésuméLa bibliométrie, qu’elle soit utilisée pour la recherche ou pour l’évaluation de la recherche, repose sur de vastes bases de données multidisciplinaires regroupant des publications scientifiques et indices de citation. The Web of Science (WoS) était la principale infrastructure dans le domaine pendant plus 30 ans jusqu’à l’émergence de plusieurs nouveaux concurrents. OpenAlex, lancée en 2022, se démarque pour sa transparence et sa couverture étendue. Pendant qu’OpenAlex pouvait réduire voire éliminer les barrières pour l’accès aux données bibliométriques, une des préoccupations qui entravait sa large adoption pour la recherche et l'évaluation de la recherche est la qualité de ses métadonnées. Cette étude a pour but d’évaluer la qualité du travail des métadonnées dans OpenAlex et WoS, en se focalisant sur la précision du type de document. On observe que plus de 4% des publications indexées, à la fois dans OpenAlex et dans WoS, semblent être mal classées en tant qu’articles ou revues de recherche, et que la grande majorité (environ 97%) de ces erreurs se présentent dans OpenAlex. En relevant ces divergences et erreurs d’attribution dans les types de documents, cette recherche a pour but d’améliorer l’attention portée aux problèmes de qualité des données qui pourraient impacter les recherches bibliométriques et les résultats de l’évaluation. Mots-clésBibliométrie; évaluation de la recherche; OpenAlex; Web of Science; Bibliothèque et Science de l’Information; données ouvertes, métadonnées, type de document",2025-05-23,2025-07-25 18:49,2025-12-03 15:24,2025-07-25 18:49,,,,,,,,,,,,,,en,"Copyright (c) 2025 Philippe Mongeon, Madelaine Hare, Geoff Krause, Rebecca Marjoram, Poppy Riddle, Rémi Toupin, Summer Wilson",,,,journals.library.ualberta.ca,,,,/Users/poppyriddle/Zotero/storage/W9EI88LU/Mongeon et al. - 2025 - Investigating Document Type Discrepancies between OpenAlex and the Web of Science.pdf,,CORPUS,Bibliometrics; Document type; Library and Information Science; Metadata; Open data; OpenAlex; Research evaluation; Web of Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PBZYW7HH,preprint,2025,"Mongeon, Philippe; Hare, Madelaine; Riddle, Poppy; Wilson, Summer; Krause, Geoff; Marjoram, Rebecca; Toupin, Rémi","Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science",,,,10.48550/arXiv.2508.18620,http://arxiv.org/abs/2508.18620,"Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.",2025-08-26,2025-10-14 18:36,2025-12-03 15:24,2025-10-14 18:36,,,,,,,,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2508.18620 [cs],,"/Users/poppyriddle/Zotero/storage/4Y2KEALJ/Mongeon et al. - 2025 - Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between Open.pdf; /Users/poppyriddle/Zotero/storage/5FUIGB8E/2508.html",,CORE PAPER; CORPUS,Computer Science - Digital Libraries,,,,,,,,,,,,,,,,,,,arXiv:2508.18620,,,,,,,,,,,,,,,,,,,,,,,,,,,
V9NME5H6,journalArticle,2018,"Savolainen, Reijo",Pioneering models for information interaction in the context of information seeking and retrieval,Journal of Documentation,,0022-0418,10.1108/JD-11-2017-0154,http://www.emerald.com/jd/article/74/5/966-986/214117,"Purpose               The purpose of this paper is to clarify the conceptual issues of information behaviour research by reviewing the approaches to information interaction in the context of information seeking and retrieval (IS&R).                                         Design/methodology/approach               The study uses the conceptual analysis focussing on four pioneering models for interactive IS&R proposed by Belkin, Ingwersen and Ingwersen and Järvelin.                                         Findings               A main characteristic of models for information interaction is the tripartite setting identifying information resources accessible through information systems, intermediary/interface and user. Dialogue is a fundamental constituent of information interaction. Early models proposed by Belkin and Ingwersen focussed on the dialogue occurring in user-intermediary interaction, while more recent frameworks developed by Ingwersen and Järvelin devote more attention to dialogue constitutive of user-information system interaction.                                         Research limitations/implications               As the study focusses on four models developed within the period of 1984-2005, the findings cannot be generalised to depict the phenomena of information interaction as a whole. Further research is needed to model the specific features of information interaction occurring in the networked information environments in particular.                                         Originality/value               The study pioneers by providing an in-depth analysis of the ways in which pioneering researchers have conceptualised the phenomena of interaction in the context of IS&R. The findings contribute to the elaboration of the conceptual space of information behaviour research.",2018-08-03,2025-11-04 20:28,2025-12-03 15:27,2025-11-04 20:28,966-986,,5,74,,JD,,,,,,,,en,https://www.emerald.com/insight/site-policies,,,,DOI.org (Crossref),,,,/Users/poppyriddle/Zotero/storage/5QT38WCT/Savolainen - 2018 - Pioneering models for information interaction in the context of information seeking and retrieval.pdf,,CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RF2WLKTZ,journalArticle,2022,"Pawlick-Potts, Danica; Humboldt-Universität Zu Berlin",Is anybody in there?: Towards a model of affect and trust in human – AI information interactions,,,,10.18452/25258,https://edoc.hu-berlin.de/handle/18452/25986,"Advancements in search engines that utilize machine learning increase the likelihood that users will perceive these systems as worthy of trust. The nature and implications of trust in the context of algorithmic systems that utilize machine learning is examined and the resulting conception of trust is modelled. While current artificial intelligence does not meet the requirements of moral autonomy necessary to be considered trustworthy, people may still engage in misplaced trust based on the perception of moral autonomy. Users who place their trust in algorithmic systems limit their critical engagement with, and assessment of, the information interaction. A preliminary high-level model of trust’s role in information interactions adapting Ingwersen and Jarvelin’s Integrative Model for Interactive Information Seeking and Retrieval is proposed using the Google search engine as an example. We need to recognize that is it possible for users to react to information systems in a social manner that may lead to the formation of trust attitudes. As information professionals we want to develop interventions that will encourage users to stay critically engaged with their interactions with information systems, even when they perceive them to be autonomous.",Sep-22,2025-11-04 20:46,2025-12-03 15:25,2025-11-04 20:46,,,,,,,Is anybody in there?,,,,,,,en,"(CC BY 4.0) Attribution 4.0 International, Creative Commons Attribution 4.0 International",,,,DOI.org (Datacite),,Publisher: Humboldt-Universität zu Berlin,,/Users/poppyriddle/Zotero/storage/AL4GRZ4M/Pawlick-Potts - 2022 - Is anybody in there Towards a model of affect and trust in human – AI information interactions.pdf,,core journals; CORE PAPER; CORPUS,020 Bibliotheks- und Informationswissenschaften; conceptual models; human-computer interaction; information interactions; trust,,,,Humboldt-Universität Zu Berlin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5MQJYTDN,journalArticle,2025,"Antal, Margit; Buza, Krisztian",Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas,"Acta Universitatis Sapientiae, Informatica",,2066-7760,10.1007/s44427-025-00006-3,https://doi.org/10.1007/s44427-025-00006-3,"Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks. Our dataset and the corresponding evaluation code are publicly accessible at https://github.com/margitantal68/rag_paper.",2025-07-16,2025-11-06 17:16,2025-12-03 15:10,2025-11-06 17:16,5,,1,17,,Acta Univ. Sapientiae Inform.,Evaluating Open-Source LLMs in RAG Systems,,,,,,,en,,,,,Springer Link,,,,/Users/poppyriddle/Zotero/storage/4J7CIJH6/Antal and Buza - 2025 - Evaluating Open-Source LLMs in RAG Systems A Benchmark on Diploma Theses Abstracts Using Ragas.pdf,,CORE PAPER; CORPUS,Elasticsearch; Evaluation; LLM; RAG; Ragas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
86GYM68L,conferencePaper,2024,"Es, Shahul; James, Jithin; Espinosa Anke, Luis; Schockaert, Steven",RAGAs: Automated Evaluation of Retrieval Augmented Generation,Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations,,,10.18653/v1/2024.eacl-demo.16,https://aclanthology.org/2024.eacl-demo.16/,"We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAGAs is available at [https://github.com/explodinggradients/ragas]. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.",Mar-24,2025-11-06 17:20,2025-12-03 15:14,2025-11-06 17:20,150–158,,,,,,RAGAs,,,,,Association for Computational Linguistics,"St. Julians, Malta",,,,,,ACLWeb,,,,/Users/poppyriddle/Zotero/storage/U54DQLRV/Es et al. - 2024 - RAGAs Automated Evaluation of Retrieval Augmented Generation.pdf,,core journals; CORE PAPER; CORPUS,,"Aletras, Nikolaos; De Clercq, Orphee",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MIVNZ8WZ,conferencePaper,2024,"Wang, Shuai; Khramtsova, Ekaterina; Zhuang, Shengyao; Zuccon, Guido",FeB4RAG: Evaluating Federated Search in the Context of Retrieval Augmented Generation,Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval,979-8-4007-0431-4,,10.1145/3626772.3657853,https://doi.org/10.1145/3626772.3657853,"Federated search systems aggregate results from multiple search engines, selecting appropriate sources to enhance result quality and align with user intent. With the increasing uptake of Retrieval-Augmented Generation (RAG) pipelines, federated search can play a pivotal role in sourcing relevant information across heterogeneous data sources to generate informed responses. However, existing datasets, such as those developed in the past TREC FedWeb tracks, predate the RAG paradigm shift and lack representation of modern information retrieval challenges.To bridge this gap, we present FeB4RAG, a novel dataset specifically designed for federated search within RAG frameworks. This dataset, derived from 16 sub-collections of the widely used BEIR benchmarking collection, includes 790 information requests (akin to conversational queries) tailored for chatbot applications, along with top results returned by each resource and associated LLM-derived relevance judgements. Additionally, to support the need for this collection, we demonstrate the impact on response generation of a high quality federated search system for RAG compared to a naive approach to federated search. We do so by comparing answers generated by the RAG pipeline with a qualitative side-by-side comparison. Our collection fosters and supports the development and evaluation of new federated search methods, especially in the context of RAG pipelines. The resource is publicly available at https://github.com/ielab/FeB4RAG.",2024-07-11,2025-11-06 17:46,2025-12-03 15:31,2025-11-06,763–773,,,,,,FeB4RAG,SIGIR '24,,,,Association for Computing Machinery,"New York, NY, USA",,,,,,ACM Digital Library,,,,/Users/poppyriddle/Zotero/storage/SJBNEWT8/Wang et al. - 2024 - FeB4RAG Evaluating Federated Search in the Context of Retrieval Augmented Generation.pdf,,CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HBSILQCF,preprint,2023,"Liu, Yi; Huang, Lianzhe; Li, Shicheng; Chen, Sishuo; Zhou, Hao; Meng, Fandong; Zhou, Jie; Sun, Xu",RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge,,,,10.48550/arXiv.2311.08147,http://arxiv.org/abs/2311.08147,"LLMs and AI chatbots have improved people's efficiency in various fields. However, the necessary knowledge for answering the question may be beyond the models' knowledge boundaries. To mitigate this issue, many researchers try to introduce external knowledge, such as knowledge graphs and Internet contents, into LLMs for up-to-date information. However, the external information from the Internet may include counterfactual information that will confuse the model and lead to an incorrect response. Thus there is a pressing need for LLMs to possess the ability to distinguish reliable information from external knowledge. Therefore, to evaluate the ability of LLMs to discern the reliability of external knowledge, we create a benchmark from existing knowledge bases. Our benchmark consists of two tasks, Question Answering and Text Generation, and for each task, we provide models with a context containing counterfactual information. Evaluation results show that existing LLMs are susceptible to interference from unreliable external knowledge with counterfactual information, and simple intervention methods make limited contributions to the alleviation of this issue.",2023-11-14,2025-11-06 17:51,2025-12-03 15:23,2025-11-06 17:51,,,,,,,RECALL,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2311.08147 [cs],,/Users/poppyriddle/Zotero/storage/RD95FUGV/Liu et al. - 2023 - RECALL A Benchmark for LLMs Robustness against External Counterfactual Knowledge.pdf; /Users/poppyriddle/Zotero/storage/NZ99ERZI/2311.html,,CORE PAPER; CORPUS,Computer Science - Artificial Intelligence; Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2311.08147,,,,,,,,,,,,,,,,,,,,,,,,,,,
3FQJDZSA,preprint,2019,"Jin, Qiao; Dhingra, Bhuwan; Liu, Zhengping; Cohen, William W.; Lu, Xinghua",PubMedQA: A Dataset for Biomedical Research Question Answering,,,,10.48550/arXiv.1909.06146,http://arxiv.org/abs/1909.06146,"We introduce PubMedQA, a novel biomedical question answering (QA) dataset collected from PubMed abstracts. The task of PubMedQA is to answer research questions with yes/no/maybe (e.g.: Do preoperative statins reduce atrial fibrillation after coronary artery bypass grafting?) using the corresponding abstracts. PubMedQA has 1k expert-annotated, 61.2k unlabeled and 211.3k artificially generated QA instances. Each PubMedQA instance is composed of (1) a question which is either an existing research article title or derived from one, (2) a context which is the corresponding abstract without its conclusion, (3) a long answer, which is the conclusion of the abstract and, presumably, answers the research question, and (4) a yes/no/maybe answer which summarizes the conclusion. PubMedQA is the first QA dataset where reasoning over biomedical research texts, especially their quantitative contents, is required to answer the questions. Our best performing model, multi-phase fine-tuning of BioBERT with long answer bag-of-word statistics as additional supervision, achieves 68.1% accuracy, compared to single human performance of 78.0% accuracy and majority-baseline of 55.2% accuracy, leaving much room for improvement. PubMedQA is publicly available at https://pubmedqa.github.io.",2019-09-13,2025-11-10 18:24,2025-12-03 15:21,2025-11-10 18:24,,,,,,,PubMedQA,,,,,arXiv,,,,,,,arXiv.org,,arXiv:1909.06146 [cs],,/Users/poppyriddle/Zotero/storage/WRP7CBHM/Jin et al. - 2019 - PubMedQA A Dataset for Biomedical Research Question Answering.pdf; /Users/poppyriddle/Zotero/storage/9XIF94W8/1909.html,,CORE PAPER; CORPUS,Computer Science - Computation and Language; Computer Science - Machine Learning; Quantitative Biology - Quantitative Methods,,,,,,,,,,,,,,,,,,,arXiv:1909.06146,,,,,,,,,,,,,,,,,,,,,,,,,,,
EAJLNUYV,conferencePaper,2025,"Wu, Jinyang; Zhang, Shuai; Che, Feihu; Feng, Mingkuan; Shao, Pengpeng; Tao, Jianhua",Pandora's Box or Aladdin's Lamp: A Comprehensive Analysis Revealing the Role of RAG Noise in Large Language Models,Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),979-8-89176-251-0,,10.18653/v1/2025.acl-long.250,https://aclanthology.org/2025.acl-long.250/,"Retrieval-Augmented Generation (RAG) has emerged as a crucial method for addressing hallucinations in large language models (LLMs). While recent research has extended RAG models to complex noisy scenarios, these explorations often confine themselves to limited noise types and presuppose that noise is inherently detrimental to LLMs, potentially deviating from real-world retrieval environments and restricting practical applicability. In this paper, we define seven distinct noise types from a linguistic perspective and establish a Noise RAG Benchmark (NoiserBench), a comprehensive evaluation framework encompassing multiple datasets and reasoning tasks. Through empirical evaluation of eight representative LLMs with diverse architectures and scales, we reveal that these noises can be further categorized into two practical groups: noise that is beneficial to LLMs (aka beneficial noise) and noise that is harmful to LLMs (aka harmful noise). While harmful noise generally impairs performance, beneficial noise may enhance several aspects of model capabilities and overall performance. Our analysis offers insights for developing robust RAG solutions and mitigating hallucinations across diverse retrieval scenarios. Code is available at https://github.com/jinyangwu/NoiserBench.",Jul-25,2025-11-17 18:13,2025-12-03 15:31,2025-11-17 18:13,5019–5039,,,,,,Pandora's Box or Aladdin's Lamp,,,,,Association for Computational Linguistics,"Vienna, Austria",,,,,,ACLWeb,,,,/Users/poppyriddle/Zotero/storage/QPFW2GS8/Wu et al. - 2025 - Pandora's Box or Aladdin's Lamp A Comprehensive Analysis Revealing the Role of RAG Noise in Large L.pdf,,CORE PAPER; CORPUS,,"Che, Wanxiang; Nabende, Joyce; Shutova, Ekaterina; Pilehvar, Mohammad Taher",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,ACL 2025,,,,,,,,,,,,,,,
PJN5TSDQ,preprint,2025,"Stambolic, Vasilije; Dhar, Aritra; Cavigelli, Lukas",RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation,,,,10.48550/arXiv.2510.11195,http://arxiv.org/abs/2510.11195,"Retrieval-Augmented Generation (RAG) increases the reliability and trustworthiness of the LLM response and reduces hallucination by eliminating the need for model retraining. It does so by adding external data into the LLM's context. We develop a new class of black-box attack, RAG-Pull, that inserts hidden UTF characters into queries or external code repositories, redirecting retrieval toward malicious code, thereby breaking the models' safety alignment. We observe that query and code perturbations alone can shift retrieval toward attacker-controlled snippets, while combined query-and-target perturbations achieve near-perfect success. Once retrieved, these snippets introduce exploitable vulnerabilities such as remote code execution and SQL injection. RAG-Pull's minimal perturbations can alter the model's safety alignment and increase preference towards unsafe code, therefore opening up a new class of attacks on LLMs.",2025-10-13,2025-11-17 19:44,2025-12-03 15:29,2025-11-17 19:44,,,,,,,RAG-Pull,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2510.11195 [cs],,/Users/poppyriddle/Zotero/storage/XXUGV2MC/Stambolic et al. - 2025 - RAG-Pull Imperceptible Attacks on RAG Systems for Code Generation.pdf; /Users/poppyriddle/Zotero/storage/TC65T25Z/2510.html,,CORE PAPER; CORPUS,Computer Science - Artificial Intelligence; Computer Science - Cryptography and Security,,,,,,,,,,,,,,,,,,,arXiv:2510.11195,,,,,,,,,,,,,,,,,,,,,,,,,,,
Q793YMUK,preprint,2025,"Zeng, Yixiao; Cao, Tianyu; Wang, Danqing; Zhao, Xinran; Qiu, Zimeng; Ziyadi, Morteza; Wu, Tongshuang; Li, Lei",RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems,,,,10.48550/arXiv.2506.00789,http://arxiv.org/abs/2506.00789,"Retrieval-Augmented Generation (RAG) enhances recency and factuality in answers. However, existing evaluations rarely test how well these systems cope with real-world noise, conflicting between internal and external retrieved contexts, or fast-changing facts. We introduce Retrieval-Aware Robustness Evaluation (RARE), a unified framework and large-scale benchmark that jointly stress-tests query and document perturbations over dynamic, time-sensitive corpora. One of the central features of RARE is a knowledge-graph-driven synthesis pipeline (RARE-Get) that automatically extracts single and multi-hop relations from the customized corpus and generates multi-level question sets without manual intervention. Leveraging this pipeline, we construct a dataset (RARE-Set) spanning 527 expert-level time-sensitive finance, economics, and policy documents and 48295 questions whose distribution evolves as the underlying sources change. To quantify resilience, we formalize retrieval-conditioned robustness metrics (RARE-Met) that capture a model's ability to remain correct or recover when queries, documents, or real-world retrieval results are systematically altered. Our findings reveal that RAG systems are unexpectedly sensitive to perturbations. Moreover, they consistently demonstrate lower robustness on multi-hop queries compared to single-hop queries across all domains.",2025-10-27,2025-11-18 0:30,2025-12-03 15:48,2025-11-18 0:30,,,,,,,RARE,,,,,arXiv,,,,,,,arXiv.org,,arXiv:2506.00789 [cs],,/Users/poppyriddle/Zotero/storage/3BPAZUH9/Zeng et al. - 2025 - RARE Retrieval-Aware Robustness Evaluation for Retrieval-Augmented Generation Systems.pdf; /Users/poppyriddle/Zotero/storage/IQUHFDRB/2506.html,,CORE PAPER; CORPUS,Computer Science - Computation and Language,,,,,,,,,,,,,,,,,,,arXiv:2506.00789,,,,,,,,,,,,,,,,,,,,,,,,,,,
EKYLGV7U,journalArticle,2025,"Taylor, Jaime; Dagan, Kelly; Youngberg, Margaret; Kaufman, Therese; Radding, Johanna",A Survey of AI tools in Library Tech: Accelerating into and Unlocking Streamlined Enhanced Convenient Empowering Game-Changers,Journal of Electronic Resources Librarianship,,1941-126X,10.1080/1941126X.2025.2497738,https://doi.org/10.1080/1941126X.2025.2497738,,2025-04-03,2025-11-18 12:39,2025-12-03 15:29,2025-11-18 12:39,217-229,,2,37,,,A Survey of AI tools in Library Tech,,,,,,,,,,,,Taylor and Francis+NEJM,,Publisher: Routledge _eprint: https://doi.org/10.1080/1941126X.2025.2497738,,/Users/poppyriddle/Zotero/storage/C8CWPGY8/Taylor et al. - 2025 - A Survey of AI tools in Library Tech Accelerating into and Unlocking Streamlined Enhanced Convenien.pdf,,CORE PAPER; CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U48T2NVJ,preprint,2025,"Cao, Tianyu; Bhandari, Neel; Yerukola, Akhila; Asai, Akari; Sap, Maarten",Out of Style: RAG's Fragility to Linguistic Variation,,,,10.48550/ARXIV.2504.08231,https://arxiv.org/abs/2504.08231,"Despite the impressive performance of Retrieval-augmented Generation (RAG) systems across various NLP benchmarks, their robustness in handling real-world user-LLM interaction queries remains largely underexplored. This presents a critical gap for practical deployment, where user queries exhibit greater linguistic variations and can trigger cascading errors across interdependent RAG components. In this work, we systematically analyze how varying four linguistic dimensions (formality, readability, politeness, and grammatical correctness) impact RAG performance. We evaluate two retrieval models and nine LLMs, ranging from 3 to 72 billion parameters, across four information-seeking Question Answering (QA) datasets. Our results reveal that linguistic reformulations significantly impact both retrieval and generation stages, leading to a relative performance drop of up to 40.41% in Recall@5 scores for less formal queries and 38.86% in answer match scores for queries containing grammatical errors. Notably, RAG systems exhibit greater sensitivity to such variations compared to LLM-only generations, highlighting their vulnerability to error propagation due to linguistic shifts. These findings highlight the need for improved robustness techniques to enhance reliability in diverse user interactions.",2025,2025-11-20 17:07,2025-12-03 15:35,2025-11-20 17:07,,,,,,,Out of Style,,,,,arXiv,,,Creative Commons Attribution 4.0 International,,,,DOI.org (Datacite),,Version Number: 1,,/Users/poppyriddle/Zotero/storage/S76DMQUK/Cao et al. - 2025 - Out of Style RAG's Fragility to Linguistic Variation.pdf; /Users/poppyriddle/Zotero/storage/3DJ5DKUV/2504.html,,CORPUS,Computation and Language (cs.CL); Computer Science - Computation and Language; FOS: Computer and information sciences,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HUSR5F3J,journalArticle,2025,"Kumar, Vinit; Chandrappa; Harinarayana, NS",Exploring dimensions of metadata quality assessment: A scoping review,Journal of Librarianship and Information Science,,0961-0006,10.1177/09610006241239080,https://doi.org/10.1177/09610006241239080,"Assessing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.",2025-09-01,2025-11-20 23:59,2025-12-03 15:39,2025-11-20 23:59,661-673,,3,57,,,Exploring dimensions of metadata quality assessment,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications Ltd,,/Users/poppyriddle/Zotero/storage/FFBTBFQ8/Kumar et al. - 2025 - Exploring dimensions of metadata quality assessment A scoping review.pdf,,CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RJDFE3Z3,journalArticle,2022,"Nguyen, Ba Xuan; Luczak-Roesch, Markus; Dinneen, Jesse David; Larivière, Vincent",Assessing the quality of bibliographic data sources for measuring international research collaboration,Quantitative Science Studies,,2641-3337,10.1162/qss_a_00211,https://doi.org/10.1162/qss_a_00211,"Measuring international research collaboration (IRC) is essential to various research assessment tasks but the effect of various measurement decisions, including which data sources to use, has not been thoroughly studied. To better understand the effect of data source choice on IRC measurement, we design and implement a data quality assessment framework specifically for bibliographic data by reviewing and selecting available dimensions and designing appropriate computable metrics, and then validate the framework by applying it to four popular sources of bibliographic data: Microsoft Academic Graph, Web of Science (WoS), Dimensions, and the ACM Digital Library. Successful validation of the framework suggests it is consistent with the popular conceptual framework of information quality proposed by Wang and Strong (1996) and adequately identifies the differences in quality in the sources examined. Application of the framework reveals that WoS has the highest overall quality among the sets considered; and that the differences in quality can be explained primarily by how the data sources are organized. Our study comprises a methodological contribution that enables researchers to apply this IRC measurement tool in their studies and makes an empirical contribution by further characterizing four popular sources of bibliographic data and their impact on IRC measurement.",2022-11-01,2025-11-21 15:01,2025-12-03 15:41,2025-11-21 15:01,529-559,,3,3,,Quantitative Science Studies,,,,,,,,,,,,,Silverchair,,,,/Users/poppyriddle/Zotero/storage/ETECQJEV/Nguyen et al. - 2022 - Assessing the quality of bibliographic data sources for measuring international research collaborati.pdf; /Users/poppyriddle/Zotero/storage/T6DTXS3N/qss_a_00211.html,,CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YR5TSPTK,journalArticle,2016,"Neumaier, Sebastian; Umbrich, Jürgen; Polleres, Axel",Automated Quality Assessment of Metadata across Open Data Portals,J. Data and Information Quality,,1936-1955,10.1145/2964909,https://dl.acm.org/doi/10.1145/2964909,"The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources.In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.",2016-10-25,2025-11-21 15:08,2025-12-03 15:41,2025-11-21 15:08,2:1–2:29,,1,8,,,,,,,,,,,,,,,ACM Digital Library,,,,/Users/poppyriddle/Zotero/storage/5YS3EVSK/Neumaier et al. - 2016 - Automated Quality Assessment of Metadata across Open Data Portals.pdf,,CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
U227C9CK,journalArticle,2015,"Zaveri, Amrapali; Rula, Anisa; Maurino, Andrea; Pietrobon, Ricardo; Lehmann, Jens; Auer, Sören",Quality assessment for Linked Data: A Survey: A systematic literature review and conceptual framework,Semantic Web,,1570-0844,10.3233/SW-150175,https://journals.sagepub.com/action/showAbstract,"The development and standardization of Semantic Web technologies has resulted in an unprecedented volume of data being published on the Web as Linked Data (LD). However, we observe widely varying data quality ranging from extensively curated datasets to crowdsourced and extracted data of relatively low quality. In this article, we present the results of a systematic review of approaches for assessing the quality of LD. We gather existing approaches and analyze them qualitatively. In particular, we unify and formalize commonly used terminologies across papers related to data quality and provide a comprehensive list of 18 quality dimensions and 69 metrics. Additionally, we qualitatively analyze the 30 core approaches and 12 tools using a set of attributes. The aim of this article is to provide researchers and data curators a comprehensive understanding of existing work, thereby encouraging further experimentation and development of new approaches focused towards data quality, specifically for LD.",2015-11-10,2025-11-26 15:51,2025-12-03 15:48,2025-11-26 15:51,63-93,,1,7,,,Quality assessment for Linked Data,,,,,,,EN,,,,,SAGE Journals,,Publisher: SAGE Publications,,/Users/poppyriddle/Zotero/storage/B2348CWX/Zaveri et al. - 2015 - Quality assessment for Linked Data A Survey A systematic literature review and conceptual framewor.pdf,,cited; CORPUS; to print,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
KGWD7P3G,conferencePaper,1998,"Moen, W.E.; Stewart, E.L.; McClure, C.R.",Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS),Proceedings IEEE International Forum on Research and Technology Advances in Digital Libraries -ADL'98-,,,10.1109/ADL.1998.670425,https://ieeexplore.ieee.org/abstract/document/670425,"Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.",Apr-98,2025-11-26 15:55,2025-12-03 15:41,2025-11-26 15:55,246-255,,,,,,Assessing metadata quality,,,,,,,,,,,,IEEE Xplore,,ISSN: 1092-9959,,/Users/poppyriddle/Zotero/storage/GCZ65NDX/Moen et al. - 1998 - Assessing metadata quality findings and methodological considerations from an evaluation of the US.pdf,,cited; CORPUS; to print,Content management; Information analysis; Information management; Information retrieval; Libraries,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,IEEE International Forum on Research and Technology Advances in Digital Libraries -ADL'98-,,,,,,,,,,,,,,,
JGEWZEKE,journalArticle,2016,"Meschenmoser, Phillip; Meuschke, Norman; Hotz, Manuel; Gipp, Bela",Scraping Scientific Web Repositories: Challenges and Solutions for Automated Content Extraction,D-Lib Magazine,,,10.1045/september2016‐meschenmoser,https://gipplab.uni-goettingen.de/wp-content/papercite-data/pdf/meschenmoser2016a.pdf,,2016,2025-11-27 8:45,2025-12-03 15:41,2025-11-27 8:44,,,10-Sep,22,,,,,,,,,,en,,,,,,,,,/Users/poppyriddle/Zotero/storage/AMFSNUJ4/gipplab.uni-goettingen.dewp-contentpapercite-datapdfmeschenmoser2016a.pdf.pdf,,cited; CORPUS; to print,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RU5SWZZY,conferencePaper,2024,"Wahed, Mutaz Abdel; Alzboon, Mowafaq Salem; Alqaraleh, Muhyeeddin; Ayman, Jaradat; Al-Batah, Mohammad; Bader, Ahmad Fuad","Automating Web Data Collection: Challenges, Solutions, and Python-Based Strategies for Effective Web Scraping","2024 7th International Conference on Internet Applications, Protocols, and Services (NETAPPS)",,,10.1109/NETAPPS63333.2024.10823528,https://ieeexplore.ieee.org/document/10823528,"The process of collecting and retrieving such a massive amount of data is difficult, especially when manual approach is the only option. Instead, we can use web scraping to automate the process of collecting web data using bots or automated scripts known as web scrapers. Web scraping as data mining technology is gaining popularity among data scientists and analysts, it is also causing controversy due to ethical concerns about the ability of get data due to privacy and copyright using automated programs or tools. This paper aims to classify and estimate the various challenges and solutions associated with web scraping algorithms, specifically those utilizing python libraries, and addressing the ethical implication associated with this technology.",Nov-24,2025-11-27 8:50,2025-12-03 15:45,2025-11-27 8:50,06-Jan,,,,,,Automating Web Data Collection,,,,,,,,,,,,IEEE Xplore,,,,"/Users/poppyriddle/Zotero/storage/APIRCHNW/Wahed et al. - 2024 - Automating Web Data Collection Challenges, Solutions, and Python-Based Strategies for Effective Web.pdf",,cited; CORPUS; to print,big data; Bots; data mining; Data privacy; Ethics; Guidelines; Internet; IP networks; Law; Libraries; Manuals; Protocols; Python; Web Scraping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"2024 7th International Conference on Internet Applications, Protocols, and Services (NETAPPS)",,,,,,,,,,,,,,,
FGGVSQIE,conferencePaper,2024,"Arachchi, Naveen Hedalla; Dayarathne, Ranul Navojith; Aluthdeniya, Nipuna Dilshan; Ranasinghe, Wanuja; Ganegoda, Gamage Upeksha",Layout Aware Research Paper Parsing and Draft Research Paper Layout-Error Detection Using NLP and Rule-based Techniques,2024 9th International Conference on Information Technology Research (ICITR),,,10.1109/ICITR64794.2024.10857771,https://ieeexplore.ieee.org/document/10857771,"The growing demand for precise formatting and content organization in academic writing has created a need for tools to help undergraduates prepare high-quality research papers for better conference acceptance. This paper introduces a layout-aware content extraction and error detection system, employing a fine-tuned layout parser classification model alongside rule-based algorithms to enhance section classification accuracy and formatting compliance in research papers. The proposed system achieves an average accuracy of 0.8737 and demonstrates better performance across various sections with an average cosine similarity exceeding 0.8900, reflecting its effectiveness in preserving the content. Additionally, the error detection component identifies layout issues, such as section availability, reference duplication, and column misalignment, with high accuracy and F1 scores, showcasing its potential to streamline the drafting process. Unlike existing methods focused solely on text or entity extraction, this system integrates layout-aware content extraction with error detection to provide a comprehensive solution. It addresses diverse academic publisher standards and layout complexities, assisting students in refining their drafts to ensure higher quality and compliance. Future enhancements will incorporate transformer-based models and expanded datasets to improve adaptability, semantic understanding, and the detection of additional formatting errors, further advancing research paper quality.",Dec-24,2025-11-27 8:59,2025-12-03 15:33,2025-11-27 8:59,06-Jan,,,,,,,,,,,,,,,,,,IEEE Xplore,,ISSN: 2831-3399,,/Users/poppyriddle/Zotero/storage/R59PB37F/Arachchi et al. - 2024 - Layout Aware Research Paper Parsing and Draft Research Paper Layout-Error Detection Using NLP and Ru.pdf,,cited; CORPUS; to print,Accuracy; Adaptation models; Classification algorithms; Layout; Layout aware; Multiclass Classification; NLP; Refining; Research paper parser; Rule-based Techniques; Semantics; Standards organizations; Text extraction; Training; Transformers; Writing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,2024 9th International Conference on Information Technology Research (ICITR),,,,,,,,,,,,,,,
XN4D6LDH,journalArticle,2022,"Shen, Zejiang; Lo, Kyle; Wang Lucy Lu; Kuehl, Bailey; Weld, Daniel S.; Downey, Doug",VILA: Improving Structured Content Extraction from Scientific PDFs Using Visual Layout Groups,Transactions of the Association for Computational Linguistics,,,10.1162/tacl a 00466,https://www.proquest.com/docview/2893947179?parentSessionId=ffVWVMhcvXcNeZv%2FI%2FRFP4zqDhaN9mCKf95Y5Njao9s%3D&pq-origsite=primo&accountid=10406&sourcetype=Scholarly%20Journals,,2022,2025-11-27 9:07,2025-12-03 15:44,2025-11-27 9:07,376–392,,,10,,,,,,,,,,,,,,,,,,,/Users/poppyriddle/Zotero/storage/F9HKAUER/VILA Improving Structured Content Extraction from Scientific PDFs Using Visual Layout Groups - ProQ.pdf; /Users/poppyriddle/Zotero/storage/F9C3T7UR/2893947179.html,,cited; CORPUS; to print,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TKW2M2I8,conferencePaper,2024,"Fan, Wenqi; Ding, Yujuan; Ning, Liangbo; Wang, Shijie; Li, Hengyun; Yin, Dawei; Chua, Tat-Seng; Li, Qing",A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models,Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,979-8-4007-0490-1,,10.1145/3637528.3671470,https://dl.acm.org/doi/10.1145/3637528.3671470,,2024-08-25,2025-12-03 15:15,2025-12-03 15:16,2025-12-03 15:15,6491-6501,,,,,,A Survey on RAG Meeting LLMs,,,,,ACM,Barcelona Spain,en,,,,,DOI.org (Crossref),,,,,,CORPUS,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,KDD '24: The 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining,,,,,,,,,,,,,,,
N2JRAEF5,preprint,2022,"Priem, Jason; Piwowar, Heather; Orr, Richard","OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts",,,,10.48550/ARXIV.2205.01833,https://arxiv.org/abs/2205.01833,"OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.",2022,2025-12-03 15:26,2025-12-03 15:26,2025-12-03 15:26,,,,,,,OpenAlex,,,,,arXiv,,,Creative Commons Zero v1.0 Universal,,,,DOI.org (Datacite),,Version Number: 2,,,,CORPUS,Digital Libraries (cs.DL); FOS: Computer and information sciences,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,