{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere API and SciBERT with BM25 as pre-retriever for RAG\n",
    "This notebook uses a Cohere API for generating responses to text. A query input is required from the user. \n",
    "SciBERT is used for embeddings in a dense vector array for the query. \n",
    "This version is different in that it uses BM25 as a pre-retriever for the input text to reduce how many documents are processed by SciBERT (embeddings) and the generator.\n",
    "A DOI is supplied with the text as both an identifier and locator. \n",
    "\n",
    "## pipeline\n",
    "1. BM25 Retrieval\n",
    "    - BM25 is used to retrieve top-k candidate documents based on keyword matching\n",
    "2. Dense embedding retrieval\n",
    "    - query is embedded using SciBERT and the retrieved documents.\n",
    "3. Re-ranking\n",
    "    - cosine similarity between query embedding and document embedding to rerank candidate docs\n",
    "4. Generation\n",
    "    - docs and query are fed to generator for answer creation. \n",
    "\n",
    "\n",
    "### reference\n",
    "\n",
    "- rank_bm25: https://github.com/dorianbrown/rank_bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import cohere\n",
    "from cohere import Client\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import logging # finding where functions are taking too long\n",
    "#for BM25s\n",
    "import bm25s\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import pickle\n",
    "# models specific\n",
    "from generate_embeddings import generate_embeddings\n",
    "from bm25_retriever import bm25_retriever\n",
    "\n",
    "\n",
    "def main():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    global key,email\n",
    "    key = os.getenv('COHERE_API_KEY')\n",
    "    email = os.getenv('EMAIL')\n",
    "\n",
    "    #verify if it worked\n",
    "    if email is not None and key is not None:\n",
    "        print(\"all is good, beautiful!\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is callable\n"
     ]
    }
   ],
   "source": [
    "# Initialize Cohere client\n",
    "co = cohere.Client(key) \n",
    "\n",
    "# Load SciBERT model and tokenizer\n",
    "\"\"\"\n",
    "documentation can be found here: https://huggingface.co/docs/transformers/v4.50.0/en/model_doc/auto#transformers.AutoTokenizer\n",
    "\n",
    "\"\"\"\n",
    "# Initialize tokenizer with custom parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    max_len=512,\n",
    "    use_fast=True,  # Use the fast tokenizer\n",
    "    do_lower_case=False,  # Preserve case\n",
    "    add_prefix_space=False,  # No prefix space\n",
    "    never_split=[\"[DOC]\", \"[REF]\"],  # Tokens to never split\n",
    "    #additional_special_tokens=[\"<doi>\", \"</doi>\"],  # Add custom special tokens ***RE-EVALUATE*** (tuple or list of str or tokenizers.AddedToken, optional) â€” A tuple or a list of additional special tokens. Add them here to ensure they are skipped when decoding with skip_special_tokens is set to True. If they are not part of the vocabulary, they will be added at the end of the vocabulary.\n",
    "    skip_special_tokens=False,\n",
    ")\n",
    "\n",
    "# this is the SciBERT model that is used to embed the text and query.\n",
    "# other models: 'allenai-specter', \n",
    "#documentation here: https://huggingface.co/docs/transformers/model_doc/auto\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "#verify that the model is callable\n",
    "if callable(model):\n",
    "    print(\"Model is callable\")\n",
    "else:\n",
    "    print(\"Model is not callable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V4 BM25 Pre-retriever\n",
    "functions in V3 have been converted to modules.\n",
    "Includes the following:\n",
    "- BM25 pre-retriever\n",
    "- SciBERT embedding of query and pre-retrieved documents\n",
    "- cosine similarity between embeddings of query and documents\n",
    "- response instruction\n",
    "- context includes DOI, Title, and Abstract as augmentation to query and instruction.\n",
    "- response\n",
    "- follow up with retrieved documents for verfication. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
