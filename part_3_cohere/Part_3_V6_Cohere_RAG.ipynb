{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere RAG with dense retriever and ReRank model\n",
    "- references: https://docs.cohere.com/v2/docs/rag-complete-example\n",
    "<br>\n",
    "“This work was supported by compute credits from a Cohere Labs Research Grant, these grants are designed to support academic partners conducting research with the goal of releasing scientific artifacts and data for good projects.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import cohere\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import sys\n",
    "from colorama import Fore, Style, Back\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mall is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "# load secret from local .env file\n",
    "def get_key():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    _key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "    #verify if it worked\n",
    "    if _key is not None:\n",
    "        print(Fore.GREEN + \"all is good, beautiful!\")\n",
    "        return _key\n",
    "    else:\n",
    "        print(Fore.LIGHTRED_EX + \"API Key is missing\")\n",
    "\n",
    "# initilize client\n",
    "co = cohere.ClientV2(get_key())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0]\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append(f\"{doi} {text}\\n\")\n",
    "    return documents_with_doi\n",
    "\n",
    "# initialize empty search query\n",
    "search_queries = []\n",
    "# Embed the documents\n",
    "def document_embed(documents:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the documents from a list provided from read_documents_with_doi()\n",
    "    NOTE: Change the model based on the test condition\n",
    "    \"\"\"\n",
    "    doc_emb = co.embed(\n",
    "        model=\"embed-v4.0\",\n",
    "        #model=\"embed-english-v3.0\",\n",
    "        input_type=\"search_document\",\n",
    "        texts=[doc for doc in documents],\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return doc_emb\n",
    "\n",
    "# Embed the search query\n",
    "def query_embed(search_queries:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the query from a list provided in search_queries variable\n",
    "    NOTE: change model depending on test condition\n",
    "    \"\"\"\n",
    "    query_emb = co.embed(\n",
    "        model=\"embed-v4.0\",\n",
    "        #model=\"embed-english-v3.0\",\n",
    "        input_type=\"search_query\",\n",
    "        texts=search_queries,\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return query_emb\n",
    "\n",
    "# retrieve top_k and compute similarity using dot product\n",
    "def retrieve_top_k(top_k, query_embedded, documents_embedded, documents)->List[str]:\n",
    "    \"\"\"\n",
    "    returns the top_k documents based on dot product similarity\n",
    "    \"\"\"\n",
    "\n",
    "    scores = np.dot(query_embedded, np.transpose(documents_embedded))[0]#ordered list!\n",
    "    # takes top scores, and returns sorted list and returns indices sliced by top_k\n",
    "    max_idx = np.argsort(-scores)[:top_k]\n",
    "    # returns documents by index\n",
    "    retrieved_docs = [documents[item] for item in max_idx]\n",
    "    # returns a list of documents\n",
    "    return retrieved_docs\n",
    "\n",
    "def rerank_documents(retrieved_documents,search_queries,threshold,top_k)->List[str]:\n",
    "    \"\"\"\n",
    "    takes retrieved_documents as input along with search_queries and runs them through the \n",
    "    rerank model from cohere for semantic similarity. \n",
    "\n",
    "    top_n = top_k\n",
    "    Limits those returned by a threshold score. this is to reduce those that are irrelevant.\n",
    "\n",
    "    NOTE: change the model based on the test condition\n",
    "    \"\"\"\n",
    "    # Rerank the documents\n",
    "    results = co.rerank(\n",
    "        model=\"rerank-v3.5\",\n",
    "        #model=\"rerank-english-v3.0\",\n",
    "        query=search_queries[0],\n",
    "        documents=[doc for doc in retrieved_documents],\n",
    "        top_n=top_k,\n",
    "        max_tokens_per_doc=4096,# defaults to 4096\n",
    "    )\n",
    "\n",
    "    # Display the reranking results\n",
    "    #for idx, result in enumerate(results.results):\n",
    "    #    print(f\"Rank: {idx+1}\")\n",
    "    #    print(f\"Score: {result.relevance_score}\")\n",
    "    #    print(f\"Document: {retrieved_documents[result.index]}\\n\")\n",
    "\n",
    "    #returns only those over threshold\n",
    "    reranked_docs = [\n",
    "        retrieved_documents[result.index] for result in results.results if result.relevance_score >=threshold\n",
    "    ]\n",
    "    reranked_with_score = [(result.relevance_score, retrieved_documents[result.index].split(\"\\n\")[0].strip(\"DOI: \")) for result in results.results if result.relevance_score >=threshold]\n",
    "\n",
    "    print(f\"reranked_documents: {reranked_docs}\")\n",
    "    print(f\"length of reranked_documents: {len(reranked_docs)}\")\n",
    "\n",
    "    return reranked_docs, reranked_with_score\n",
    "\n",
    "def cohere_rag_pipeline(directory_path,search_queries,top_k,threshold):\n",
    "\n",
    "    # retrieve documents from directory\n",
    "    documents = read_documents_with_doi(directory_path)\n",
    "    print(f\"Length of documents: {len(documents)}\")\n",
    "\n",
    "    # randomize order of documents\n",
    "    random.shuffle(documents) # this was added for december run - it seemed to have a big impact on precision, recall, etc. \n",
    "\n",
    "    # embed the documents\n",
    "    documents_embedded = document_embed(documents)\n",
    "\n",
    "    #embed the query:\n",
    "    query_embedded = query_embed(search_queries)\n",
    "\n",
    "    # retrieve the top_k documents\n",
    "    retrieved_documents = retrieve_top_k(top_k, query_embedded, documents_embedded, documents)\n",
    "\n",
    "    # rerank the documents using the Rerank model from Cohere\n",
    "    reranked_documents, reranked_DOIs_with_score = rerank_documents(retrieved_documents,search_queries,threshold,top_k)\n",
    "    # set system instructions\n",
    "    instructions = \"\"\"\n",
    "                    You are an academic research assistant.\n",
    "                    You must include the DOI in your response.\n",
    "                    If there is no content provided, ask for a different question.\n",
    "                    Please structure your response like this:\n",
    "                    Summary: summary statement here. \n",
    "                    DOI: summary of the text associated with this DOI.\n",
    "                    Address me as, 'my lady'.\n",
    "                    \"\"\"\n",
    "    # create messages to model\n",
    "    messages = [{\"role\":\"user\",\n",
    "                \"content\": search_queries[0]},\n",
    "                {\"role\":\"system\",\n",
    "                \"content\":instructions}]\n",
    "\n",
    "    # Generate the response NOTE: change the model for the test condition!\n",
    "    resp = co.chat(\n",
    "        model=\"command-a-03-2025\",\n",
    "        messages=messages,\n",
    "        documents=reranked_documents,\n",
    "    )\n",
    "\n",
    "    return resp, reranked_documents, reranked_DOIs_with_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#  run here to test functions avove\n",
    "# ****** Pipeline ********\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.1\n",
    "\n",
    "response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "# Display the response\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"{response.message.content[0].text}\")\n",
    "print(Fore.LIGHTCYAN_EX + f\"------\\nReranked documents:\")\n",
    "for doc in reranked_documents_end:\n",
    "    print(doc)\n",
    "\n",
    "# Display the citations and source documents\n",
    "if response.message.citations:\n",
    "    print(Fore.LIGHTYELLOW_EX + \"\\nCITATIONS:\")\n",
    "    for citation in response.message.citations:\n",
    "        print(f\"source text: {citation.text},\\nsource: {citation.sources[0].document.get('content').split(\"\\n\")[0]}\\n------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# Analysis\n",
    "Precision, recall, accuracy, F1 scores and faithfulness\n",
    "## Precision, recall, F1 score\n",
    "### references\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "- https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from colorama import Fore, Back, Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## automated version \n",
    "Currently Works!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Documents score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query, Precision, Recall, F1-Score, Accuracy, Balanced accuracy, Faithfulness score, Documents score, Response]\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial dataframe to capture results from each query and results\n",
    "#ONLY DO THIS AT THE BEGINNING OF THE ANALYSIS PROCEDURE, OTHERWISE, IT WILL ERASE THE PREVIOUS RESULTS!!\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Query','Precision','Recall','F1-Score','Accuracy', 'Balanced accuracy', 'Faithfulness score', 'Documents score', 'Response'])\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m Golden set loaded!\n"
     ]
    }
   ],
   "source": [
    "golden_set_df = pd.read_excel(\"golden_set.xlsx\")\n",
    "#golden_set_df_test = golden_set_df.head(3)\n",
    "#golden_set_df\n",
    "print(Fore.LIGHTGREEN_EX + f\" Golden set loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of documents: 96\n",
      "Length of corpus: 96\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', \"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.1177/09610006241239080', '10.1109/ADL.1998.670425', '10.5281/zenodo.14006424']\n",
      "\u001b[93mSummary: Three studies examined the abstract in metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - This study compared the amount of metadata and the completeness degree of research publications in new academic databases.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - This study identified the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment.\n",
      "\n",
      "DOI: 10.5281/zenodo.14006424 - This study analyzed OpenAlex's coverage and metadata availability of African-based publications.\n",
      "\n",
      "My lady, I hope this information is helpful.\n",
      "96\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.250\n",
      "Recall: 0.200\n",
      "F1-Score: 0.222\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.63179934, '10.1162/qss_a_00286'), (0.58919704, '10.1177/09610006241239080'), (0.55420566, '10.1109/ADL.1998.670425'), (0.18688062, '10.5281/zenodo.14006424')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1108/JD-10-2022-0234\\n Title: すべての学術分野にわたる引用と参照の習慣の分析: 書誌の参照と引用の実践におけるアプローチと傾向 An analysis of citing and referencing habits across all scholarly disciplines: approaches and trends in bibliographic referencing and citing practices\\nAbstract: この研究で、著者らは、学術文献における引用および参照の誤りについて現在考えられる原因を特定し、スウィートランド氏が1989年の論文で提供したスナップショットから何かが変わったかどうかを比較したいと考えている。,著者らは、27の主題分野にわたる147のジャーナルに掲載された729件の論文から、参考要素、すなわち書誌的参照、言及、引用、およびそれぞれの本文中の参照ポインタを分析した。,分析の結果は、書誌的事項が指摘された。著者らの知る限り、この研究は、Sweetland (1989) 以来、文献における参照および引用の慣行における誤りを分析したものとしては、最近入手可能な最良のものである。 In this study, the authors want to identify current possible causes for citing and referencing errors in scholarly literature to compare if something changed from the snapshot provided by Sweetland in his 1989 paper.,The authors analysed reference elements, i.e. bibliographic references, mentions, quotations and respective in-text reference pointers, from 729 articles published in 147 journals across the 27 subject areas.,The outcomes of the analysis pointed out that bibliographic errors have been perpetuated for decades and that their possible causes have increased, despite the encouraged use of technological facilities, i.e. the reference managers.,As far as the authors know, the study is the best recent available analysis of errors in referencing and citing practices in the literature since Sweetland (1989).\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1108/JD-10-2022-0234', '10.1007/s11192-022-04367-w', '10.1162/qss_a_00112']\n",
      "\u001b[93mSummary: Three studies examined citations.\n",
      "\n",
      "DOI: 10.1108/JD-10-2022-0234 - This study examined the errors in referencing and citing practices in the literature since Sweetland (1989).\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w - This study examined the invalid citations due to DOI errors in Crossref data.\n",
      "\n",
      "DOI: 10.1162/qss_a_00112 - This study examined the completeness and accuracy of citation links in five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 1.000\n",
      "Recall: 0.600\n",
      "F1-Score: 0.750\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.800\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.3570971, '10.1108/JD-10-2022-0234'), (0.21608633, '10.1007/s11192-022-04367-w'), (0.13627496, '10.1162/qss_a_00112')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1007/s11192-022-04367-w', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: Crossref is a community-owned sustainable source of scholarly metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "\n",
      "Crossref is a community-owned sustainable source of scholarly metadata. It contains over 106 million records and expands at an average rate of 11% a year. It has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH.\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "96\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.800\n",
      "Recall: 0.800\n",
      "F1-Score: 0.800\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.895\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.7781275, '10.1162/qss_a_00022'), (0.72395444, '10.1162/qss_a_00210'), (0.55623144, '10.31222/osf.io/smxe5'), (0.44883752, '10.1007/s11192-022-04367-w'), (0.17410146, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n', 'DOI: 10.1007/s11192-022-04289-7\\n Title: 最も多く見つかる場所を検索: 56 の書誌データベースの懲戒範囲の比較 Search where you will find most: Comparing the disciplinary coverage of 56 bibliographic databases\\nAbstract: この論文では、新しいサイエントメトリクス手法を紹介し、それを学術界で人気のある英語に焦点を当てた書誌データベースの多くの主題範囲を推定するために適用します。この方法では、クエリ結果を共通の分母として使用して、さまざまな検索エンジン、リポジトリ、デジタル ライブラリ、その他の書誌データベースを比較します。この方法は、データベース カバレッジのより小さいセットを分析する既存のサンプリング ベースのアプローチを拡張します。この調査結果では、56 のデータベースの相対的および絶対的な対象範囲が示されており、これまで入手できなかった情報が示されています。データベースの絶対的な対象範囲を知ることで、特にルックアップ検索や探索的検索に関連する、高い再現率/感度が必要な検索に最も包括的なデータベースを選択できます。データベースの相対的な対象範囲を知ることで、特に体系的な検索に関連する、高い精度と特異性が必要な検索に特化したデータベースを選択できます。この調査結果は、Google Scholar、Scopus、または Web of Science の専門分野の範囲の違いだけでなく、分析頻度が低いデータベースの違いも示しています。たとえば、研究者は、Meta (廃止)、Embase、または Europe PMC が、医学やその他の健康分野の PubMed よりも多くの記録をカバーしていることが判明したことに驚くかもしれません。これらの発見は、研究者が新しく導入されたオプションに対しても頼りになるデータベースを再評価するよう促すはずです。より包括的なデータベースを使用して検索すると、特にシステマティック レビューやメタ分析など、最も適合するデータベースの選択に特別な考慮が必要な場合に、検索結果が向上します。この比較は、図書館員やその他の情報専門家が高価なデータベース調達戦略を再評価するのにも役立ちます。機関にアクセスできない研究者は、どのオープン データベースが自分の専門分野において最も包括的である可能性が高いかを学びます。 This paper introduces a novel scientometrics method and applies it to estimate the subject coverages of many of the popular English-focused bibliographic databases in academia. The method uses query results as a common denominator to compare a wide variety of search engines, repositories, digital libraries, and other bibliographic databases. The method extends existing sampling-based approaches that analyze smaller sets of database coverages. The findings show the relative and absolute subject coverages of 56 databases—information that has often not been available before. Knowing the databases’ absolute subject coverage allows the selection of the most comprehensive databases for searches requiring high recall/sensitivity, particularly relevant in lookup or exploratory searches. Knowing the databases’ relative subject coverage allows the selection of specialized databases for searches requiring high precision/specificity, particularly relevant in systematic searches. The findings illustrate not only differences in the disciplinary coverage of Google Scholar, Scopus, or Web of Science, but also of less frequently analyzed databases. For example, researchers might be surprised how Meta (discontinued), Embase, or Europe PMC are found to cover more records than PubMed in Medicine and other health subjects. These findings should encourage researchers to re-evaluate their go-to databases, also against newly introduced options. Searching with more comprehensive databases can improve finding, particularly when selecting the most fitting databases needs particular thought, such as in systematic reviews and meta-analyses. This comparison can also help librarians and other information experts re-evaluate expensive database procurement strategies. Researchers without institutional access learn which open databases are likely most comprehensive in their disciplines.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.48550/arXiv.2508.18620', '10.1162/qss_a_00286', '10.5281/zenodo.14006424', '10.1007/s11192-022-04289-7']\n",
      "\u001b[93mSummary: Several papers evaluate the linguistic coverage or language-related metadata in scholarly databases.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 - Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness - This paper assesses the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 - Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science - This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - Completeness degree of publication metadata in eight free-access scholarly databases - This study compares the amount of metadata and the completeness degree of research publications in new academic databases, including an analysis of language-related metadata.\n",
      "\n",
      "DOI: 10.5281/zenodo.14006424 - Coverage and metadata availability of African publications in OpenAlex: A comparative analysis - This paper analyzes OpenAlex's coverage and metadata availability of African-based publications, including a comparison with other databases and an assessment of language-related metadata.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need additional assistance, please let me know.\n",
      "96\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.200\n",
      "Recall: 0.200\n",
      "F1-Score: 0.200\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.9108078, '10.48550/arXiv.2409.10633'), (0.42706275, '10.48550/arXiv.2508.18620'), (0.41563448, '10.1162/qss_a_00286'), (0.19475074, '10.5281/zenodo.14006424'), (0.16595806, '10.1007/s11192-022-04289-7')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: COVID-19 研究への資金提供: オープン データ インフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 資金提供機関が提供する資金の成果を分析するには、資金提供によって得られた出版物を追跡できることが不可欠です。私たちは、Crossref での資金調達データのオープンな利用可能性を調査し、新型コロナウイルス感染症 (COVID-19) に関連する研究を報告する出版物の資金調達データに焦点を当てています。また、Scopus と Web of Science という 2 つの独自の書誌データベースで利用可能な資金調達データとの比較も示します。私たちの分析では、Crossref の資金調達データの範囲が限られていることが明らかになりました。また、特に Scopus における資金調達データの品質に関連する問題も示しています。 Crossref での資金調達データのオープンな可用性を向上させるための推奨事項を提供します。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.5281/zenodo.14006424', '10.31222/osf.io/smxe5', '10.1162/qss_a_00286', '10.1162/qss_a_00212']\n",
      "\u001b[93mSummary: I have found four papers that address funding metadata, its availability, or its analysis in scholarly databases.\n",
      "\n",
      "DOI: 10.1162/qss_a_00210 - This paper addresses the availability and completeness of open funder metadata, using a case study of publications funded by the Dutch Research Council.\n",
      "\n",
      "DOI: 10.5281/zenodo.14006424 - This paper analyses OpenAlex's coverage and metadata availability of African-based publications, comparing it with Scopus, Web of Science, and African Journals Online.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This paper presents an overview of the availability of six metadata elements in Crossref, including funding information.\n",
      "\n",
      "DOI: 10.1162/qss_a_00212 - This paper studies the open availability of funding data in Crossref, focusing on COVID-19 research, and compares it with Scopus and Web of Science.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need more details, please let me know.\n",
      "96\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.800\n",
      "Recall: 1.000\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.990\n",
      "Balanced accuracy: 0.995\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.68798697, '10.1162/qss_a_00210'), (0.48616135, '10.5281/zenodo.14006424'), (0.4024631, '10.31222/osf.io/smxe5'), (0.31545174, '10.1162/qss_a_00286'), (0.30762884, '10.1162/qss_a_00212')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", \"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.48550/arXiv.2312.10997', '10.1145/3637528.3671470', '10.6109/jkiice.2023.27.12.1489', '10.1007/s44427-025-00006-3']\n",
      "\u001b[93mMy lady, here are the papers that discuss the use of Retrieval-Augmented Generation (RAG) in applications:\n",
      "\n",
      "**Summary:** Retrieval-Augmented Generation (RAG) is discussed in various papers focusing on its application in enhancing Large Language Models (LLMs) by incorporating external knowledge sources, improving accuracy, and enabling dynamic knowledge updates.\n",
      "\n",
      "**DOI: 10.1016/j.caeai.2025.100417**\n",
      "This paper focuses on the application of RAG in educational scenarios, addressing challenges like hallucination and static internal knowledge in LLMs. It explores practical applications in interactive learning systems, educational content generation, and large-scale deployment in educational ecosystems.\n",
      "\n",
      "**DOI: 10.48550/arXiv.2312.10997**\n",
      "This comprehensive review paper examines the progression of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG. It discusses the integration of external databases to enhance LLM accuracy, particularly for knowledge-intensive tasks.\n",
      "\n",
      "**DOI: 10.1145/3637528.3671470**\n",
      "This survey highlights RAG's role in providing reliable external knowledge, especially in AI-Generated Content (AIGC). It discusses Retrieval-Augmented Large Language Models (RA-LLMs) and their ability to enhance content quality by utilizing external knowledge bases.\n",
      "\n",
      "**DOI: 10.6109/jkiice.2023.27.12.1489**\n",
      "This paper proposes a Metadata Retrieval-Augmented Generation (M-RAG) method for open-domain question answering, demonstrating performance improvements compared to conventional RAG methods.\n",
      "\n",
      "**DOI: 10.1007/s44427-025-00006-3**\n",
      "This study evaluates open-source LLMs in RAG systems using a benchmark dataset, focusing on retrieval effectiveness and answer quality for academic and structured knowledge tasks.\n",
      "\n",
      "These papers collectively showcase the diverse applications and advancements of RAG in various domains, including education, question answering, and LLM enhancement.\n",
      "96\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) when used in applications?']:\n",
      "Precision: 0.200\n",
      "Recall: 0.200\n",
      "F1-Score: 0.200\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.8958969, '10.1016/j.caeai.2025.100417'), (0.81418616, '10.48550/arXiv.2312.10997'), (0.7173491, '10.1145/3637528.3671470'), (0.69672287, '10.6109/jkiice.2023.27.12.1489'), (0.6675167, '10.1007/s44427-025-00006-3')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1007/s11192-022-04367-w', '10.1162/qss_a_00112']\n",
      "\u001b[93mSummary: Crossref is a sustainable source of community-owned scholarly metadata that collects and makes available academic metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.95628047, '10.1162/qss_a_00022'), (0.47548413, '10.1162/qss_a_00210'), (0.44680908, '10.31222/osf.io/smxe5'), (0.21246108, '10.1007/s11192-022-04367-w'), (0.19260046, '10.1162/qss_a_00112')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.29173/cais1943\\n Title: OpenAlex と Web of Science の間のドキュメント タイプの不一致を調査する Investigating Document Type Discrepancies between OpenAlex and the Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された OpenAlex は、そのオープン性と広範なカバレッジで際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この研究は、ドキュメント タイプの精度に焦点を当て、OpenAlex と WoS における作品のメタデータの品質を評価することを目的としています。 OpenAlex と WoS の両方でインデックス付けされている出版物の 4% 以上が研究論文またはレビューとして誤って分類されているようであり、これらのエラーの大部分 (約 97%) が OpenAlex で発生していることが観察されています。この研究は、文書タイプの不一致や誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, launched in 2022, stands out for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinder its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess the metadata quality of works in OpenAlex and WoS, focusing on document type accuracy. We observe that over 4% of the publications indexed in both OpenAlex and WoS appear to be misclassified as research articles or reviews, and that the vast majority (about 97%) of these errors occur in OpenAlex. By addressing discrepancies and misattributions in document types this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', \"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.3145/epi.2023.mar.09\\n Title: Microsoft Academic Graph から OpenAlex に切り替える場合、書誌情報学に関連するメタデータはどれが同じで、どれが異なりますか? Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\\nAbstract: Microsoft Academic Graph (MAG) の廃止の発表に伴い、非営利団体 OurResearch は OpenAlex という名前で同様のリソースを提供すると発表しました。したがって、最新の MAG スナップショットと初期の OpenAlex スナップショットの書誌学的分析に関連するメタデータを比較します。 MAG の実質的にすべての著作物は、書誌データの出版年、巻数、最初と最後のページ、DOI、および引用分析の重要な要素である参考文献の数を保存しながら OpenAlex に転送されました。 MAG ドキュメントの 90% 以上が OpenAlex に同等のドキュメント タイプを持っています。残りのうち、特に OpenAlex 文書タイプの Journal-article および Book-chapter への再分類は正しいようで、その割合は 7% 以上に達しており、文書タイプの仕様は MAG から OpenAlex に大幅に改善されました。書誌学的関連メタデータの別の項目として、MAG と OpenAlex における紙ベースの主題分類を調べました。 OpenAlex では、MAG よりもはるかに多くの主題分類が割り当てられているドキュメントが見つかりました。第 1 レベルと第 2 レベルでは、分類構造はほぼ同じです。主題の再分類に関するデータを表とグラフの両方のレベルで表示します。分野で正規化された書誌学的評価に対する豊富な主題の再分類の影響の評価は、この論文の範囲には含まれません。この未解決の質問とは別に、OpenAlex は全体的に、文書タイプの割り当てがより広範囲にカバーされているため、2021 年より前の出版物については少なくとも MAG と同等かそれ以上に書誌学的分析に適しているようです。 With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2508.18620', '10.29173/cais1943', '10.48550/arXiv.2409.10633', '10.3145/epi.2023.mar.09']\n",
      "\u001b[93mSummary: OpenAlex is a bibliographic database that was launched in 2022. It is open-source and inclusive, and promises comprehensive research information. It is already in use by scholars and research institutions, but the quality of its metadata is currently being assessed.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663\n",
      "OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. However, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620\n",
      "OpenAlex may reduce or eliminate barriers to accessing bibliometric data, but one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata.\n",
      "\n",
      "DOI: 10.29173/cais1943\n",
      "OpenAlex stands out for its openness and extensive coverage. However, one of the concerns that hinder its broader adoption for research and research evaluation is the quality of its metadata.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633\n",
      "OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages.\n",
      "\n",
      "DOI: 10.3145/epi.2023.mar.09\n",
      "OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.8455478, '10.48550/arXiv.2404.17663'), (0.7178241, '10.48550/arXiv.2508.18620'), (0.69024646, '10.29173/cais1943'), (0.64344186, '10.48550/arXiv.2409.10633'), (0.6353361, '10.3145/epi.2023.mar.09')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1007/s11192-015-1765-5\\n Title: Web of Science と Scopus のジャーナルの報道: 比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌学的手法は、研究の評価など、さまざまな目的で複数の分野で使用されています。ほとんどの書誌学的分析には、トムソン・ロイターの Web of Science (WoS) とエルゼビアの Scopus というデータ ソースが共通しています。この調査の目的は、これら 2 つのデータベースのジャーナルの対象範囲を説明し、特定の分野、出版国、言語が過小評価されているかどうかを評価することです。これを行うために、WoS (13,605 ジャーナル) および Scopus (20,346 ジャーナル) のアクティブな学術ジャーナルの範囲を、ウルリッヒの広範な定期刊行物ディレクトリ (63,013 ジャーナル) と比較しました。結果は、WoS または Scopus を研究評価に使用すると、社会科学、芸術、人文科学に不利益をもたらす、自然科学と工学、生物医学研究に有利なバイアスを導入する可能性があることを示しています。同様に、英語のジャーナルが過大評価され、他の言語に損害を与えています。どちらのデータベースもこれらのバイアスを共有していますが、その範囲は大きく異なります。そのため、書誌情報分析の結果は使用するデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈において、特に異なる分野、機関、国、または言語を比較する場合には、WoS と Scopus を慎重に使用する必要があることを示唆しています。書誌学コミュニティは、分野固有の引用インデックスや全国的な引用インデックスなど、WoS や Scopus ではカバーされていない科学的成果を含む手法や指標を開発する努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.29173/cais1943\\n Title: OpenAlex と Web of Science の間のドキュメント タイプの不一致を調査する Investigating Document Type Discrepancies between OpenAlex and the Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された OpenAlex は、そのオープン性と広範なカバレッジで際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この研究は、ドキュメント タイプの精度に焦点を当て、OpenAlex と WoS における作品のメタデータの品質を評価することを目的としています。 OpenAlex と WoS の両方でインデックス付けされている出版物の 4% 以上が研究論文またはレビューとして誤って分類されているようであり、これらのエラーの大部分 (約 97%) が OpenAlex で発生していることが観察されています。この研究は、文書タイプの不一致や誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, launched in 2022, stands out for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinder its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess the metadata quality of works in OpenAlex and WoS, focusing on document type accuracy. We observe that over 4% of the publications indexed in both OpenAlex and WoS appear to be misclassified as research articles or reviews, and that the vast majority (about 97%) of these errors occur in OpenAlex. By addressing discrepancies and misattributions in document types this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.1162/qss_a_00211\\n Title: 国際的な研究協力を測定するための書誌データソースの品質を評価する Assessing the quality of bibliographic data sources for measuring international research collaboration\\nAbstract: 国際研究協力 (IRC) の測定は、さまざまな研究評価タスクに不可欠ですが、どのデータソースを使用するかなど、さまざまな測定決定の影響については十分に研究されていません。データ ソースの選択が IRC 測定に及ぼす影響をより深く理解するために、利用可能なディメンションを確認して選択し、適切な計算可能なメトリクスを設計することにより、書誌データに特化したデータ品質評価フレームワークを設計および実装し、次にそれを書誌データの 4 つの一般的なソース (Microsoft Academic Graph、Web of Science (WoS)、Dimensions、ACM Digital Library) に適用してフレームワークを検証します。このフレームワークの検証が成功した場合、それが Wang と Strong (1996) によって提案された情報品質の一般的な概念フレームワークと一致しており、調査された情報源の品質の違いが適切に特定されることが示唆されます。フレームワークを適用すると、検討したセットの中で WS が全体的な品質が最も高いことがわかります。そして、品質の違いは主にデータ ソースの編成方法によって説明できると考えられます。私たちの研究は、研究者がこの IRC 測定ツールを研究に適用できるようにする方法論的な貢献で構成されており、書誌データの 4 つの一般的な情報源とその IRC 測定への影響をさらに特徴付けることによって実証的な貢献をしています。 Measuring international research collaboration (IRC) is essential to various research assessment tasks but the effect of various measurement decisions, including which data sources to use, has not been thoroughly studied. To better understand the effect of data source choice on IRC measurement, we design and implement a data quality assessment framework specifically for bibliographic data by reviewing and selecting available dimensions and designing appropriate computable metrics, and then validate the framework by applying it to four popular sources of bibliographic data: Microsoft Academic Graph, Web of Science (WoS), Dimensions, and the ACM Digital Library. Successful validation of the framework suggests it is consistent with the popular conceptual framework of information quality proposed by Wang and Strong (1996) and adequately identifies the differences in quality in the sources examined. Application of the framework reveals that WoS has the highest overall quality among the sets considered; and that the differences in quality can be explained primarily by how the data sources are organized. Our study comprises a methodological contribution that enables researchers to apply this IRC measurement tool in their studies and makes an empirical contribution by further characterizing four popular sources of bibliographic data and their impact on IRC measurement.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1007/s11192-015-1765-5', '10.48550/arXiv.2508.18620', '10.29173/cais1943', '10.1162/qss_a_00112', '10.1162/qss_a_00211']\n",
      "\u001b[93mSummary: I'm sorry, my lady, I could not find any information about the strengths and weaknesses of Web of Science (WoS) as a bibliometric database.\n",
      "\n",
      "DOI: 10.1007/s11192-015-1765-5 discusses the journal coverage of Web of Science and Scopus, and how the use of either for research evaluation may introduce biases that favour Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 and 10.29173/cais1943 both discuss the discrepancies between OpenAlex and Web of Science, and how OpenAlex may reduce or eliminate barriers to accessing bibliometric data, but one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00112 presents a large-scale comparison of five multidisciplinary bibliographic data sources, including Web of Science, and discusses the strengths and weaknesses of the different data sources.\n",
      "\n",
      "DOI: 10.1162/qss_a_00211 discusses the quality of bibliographic data sources for measuring international research collaboration, and how WoS has the highest overall quality among the sets considered.\n",
      "96\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.400\n",
      "Recall: 0.400\n",
      "F1-Score: 0.400\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.6334335, '10.1007/s11192-015-1765-5'), (0.56431514, '10.48550/arXiv.2508.18620'), (0.54274267, '10.29173/cais1943'), (0.45914948, '10.1162/qss_a_00112'), (0.2705689, '10.1162/qss_a_00211')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.17821/srels/2024/v61i5/171583\\n Title: 図書館における質問回答ベースの検索システムの設計: オープンソースの検索拡張生成 (RAG) パイプラインのアプリケーション Designing Question-Answer Based Search System in Libraries: Application of Open Source Retrieval Augmented Generation (RAG) Pipeline\\nAbstract: この研究の主な目的は、プロトタイプを準備し、図書館が検索拡張生成 (RAG) フレームワークを通じてオープンソース ソフトウェア ツールと大規模言語モデル (LLM) を使用して低コストの会話型検索システムを開発できることを実証することです。 LLM は幻覚を起こし、時代遅れで文脈を理解していない応答を返すことがよくあります。ただし、この実験は、LLM が一連の関連文書で強化された場合、文脈に応じた適切な応答を提供できることを示しています。回答を生成する前に関連ドキュメントで LLM を拡張することは、検索拡張生成として知られています。この方法論には、LangChain などのツール、ChromaDB などのベクトル データベース、Llama3 (700 億のパラメーター ベースのモデル) などのオープンソース LLM を使用して RAG パイプラインを作成することが含まれていました。開発されたプロトタイプには、収集、処理され、パイプラインに取り込まれたチャンドラヤーン 3 ミッションに関する 250 以上の関連文書のデータセットが含まれています。最後に、研究では標準的な LLM と RAG 拡張を備えた LLM からの応答を比較しました。主な調査結果から、標準的な LLM (RAG なし) は、チャンドラヤーン 3 に関連するクエリに対して自信を持って不正確で幻覚のような応答を生成するのに対し、RAG を使用する LLM は、応答を生成する前に関連文書のセットが提供されると、一貫して正確で有益な、文脈に沿った応答を提供することが明らかになりました。この調査では、オープンソースの RAG ベースのシステムは、情報検索を強化し、図書館を動的な情報サービスに変えるための費用対効果の高いソリューションを図書館に提供すると結論付けています。 This study primarily aims to prepare a prototype and demonstrate that libraries can develop a low-cost conversational search system using open-source software tools and Large Language Models (LLMs) through a Retrieval-Augmented Generation (RAG) framework. LLMs often hallucinate and provide outdated and non-contextualized responses. However, this experiment shows that LLMs can deliver contextualized, relevant responses when augmented with a set of relevant documents. Augmenting LLMs with relevant documents before generating answers is known as retrieval-augmented generation. The methodology involved creating a RAG pipeline using tools like LangChain, vector databases like ChromaDB, and open-source LLMs like Llama3 (a 70-billion parameter-based model). The prototype developed includes a dataset of 250+ relevant documents on the Chandrayaan-3 mission that was collected, processed, and ingested into the pipeline. Finally, the study compared responses from standard LLMs and LLMs with RAG augmentation. Key findings revealed that standard LLMs (without RAG) produced confidently incorrect, hallucinated responses against queries related to Chandrayaan-3, while LLMs with RAG consistently provided accurate, informative, and contextualized answers when supplied with a set of relevant documents before generating the response. The study concluded that open-source RAG-based systems offer a cost-effective solution for libraries to enhance information retrieval and transform libraries into dynamic information services.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.48550/arXiv.2505.13557\\n Title: AMAQA: RAG システム用のメタデータベースの QA データセット AMAQA: A Metadata-based QA Dataset for RAG Systems\\nAbstract: 検索拡張生成 (RAG) システムは、質問応答 (QA) タスクで広く使用されていますが、現在のベンチマークにはメタデータの統合が不足しており、テキスト データと外部情報の両方を必要とするシナリオでの評価が妨げられています。これに対処するために、テキストとメタデータを組み合わせたタスクを評価するように設計された新しいオープンアクセス QA データセットである AMAQA を紹介します。メタデータの統合は、サイバーセキュリティやインテリジェンスなど、関連情報へのタイムリーなアクセスが重要な、大量のデータの迅速な分析が必要な分野で特に重要です。 AMAQA には、26 のパブリック Telegram グループから収集された約 110 万件の英語メッセージが含まれており、タイムスタンプ、トピック、感情の調子、毒性指標などのメタデータが充実しており、特定の基準に基づいてドキュメントをフィルタリングすることで、正確で文脈に応じたクエリを実行できます。また、450 の高品質 QA ペアも含まれており、メタデータ主導の QA および RAG システムの研究を進めるための貴重なリソースとなります。私たちの知る限り、AMAQA は、メッセージで扱われるトピックなどのメタデータとラベルを組み込んだ最初のシングルホップ QA ベンチマークです。私たちはベンチマークで広範なテストを実施し、将来の研究のための新しい基準を確立します。メタデータを活用すると精度が 0.12 から 0.61 に向上し、構造化コンテキストの価値が強調されることがわかりました。これに基づいて、提供されたコンテキストを反復処理し、ノイズの多いドキュメントで強化することで LLM 入力を洗練するためのいくつかの戦略を検討し、最良のベースラインよりもさらに 3 ポイントの向上を達成し、単純なメタデータ フィルタリングよりも 14 ポイントの改善を達成しました。 Retrieval-augmented generation (RAG) systems are widely used in question-answering (QA) tasks, but current benchmarks lack metadata integration, hindering evaluation in scenarios requiring both textual data and external information. To address this, we present AMAQA, a new open-access QA dataset designed to evaluate tasks combining text and metadata. The integration of metadata is especially important in fields that require rapid analysis of large volumes of data, such as cybersecurity and intelligence, where timely access to relevant information is critical. AMAQA includes about 1.1 million English messages collected from 26 public Telegram groups, enriched with metadata such as timestamps, topics, emotional tones, and toxicity indicators, which enable precise and contextualized queries by filtering documents based on specific criteria. It also includes 450 high-quality QA pairs, making it a valuable resource for advancing research on metadata-driven QA and RAG systems. To the best of our knowledge, AMAQA is the first single-hop QA benchmark to incorporate metadata and labels such as topics covered in the messages. We conduct extensive tests on the benchmark, establishing a new standard for future research. We show that leveraging metadata boosts accuracy from 0.12 to 0.61, highlighting the value of structured context. Building on this, we explore several strategies to refine the LLM input by iterating over provided context and enriching it with noisy documents, achieving a further 3-point gain over the best baseline and a 14-point improvement over simple metadata filtering.\\n', \"DOI: 10.48550/arXiv.2505.18247\\n Title: MetaGen Blended RAG: 専門分野の質問応答でゼロショットの精度を解放 MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索拡張生成 (RAG) は、ファイアウォールの背後に隔離されていることが多く、事前トレーニング中に LLM が認識できない複雑で特殊な用語が豊富に含まれる、ドメイン固有のエンタープライズ データセットに苦戦します。医学、ネットワーキング、法律などの分野にわたるセマンティックのばらつきが RAG のコンテキストの精度を妨げる一方、ソリューションを微調整するのはコストがかかり、時間がかかり、新しいデータが出現したときの汎用性が欠けています。微調整を行わずにレトリーバーでゼロショット精度を達成することは依然として重要な課題です。私たちは、メタデータ生成パイプラインと密ベクトルと疎ベクトルを使用したハイブリッド クエリ インデックスを通じてセマンティック リトリーバーを強化する新しいエンタープライズ検索アプローチである「MetaGen Blended RAG」を紹介します。主要な概念、トピック、頭字語を活用することで、私たちのメソッドはメタデータを強化したセマンティック インデックスと強化されたハイブリッド クエリを作成し、微調整することなく堅牢でスケーラブルなパフォーマンスを実現します。生物医学の PubMedQA データセットでは、MetaGen Blended RAG は 82% の検索精度と 77% の RAG 精度を達成し、以前のすべてのゼロショット RAG ベンチマークを上回り、そのデータセット上の微調整モデルに匹敵するだけでなく、SQuAD や NQ などのデータセットでも優れています。このアプローチは、特殊なドメイン全体にわたって比類のない一般化を備えたセマンティック検索ツールを構築する新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.17821/srels/2024/v61i5/171583', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2505.13557', '10.48550/arXiv.2505.18247']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a technique that enhances question answering and information retrieval systems by combining the strengths of both retrieval and generation models.\n",
      "\n",
      "DOI: 10.17821/srels/2024/v61i5/171583 - This study demonstrates that RAG can improve the accuracy and contextual relevance of responses from Large Language Models (LLMs) by augmenting them with relevant documents before generating answers. The research shows that standard LLMs without RAG often produce incorrect and hallucinated responses, while RAG-augmented LLMs provide accurate, informative, and contextually appropriate answers.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - This paper introduces Multi-Meta-RAG, a method that improves RAG for multi-hop queries by using database filtering with LLM-extracted metadata. This approach enhances the selection of relevant documents from various sources, thereby improving the performance of RAG systems in complex question answering tasks.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - The study proposes a Metadata Retrieval-Augmented Generation (M-RAG) method that enhances open-domain question answering systems by utilizing embeddings with metadata and generative models. This method allows the model to understand the order and context of documents, leading to more accurate and sourced responses.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.13557 - This research introduces AMAQA, a metadata-based QA dataset designed to evaluate RAG systems in scenarios requiring both textual data and external information. The integration of metadata significantly boosts accuracy, highlighting the importance of structured context in RAG systems.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247 - This paper presents MetaGen Blended RAG, a novel approach that enhances semantic retrievers through metadata generation and hybrid query indexes. This method achieves high zero-shot precision in specialized domains, outperforming previous RAG benchmarks and fine-tuned models.\n",
      "\n",
      "My lady, RAG is used to improve question answering and information retrieval systems by augmenting LLMs with relevant documents, metadata, and advanced retrieval techniques, thereby enhancing accuracy, contextual relevance, and performance in complex tasks.\n",
      "96\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.81133276, '10.17821/srels/2024/v61i5/171583'), (0.72348577, '10.48550/arXiv.2406.13213'), (0.7211353, '10.6109/jkiice.2023.27.12.1489'), (0.72019166, '10.48550/arXiv.2505.13557'), (0.5614318, '10.48550/arXiv.2505.18247')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1045/september2016‐meschenmoser\\n Title: 科学 Web リポジトリのスクレイピング: 自動コンテンツ抽出の課題と解決策 Scraping Scientific Web Repositories: Challenges and Solutions for Automated Content Extraction\\nAbstract: 多くの科学 Web リポジトリは、科学出版物の可視性とアクセシビリティを向上させるだけでなく、h インデックスなどの指標を表示することによって、研究者の定量的および定性的な出版パフォーマンスも評価しています。これらの指標は、研究機関やその他の関係者にとって、採用や資金調達の決定など、影響力のある意思決定プロセスをサポートするために重要になっています。ただし、科学 Web リポジトリは通常、単純なパフォーマンス メトリクスと限られた分析オプションのみを提供します。さらに、パフォーマンス指標を計算するためのデータとアルゴリズムは通常公開されていません。したがって、システムがどの出版物を計算に含めるか、またシステムが結果をどのようにランク付けするかは透明性がなく、検証可能ではありません。多くの研究者は、これらのシステムの透明性を高めるために、基礎となるサイエントメトリーの生データにアクセスすることに興味を持っています。このペーパーでは、課題について説明し、科学 Web リポジトリ内のそのようなデータにプログラムでアクセスするための戦略を示します。 Google Scholar データに基づいて研究パフォーマンスを比較できるオープンソース ツール (MIT ライセンス) の一部として戦略を示します。ツールに含まれるスクレイパーは、リポジトリの運営者から同意が得られた場合にのみ使用する必要があることを強調したいと思います。私たちの経験では、研究目標が明確に説明され、プロジェクトが非営利的な性質のものであれば、同意が得られることがよくあります。 ide from improving the visibility and accessibility of scientific publications, many scientific Web repositories also assess researchers' quantitative and qualitative publication performance, e.g., by displaying metrics such as the h‐index. These metrics have become important for research institutions and other stakeholders to support impactful decision making processes such as hiring or funding decisions. However, scientific Web repositories typically offer only simple performance metrics and limited analysis options. Moreover, the data and algorithms to compute performance metrics are usually not published. Hence, it is not transparent or verifiable which publications the systems include in the computation and how the systems rank the results. Many researchers are interested in accessing the underlying scientometric raw data to increase the transparency of these systems. In this paper, we discuss the challenges and present strategies to programmatically access such data in scientific Web repositories. We demonstrate the strategies as part of an open source tool (MIT license) that allows research performance comparisons based on Google Scholar data. We would like to emphasize that the scraper included in the tool should only be used if consent was given by the operator of a repository. In our experience, consent is often given if the research goals are clearly explained and the project is of a non‐commercial nature.\\n\"]\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1045/september2016‐meschenmoser']\n",
      "\u001b[93mSummary: I'm sorry, my lady, I could not find any information about the challenges in normalizing citation metrics across scientific fields.\n",
      "\n",
      "DOI: 10.1045/september2016‐meschenmoser - This paper discusses the challenges and solutions for automated content extraction from scientific web repositories.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.000\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.495\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.14296049, '10.1045/september2016‐meschenmoser')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1007/s11192-022-04289-7\\n Title: 最も多く見つかる場所を検索: 56 の書誌データベースの懲戒範囲の比較 Search where you will find most: Comparing the disciplinary coverage of 56 bibliographic databases\\nAbstract: この論文では、新しいサイエントメトリクス手法を紹介し、それを学術界で人気のある英語に焦点を当てた書誌データベースの多くの主題範囲を推定するために適用します。この方法では、クエリ結果を共通の分母として使用して、さまざまな検索エンジン、リポジトリ、デジタル ライブラリ、その他の書誌データベースを比較します。この方法は、データベース カバレッジのより小さいセットを分析する既存のサンプリング ベースのアプローチを拡張します。この調査結果では、56 のデータベースの相対的および絶対的な対象範囲が示されており、これまで入手できなかった情報が示されています。データベースの絶対的な対象範囲を知ることで、特にルックアップ検索や探索的検索に関連する、高い再現率/感度が必要な検索に最も包括的なデータベースを選択できます。データベースの相対的な対象範囲を知ることで、特に体系的な検索に関連する、高い精度と特異性が必要な検索に特化したデータベースを選択できます。この調査結果は、Google Scholar、Scopus、または Web of Science の専門分野の範囲の違いだけでなく、分析頻度が低いデータベースの違いも示しています。たとえば、研究者は、Meta (廃止)、Embase、または Europe PMC が、医学やその他の健康分野の PubMed よりも多くの記録をカバーしていることが判明したことに驚くかもしれません。これらの発見は、研究者が新しく導入されたオプションに対しても頼りになるデータベースを再評価するよう促すはずです。より包括的なデータベースを使用して検索すると、特にシステマティック レビューやメタ分析など、最も適合するデータベースの選択に特別な考慮が必要な場合に、検索結果が向上します。この比較は、図書館員やその他の情報専門家が高価なデータベース調達戦略を再評価するのにも役立ちます。機関にアクセスできない研究者は、どのオープン データベースが自分の専門分野において最も包括的である可能性が高いかを学びます。 This paper introduces a novel scientometrics method and applies it to estimate the subject coverages of many of the popular English-focused bibliographic databases in academia. The method uses query results as a common denominator to compare a wide variety of search engines, repositories, digital libraries, and other bibliographic databases. The method extends existing sampling-based approaches that analyze smaller sets of database coverages. The findings show the relative and absolute subject coverages of 56 databases—information that has often not been available before. Knowing the databases’ absolute subject coverage allows the selection of the most comprehensive databases for searches requiring high recall/sensitivity, particularly relevant in lookup or exploratory searches. Knowing the databases’ relative subject coverage allows the selection of specialized databases for searches requiring high precision/specificity, particularly relevant in systematic searches. The findings illustrate not only differences in the disciplinary coverage of Google Scholar, Scopus, or Web of Science, but also of less frequently analyzed databases. For example, researchers might be surprised how Meta (discontinued), Embase, or Europe PMC are found to cover more records than PubMed in Medicine and other health subjects. These findings should encourage researchers to re-evaluate their go-to databases, also against newly introduced options. Searching with more comprehensive databases can improve finding, particularly when selecting the most fitting databases needs particular thought, such as in systematic reviews and meta-analyses. This comparison can also help librarians and other information experts re-evaluate expensive database procurement strategies. Researchers without institutional access learn which open databases are likely most comprehensive in their disciplines.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.48550/arXiv.2303.17661', '10.1007/s11192-022-04367-w', '10.1007/s11192-022-04289-7', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: Metadata quality is crucial for digital objects to be discovered through digital library interfaces.\n",
      "DOI: 10.48550/arXiv.2303.17661 - Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w - This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04289-7 - This paper introduces a novel scientometrics method and applies it to estimate the subject coverages of many of the popular English-focused bibliographic databases in academia. The method uses query results as a common denominator to compare a wide variety of search engines, repositories, digital libraries, and other bibliographic databases. The method extends existing sampling-based approaches that analyze smaller sets of database coverages. The findings show the relative and absolute subject coverages of 56 databases—information that has often not been available before. Knowing the databases’ absolute subject coverage allows the selection of the most comprehensive databases for searches requiring high recall/sensitivity, particularly relevant in lookup or exploratory searches. Knowing the databases’ relative subject coverage allows the selection of specialized databases for searches requiring high precision/specificity, particularly relevant in systematic searches. The findings illustrate not only differences in the disciplinary coverage of Google Scholar, Scopus, or Web of Science, but also of less frequently analyzed databases. For example, researchers might be surprised how Meta (discontinued), Embase, or Europe PMC are found to cover more records than PubMed in Medicine and other health subjects. These findings should encourage researchers to re-evaluate their go-to databases, also against newly introduced options. Searching with more comprehensive databases can improve finding, particularly when selecting the most fitting databases needs particular thought, such as in systematic reviews and meta-analyses. This comparison can also help librarians and other information experts re-evaluate expensive database procurement strategies. Researchers without institutional access learn which open databases are likely most comprehensive in their disciplines.\n",
      "\n",
      "DOI: 10.5281/ZENODO.6188748 - This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.250\n",
      "Recall: 0.200\n",
      "F1-Score: 0.222\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.3829032, '10.48550/arXiv.2303.17661'), (0.3650723, '10.1007/s11192-022-04367-w'), (0.14534597, '10.1007/s11192-022-04289-7'), (0.10071066, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', \"DOI: 10.48550/arXiv.2404.13948\\n Title: RAG の背中を打ち砕いた ypos: 低レベルの摂動を介して野生のドキュメントをシミュレートすることによる、RAG パイプラインへの遺伝的攻撃 ypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\\nAbstract: 最近の大規模言語モデル (LLM) の適用可能性がさまざまなドメインや現実世界のアプリケーションに拡大するにつれて、その堅牢性がますます重要になってきています。検索拡張生成 (RAG) は、LLM の制限に対処するための有望なソリューションですが、RAG の堅牢性に関する既存の研究では、RAG コンポーネント間の相互接続関係や、軽微なテキスト エラーなど、現実のデータベースに蔓延する潜在的な脅威が見落とされていることがよくあります。この研究では、RAG の堅牢性を評価する際にまだ解明されていない 2 つの側面を調査します。1 つは低レベルの摂動によるノイズの多いドキュメントに対する脆弱性、2 つは RAG の堅牢性の全体的な評価です。さらに、これらの側面をターゲットとした新しい攻撃方法である RAG への遺伝的攻撃を紹介します。具体的には、GARAG は、各コンポーネント内の脆弱性を明らかにし、ノイズの多いドキュメントに対してシステム全体の機能をテストするように設計されています。 \\\\textit{GARAG} を標準 QA データセットに適用し、さまざまな取得者と LLM を組み込むことで、RAG の堅牢性を検証します。実験結果は、GARAG が一貫して高い攻撃成功率を達成していることを示しています。また、各コンポーネントのパフォーマンスとその相乗効果を著しく損なうため、わずかなテキストの不正確さが現実世界の RAG システムを混乱させる大きなリスクを浮き彫りにします。 The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1 vulnerability to noisy documents through low-level perturbations and 2 a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG, which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.48550/arXiv.2406.13213', '10.1145/3637528.3671470', '10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2404.13948']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of Large Language Models (LLMs), such as hallucinations, outdated knowledge, and non-transparent reasoning processes. RAG enhances the accuracy and credibility of generated content by incorporating knowledge from external databases, enabling continuous knowledge updates and integration of domain-specific information.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997: This paper provides a comprehensive review of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG. It examines the tripartite foundation of RAG frameworks: retrieval, generation, and augmentation techniques. The study highlights state-of-the-art technologies in each component, offering insights into RAG system advancements, evaluation frameworks, and future research directions.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213: This research introduces Multi-Meta-RAG, a method to improve RAG for multi-hop queries using LLM-extracted metadata for database filtering. It addresses the performance limitations of traditional RAG applications in handling complex queries, demonstrating significant improvements on the MultiHop-RAG benchmark.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470: The study reviews Retrieval-Augmented Large Language Models (RA-LLMs), focusing on their ability to leverage external knowledge bases for high-quality content generation. It covers three technical perspectives, discusses current limitations, and outlines promising future research directions.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16: This paper introduces RAGAs, a reference-free evaluation framework for RAG pipelines. It assesses RAG systems' ability to retrieve relevant context, faithfully utilize it, and generate high-quality outputs. RAGAs aims to accelerate evaluation cycles, crucial for the rapid adoption of LLMs.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.13948: This research investigates the robustness of RAG, focusing on vulnerability to noisy documents and holistic system evaluation. It introduces GARAG, a genetic attack method, to test RAG components' resilience, revealing significant risks from minor textual inaccuracies.\n",
      "\n",
      "My lady, if you have any further questions or need more details, please let me know.\n",
      "96\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.400\n",
      "Recall: 0.400\n",
      "F1-Score: 0.400\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.5488434, '10.48550/arXiv.2312.10997'), (0.52685463, '10.48550/arXiv.2406.13213'), (0.4893821, '10.1145/3637528.3671470'), (0.4833807, '10.18653/v1/2024.eacl-demo.16'), (0.3458787, '10.48550/arXiv.2404.13948')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n', \"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1145/2964909', '10.1109/ADL.1998.670425', '10.1177/09610006241239080', '10.5860/crl.86.1.101', '10.48550/arXiv.2303.17661']\n",
      "\u001b[93mSummary: The key challenges in metadata quality assessment include inconsistent definitions and measurements of quality dimensions, tensions between sociocultural representations and standardized systems, and incompleteness, inconsistency, and inaccuracy in metadata values. Advancements include the development of automated quality assessment frameworks, application of AI methods for metadata enhancement, and methodological contributions to qualitative synthesis of metadata quality literature.\n",
      "\n",
      "DOI: 10.1145/2964909 - This study addresses the challenges in metadata quality by developing a generic metadata quality assessment framework for open data portals. It maps specific metadata from widely used portal software frameworks to a standardized schema and defines quality metrics for automated evaluation.\n",
      "\n",
      "DOI: 10.1109/ADL.1998.670425 - This research explores qualitative and quantitative techniques for assessing metadata records, highlighting the need for diverse criteria to evaluate different types of metadata.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - A scoping review identifies commonly measured dimensions in metadata quality assessment but notes a lack of consensus on definitions and measurements, emphasizing the need for further research.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 - This study investigates how metadata quality issues arise from sociocultural and resource constraints, identifying 32 unique issues categorized to understand their impact on individuals and communities.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661 - Introducing MetaEnhance, a framework that uses AI to improve metadata quality for electronic theses and dissertations, achieving high accuracy in error detection and correction.\n",
      "\n",
      "My lady, I hope this overview meets your requirements. If you have any further questions or need additional details, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in metadata quality assessment?']:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.000\n",
      "Accuracy: 0.896\n",
      "Balanced accuracy: 0.473\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.6636041, '10.1145/2964909'), (0.4874788, '10.1109/ADL.1998.670425'), (0.38741276, '10.1177/09610006241239080'), (0.37305376, '10.5860/crl.86.1.101'), (0.2738236, '10.48550/arXiv.2303.17661')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.1080/19386389.2011.570654\\n Title: メタデータ レコードの問題の分析 An Analysis of Problems in Metadata Records\\nAbstract: メタデータはデジタル ライブラリにおいて重要な役割を果たします。しかし、有用であるためには、メタデータ レコードに問題がなくなければなりません。メタデータに問題があると、リソースが正しく表現されず、ユーザーはメタデータのメリットを享受できなくなります。このような問題を排除しないにしても、最小限に抑えるには、メタデータ レコードで発生する可能性のある問題の種類を理解することが不可欠です。この論文では、文献で報告されているメタデータ レコードで見つかった問題を比較および分析します。メタデータの問題の 5 つのカテゴリを特定できることがわかります。これらは、不正な値、不正な要素、情報の欠落、情報損失、および一貫性のない値の表現の問題です。これらの問題がメタデータによって提供できるサービスに悪影響を与えることを考慮すると、メタデータの使用から得られる利点と、メタデータ レコードの作成に費やされるコストと労力のバランスが確保されるように、予防または是正措置を講じる必要があります。 Metadata plays an important role in digital libraries. But to be useful, metadata records must be problem free. When problems are present in the metadata, resources are not correctly represented and users are not able to reap the benefits of metadata. To minimize, if not eliminate, such problems, it is essential to understand the kinds of problems that can occur in metadata records. In this paper, problems found in metadata records as reported in the literature are compared and analyzed. It is found that five categories of metadata problems can be identified. These are the problems of Incorrect Values, Incorrect Elements, Missing Information, Information Loss, and Inconsistent Value Representation. Given that these problems are detrimental to the services that can be provided by metadata, preventive or corrective measures need to be put in place so as to ensure that the benefits derived from using metadata balance the costs and efforts spent in the creation of metadata records.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1007/s11192-023-04923-y\\n Title: OpenAlex に機関が存在しない: 考えられる理由、影響、および解決策 Missing institutions in OpenAlex: possible reasons, implications, and solutions\\nAbstract: オープン サイエンスの到来により、高いデータ品質を備えたオープン データ プラットフォームが必要になります。 2022 年 1 月に開始されたグローバル研究システムの完全にオープンなカタログである OpenAlex は、データへの簡単なアクセスと、量的科学研究で広く使用されている幅広いデータ範囲という 2 つの主な利点を備えています。注目すべきことに、OpenAlex はライデン大学ランキングの重要なデータ ソースとして採用されています。ただし、OpenAlex の雑誌記事メタデータには機関が欠落しているという深刻なデータ品質の問題があります。この研究では、完全な機関情報 (FII)、部分的に欠落している機関情報 (PMII)、および完全に欠落している機関情報 (CMII) という 3 つのタイプの機関情報を定義することにより、問題の考えられる理由とその結果と解決策を調査します。私たちの結果は、OpenAlex のジャーナル記事の 60% 以上で機関の欠落の問題が発生していることを示しています。この問題は、初期のメタデータや社会科学、人文科学で特に蔓延しています。データのサブサンプルを使用して、問題の考えられる理由、歪んだ結果をもたらす可能性のあるリスク、行方不明の機関の問題に対する考えられる解決策をさらに調査します。その目的は、オープン リソースにおけるデータ品質の向上の重要性を高め、それによって量的科学の研究やより広範な文脈においてオープン リソースの責任ある使用をサポートすることです。 The advent of open science calls for open data platforms with high data quality. As a fully open catalog of the global research system launched in January 2022, OpenAlex features two main advantages of easy data accessibility and broad data coverage, which has been widely used in quantitative science studies. Remarkably, OpenAlex is adopted as an important data source for Leiden university ranking. However, there is a severe data quality problem of missing institutions in journal article metadata in OpenAlex. This study investigates the possible reasons for the problem and its consequences and solutions by defining three types of institutional information—full institutional information (FII), partially missing institutional information (PMII) and completely missing institutional information (CMII). Our results show that the problem of missing institutions occurs in more than 60% of the journal articles in OpenAlex. The problem is particularly widespread in metadata from the early years and in the social sciences and humanities. Using sub-samples of the data, we further explore the possible reasons for the problem, the risk it might represent for distorted results, and possible solutions to the problem of missing institutions. The aim is to raise the importance of data quality improvements in open resources, and thus to support the responsible use of open resources in quantitative science studies and also in broader contexts.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.5860/crl.86.1.101', '10.48550/arXiv.2303.17661', '10.1080/19386389.2011.570654', '10.1177/09610006241239080', '10.1007/s11192-023-04923-y']\n",
      "\u001b[93mSummary: Metadata quality is crucial for digital object discovery and user experience, but it often faces challenges like incompleteness, inconsistency, and inaccuracy. Solutions include automated methods, standardization, and improved assessment tools.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661 - Metadata quality is vital for digital library interfaces, but issues like incomplete, inconsistent, and incorrect values are common. The study proposes MetaEnhance, an AI-based framework, to automatically detect and correct errors in electronic theses and dissertations (ETDs). Testing showed high accuracy in error detection and correction.\n",
      "\n",
      "DOI: 10.1080/19386389.2011.570654 - Metadata plays a critical role in digital libraries, but problems like incorrect values, missing information, and inconsistent representation hinder its effectiveness. The paper emphasizes the need for preventive and corrective measures to balance the benefits of metadata with the costs of creation.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - Metadata is essential for data retrieval, organization, and user experience. A scoping review identified completeness, accuracy, consistency, and accessibility as key dimensions in metadata quality assessment. However, there is no consensus on definitions and measurements, highlighting the need for further research and flexible assessment tools.\n",
      "\n",
      "DOI: 10.1007/s11192-023-04923-y - OpenAlex, an open data platform, faces a significant issue with missing institutions in journal article metadata. The study defines three types of institutional information and finds that over 60% of articles lack complete institutional data. Solutions are proposed to improve data quality and support responsible use in quantitative science.\n",
      "\n",
      "My lady, these documents highlight the importance of addressing metadata quality issues through innovative solutions and standardized practices.\n",
      "96\n",
      "For query: ['What are the key challenges and potential solutions for improving metadata quality?']:\n",
      "Precision: 0.800\n",
      "Recall: 0.800\n",
      "F1-Score: 0.800\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.895\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.5573882, '10.5860/crl.86.1.101'), (0.46701634, '10.48550/arXiv.2303.17661'), (0.43467787, '10.1080/19386389.2011.570654'), (0.40952834, '10.1177/09610006241239080'), (0.29672605, '10.1007/s11192-023-04923-y')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 14 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", \"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", \"DOI: 10.48550/arXiv.2505.18247\\n Title: MetaGen Blended RAG: 専門分野の質問応答でゼロショットの精度を解放 MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索拡張生成 (RAG) は、ファイアウォールの背後に隔離されていることが多く、事前トレーニング中に LLM が認識できない複雑で特殊な用語が豊富に含まれる、ドメイン固有のエンタープライズ データセットに苦戦します。医学、ネットワーキング、法律などの分野にわたるセマンティックのばらつきが RAG のコンテキストの精度を妨げる一方、ソリューションを微調整するのはコストがかかり、時間がかかり、新しいデータが出現したときの汎用性が欠けています。微調整を行わずにレトリーバーでゼロショット精度を達成することは依然として重要な課題です。私たちは、メタデータ生成パイプラインと密ベクトルと疎ベクトルを使用したハイブリッド クエリ インデックスを通じてセマンティック リトリーバーを強化する新しいエンタープライズ検索アプローチである「MetaGen Blended RAG」を紹介します。主要な概念、トピック、頭字語を活用することで、私たちのメソッドはメタデータを強化したセマンティック インデックスと強化されたハイブリッド クエリを作成し、微調整することなく堅牢でスケーラブルなパフォーマンスを実現します。生物医学の PubMedQA データセットでは、MetaGen Blended RAG は 82% の検索精度と 77% の RAG 精度を達成し、以前のすべてのゼロショット RAG ベンチマークを上回り、そのデータセット上の微調整モデルに匹敵するだけでなく、SQuAD や NQ などのデータセットでも優れています。このアプローチは、特殊なドメイン全体にわたって比類のない一般化を備えたセマンティック検索ツールを構築する新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\", 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1016/j.caeai.2025.100417', '10.48550/arXiv.2505.18247', '10.1007/s44427-025-00006-3', '10.1145/3637528.3671470']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) systems face challenges such as hallucination, outdated knowledge, non-transparent reasoning processes, computational costs, and evaluating system effectiveness. Advancements include improved factual accuracy, dynamic knowledge updates, and enhanced performance in specialized domains.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - This paper discusses the progression of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG, and highlights the state-of-the-art technologies in retrieval, generation, and augmentation techniques. It also introduces an up-to-date evaluation framework and benchmark.\n",
      "\n",
      "DOI: 10.1016/j.caeai.2025.100417 - This study focuses on RAG in educational applications, discussing challenges like mitigating hallucinations, ensuring knowledge completeness and timeliness, and reducing computational costs. It explores practical applications in interactive learning systems and educational content generation.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247 - Introducing MetaGen Blended RAG, this paper addresses challenges in domain-specific enterprise datasets and achieves high zero-shot precision without fine-tuning, demonstrating robust performance in specialized domains like biomedicine.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3 - This research evaluates open-source LLMs in RAG systems using the Ragas framework, focusing on retrieval effectiveness and answer quality in academic and structured knowledge tasks.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470 - This survey reviews Retrieval-Augmented Large Language Models (RA-LLMs), discussing their ability to provide reliable external knowledge and enhance AI-generated content quality. It also explores current limitations and future research directions.\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions or need additional details, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in Retrieval-Augmented Generation (RAG) systems?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9224337, '10.48550/arXiv.2312.10997'), (0.87681407, '10.1016/j.caeai.2025.100417'), (0.7272211, '10.48550/arXiv.2505.18247'), (0.6641271, '10.1007/s44427-025-00006-3'), (0.6102915, '10.1145/3637528.3671470')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 15 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', \"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.1177/09610006241239080', '10.1109/ADL.1998.670425', '10.5281/zenodo.14006424']\n",
      "\u001b[93mSummary: Three studies examined the abstract in metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - This study compared the amount of metadata and the completeness degree of research publications in new academic databases.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - This study identified the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment.\n",
      "\n",
      "DOI: 10.5281/zenodo.14006424 - This study analysed OpenAlex's coverage and metadata availability of African-based publications.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.250\n",
      "Recall: 0.200\n",
      "F1-Score: 0.222\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.63179934, '10.1162/qss_a_00286'), (0.58919704, '10.1177/09610006241239080'), (0.55420566, '10.1109/ADL.1998.670425'), (0.18688062, '10.5281/zenodo.14006424')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1108/JD-10-2022-0234\\n Title: すべての学術分野にわたる引用と参照の習慣の分析: 書誌の参照と引用の実践におけるアプローチと傾向 An analysis of citing and referencing habits across all scholarly disciplines: approaches and trends in bibliographic referencing and citing practices\\nAbstract: この研究で、著者らは、学術文献における引用および参照の誤りについて現在考えられる原因を特定し、スウィートランド氏が1989年の論文で提供したスナップショットから何かが変わったかどうかを比較したいと考えている。,著者らは、27の主題分野にわたる147のジャーナルに掲載された729件の論文から、参考要素、すなわち書誌的参照、言及、引用、およびそれぞれの本文中の参照ポインタを分析した。,分析の結果は、書誌的事項が指摘された。著者らの知る限り、この研究は、Sweetland (1989) 以来、文献における参照および引用の慣行における誤りを分析したものとしては、最近入手可能な最良のものである。 In this study, the authors want to identify current possible causes for citing and referencing errors in scholarly literature to compare if something changed from the snapshot provided by Sweetland in his 1989 paper.,The authors analysed reference elements, i.e. bibliographic references, mentions, quotations and respective in-text reference pointers, from 729 articles published in 147 journals across the 27 subject areas.,The outcomes of the analysis pointed out that bibliographic errors have been perpetuated for decades and that their possible causes have increased, despite the encouraged use of technological facilities, i.e. the reference managers.,As far as the authors know, the study is the best recent available analysis of errors in referencing and citing practices in the literature since Sweetland (1989).\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1108/JD-10-2022-0234', '10.1007/s11192-022-04367-w', '10.1162/qss_a_00112']\n",
      "\u001b[93mSummary: Three studies examined citations.\n",
      "\n",
      "DOI: 10.1108/JD-10-2022-0234 - This study examined errors in referencing and citing practices in scholarly literature.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w - This study examined invalid citations due to DOI errors in Crossref data.\n",
      "\n",
      "DOI: 10.1162/qss_a_00112 - This study examined differences in the completeness and accuracy of citation links in five multidisciplinary bibliographic data sources.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 1.000\n",
      "Recall: 0.600\n",
      "F1-Score: 0.750\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.800\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.3570971, '10.1108/JD-10-2022-0234'), (0.21608633, '10.1007/s11192-022-04367-w'), (0.13627496, '10.1162/qss_a_00112')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1007/s11192-022-04367-w', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: Crossref is a community-owned sustainable source of scholarly metadata that is available through a number of APIs.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.800\n",
      "Recall: 0.800\n",
      "F1-Score: 0.800\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.895\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.7781275, '10.1162/qss_a_00022'), (0.72395444, '10.1162/qss_a_00210'), (0.55623144, '10.31222/osf.io/smxe5'), (0.44883752, '10.1007/s11192-022-04367-w'), (0.17410146, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n', 'DOI: 10.1007/s11192-022-04289-7\\n Title: 最も多く見つかる場所を検索: 56 の書誌データベースの懲戒範囲の比較 Search where you will find most: Comparing the disciplinary coverage of 56 bibliographic databases\\nAbstract: この論文では、新しいサイエントメトリクス手法を紹介し、それを学術界で人気のある英語に焦点を当てた書誌データベースの多くの主題範囲を推定するために適用します。この方法では、クエリ結果を共通の分母として使用して、さまざまな検索エンジン、リポジトリ、デジタル ライブラリ、その他の書誌データベースを比較します。この方法は、データベース カバレッジのより小さいセットを分析する既存のサンプリング ベースのアプローチを拡張します。この調査結果では、56 のデータベースの相対的および絶対的な対象範囲が示されており、これまで入手できなかった情報が示されています。データベースの絶対的な対象範囲を知ることで、特にルックアップ検索や探索的検索に関連する、高い再現率/感度が必要な検索に最も包括的なデータベースを選択できます。データベースの相対的な対象範囲を知ることで、特に体系的な検索に関連する、高い精度と特異性が必要な検索に特化したデータベースを選択できます。この調査結果は、Google Scholar、Scopus、または Web of Science の専門分野の範囲の違いだけでなく、分析頻度が低いデータベースの違いも示しています。たとえば、研究者は、Meta (廃止)、Embase、または Europe PMC が、医学やその他の健康分野の PubMed よりも多くの記録をカバーしていることが判明したことに驚くかもしれません。これらの発見は、研究者が新しく導入されたオプションに対しても頼りになるデータベースを再評価するよう促すはずです。より包括的なデータベースを使用して検索すると、特にシステマティック レビューやメタ分析など、最も適合するデータベースの選択に特別な考慮が必要な場合に、検索結果が向上します。この比較は、図書館員やその他の情報専門家が高価なデータベース調達戦略を再評価するのにも役立ちます。機関にアクセスできない研究者は、どのオープン データベースが自分の専門分野において最も包括的である可能性が高いかを学びます。 This paper introduces a novel scientometrics method and applies it to estimate the subject coverages of many of the popular English-focused bibliographic databases in academia. The method uses query results as a common denominator to compare a wide variety of search engines, repositories, digital libraries, and other bibliographic databases. The method extends existing sampling-based approaches that analyze smaller sets of database coverages. The findings show the relative and absolute subject coverages of 56 databases—information that has often not been available before. Knowing the databases’ absolute subject coverage allows the selection of the most comprehensive databases for searches requiring high recall/sensitivity, particularly relevant in lookup or exploratory searches. Knowing the databases’ relative subject coverage allows the selection of specialized databases for searches requiring high precision/specificity, particularly relevant in systematic searches. The findings illustrate not only differences in the disciplinary coverage of Google Scholar, Scopus, or Web of Science, but also of less frequently analyzed databases. For example, researchers might be surprised how Meta (discontinued), Embase, or Europe PMC are found to cover more records than PubMed in Medicine and other health subjects. These findings should encourage researchers to re-evaluate their go-to databases, also against newly introduced options. Searching with more comprehensive databases can improve finding, particularly when selecting the most fitting databases needs particular thought, such as in systematic reviews and meta-analyses. This comparison can also help librarians and other information experts re-evaluate expensive database procurement strategies. Researchers without institutional access learn which open databases are likely most comprehensive in their disciplines.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.48550/arXiv.2508.18620', '10.1162/qss_a_00286', '10.5281/zenodo.14006424', '10.1007/s11192-022-04289-7']\n",
      "\u001b[93mSummary: Several papers evaluate the linguistic coverage or language-related metadata in scholarly databases.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 - Evaluates the linguistic coverage of OpenAlex by assessing the completeness and accuracy of its language-related metadata through a comparison with Web of Science (WoS) and manual validation of 6,836 articles.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 - Investigates discrepancies in language metadata between OpenAlex and WoS, aiming to enhance awareness of data quality issues in bibliometric research.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - Compares the completeness of publication metadata, including language, in eight free-access scholarly databases, highlighting differences between academic search engines and third-party databases.\n",
      "\n",
      "DOI: 10.5281/zenodo.14006424 - Analyzes the coverage and metadata availability of African publications in OpenAlex, comparing it with Scopus, WoS, and African Journals Online (AJOL), with a focus on language inclusivity.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04289-7 - Introduces a method to estimate the subject coverage of 56 bibliographic databases, including language-focused databases, providing insights into their relative and absolute subject coverages.\n",
      "\n",
      "My lady, these papers provide a comprehensive overview of the evaluation of linguistic coverage and language-related metadata in various scholarly databases.\n",
      "96\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.200\n",
      "Recall: 0.200\n",
      "F1-Score: 0.200\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9108078, '10.48550/arXiv.2409.10633'), (0.42706275, '10.48550/arXiv.2508.18620'), (0.41563448, '10.1162/qss_a_00286'), (0.19475074, '10.5281/zenodo.14006424'), (0.16595806, '10.1007/s11192-022-04289-7')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: COVID-19 研究への資金提供: オープン データ インフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 資金提供機関が提供する資金の成果を分析するには、資金提供によって得られた出版物を追跡できることが不可欠です。私たちは、Crossref での資金調達データのオープンな利用可能性を調査し、新型コロナウイルス感染症 (COVID-19) に関連する研究を報告する出版物の資金調達データに焦点を当てています。また、Scopus と Web of Science という 2 つの独自の書誌データベースで利用可能な資金調達データとの比較も示します。私たちの分析では、Crossref の資金調達データの範囲が限られていることが明らかになりました。また、特に Scopus における資金調達データの品質に関連する問題も示しています。 Crossref での資金調達データのオープンな可用性を向上させるための推奨事項を提供します。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.5281/zenodo.14006424', '10.31222/osf.io/smxe5', '10.1162/qss_a_00286', '10.1162/qss_a_00212']\n",
      "\u001b[93mSummary: Several papers address funding metadata, its availability, or its analysis in scholarly databases.\n",
      "\n",
      "DOI: 10.1162/qss_a_00210 - This paper investigates the availability and completeness of open funder metadata, specifically focusing on publications funded by the Dutch Research Council (NWO). It highlights the challenges in assessing the coverage of funder metadata due to the unknown number of articles resulting from funded research. The study finds that only 67% of articles funded by NWO contain funding information in Crossref, with variations in acknowledging NWO as the funder name or linking Funder IDs to NWO.\n",
      "\n",
      "DOI: 10.5281/zenodo.14006424 - This paper analyzes the coverage and metadata availability of African publications in OpenAlex, comparing it with Scopus, Web of Science (WoS), and African Journals Online (AJOL). The study finds that OpenAlex offers the most extensive publication coverage but performs worse regarding funder information. However, metadata availability in OpenAlex is better for publications also indexed in Scopus or WoS.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This paper presents an overview of the availability of six metadata elements in Crossref, including funding information. The analysis shows that the availability of these metadata elements has improved over time, but many publishers need to make additional efforts to realize full openness of bibliographic metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - This study compares the amount of metadata and completeness degree of research publications in eight free-access scholarly databases. The results show that third-party databases have more metadata quality and a higher completeness rate compared to academic search engines. However, the main problem of third-party databases is the loss of information derived from integrating different sources.\n",
      "\n",
      "DOI: 10.1162/qss_a_00212 - This paper focuses on the open availability of funding data in Crossref, specifically for publications related to COVID-19 research. It compares funding data in Crossref with two proprietary bibliometric databases, Scopus and Web of Science, and provides recommendations for improving the open availability of funding data in Crossref.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or would like more details on a specific paper, please let me know.\n",
      "96\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.800\n",
      "Recall: 1.000\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.990\n",
      "Balanced accuracy: 0.995\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.68798697, '10.1162/qss_a_00210'), (0.48616135, '10.5281/zenodo.14006424'), (0.4024631, '10.31222/osf.io/smxe5'), (0.31545174, '10.1162/qss_a_00286'), (0.30762884, '10.1162/qss_a_00212')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", \"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.48550/arXiv.2312.10997', '10.1145/3637528.3671470', '10.6109/jkiice.2023.27.12.1489', '10.1007/s44427-025-00006-3']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a technique that enhances the capabilities of Large Language Models (LLMs) by incorporating external knowledge sources.\n",
      "\n",
      "DOI: 10.1016/j.caeai.2025.100417 - This paper discusses the use of RAG in educational applications, specifically in interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - This paper provides a comprehensive review of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG, and their applications in enhancing the accuracy and credibility of LLMs, particularly for knowledge-intensive tasks.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470 - This paper discusses the use of RAG in Retrieval-Augmented Large Language Models (RA-LLMs) to improve the quality of generated content by utilizing external knowledge bases, with a focus on AI-Generated Content (AIGC) applications.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - This paper proposes a Metadata Retrieval-Augmented Generation (M-RAG) method for enhancing open-domain question answering systems, demonstrating significant performance improvements over conventional RAG methods.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3 - This paper evaluates the effectiveness of open-source LLMs in RAG systems using a benchmark dataset derived from diploma theses abstracts, providing insights into optimizing RAG implementations for academic and structured knowledge tasks.\n",
      "\n",
      "My lady, these papers discuss the use of RAG in various applications, including education, knowledge-intensive tasks, AI-generated content, open-domain question answering, and academic research.\n",
      "96\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) when used in applications?']:\n",
      "Precision: 0.200\n",
      "Recall: 0.200\n",
      "F1-Score: 0.200\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.8958969, '10.1016/j.caeai.2025.100417'), (0.81418616, '10.48550/arXiv.2312.10997'), (0.7173491, '10.1145/3637528.3671470'), (0.69672287, '10.6109/jkiice.2023.27.12.1489'), (0.6675167, '10.1007/s44427-025-00006-3')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1007/s11192-022-04367-w', '10.1162/qss_a_00112']\n",
      "\u001b[93mSummary: Crossref is a community-owned source of scholarly metadata that is available to publishers, authors, librarians, funders and researchers.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.95628047, '10.1162/qss_a_00022'), (0.47548413, '10.1162/qss_a_00210'), (0.44680908, '10.31222/osf.io/smxe5'), (0.21246108, '10.1007/s11192-022-04367-w'), (0.19260046, '10.1162/qss_a_00112')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.29173/cais1943\\n Title: OpenAlex と Web of Science の間のドキュメント タイプの不一致を調査する Investigating Document Type Discrepancies between OpenAlex and the Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された OpenAlex は、そのオープン性と広範なカバレッジで際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この研究は、ドキュメント タイプの精度に焦点を当て、OpenAlex と WoS における作品のメタデータの品質を評価することを目的としています。 OpenAlex と WoS の両方でインデックス付けされている出版物の 4% 以上が研究論文またはレビューとして誤って分類されているようであり、これらのエラーの大部分 (約 97%) が OpenAlex で発生していることが観察されています。この研究は、文書タイプの不一致や誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, launched in 2022, stands out for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinder its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess the metadata quality of works in OpenAlex and WoS, focusing on document type accuracy. We observe that over 4% of the publications indexed in both OpenAlex and WoS appear to be misclassified as research articles or reviews, and that the vast majority (about 97%) of these errors occur in OpenAlex. By addressing discrepancies and misattributions in document types this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', \"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.3145/epi.2023.mar.09\\n Title: Microsoft Academic Graph から OpenAlex に切り替える場合、書誌情報学に関連するメタデータはどれが同じで、どれが異なりますか? Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\\nAbstract: Microsoft Academic Graph (MAG) の廃止の発表に伴い、非営利団体 OurResearch は OpenAlex という名前で同様のリソースを提供すると発表しました。したがって、最新の MAG スナップショットと初期の OpenAlex スナップショットの書誌学的分析に関連するメタデータを比較します。 MAG の実質的にすべての著作物は、書誌データの出版年、巻数、最初と最後のページ、DOI、および引用分析の重要な要素である参考文献の数を保存しながら OpenAlex に転送されました。 MAG ドキュメントの 90% 以上が OpenAlex に同等のドキュメント タイプを持っています。残りのうち、特に OpenAlex 文書タイプの Journal-article および Book-chapter への再分類は正しいようで、その割合は 7% 以上に達しており、文書タイプの仕様は MAG から OpenAlex に大幅に改善されました。書誌学的関連メタデータの別の項目として、MAG と OpenAlex における紙ベースの主題分類を調べました。 OpenAlex では、MAG よりもはるかに多くの主題分類が割り当てられているドキュメントが見つかりました。第 1 レベルと第 2 レベルでは、分類構造はほぼ同じです。主題の再分類に関するデータを表とグラフの両方のレベルで表示します。分野で正規化された書誌学的評価に対する豊富な主題の再分類の影響の評価は、この論文の範囲には含まれません。この未解決の質問とは別に、OpenAlex は全体的に、文書タイプの割り当てがより広範囲にカバーされているため、2021 年より前の出版物については少なくとも MAG と同等かそれ以上に書誌学的分析に適しているようです。 With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2508.18620', '10.29173/cais1943', '10.48550/arXiv.2409.10633', '10.3145/epi.2023.mar.09']\n",
      "\u001b[93mSummary: OpenAlex is a bibliographic database that launched in 2022 and is distinguished by its openness and extensive coverage. It has the potential to reduce or eliminate barriers to accessing bibliometric data, but concerns about the quality of its metadata hinder its broader adoption for research and evaluation.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. However, metadata accuracy and completeness issues require additional research to fully understand and address its limitations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 - This study focuses on metadata quality in OpenAlex and Web of Science (WoS), specifically document type, publication year, language, and author count. It aims to address discrepancies and misattributions in metadata to enhance awareness of data quality issues.\n",
      "\n",
      "DOI: 10.29173/cais1943 - Over 4% of publications indexed in both OpenAlex and WoS are misclassified as research articles or reviews, with 97% of these errors occurring in OpenAlex. This study highlights the need for improved metadata quality in OpenAlex.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 - OpenAlex exhibits a more balanced linguistic coverage than WoS, but its language metadata is not always accurate. Critical use of OpenAlex can provide comprehensive analyses of languages in scholarly publishing, but further work is needed to ensure metadata quality.\n",
      "\n",
      "DOI: 10.3145/epi.2023.mar.09 - OpenAlex has improved document type specifications compared to Microsoft Academic Graph (MAG) and is at least as suited for bibliometric analyses as MAG for publication years before 2021. It also has more documents with subject classification assignments.\n",
      "\n",
      "My lady, I hope this summary of OpenAlex's key features and limitations as a bibliometric database is helpful. If you have any further questions or would like more information, please let me know.\n",
      "96\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.8455478, '10.48550/arXiv.2404.17663'), (0.7178241, '10.48550/arXiv.2508.18620'), (0.69024646, '10.29173/cais1943'), (0.64344186, '10.48550/arXiv.2409.10633'), (0.6353361, '10.3145/epi.2023.mar.09')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1007/s11192-015-1765-5\\n Title: Web of Science と Scopus のジャーナルの報道: 比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌学的手法は、研究の評価など、さまざまな目的で複数の分野で使用されています。ほとんどの書誌学的分析には、トムソン・ロイターの Web of Science (WoS) とエルゼビアの Scopus というデータ ソースが共通しています。この調査の目的は、これら 2 つのデータベースのジャーナルの対象範囲を説明し、特定の分野、出版国、言語が過小評価されているかどうかを評価することです。これを行うために、WoS (13,605 ジャーナル) および Scopus (20,346 ジャーナル) のアクティブな学術ジャーナルの範囲を、ウルリッヒの広範な定期刊行物ディレクトリ (63,013 ジャーナル) と比較しました。結果は、WoS または Scopus を研究評価に使用すると、社会科学、芸術、人文科学に不利益をもたらす、自然科学と工学、生物医学研究に有利なバイアスを導入する可能性があることを示しています。同様に、英語のジャーナルが過大評価され、他の言語に損害を与えています。どちらのデータベースもこれらのバイアスを共有していますが、その範囲は大きく異なります。そのため、書誌情報分析の結果は使用するデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈において、特に異なる分野、機関、国、または言語を比較する場合には、WoS と Scopus を慎重に使用する必要があることを示唆しています。書誌学コミュニティは、分野固有の引用インデックスや全国的な引用インデックスなど、WoS や Scopus ではカバーされていない科学的成果を含む手法や指標を開発する努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.29173/cais1943\\n Title: OpenAlex と Web of Science の間のドキュメント タイプの不一致を調査する Investigating Document Type Discrepancies between OpenAlex and the Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された OpenAlex は、そのオープン性と広範なカバレッジで際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この研究は、ドキュメント タイプの精度に焦点を当て、OpenAlex と WoS における作品のメタデータの品質を評価することを目的としています。 OpenAlex と WoS の両方でインデックス付けされている出版物の 4% 以上が研究論文またはレビューとして誤って分類されているようであり、これらのエラーの大部分 (約 97%) が OpenAlex で発生していることが観察されています。この研究は、文書タイプの不一致や誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, launched in 2022, stands out for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinder its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess the metadata quality of works in OpenAlex and WoS, focusing on document type accuracy. We observe that over 4% of the publications indexed in both OpenAlex and WoS appear to be misclassified as research articles or reviews, and that the vast majority (about 97%) of these errors occur in OpenAlex. By addressing discrepancies and misattributions in document types this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.1162/qss_a_00211\\n Title: 国際的な研究協力を測定するための書誌データソースの品質を評価する Assessing the quality of bibliographic data sources for measuring international research collaboration\\nAbstract: 国際研究協力 (IRC) の測定は、さまざまな研究評価タスクに不可欠ですが、どのデータソースを使用するかなど、さまざまな測定決定の影響については十分に研究されていません。データ ソースの選択が IRC 測定に及ぼす影響をより深く理解するために、利用可能なディメンションを確認して選択し、適切な計算可能なメトリクスを設計することにより、書誌データに特化したデータ品質評価フレームワークを設計および実装し、次にそれを書誌データの 4 つの一般的なソース (Microsoft Academic Graph、Web of Science (WoS)、Dimensions、ACM Digital Library) に適用してフレームワークを検証します。このフレームワークの検証が成功した場合、それが Wang と Strong (1996) によって提案された情報品質の一般的な概念フレームワークと一致しており、調査された情報源の品質の違いが適切に特定されることが示唆されます。フレームワークを適用すると、検討したセットの中で WS が全体的な品質が最も高いことがわかります。そして、品質の違いは主にデータ ソースの編成方法によって説明できると考えられます。私たちの研究は、研究者がこの IRC 測定ツールを研究に適用できるようにする方法論的な貢献で構成されており、書誌データの 4 つの一般的な情報源とその IRC 測定への影響をさらに特徴付けることによって実証的な貢献をしています。 Measuring international research collaboration (IRC) is essential to various research assessment tasks but the effect of various measurement decisions, including which data sources to use, has not been thoroughly studied. To better understand the effect of data source choice on IRC measurement, we design and implement a data quality assessment framework specifically for bibliographic data by reviewing and selecting available dimensions and designing appropriate computable metrics, and then validate the framework by applying it to four popular sources of bibliographic data: Microsoft Academic Graph, Web of Science (WoS), Dimensions, and the ACM Digital Library. Successful validation of the framework suggests it is consistent with the popular conceptual framework of information quality proposed by Wang and Strong (1996) and adequately identifies the differences in quality in the sources examined. Application of the framework reveals that WoS has the highest overall quality among the sets considered; and that the differences in quality can be explained primarily by how the data sources are organized. Our study comprises a methodological contribution that enables researchers to apply this IRC measurement tool in their studies and makes an empirical contribution by further characterizing four popular sources of bibliographic data and their impact on IRC measurement.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1007/s11192-015-1765-5', '10.48550/arXiv.2508.18620', '10.29173/cais1943', '10.1162/qss_a_00112', '10.1162/qss_a_00211']\n",
      "\u001b[93mSummary: Web of Science (WoS) is a bibliometric database that has been used for over 30 years and is considered to have the highest overall quality when compared to other bibliometric databases. However, it has been found to favour Natural Sciences and Engineering as well as Biomedical Research over Social Sciences, Arts and Humanities.\n",
      "\n",
      "DOI: 10.1007/s11192-015-1765-5\n",
      "\n",
      "My lady, I have found information on the strengths and weaknesses of WoS as a bibliometric database.\n",
      "\n",
      "WoS has been used for over 30 years and is considered to have the highest overall quality when compared to other bibliometric databases. However, it has been found to favour Natural Sciences and Engineering as well as Biomedical Research over Social Sciences, Arts and Humanities.\n",
      "\n",
      "Is there anything else I can help you with?\n",
      "96\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.400\n",
      "Recall: 0.400\n",
      "F1-Score: 0.400\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.6334335, '10.1007/s11192-015-1765-5'), (0.56431514, '10.48550/arXiv.2508.18620'), (0.54274267, '10.29173/cais1943'), (0.45914948, '10.1162/qss_a_00112'), (0.2705689, '10.1162/qss_a_00211')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.17821/srels/2024/v61i5/171583\\n Title: 図書館における質問回答ベースの検索システムの設計: オープンソースの検索拡張生成 (RAG) パイプラインのアプリケーション Designing Question-Answer Based Search System in Libraries: Application of Open Source Retrieval Augmented Generation (RAG) Pipeline\\nAbstract: この研究の主な目的は、プロトタイプを準備し、図書館が検索拡張生成 (RAG) フレームワークを通じてオープンソース ソフトウェア ツールと大規模言語モデル (LLM) を使用して低コストの会話型検索システムを開発できることを実証することです。 LLM は幻覚を起こし、時代遅れで文脈を理解していない応答を返すことがよくあります。ただし、この実験は、LLM が一連の関連文書で強化された場合、文脈に応じた適切な応答を提供できることを示しています。回答を生成する前に関連ドキュメントで LLM を拡張することは、検索拡張生成として知られています。この方法論には、LangChain などのツール、ChromaDB などのベクトル データベース、Llama3 (700 億のパラメーター ベースのモデル) などのオープンソース LLM を使用して RAG パイプラインを作成することが含まれていました。開発されたプロトタイプには、収集、処理され、パイプラインに取り込まれたチャンドラヤーン 3 ミッションに関する 250 以上の関連文書のデータセットが含まれています。最後に、研究では標準的な LLM と RAG 拡張を備えた LLM からの応答を比較しました。主な調査結果から、標準的な LLM (RAG なし) は、チャンドラヤーン 3 に関連するクエリに対して自信を持って不正確で幻覚のような応答を生成するのに対し、RAG を使用する LLM は、応答を生成する前に関連文書のセットが提供されると、一貫して正確で有益な、文脈に沿った応答を提供することが明らかになりました。この調査では、オープンソースの RAG ベースのシステムは、情報検索を強化し、図書館を動的な情報サービスに変えるための費用対効果の高いソリューションを図書館に提供すると結論付けています。 This study primarily aims to prepare a prototype and demonstrate that libraries can develop a low-cost conversational search system using open-source software tools and Large Language Models (LLMs) through a Retrieval-Augmented Generation (RAG) framework. LLMs often hallucinate and provide outdated and non-contextualized responses. However, this experiment shows that LLMs can deliver contextualized, relevant responses when augmented with a set of relevant documents. Augmenting LLMs with relevant documents before generating answers is known as retrieval-augmented generation. The methodology involved creating a RAG pipeline using tools like LangChain, vector databases like ChromaDB, and open-source LLMs like Llama3 (a 70-billion parameter-based model). The prototype developed includes a dataset of 250+ relevant documents on the Chandrayaan-3 mission that was collected, processed, and ingested into the pipeline. Finally, the study compared responses from standard LLMs and LLMs with RAG augmentation. Key findings revealed that standard LLMs (without RAG) produced confidently incorrect, hallucinated responses against queries related to Chandrayaan-3, while LLMs with RAG consistently provided accurate, informative, and contextualized answers when supplied with a set of relevant documents before generating the response. The study concluded that open-source RAG-based systems offer a cost-effective solution for libraries to enhance information retrieval and transform libraries into dynamic information services.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.48550/arXiv.2505.13557\\n Title: AMAQA: RAG システム用のメタデータベースの QA データセット AMAQA: A Metadata-based QA Dataset for RAG Systems\\nAbstract: 検索拡張生成 (RAG) システムは、質問応答 (QA) タスクで広く使用されていますが、現在のベンチマークにはメタデータの統合が不足しており、テキスト データと外部情報の両方を必要とするシナリオでの評価が妨げられています。これに対処するために、テキストとメタデータを組み合わせたタスクを評価するように設計された新しいオープンアクセス QA データセットである AMAQA を紹介します。メタデータの統合は、サイバーセキュリティやインテリジェンスなど、関連情報へのタイムリーなアクセスが重要な、大量のデータの迅速な分析が必要な分野で特に重要です。 AMAQA には、26 のパブリック Telegram グループから収集された約 110 万件の英語メッセージが含まれており、タイムスタンプ、トピック、感情の調子、毒性指標などのメタデータが充実しており、特定の基準に基づいてドキュメントをフィルタリングすることで、正確で文脈に応じたクエリを実行できます。また、450 の高品質 QA ペアも含まれており、メタデータ主導の QA および RAG システムの研究を進めるための貴重なリソースとなります。私たちの知る限り、AMAQA は、メッセージで扱われるトピックなどのメタデータとラベルを組み込んだ最初のシングルホップ QA ベンチマークです。私たちはベンチマークで広範なテストを実施し、将来の研究のための新しい基準を確立します。メタデータを活用すると精度が 0.12 から 0.61 に向上し、構造化コンテキストの価値が強調されることがわかりました。これに基づいて、提供されたコンテキストを反復処理し、ノイズの多いドキュメントで強化することで LLM 入力を洗練するためのいくつかの戦略を検討し、最良のベースラインよりもさらに 3 ポイントの向上を達成し、単純なメタデータ フィルタリングよりも 14 ポイントの改善を達成しました。 Retrieval-augmented generation (RAG) systems are widely used in question-answering (QA) tasks, but current benchmarks lack metadata integration, hindering evaluation in scenarios requiring both textual data and external information. To address this, we present AMAQA, a new open-access QA dataset designed to evaluate tasks combining text and metadata. The integration of metadata is especially important in fields that require rapid analysis of large volumes of data, such as cybersecurity and intelligence, where timely access to relevant information is critical. AMAQA includes about 1.1 million English messages collected from 26 public Telegram groups, enriched with metadata such as timestamps, topics, emotional tones, and toxicity indicators, which enable precise and contextualized queries by filtering documents based on specific criteria. It also includes 450 high-quality QA pairs, making it a valuable resource for advancing research on metadata-driven QA and RAG systems. To the best of our knowledge, AMAQA is the first single-hop QA benchmark to incorporate metadata and labels such as topics covered in the messages. We conduct extensive tests on the benchmark, establishing a new standard for future research. We show that leveraging metadata boosts accuracy from 0.12 to 0.61, highlighting the value of structured context. Building on this, we explore several strategies to refine the LLM input by iterating over provided context and enriching it with noisy documents, achieving a further 3-point gain over the best baseline and a 14-point improvement over simple metadata filtering.\\n', \"DOI: 10.48550/arXiv.2505.18247\\n Title: MetaGen Blended RAG: 専門分野の質問応答でゼロショットの精度を解放 MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索拡張生成 (RAG) は、ファイアウォールの背後に隔離されていることが多く、事前トレーニング中に LLM が認識できない複雑で特殊な用語が豊富に含まれる、ドメイン固有のエンタープライズ データセットに苦戦します。医学、ネットワーキング、法律などの分野にわたるセマンティックのばらつきが RAG のコンテキストの精度を妨げる一方、ソリューションを微調整するのはコストがかかり、時間がかかり、新しいデータが出現したときの汎用性が欠けています。微調整を行わずにレトリーバーでゼロショット精度を達成することは依然として重要な課題です。私たちは、メタデータ生成パイプラインと密ベクトルと疎ベクトルを使用したハイブリッド クエリ インデックスを通じてセマンティック リトリーバーを強化する新しいエンタープライズ検索アプローチである「MetaGen Blended RAG」を紹介します。主要な概念、トピック、頭字語を活用することで、私たちのメソッドはメタデータを強化したセマンティック インデックスと強化されたハイブリッド クエリを作成し、微調整することなく堅牢でスケーラブルなパフォーマンスを実現します。生物医学の PubMedQA データセットでは、MetaGen Blended RAG は 82% の検索精度と 77% の RAG 精度を達成し、以前のすべてのゼロショット RAG ベンチマークを上回り、そのデータセット上の微調整モデルに匹敵するだけでなく、SQuAD や NQ などのデータセットでも優れています。このアプローチは、特殊なドメイン全体にわたって比類のない一般化を備えたセマンティック検索ツールを構築する新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.17821/srels/2024/v61i5/171583', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2505.13557', '10.48550/arXiv.2505.18247']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a technique that enhances question answering and information retrieval systems by combining the strengths of both retrieval and generation models. It improves the accuracy, relevance, and contextualization of responses by leveraging external knowledge sources and metadata.\n",
      "\n",
      "DOI: 10.17821/srels/2024/v61i5/171583 - This study demonstrates that RAG can significantly improve the performance of Large Language Models (LLMs) in question answering tasks by providing relevant documents before generating responses. The results show that RAG-augmented LLMs produce more accurate, informative, and contextualized answers compared to standard LLMs without RAG.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - This paper introduces Multi-Meta-RAG, a novel method that enhances RAG for multi-hop queries by using database filtering with LLM-extracted metadata. This approach improves the selection of relevant documents from various sources, leading to better performance on the MultiHop-RAG benchmark.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - The proposed Metadata Retrieval-Augmented Generation (M-RAG) method in this paper enhances open-domain question answering systems by utilizing embeddings with metadata and generative models. The results show a significant performance improvement compared to conventional RAG methods and external inference ODQA systems.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.13557 - This study presents AMAQA, a new open-access QA dataset designed to evaluate tasks combining text and metadata. The integration of metadata in RAG systems is shown to boost accuracy and highlight the value of structured context, particularly in fields requiring rapid analysis of large volumes of data.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247 - The introduction of MetaGen Blended RAG in this paper demonstrates a novel approach to enhancing semantic retrievers for specialized domain question answering. By leveraging metadata and hybrid query indexes, this method achieves robust, scalable performance without fine-tuning, surpassing prior zero-shot RAG benchmarks.\n",
      "\n",
      "My lady, I hope this summary provides a comprehensive overview of how RAG is used to improve question answering and information retrieval systems. If you have any further questions or would like more details on a specific DOI, please don't hesitate to ask.\n",
      "96\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.81133276, '10.17821/srels/2024/v61i5/171583'), (0.72348577, '10.48550/arXiv.2406.13213'), (0.7211353, '10.6109/jkiice.2023.27.12.1489'), (0.72019166, '10.48550/arXiv.2505.13557'), (0.5614318, '10.48550/arXiv.2505.18247')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1045/september2016‐meschenmoser\\n Title: 科学 Web リポジトリのスクレイピング: 自動コンテンツ抽出の課題と解決策 Scraping Scientific Web Repositories: Challenges and Solutions for Automated Content Extraction\\nAbstract: 多くの科学 Web リポジトリは、科学出版物の可視性とアクセシビリティを向上させるだけでなく、h インデックスなどの指標を表示することによって、研究者の定量的および定性的な出版パフォーマンスも評価しています。これらの指標は、研究機関やその他の関係者にとって、採用や資金調達の決定など、影響力のある意思決定プロセスをサポートするために重要になっています。ただし、科学 Web リポジトリは通常、単純なパフォーマンス メトリクスと限られた分析オプションのみを提供します。さらに、パフォーマンス指標を計算するためのデータとアルゴリズムは通常公開されていません。したがって、システムがどの出版物を計算に含めるか、またシステムが結果をどのようにランク付けするかは透明性がなく、検証可能ではありません。多くの研究者は、これらのシステムの透明性を高めるために、基礎となるサイエントメトリーの生データにアクセスすることに興味を持っています。このペーパーでは、課題について説明し、科学 Web リポジトリ内のそのようなデータにプログラムでアクセスするための戦略を示します。 Google Scholar データに基づいて研究パフォーマンスを比較できるオープンソース ツール (MIT ライセンス) の一部として戦略を示します。ツールに含まれるスクレイパーは、リポジトリの運営者から同意が得られた場合にのみ使用する必要があることを強調したいと思います。私たちの経験では、研究目標が明確に説明され、プロジェクトが非営利的な性質のものであれば、同意が得られることがよくあります。 ide from improving the visibility and accessibility of scientific publications, many scientific Web repositories also assess researchers' quantitative and qualitative publication performance, e.g., by displaying metrics such as the h‐index. These metrics have become important for research institutions and other stakeholders to support impactful decision making processes such as hiring or funding decisions. However, scientific Web repositories typically offer only simple performance metrics and limited analysis options. Moreover, the data and algorithms to compute performance metrics are usually not published. Hence, it is not transparent or verifiable which publications the systems include in the computation and how the systems rank the results. Many researchers are interested in accessing the underlying scientometric raw data to increase the transparency of these systems. In this paper, we discuss the challenges and present strategies to programmatically access such data in scientific Web repositories. We demonstrate the strategies as part of an open source tool (MIT license) that allows research performance comparisons based on Google Scholar data. We would like to emphasize that the scraper included in the tool should only be used if consent was given by the operator of a repository. In our experience, consent is often given if the research goals are clearly explained and the project is of a non‐commercial nature.\\n\"]\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1045/september2016‐meschenmoser']\n",
      "\u001b[93mSummary: I'm sorry, my lady, I could not find any information about the main challenges in normalizing citation metrics across scientific fields.\n",
      "\n",
      "DOI: 10.1045/september2016‐meschenmoser discusses scientific web repositories and the challenges and solutions for automated content extraction.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.000\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.495\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.14296049, '10.1045/september2016‐meschenmoser')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1007/s11192-022-04289-7\\n Title: 最も多く見つかる場所を検索: 56 の書誌データベースの懲戒範囲の比較 Search where you will find most: Comparing the disciplinary coverage of 56 bibliographic databases\\nAbstract: この論文では、新しいサイエントメトリクス手法を紹介し、それを学術界で人気のある英語に焦点を当てた書誌データベースの多くの主題範囲を推定するために適用します。この方法では、クエリ結果を共通の分母として使用して、さまざまな検索エンジン、リポジトリ、デジタル ライブラリ、その他の書誌データベースを比較します。この方法は、データベース カバレッジのより小さいセットを分析する既存のサンプリング ベースのアプローチを拡張します。この調査結果では、56 のデータベースの相対的および絶対的な対象範囲が示されており、これまで入手できなかった情報が示されています。データベースの絶対的な対象範囲を知ることで、特にルックアップ検索や探索的検索に関連する、高い再現率/感度が必要な検索に最も包括的なデータベースを選択できます。データベースの相対的な対象範囲を知ることで、特に体系的な検索に関連する、高い精度と特異性が必要な検索に特化したデータベースを選択できます。この調査結果は、Google Scholar、Scopus、または Web of Science の専門分野の範囲の違いだけでなく、分析頻度が低いデータベースの違いも示しています。たとえば、研究者は、Meta (廃止)、Embase、または Europe PMC が、医学やその他の健康分野の PubMed よりも多くの記録をカバーしていることが判明したことに驚くかもしれません。これらの発見は、研究者が新しく導入されたオプションに対しても頼りになるデータベースを再評価するよう促すはずです。より包括的なデータベースを使用して検索すると、特にシステマティック レビューやメタ分析など、最も適合するデータベースの選択に特別な考慮が必要な場合に、検索結果が向上します。この比較は、図書館員やその他の情報専門家が高価なデータベース調達戦略を再評価するのにも役立ちます。機関にアクセスできない研究者は、どのオープン データベースが自分の専門分野において最も包括的である可能性が高いかを学びます。 This paper introduces a novel scientometrics method and applies it to estimate the subject coverages of many of the popular English-focused bibliographic databases in academia. The method uses query results as a common denominator to compare a wide variety of search engines, repositories, digital libraries, and other bibliographic databases. The method extends existing sampling-based approaches that analyze smaller sets of database coverages. The findings show the relative and absolute subject coverages of 56 databases—information that has often not been available before. Knowing the databases’ absolute subject coverage allows the selection of the most comprehensive databases for searches requiring high recall/sensitivity, particularly relevant in lookup or exploratory searches. Knowing the databases’ relative subject coverage allows the selection of specialized databases for searches requiring high precision/specificity, particularly relevant in systematic searches. The findings illustrate not only differences in the disciplinary coverage of Google Scholar, Scopus, or Web of Science, but also of less frequently analyzed databases. For example, researchers might be surprised how Meta (discontinued), Embase, or Europe PMC are found to cover more records than PubMed in Medicine and other health subjects. These findings should encourage researchers to re-evaluate their go-to databases, also against newly introduced options. Searching with more comprehensive databases can improve finding, particularly when selecting the most fitting databases needs particular thought, such as in systematic reviews and meta-analyses. This comparison can also help librarians and other information experts re-evaluate expensive database procurement strategies. Researchers without institutional access learn which open databases are likely most comprehensive in their disciplines.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.48550/arXiv.2303.17661', '10.1007/s11192-022-04367-w', '10.1007/s11192-022-04289-7', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: I found two documents that address your question.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661\n",
      "Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. This study investigates methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. The proposed framework, MetaEnhance, utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. Evaluation on a benchmark of 500 ETDs shows that MetaEnhance achieves nearly perfect F1-scores in detecting errors and F1-scores ranging from 0.85 to 1.00 in correcting errors for five of seven fields.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w\n",
      "This study aims to identify classes of DOI mistakes in open bibliographic metadata available in Crossref, determine responsible publishers, and assess the potential for automatic correction. By analyzing invalid cited DOIs gathered by OpenCitations, the research identifies patterns of factual errors and defines regular expressions to catch and correct them. The findings show that a few publishers are responsible for the majority of invalid citations. The study extends the taxonomy of DOI name errors and proposes a DOI cleaning mechanism that can be integrated into existing processes for automatic correction.\n",
      "\n",
      "My lady, would you like to know more about either of these studies?\n",
      "96\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.250\n",
      "Recall: 0.200\n",
      "F1-Score: 0.222\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.3829032, '10.48550/arXiv.2303.17661'), (0.3650723, '10.1007/s11192-022-04367-w'), (0.14534597, '10.1007/s11192-022-04289-7'), (0.10071066, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', \"DOI: 10.48550/arXiv.2404.13948\\n Title: RAG の背中を打ち砕いた ypos: 低レベルの摂動を介して野生のドキュメントをシミュレートすることによる、RAG パイプラインへの遺伝的攻撃 ypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\\nAbstract: 最近の大規模言語モデル (LLM) の適用可能性がさまざまなドメインや現実世界のアプリケーションに拡大するにつれて、その堅牢性がますます重要になってきています。検索拡張生成 (RAG) は、LLM の制限に対処するための有望なソリューションですが、RAG の堅牢性に関する既存の研究では、RAG コンポーネント間の相互接続関係や、軽微なテキスト エラーなど、現実のデータベースに蔓延する潜在的な脅威が見落とされていることがよくあります。この研究では、RAG の堅牢性を評価する際にまだ解明されていない 2 つの側面を調査します。1 つは低レベルの摂動によるノイズの多いドキュメントに対する脆弱性、2 つは RAG の堅牢性の全体的な評価です。さらに、これらの側面をターゲットとした新しい攻撃方法である RAG への遺伝的攻撃を紹介します。具体的には、GARAG は、各コンポーネント内の脆弱性を明らかにし、ノイズの多いドキュメントに対してシステム全体の機能をテストするように設計されています。 \\\\textit{GARAG} を標準 QA データセットに適用し、さまざまな取得者と LLM を組み込むことで、RAG の堅牢性を検証します。実験結果は、GARAG が一貫して高い攻撃成功率を達成していることを示しています。また、各コンポーネントのパフォーマンスとその相乗効果を著しく損なうため、わずかなテキストの不正確さが現実世界の RAG システムを混乱させる大きなリスクを浮き彫りにします。 The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1 vulnerability to noisy documents through low-level perturbations and 2 a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG, which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.48550/arXiv.2406.13213', '10.1145/3637528.3671470', '10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2404.13948']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, such as hallucinations, outdated knowledge, and non-transparent reasoning processes. It enhances the accuracy and credibility of generation, particularly for knowledge-intensive tasks, by incorporating knowledge from external databases.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - RAG synergistically merges LLMs' intrinsic knowledge with external databases, improving accuracy and credibility in knowledge-intensive tasks.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - RAG enables LLMs to answer queries over unseen document collections, but traditional RAG applications perform poorly in multi-hop questions.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470 - RAG provides reliable and up-to-date external knowledge, assisting generative AI in producing high-quality outputs.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16 - RAG systems reduce the risk of hallucinations by providing LLMs with knowledge from reference textual databases.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.13948 - RAG is a promising solution for addressing LLM limitations, but its robustness is often overlooked in existing studies.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need more details, please let me know.\n",
      "96\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.400\n",
      "Recall: 0.400\n",
      "F1-Score: 0.400\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.5488434, '10.48550/arXiv.2312.10997'), (0.52685463, '10.48550/arXiv.2406.13213'), (0.4893821, '10.1145/3637528.3671470'), (0.4833807, '10.18653/v1/2024.eacl-demo.16'), (0.3458787, '10.48550/arXiv.2404.13948')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n', \"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1145/2964909', '10.1109/ADL.1998.670425', '10.1177/09610006241239080', '10.5860/crl.86.1.101', '10.48550/arXiv.2303.17661']\n",
      "\u001b[93mSummary: Metadata quality assessment is a critical area of research, with challenges including defining and measuring quality dimensions, cultural and contextual influences, and automating assessment processes. Advancements include the development of frameworks like MetaEnhance, standardized metadata schemas, and methodological contributions from studies on GILS and open data portals.\n",
      "\n",
      "DOI: 10.1145/2964909 - This study focuses on automated quality assessment of metadata across open data portals, addressing challenges in searchability, discoverability, and usability due to increasing data volumes. It proposes a generic framework that maps metadata from different portal software to a standardized schema and defines quality metrics for automated evaluation.\n",
      "\n",
      "DOI: 10.1109/ADL.1998.670425 - This research discusses methodological considerations in assessing metadata quality, using the US Government Information Locator Service (GILS) as a case study. It highlights the need for diverse criteria and procedures to evaluate different types of metadata, contributing to the dialogue on metadata quality assessment.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - This scoping review identifies commonly measured dimensions of metadata quality, such as completeness, accuracy, and consistency, but notes a lack of consensus on their definitions and measurements. It emphasizes the need for further research and improved flexibility in assessment tools.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 - This study explores metadata quality issues across cultures, identifying tensions between sociocultural representations and standardized systems. It categorizes 32 unique issues and examines how metadata interacts with communities to reflect cultural meanings.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661 - This research introduces MetaEnhance, a framework using AI to improve metadata quality for electronic theses and dissertations. It demonstrates high effectiveness in error detection and correction, showcasing advancements in automated metadata enhancement.\n",
      "\n",
      "My lady, these findings highlight both the complexities and progress in metadata quality assessment, offering insights into methodological, technological, and cultural dimensions of the field.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in metadata quality assessment?']:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.000\n",
      "Accuracy: 0.896\n",
      "Balanced accuracy: 0.473\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.6636041, '10.1145/2964909'), (0.4874788, '10.1109/ADL.1998.670425'), (0.38741276, '10.1177/09610006241239080'), (0.37305376, '10.5860/crl.86.1.101'), (0.2738236, '10.48550/arXiv.2303.17661')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.1080/19386389.2011.570654\\n Title: メタデータ レコードの問題の分析 An Analysis of Problems in Metadata Records\\nAbstract: メタデータはデジタル ライブラリにおいて重要な役割を果たします。しかし、有用であるためには、メタデータ レコードに問題がなくなければなりません。メタデータに問題があると、リソースが正しく表現されず、ユーザーはメタデータのメリットを享受できなくなります。このような問題を排除しないにしても、最小限に抑えるには、メタデータ レコードで発生する可能性のある問題の種類を理解することが不可欠です。この論文では、文献で報告されているメタデータ レコードで見つかった問題を比較および分析します。メタデータの問題の 5 つのカテゴリを特定できることがわかります。これらは、不正な値、不正な要素、情報の欠落、情報損失、および一貫性のない値の表現の問題です。これらの問題がメタデータによって提供できるサービスに悪影響を与えることを考慮すると、メタデータの使用から得られる利点と、メタデータ レコードの作成に費やされるコストと労力のバランスが確保されるように、予防または是正措置を講じる必要があります。 Metadata plays an important role in digital libraries. But to be useful, metadata records must be problem free. When problems are present in the metadata, resources are not correctly represented and users are not able to reap the benefits of metadata. To minimize, if not eliminate, such problems, it is essential to understand the kinds of problems that can occur in metadata records. In this paper, problems found in metadata records as reported in the literature are compared and analyzed. It is found that five categories of metadata problems can be identified. These are the problems of Incorrect Values, Incorrect Elements, Missing Information, Information Loss, and Inconsistent Value Representation. Given that these problems are detrimental to the services that can be provided by metadata, preventive or corrective measures need to be put in place so as to ensure that the benefits derived from using metadata balance the costs and efforts spent in the creation of metadata records.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1007/s11192-023-04923-y\\n Title: OpenAlex に機関が存在しない: 考えられる理由、影響、および解決策 Missing institutions in OpenAlex: possible reasons, implications, and solutions\\nAbstract: オープン サイエンスの到来により、高いデータ品質を備えたオープン データ プラットフォームが必要になります。 2022 年 1 月に開始されたグローバル研究システムの完全にオープンなカタログである OpenAlex は、データへの簡単なアクセスと、量的科学研究で広く使用されている幅広いデータ範囲という 2 つの主な利点を備えています。注目すべきことに、OpenAlex はライデン大学ランキングの重要なデータ ソースとして採用されています。ただし、OpenAlex の雑誌記事メタデータには機関が欠落しているという深刻なデータ品質の問題があります。この研究では、完全な機関情報 (FII)、部分的に欠落している機関情報 (PMII)、および完全に欠落している機関情報 (CMII) という 3 つのタイプの機関情報を定義することにより、問題の考えられる理由とその結果と解決策を調査します。私たちの結果は、OpenAlex のジャーナル記事の 60% 以上で機関の欠落の問題が発生していることを示しています。この問題は、初期のメタデータや社会科学、人文科学で特に蔓延しています。データのサブサンプルを使用して、問題の考えられる理由、歪んだ結果をもたらす可能性のあるリスク、行方不明の機関の問題に対する考えられる解決策をさらに調査します。その目的は、オープン リソースにおけるデータ品質の向上の重要性を高め、それによって量的科学の研究やより広範な文脈においてオープン リソースの責任ある使用をサポートすることです。 The advent of open science calls for open data platforms with high data quality. As a fully open catalog of the global research system launched in January 2022, OpenAlex features two main advantages of easy data accessibility and broad data coverage, which has been widely used in quantitative science studies. Remarkably, OpenAlex is adopted as an important data source for Leiden university ranking. However, there is a severe data quality problem of missing institutions in journal article metadata in OpenAlex. This study investigates the possible reasons for the problem and its consequences and solutions by defining three types of institutional information—full institutional information (FII), partially missing institutional information (PMII) and completely missing institutional information (CMII). Our results show that the problem of missing institutions occurs in more than 60% of the journal articles in OpenAlex. The problem is particularly widespread in metadata from the early years and in the social sciences and humanities. Using sub-samples of the data, we further explore the possible reasons for the problem, the risk it might represent for distorted results, and possible solutions to the problem of missing institutions. The aim is to raise the importance of data quality improvements in open resources, and thus to support the responsible use of open resources in quantitative science studies and also in broader contexts.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.5860/crl.86.1.101', '10.48550/arXiv.2303.17661', '10.1080/19386389.2011.570654', '10.1177/09610006241239080', '10.1007/s11192-023-04923-y']\n",
      "\u001b[93mSummary: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, metadata often exhibits incomplete, inconsistent, and incorrect values.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661 - Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, metadata often exhibits incomplete, inconsistent, and incorrect values. MetaEnhance, a framework utilizing state-of-the-art artificial intelligence methods, is proposed to improve the quality of metadata fields.\n",
      "\n",
      "DOI: 10.1080/19386389.2011.570654 - Metadata plays a vital role in digital libraries, but metadata records must be problem-free to be useful. Five categories of metadata problems are identified: Incorrect Values, Incorrect Elements, Missing Information, Information Loss, and Inconsistent Value Representation. Preventive or corrective measures are necessary to ensure the benefits of metadata outweigh the costs of creation.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - Metadata quality assessment commonly uses dimensions such as completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions.\n",
      "\n",
      "DOI: 10.1007/s11192-023-04923-y - OpenAlex, an open data platform, faces a severe data quality problem of missing institutions in journal article metadata. The study investigates the possible reasons, consequences, and solutions for this problem, emphasizing the importance of data quality improvements in open resources.\n",
      "\n",
      "My lady, I hope this summary provides a comprehensive overview of the key challenges and potential solutions for improving metadata quality. If you have any further questions or would like more detailed information, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and potential solutions for improving metadata quality?']:\n",
      "Precision: 0.800\n",
      "Recall: 0.800\n",
      "F1-Score: 0.800\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.895\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.5573882, '10.5860/crl.86.1.101'), (0.46701634, '10.48550/arXiv.2303.17661'), (0.43467787, '10.1080/19386389.2011.570654'), (0.40952834, '10.1177/09610006241239080'), (0.29672605, '10.1007/s11192-023-04923-y')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 14 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", \"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", \"DOI: 10.48550/arXiv.2505.18247\\n Title: MetaGen Blended RAG: 専門分野の質問応答でゼロショットの精度を解放 MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索拡張生成 (RAG) は、ファイアウォールの背後に隔離されていることが多く、事前トレーニング中に LLM が認識できない複雑で特殊な用語が豊富に含まれる、ドメイン固有のエンタープライズ データセットに苦戦します。医学、ネットワーキング、法律などの分野にわたるセマンティックのばらつきが RAG のコンテキストの精度を妨げる一方、ソリューションを微調整するのはコストがかかり、時間がかかり、新しいデータが出現したときの汎用性が欠けています。微調整を行わずにレトリーバーでゼロショット精度を達成することは依然として重要な課題です。私たちは、メタデータ生成パイプラインと密ベクトルと疎ベクトルを使用したハイブリッド クエリ インデックスを通じてセマンティック リトリーバーを強化する新しいエンタープライズ検索アプローチである「MetaGen Blended RAG」を紹介します。主要な概念、トピック、頭字語を活用することで、私たちのメソッドはメタデータを強化したセマンティック インデックスと強化されたハイブリッド クエリを作成し、微調整することなく堅牢でスケーラブルなパフォーマンスを実現します。生物医学の PubMedQA データセットでは、MetaGen Blended RAG は 82% の検索精度と 77% の RAG 精度を達成し、以前のすべてのゼロショット RAG ベンチマークを上回り、そのデータセット上の微調整モデルに匹敵するだけでなく、SQuAD や NQ などのデータセットでも優れています。このアプローチは、特殊なドメイン全体にわたって比類のない一般化を備えたセマンティック検索ツールを構築する新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\", 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1016/j.caeai.2025.100417', '10.48550/arXiv.2505.18247', '10.1007/s44427-025-00006-3', '10.1145/3637528.3671470']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) systems face challenges such as hallucinations, outdated knowledge, and non-transparent reasoning processes, but advancements include improved accuracy, continuous knowledge updates, and domain-specific integration.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - This paper discusses the challenges faced by Large Language Models (LLMs), such as hallucination, outdated knowledge, and non-transparent reasoning processes. It highlights RAG as a promising solution by incorporating external knowledge, which enhances accuracy and allows for continuous updates. The paper provides a detailed examination of RAG paradigms and their advancements.\n",
      "\n",
      "DOI: 10.1016/j.caeai.2025.100417 - Focusing on educational applications, this paper addresses challenges like hallucinations and static internal knowledge in LLMs. It explores how RAG improves factual accuracy and enables dynamic knowledge updates, making it suitable for education. The paper discusses practical applications and future directions in educational RAG systems.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247 - This research introduces 'MetaGen Blended RAG', a novel approach to enhance semantic retrievers in domain-specific enterprise datasets. It addresses the challenge of achieving zero-shot precision without fine-tuning and demonstrates superior performance on datasets like PubMedQA, SQuAD, and NQ.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3 - This study evaluates the effectiveness of RAG systems using a benchmark dataset derived from diploma theses abstracts. It highlights the challenges in assessing both retrieval and generation components and provides insights into optimizing RAG implementations for academic tasks.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470 - This survey reviews existing research on Retrieval-Augmented Large Language Models (RA-LLMs), discussing their capabilities in providing reliable external knowledge. It also explores current limitations and future research directions in this field.\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions or need additional information, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in Retrieval-Augmented Generation (RAG) systems?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9224337, '10.48550/arXiv.2312.10997'), (0.87681407, '10.1016/j.caeai.2025.100417'), (0.7272211, '10.48550/arXiv.2505.18247'), (0.6641271, '10.1007/s44427-025-00006-3'), (0.6102915, '10.1145/3637528.3671470')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 15 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', \"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.1177/09610006241239080', '10.1109/ADL.1998.670425', '10.5281/zenodo.14006424']\n",
      "\u001b[93mSummary: Three studies examined the abstract in metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - This study compares the amount of metadata and the completeness degree of research publications in new academic databases. Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - This study identifies the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software.\n",
      "\n",
      "DOI: 10.5281/zenodo.14006424 - This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.250\n",
      "Recall: 0.200\n",
      "F1-Score: 0.222\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.63179934, '10.1162/qss_a_00286'), (0.58919704, '10.1177/09610006241239080'), (0.55420566, '10.1109/ADL.1998.670425'), (0.18688062, '10.5281/zenodo.14006424')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1108/JD-10-2022-0234\\n Title: すべての学術分野にわたる引用と参照の習慣の分析: 書誌の参照と引用の実践におけるアプローチと傾向 An analysis of citing and referencing habits across all scholarly disciplines: approaches and trends in bibliographic referencing and citing practices\\nAbstract: この研究で、著者らは、学術文献における引用および参照の誤りについて現在考えられる原因を特定し、スウィートランド氏が1989年の論文で提供したスナップショットから何かが変わったかどうかを比較したいと考えている。,著者らは、27の主題分野にわたる147のジャーナルに掲載された729件の論文から、参考要素、すなわち書誌的参照、言及、引用、およびそれぞれの本文中の参照ポインタを分析した。,分析の結果は、書誌的事項が指摘された。著者らの知る限り、この研究は、Sweetland (1989) 以来、文献における参照および引用の慣行における誤りを分析したものとしては、最近入手可能な最良のものである。 In this study, the authors want to identify current possible causes for citing and referencing errors in scholarly literature to compare if something changed from the snapshot provided by Sweetland in his 1989 paper.,The authors analysed reference elements, i.e. bibliographic references, mentions, quotations and respective in-text reference pointers, from 729 articles published in 147 journals across the 27 subject areas.,The outcomes of the analysis pointed out that bibliographic errors have been perpetuated for decades and that their possible causes have increased, despite the encouraged use of technological facilities, i.e. the reference managers.,As far as the authors know, the study is the best recent available analysis of errors in referencing and citing practices in the literature since Sweetland (1989).\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1108/JD-10-2022-0234', '10.1007/s11192-022-04367-w', '10.1162/qss_a_00112']\n",
      "\u001b[93mSummary: Two studies examined citations.\n",
      "\n",
      "DOI: 10.1108/JD-10-2022-0234 - This study examined errors in referencing and citing practices in scholarly literature.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w - This study examined invalid citations due to DOI errors in Crossref data.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 1.000\n",
      "Recall: 0.600\n",
      "F1-Score: 0.750\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.800\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.3569626, '10.1108/JD-10-2022-0234'), (0.21608633, '10.1007/s11192-022-04367-w'), (0.13634396, '10.1162/qss_a_00112')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1007/s11192-022-04367-w', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: Crossref is a community-owned sustainable source of scholarly metadata that is available through a number of APIs.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.800\n",
      "Recall: 0.800\n",
      "F1-Score: 0.800\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.895\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.7781275, '10.1162/qss_a_00022'), (0.72395444, '10.1162/qss_a_00210'), (0.55623144, '10.31222/osf.io/smxe5'), (0.44883752, '10.1007/s11192-022-04367-w'), (0.17410146, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n', 'DOI: 10.1007/s11192-022-04289-7\\n Title: 最も多く見つかる場所を検索: 56 の書誌データベースの懲戒範囲の比較 Search where you will find most: Comparing the disciplinary coverage of 56 bibliographic databases\\nAbstract: この論文では、新しいサイエントメトリクス手法を紹介し、それを学術界で人気のある英語に焦点を当てた書誌データベースの多くの主題範囲を推定するために適用します。この方法では、クエリ結果を共通の分母として使用して、さまざまな検索エンジン、リポジトリ、デジタル ライブラリ、その他の書誌データベースを比較します。この方法は、データベース カバレッジのより小さいセットを分析する既存のサンプリング ベースのアプローチを拡張します。この調査結果では、56 のデータベースの相対的および絶対的な対象範囲が示されており、これまで入手できなかった情報が示されています。データベースの絶対的な対象範囲を知ることで、特にルックアップ検索や探索的検索に関連する、高い再現率/感度が必要な検索に最も包括的なデータベースを選択できます。データベースの相対的な対象範囲を知ることで、特に体系的な検索に関連する、高い精度と特異性が必要な検索に特化したデータベースを選択できます。この調査結果は、Google Scholar、Scopus、または Web of Science の専門分野の範囲の違いだけでなく、分析頻度が低いデータベースの違いも示しています。たとえば、研究者は、Meta (廃止)、Embase、または Europe PMC が、医学やその他の健康分野の PubMed よりも多くの記録をカバーしていることが判明したことに驚くかもしれません。これらの発見は、研究者が新しく導入されたオプションに対しても頼りになるデータベースを再評価するよう促すはずです。より包括的なデータベースを使用して検索すると、特にシステマティック レビューやメタ分析など、最も適合するデータベースの選択に特別な考慮が必要な場合に、検索結果が向上します。この比較は、図書館員やその他の情報専門家が高価なデータベース調達戦略を再評価するのにも役立ちます。機関にアクセスできない研究者は、どのオープン データベースが自分の専門分野において最も包括的である可能性が高いかを学びます。 This paper introduces a novel scientometrics method and applies it to estimate the subject coverages of many of the popular English-focused bibliographic databases in academia. The method uses query results as a common denominator to compare a wide variety of search engines, repositories, digital libraries, and other bibliographic databases. The method extends existing sampling-based approaches that analyze smaller sets of database coverages. The findings show the relative and absolute subject coverages of 56 databases—information that has often not been available before. Knowing the databases’ absolute subject coverage allows the selection of the most comprehensive databases for searches requiring high recall/sensitivity, particularly relevant in lookup or exploratory searches. Knowing the databases’ relative subject coverage allows the selection of specialized databases for searches requiring high precision/specificity, particularly relevant in systematic searches. The findings illustrate not only differences in the disciplinary coverage of Google Scholar, Scopus, or Web of Science, but also of less frequently analyzed databases. For example, researchers might be surprised how Meta (discontinued), Embase, or Europe PMC are found to cover more records than PubMed in Medicine and other health subjects. These findings should encourage researchers to re-evaluate their go-to databases, also against newly introduced options. Searching with more comprehensive databases can improve finding, particularly when selecting the most fitting databases needs particular thought, such as in systematic reviews and meta-analyses. This comparison can also help librarians and other information experts re-evaluate expensive database procurement strategies. Researchers without institutional access learn which open databases are likely most comprehensive in their disciplines.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.48550/arXiv.2508.18620', '10.1162/qss_a_00286', '10.5281/zenodo.14006424', '10.1007/s11192-022-04289-7']\n",
      "\u001b[93mSummary: Several papers evaluate the linguistic coverage or language-related metadata in scholarly databases.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 - This paper assesses the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 - This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - This study compares the amount of metadata and the completeness degree of research publications in new academic databases, including an analysis of language-related metadata.\n",
      "\n",
      "DOI: 10.5281/zenodo.14006424 - This paper analyses OpenAlex’s coverage and metadata availability of African-based publications, including an evaluation of language-related metadata.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04289-7 - While not explicitly focused on language-related metadata, this paper introduces a novel scientometrics method to estimate the subject coverages of many popular English-focused bibliographic databases, which may have implications for linguistic coverage.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or would like more details on a specific paper, please let me know.\n",
      "96\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.200\n",
      "Recall: 0.200\n",
      "F1-Score: 0.200\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9108078, '10.48550/arXiv.2409.10633'), (0.42706275, '10.48550/arXiv.2508.18620'), (0.41563448, '10.1162/qss_a_00286'), (0.19475074, '10.5281/zenodo.14006424'), (0.16595806, '10.1007/s11192-022-04289-7')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: COVID-19 研究への資金提供: オープン データ インフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 資金提供機関が提供する資金の成果を分析するには、資金提供によって得られた出版物を追跡できることが不可欠です。私たちは、Crossref での資金調達データのオープンな利用可能性を調査し、新型コロナウイルス感染症 (COVID-19) に関連する研究を報告する出版物の資金調達データに焦点を当てています。また、Scopus と Web of Science という 2 つの独自の書誌データベースで利用可能な資金調達データとの比較も示します。私たちの分析では、Crossref の資金調達データの範囲が限られていることが明らかになりました。また、特に Scopus における資金調達データの品質に関連する問題も示しています。 Crossref での資金調達データのオープンな可用性を向上させるための推奨事項を提供します。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.5281/zenodo.14006424', '10.31222/osf.io/smxe5', '10.1162/qss_a_00286', '10.1162/qss_a_00212']\n",
      "\u001b[93mSummary: There are four papers that address funding metadata, its availability, or its analysis in scholarly databases.\n",
      "\n",
      "DOI: 10.1162/qss_a_00210 - This paper addresses the availability and completeness of open funder metadata.\n",
      "DOI: 10.5281/zenodo.14006424 - This paper addresses the coverage and metadata availability of African publications in OpenAlex.\n",
      "DOI: 10.31222/osf.io/smxe5 - This paper addresses Crossref as a source of open bibliographic metadata.\n",
      "DOI: 10.1162/qss_a_00212 - This paper addresses funding COVID-19 research.\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "96\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.800\n",
      "Recall: 1.000\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.990\n",
      "Balanced accuracy: 0.995\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.68798697, '10.1162/qss_a_00210'), (0.48616135, '10.5281/zenodo.14006424'), (0.4024631, '10.31222/osf.io/smxe5'), (0.31545174, '10.1162/qss_a_00286'), (0.30762884, '10.1162/qss_a_00212')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", \"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.48550/arXiv.2312.10997', '10.1145/3637528.3671470', '10.6109/jkiice.2023.27.12.1489', '10.1007/s44427-025-00006-3']\n",
      "\u001b[93mI'm sorry, my lady, I am unable to answer your question.\n",
      "96\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) when used in applications?']:\n",
      "Precision: 0.200\n",
      "Recall: 0.200\n",
      "F1-Score: 0.200\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 0\n",
      "Documents score: [(0.8958969, '10.1016/j.caeai.2025.100417'), (0.81418616, '10.48550/arXiv.2312.10997'), (0.7173491, '10.1145/3637528.3671470'), (0.69672287, '10.6109/jkiice.2023.27.12.1489'), (0.6675167, '10.1007/s44427-025-00006-3')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1007/s11192-022-04367-w', '10.1162/qss_a_00112']\n",
      "\u001b[93mSummary: Crossref is a community-owned sustainable source of scholarly metadata that is collected and made available by Crossref.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.95628047, '10.1162/qss_a_00022'), (0.47548413, '10.1162/qss_a_00210'), (0.44680908, '10.31222/osf.io/smxe5'), (0.21246108, '10.1007/s11192-022-04367-w'), (0.19260046, '10.1162/qss_a_00112')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.29173/cais1943\\n Title: OpenAlex と Web of Science の間のドキュメント タイプの不一致を調査する Investigating Document Type Discrepancies between OpenAlex and the Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された OpenAlex は、そのオープン性と広範なカバレッジで際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この研究は、ドキュメント タイプの精度に焦点を当て、OpenAlex と WoS における作品のメタデータの品質を評価することを目的としています。 OpenAlex と WoS の両方でインデックス付けされている出版物の 4% 以上が研究論文またはレビューとして誤って分類されているようであり、これらのエラーの大部分 (約 97%) が OpenAlex で発生していることが観察されています。この研究は、文書タイプの不一致や誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, launched in 2022, stands out for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinder its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess the metadata quality of works in OpenAlex and WoS, focusing on document type accuracy. We observe that over 4% of the publications indexed in both OpenAlex and WoS appear to be misclassified as research articles or reviews, and that the vast majority (about 97%) of these errors occur in OpenAlex. By addressing discrepancies and misattributions in document types this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', \"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.3145/epi.2023.mar.09\\n Title: Microsoft Academic Graph から OpenAlex に切り替える場合、書誌情報学に関連するメタデータはどれが同じで、どれが異なりますか? Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\\nAbstract: Microsoft Academic Graph (MAG) の廃止の発表に伴い、非営利団体 OurResearch は OpenAlex という名前で同様のリソースを提供すると発表しました。したがって、最新の MAG スナップショットと初期の OpenAlex スナップショットの書誌学的分析に関連するメタデータを比較します。 MAG の実質的にすべての著作物は、書誌データの出版年、巻数、最初と最後のページ、DOI、および引用分析の重要な要素である参考文献の数を保存しながら OpenAlex に転送されました。 MAG ドキュメントの 90% 以上が OpenAlex に同等のドキュメント タイプを持っています。残りのうち、特に OpenAlex 文書タイプの Journal-article および Book-chapter への再分類は正しいようで、その割合は 7% 以上に達しており、文書タイプの仕様は MAG から OpenAlex に大幅に改善されました。書誌学的関連メタデータの別の項目として、MAG と OpenAlex における紙ベースの主題分類を調べました。 OpenAlex では、MAG よりもはるかに多くの主題分類が割り当てられているドキュメントが見つかりました。第 1 レベルと第 2 レベルでは、分類構造はほぼ同じです。主題の再分類に関するデータを表とグラフの両方のレベルで表示します。分野で正規化された書誌学的評価に対する豊富な主題の再分類の影響の評価は、この論文の範囲には含まれません。この未解決の質問とは別に、OpenAlex は全体的に、文書タイプの割り当てがより広範囲にカバーされているため、2021 年より前の出版物については少なくとも MAG と同等かそれ以上に書誌学的分析に適しているようです。 With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2508.18620', '10.29173/cais1943', '10.48550/arXiv.2409.10633', '10.3145/epi.2023.mar.09']\n",
      "\u001b[93mSummary: OpenAlex is a bibliographic database that was launched in 2022. It is open-source and comprehensive, and promises to reduce or eliminate barriers to accessing bibliometric data. However, concerns about the quality of its metadata hinder its broader adoption for research and research evaluation.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. However, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 - This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\n",
      "\n",
      "DOI: 10.29173/cais1943 - This study focuses on document type accuracy and finds that over 4% of publications indexed in both OpenAlex and WoS are misclassified, with about 97% of these errors occurring in OpenAlex.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 - OpenAlex exhibits a more balanced linguistic coverage than WoS, but its language metadata is not always accurate. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing.\n",
      "\n",
      "DOI: 10.3145/epi.2023.mar.09 - OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021, or maybe even better because of the broader coverage of document type assignments.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or would like more details on a specific aspect of OpenAlex, please let me know.\n",
      "96\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.8455478, '10.48550/arXiv.2404.17663'), (0.7178241, '10.48550/arXiv.2508.18620'), (0.69024646, '10.29173/cais1943'), (0.64344186, '10.48550/arXiv.2409.10633'), (0.6353361, '10.3145/epi.2023.mar.09')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1007/s11192-015-1765-5\\n Title: Web of Science と Scopus のジャーナルの報道: 比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌学的手法は、研究の評価など、さまざまな目的で複数の分野で使用されています。ほとんどの書誌学的分析には、トムソン・ロイターの Web of Science (WoS) とエルゼビアの Scopus というデータ ソースが共通しています。この調査の目的は、これら 2 つのデータベースのジャーナルの対象範囲を説明し、特定の分野、出版国、言語が過小評価されているかどうかを評価することです。これを行うために、WoS (13,605 ジャーナル) および Scopus (20,346 ジャーナル) のアクティブな学術ジャーナルの範囲を、ウルリッヒの広範な定期刊行物ディレクトリ (63,013 ジャーナル) と比較しました。結果は、WoS または Scopus を研究評価に使用すると、社会科学、芸術、人文科学に不利益をもたらす、自然科学と工学、生物医学研究に有利なバイアスを導入する可能性があることを示しています。同様に、英語のジャーナルが過大評価され、他の言語に損害を与えています。どちらのデータベースもこれらのバイアスを共有していますが、その範囲は大きく異なります。そのため、書誌情報分析の結果は使用するデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈において、特に異なる分野、機関、国、または言語を比較する場合には、WoS と Scopus を慎重に使用する必要があることを示唆しています。書誌学コミュニティは、分野固有の引用インデックスや全国的な引用インデックスなど、WoS や Scopus ではカバーされていない科学的成果を含む手法や指標を開発する努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.29173/cais1943\\n Title: OpenAlex と Web of Science の間のドキュメント タイプの不一致を調査する Investigating Document Type Discrepancies between OpenAlex and the Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された OpenAlex は、そのオープン性と広範なカバレッジで際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この研究は、ドキュメント タイプの精度に焦点を当て、OpenAlex と WoS における作品のメタデータの品質を評価することを目的としています。 OpenAlex と WoS の両方でインデックス付けされている出版物の 4% 以上が研究論文またはレビューとして誤って分類されているようであり、これらのエラーの大部分 (約 97%) が OpenAlex で発生していることが観察されています。この研究は、文書タイプの不一致や誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, launched in 2022, stands out for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinder its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess the metadata quality of works in OpenAlex and WoS, focusing on document type accuracy. We observe that over 4% of the publications indexed in both OpenAlex and WoS appear to be misclassified as research articles or reviews, and that the vast majority (about 97%) of these errors occur in OpenAlex. By addressing discrepancies and misattributions in document types this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.1162/qss_a_00211\\n Title: 国際的な研究協力を測定するための書誌データソースの品質を評価する Assessing the quality of bibliographic data sources for measuring international research collaboration\\nAbstract: 国際研究協力 (IRC) の測定は、さまざまな研究評価タスクに不可欠ですが、どのデータソースを使用するかなど、さまざまな測定決定の影響については十分に研究されていません。データ ソースの選択が IRC 測定に及ぼす影響をより深く理解するために、利用可能なディメンションを確認して選択し、適切な計算可能なメトリクスを設計することにより、書誌データに特化したデータ品質評価フレームワークを設計および実装し、次にそれを書誌データの 4 つの一般的なソース (Microsoft Academic Graph、Web of Science (WoS)、Dimensions、ACM Digital Library) に適用してフレームワークを検証します。このフレームワークの検証が成功した場合、それが Wang と Strong (1996) によって提案された情報品質の一般的な概念フレームワークと一致しており、調査された情報源の品質の違いが適切に特定されることが示唆されます。フレームワークを適用すると、検討したセットの中で WS が全体的な品質が最も高いことがわかります。そして、品質の違いは主にデータ ソースの編成方法によって説明できると考えられます。私たちの研究は、研究者がこの IRC 測定ツールを研究に適用できるようにする方法論的な貢献で構成されており、書誌データの 4 つの一般的な情報源とその IRC 測定への影響をさらに特徴付けることによって実証的な貢献をしています。 Measuring international research collaboration (IRC) is essential to various research assessment tasks but the effect of various measurement decisions, including which data sources to use, has not been thoroughly studied. To better understand the effect of data source choice on IRC measurement, we design and implement a data quality assessment framework specifically for bibliographic data by reviewing and selecting available dimensions and designing appropriate computable metrics, and then validate the framework by applying it to four popular sources of bibliographic data: Microsoft Academic Graph, Web of Science (WoS), Dimensions, and the ACM Digital Library. Successful validation of the framework suggests it is consistent with the popular conceptual framework of information quality proposed by Wang and Strong (1996) and adequately identifies the differences in quality in the sources examined. Application of the framework reveals that WoS has the highest overall quality among the sets considered; and that the differences in quality can be explained primarily by how the data sources are organized. Our study comprises a methodological contribution that enables researchers to apply this IRC measurement tool in their studies and makes an empirical contribution by further characterizing four popular sources of bibliographic data and their impact on IRC measurement.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1007/s11192-015-1765-5', '10.48550/arXiv.2508.18620', '10.29173/cais1943', '10.1162/qss_a_00112', '10.1162/qss_a_00211']\n",
      "\u001b[93mSummary: I'm sorry, my lady, I was unable to find any information about the strengths and weaknesses of Web of Science (WoS) as a bibliometric database.\n",
      "\n",
      "DOI: 10.1007/s11192-015-1765-5 discusses the journal coverage of WoS and Scopus, and how they favour Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 and 10.29173/cais1943: both discuss the quality of metadata in OpenAlex and WoS, and how over 4% of the publications indexed in both OpenAlex and WoS appear to be misclassified as research articles or reviews, and that the vast majority (about 97%) of these errors occur in OpenAlex.\n",
      "\n",
      "DOI: 10.1162/qss_a_00112: discusses the comparison of five multidisciplinary bibliographic data sources, including WoS, and how combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature is important.\n",
      "\n",
      "DOI: 10.1162/qss_a_00211: discusses the quality of bibliographic data sources for measuring international research collaboration, and how WoS has the highest overall quality among the sets considered.\n",
      "96\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.400\n",
      "Recall: 0.400\n",
      "F1-Score: 0.400\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.6334335, '10.1007/s11192-015-1765-5'), (0.56431514, '10.48550/arXiv.2508.18620'), (0.54274267, '10.29173/cais1943'), (0.45914948, '10.1162/qss_a_00112'), (0.2705689, '10.1162/qss_a_00211')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.17821/srels/2024/v61i5/171583\\n Title: 図書館における質問回答ベースの検索システムの設計: オープンソースの検索拡張生成 (RAG) パイプラインのアプリケーション Designing Question-Answer Based Search System in Libraries: Application of Open Source Retrieval Augmented Generation (RAG) Pipeline\\nAbstract: この研究の主な目的は、プロトタイプを準備し、図書館が検索拡張生成 (RAG) フレームワークを通じてオープンソース ソフトウェア ツールと大規模言語モデル (LLM) を使用して低コストの会話型検索システムを開発できることを実証することです。 LLM は幻覚を起こし、時代遅れで文脈を理解していない応答を返すことがよくあります。ただし、この実験は、LLM が一連の関連文書で強化された場合、文脈に応じた適切な応答を提供できることを示しています。回答を生成する前に関連ドキュメントで LLM を拡張することは、検索拡張生成として知られています。この方法論には、LangChain などのツール、ChromaDB などのベクトル データベース、Llama3 (700 億のパラメーター ベースのモデル) などのオープンソース LLM を使用して RAG パイプラインを作成することが含まれていました。開発されたプロトタイプには、収集、処理され、パイプラインに取り込まれたチャンドラヤーン 3 ミッションに関する 250 以上の関連文書のデータセットが含まれています。最後に、研究では標準的な LLM と RAG 拡張を備えた LLM からの応答を比較しました。主な調査結果から、標準的な LLM (RAG なし) は、チャンドラヤーン 3 に関連するクエリに対して自信を持って不正確で幻覚のような応答を生成するのに対し、RAG を使用する LLM は、応答を生成する前に関連文書のセットが提供されると、一貫して正確で有益な、文脈に沿った応答を提供することが明らかになりました。この調査では、オープンソースの RAG ベースのシステムは、情報検索を強化し、図書館を動的な情報サービスに変えるための費用対効果の高いソリューションを図書館に提供すると結論付けています。 This study primarily aims to prepare a prototype and demonstrate that libraries can develop a low-cost conversational search system using open-source software tools and Large Language Models (LLMs) through a Retrieval-Augmented Generation (RAG) framework. LLMs often hallucinate and provide outdated and non-contextualized responses. However, this experiment shows that LLMs can deliver contextualized, relevant responses when augmented with a set of relevant documents. Augmenting LLMs with relevant documents before generating answers is known as retrieval-augmented generation. The methodology involved creating a RAG pipeline using tools like LangChain, vector databases like ChromaDB, and open-source LLMs like Llama3 (a 70-billion parameter-based model). The prototype developed includes a dataset of 250+ relevant documents on the Chandrayaan-3 mission that was collected, processed, and ingested into the pipeline. Finally, the study compared responses from standard LLMs and LLMs with RAG augmentation. Key findings revealed that standard LLMs (without RAG) produced confidently incorrect, hallucinated responses against queries related to Chandrayaan-3, while LLMs with RAG consistently provided accurate, informative, and contextualized answers when supplied with a set of relevant documents before generating the response. The study concluded that open-source RAG-based systems offer a cost-effective solution for libraries to enhance information retrieval and transform libraries into dynamic information services.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.48550/arXiv.2505.13557\\n Title: AMAQA: RAG システム用のメタデータベースの QA データセット AMAQA: A Metadata-based QA Dataset for RAG Systems\\nAbstract: 検索拡張生成 (RAG) システムは、質問応答 (QA) タスクで広く使用されていますが、現在のベンチマークにはメタデータの統合が不足しており、テキスト データと外部情報の両方を必要とするシナリオでの評価が妨げられています。これに対処するために、テキストとメタデータを組み合わせたタスクを評価するように設計された新しいオープンアクセス QA データセットである AMAQA を紹介します。メタデータの統合は、サイバーセキュリティやインテリジェンスなど、関連情報へのタイムリーなアクセスが重要な、大量のデータの迅速な分析が必要な分野で特に重要です。 AMAQA には、26 のパブリック Telegram グループから収集された約 110 万件の英語メッセージが含まれており、タイムスタンプ、トピック、感情の調子、毒性指標などのメタデータが充実しており、特定の基準に基づいてドキュメントをフィルタリングすることで、正確で文脈に応じたクエリを実行できます。また、450 の高品質 QA ペアも含まれており、メタデータ主導の QA および RAG システムの研究を進めるための貴重なリソースとなります。私たちの知る限り、AMAQA は、メッセージで扱われるトピックなどのメタデータとラベルを組み込んだ最初のシングルホップ QA ベンチマークです。私たちはベンチマークで広範なテストを実施し、将来の研究のための新しい基準を確立します。メタデータを活用すると精度が 0.12 から 0.61 に向上し、構造化コンテキストの価値が強調されることがわかりました。これに基づいて、提供されたコンテキストを反復処理し、ノイズの多いドキュメントで強化することで LLM 入力を洗練するためのいくつかの戦略を検討し、最良のベースラインよりもさらに 3 ポイントの向上を達成し、単純なメタデータ フィルタリングよりも 14 ポイントの改善を達成しました。 Retrieval-augmented generation (RAG) systems are widely used in question-answering (QA) tasks, but current benchmarks lack metadata integration, hindering evaluation in scenarios requiring both textual data and external information. To address this, we present AMAQA, a new open-access QA dataset designed to evaluate tasks combining text and metadata. The integration of metadata is especially important in fields that require rapid analysis of large volumes of data, such as cybersecurity and intelligence, where timely access to relevant information is critical. AMAQA includes about 1.1 million English messages collected from 26 public Telegram groups, enriched with metadata such as timestamps, topics, emotional tones, and toxicity indicators, which enable precise and contextualized queries by filtering documents based on specific criteria. It also includes 450 high-quality QA pairs, making it a valuable resource for advancing research on metadata-driven QA and RAG systems. To the best of our knowledge, AMAQA is the first single-hop QA benchmark to incorporate metadata and labels such as topics covered in the messages. We conduct extensive tests on the benchmark, establishing a new standard for future research. We show that leveraging metadata boosts accuracy from 0.12 to 0.61, highlighting the value of structured context. Building on this, we explore several strategies to refine the LLM input by iterating over provided context and enriching it with noisy documents, achieving a further 3-point gain over the best baseline and a 14-point improvement over simple metadata filtering.\\n', \"DOI: 10.48550/arXiv.2505.18247\\n Title: MetaGen Blended RAG: 専門分野の質問応答でゼロショットの精度を解放 MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索拡張生成 (RAG) は、ファイアウォールの背後に隔離されていることが多く、事前トレーニング中に LLM が認識できない複雑で特殊な用語が豊富に含まれる、ドメイン固有のエンタープライズ データセットに苦戦します。医学、ネットワーキング、法律などの分野にわたるセマンティックのばらつきが RAG のコンテキストの精度を妨げる一方、ソリューションを微調整するのはコストがかかり、時間がかかり、新しいデータが出現したときの汎用性が欠けています。微調整を行わずにレトリーバーでゼロショット精度を達成することは依然として重要な課題です。私たちは、メタデータ生成パイプラインと密ベクトルと疎ベクトルを使用したハイブリッド クエリ インデックスを通じてセマンティック リトリーバーを強化する新しいエンタープライズ検索アプローチである「MetaGen Blended RAG」を紹介します。主要な概念、トピック、頭字語を活用することで、私たちのメソッドはメタデータを強化したセマンティック インデックスと強化されたハイブリッド クエリを作成し、微調整することなく堅牢でスケーラブルなパフォーマンスを実現します。生物医学の PubMedQA データセットでは、MetaGen Blended RAG は 82% の検索精度と 77% の RAG 精度を達成し、以前のすべてのゼロショット RAG ベンチマークを上回り、そのデータセット上の微調整モデルに匹敵するだけでなく、SQuAD や NQ などのデータセットでも優れています。このアプローチは、特殊なドメイン全体にわたって比類のない一般化を備えたセマンティック検索ツールを構築する新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.17821/srels/2024/v61i5/171583', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2505.13557', '10.48550/arXiv.2505.18247']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a method used to improve question answering and information retrieval systems by enhancing Large Language Models (LLMs) with relevant documents before generating answers. This approach improves accuracy, context relevance, and performance in various domains.\n",
      "\n",
      "DOI: 10.17821/srels/2024/v61i5/171583 - This study demonstrates that libraries can develop low-cost conversational search systems using open-source tools and LLMs through a RAG framework. Standard LLMs often produce incorrect and hallucinated responses, but when augmented with relevant documents, they provide accurate and contextualized answers.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - This research introduces Multi-Meta-RAG, a method that improves RAG for multi-hop queries by using database filtering with LLM-extracted metadata. It enhances the selection of relevant documents from various sources, significantly improving performance on multi-hop benchmarks.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - This paper proposes M-RAG, a method for enhancing open-domain question answering by utilizing embeddings with metadata and generative models like GPT-3.5 and GPT-4. The approach improves performance by up to 46% compared to external inference systems and 6% over conventional RAG methods.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.13557 - This study introduces AMAQA, a metadata-based QA dataset for RAG systems, addressing the lack of metadata integration in current benchmarks. It demonstrates that leveraging metadata significantly boosts accuracy, especially in fields requiring rapid data analysis.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247 - This research presents MetaGen Blended RAG, a novel approach for achieving zero-shot precision in specialized domain question answering without fine-tuning. It achieves high retrieval and RAG accuracy, surpassing prior benchmarks and rivaling fine-tuned models.\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions or need additional details, please let me know.\n",
      "96\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.81133276, '10.17821/srels/2024/v61i5/171583'), (0.72348577, '10.48550/arXiv.2406.13213'), (0.7211353, '10.6109/jkiice.2023.27.12.1489'), (0.72019166, '10.48550/arXiv.2505.13557'), (0.5614318, '10.48550/arXiv.2505.18247')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1045/september2016‐meschenmoser\\n Title: 科学 Web リポジトリのスクレイピング: 自動コンテンツ抽出の課題と解決策 Scraping Scientific Web Repositories: Challenges and Solutions for Automated Content Extraction\\nAbstract: 多くの科学 Web リポジトリは、科学出版物の可視性とアクセシビリティを向上させるだけでなく、h インデックスなどの指標を表示することによって、研究者の定量的および定性的な出版パフォーマンスも評価しています。これらの指標は、研究機関やその他の関係者にとって、採用や資金調達の決定など、影響力のある意思決定プロセスをサポートするために重要になっています。ただし、科学 Web リポジトリは通常、単純なパフォーマンス メトリクスと限られた分析オプションのみを提供します。さらに、パフォーマンス指標を計算するためのデータとアルゴリズムは通常公開されていません。したがって、システムがどの出版物を計算に含めるか、またシステムが結果をどのようにランク付けするかは透明性がなく、検証可能ではありません。多くの研究者は、これらのシステムの透明性を高めるために、基礎となるサイエントメトリーの生データにアクセスすることに興味を持っています。このペーパーでは、課題について説明し、科学 Web リポジトリ内のそのようなデータにプログラムでアクセスするための戦略を示します。 Google Scholar データに基づいて研究パフォーマンスを比較できるオープンソース ツール (MIT ライセンス) の一部として戦略を示します。ツールに含まれるスクレイパーは、リポジトリの運営者から同意が得られた場合にのみ使用する必要があることを強調したいと思います。私たちの経験では、研究目標が明確に説明され、プロジェクトが非営利的な性質のものであれば、同意が得られることがよくあります。 ide from improving the visibility and accessibility of scientific publications, many scientific Web repositories also assess researchers' quantitative and qualitative publication performance, e.g., by displaying metrics such as the h‐index. These metrics have become important for research institutions and other stakeholders to support impactful decision making processes such as hiring or funding decisions. However, scientific Web repositories typically offer only simple performance metrics and limited analysis options. Moreover, the data and algorithms to compute performance metrics are usually not published. Hence, it is not transparent or verifiable which publications the systems include in the computation and how the systems rank the results. Many researchers are interested in accessing the underlying scientometric raw data to increase the transparency of these systems. In this paper, we discuss the challenges and present strategies to programmatically access such data in scientific Web repositories. We demonstrate the strategies as part of an open source tool (MIT license) that allows research performance comparisons based on Google Scholar data. We would like to emphasize that the scraper included in the tool should only be used if consent was given by the operator of a repository. In our experience, consent is often given if the research goals are clearly explained and the project is of a non‐commercial nature.\\n\"]\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1045/september2016‐meschenmoser']\n",
      "\u001b[93mSummary: I'm sorry, my lady, I am unable to answer your question as there is no information in the document about the challenges in normalizing citation metrics across scientific fields.\n",
      "\n",
      "DOI: 10.1045/september2016‐meschenmoser - This paper discusses the challenges and solutions for automated content extraction from scientific web repositories.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.000\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.495\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.14296049, '10.1045/september2016‐meschenmoser')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1007/s11192-022-04289-7\\n Title: 最も多く見つかる場所を検索: 56 の書誌データベースの懲戒範囲の比較 Search where you will find most: Comparing the disciplinary coverage of 56 bibliographic databases\\nAbstract: この論文では、新しいサイエントメトリクス手法を紹介し、それを学術界で人気のある英語に焦点を当てた書誌データベースの多くの主題範囲を推定するために適用します。この方法では、クエリ結果を共通の分母として使用して、さまざまな検索エンジン、リポジトリ、デジタル ライブラリ、その他の書誌データベースを比較します。この方法は、データベース カバレッジのより小さいセットを分析する既存のサンプリング ベースのアプローチを拡張します。この調査結果では、56 のデータベースの相対的および絶対的な対象範囲が示されており、これまで入手できなかった情報が示されています。データベースの絶対的な対象範囲を知ることで、特にルックアップ検索や探索的検索に関連する、高い再現率/感度が必要な検索に最も包括的なデータベースを選択できます。データベースの相対的な対象範囲を知ることで、特に体系的な検索に関連する、高い精度と特異性が必要な検索に特化したデータベースを選択できます。この調査結果は、Google Scholar、Scopus、または Web of Science の専門分野の範囲の違いだけでなく、分析頻度が低いデータベースの違いも示しています。たとえば、研究者は、Meta (廃止)、Embase、または Europe PMC が、医学やその他の健康分野の PubMed よりも多くの記録をカバーしていることが判明したことに驚くかもしれません。これらの発見は、研究者が新しく導入されたオプションに対しても頼りになるデータベースを再評価するよう促すはずです。より包括的なデータベースを使用して検索すると、特にシステマティック レビューやメタ分析など、最も適合するデータベースの選択に特別な考慮が必要な場合に、検索結果が向上します。この比較は、図書館員やその他の情報専門家が高価なデータベース調達戦略を再評価するのにも役立ちます。機関にアクセスできない研究者は、どのオープン データベースが自分の専門分野において最も包括的である可能性が高いかを学びます。 This paper introduces a novel scientometrics method and applies it to estimate the subject coverages of many of the popular English-focused bibliographic databases in academia. The method uses query results as a common denominator to compare a wide variety of search engines, repositories, digital libraries, and other bibliographic databases. The method extends existing sampling-based approaches that analyze smaller sets of database coverages. The findings show the relative and absolute subject coverages of 56 databases—information that has often not been available before. Knowing the databases’ absolute subject coverage allows the selection of the most comprehensive databases for searches requiring high recall/sensitivity, particularly relevant in lookup or exploratory searches. Knowing the databases’ relative subject coverage allows the selection of specialized databases for searches requiring high precision/specificity, particularly relevant in systematic searches. The findings illustrate not only differences in the disciplinary coverage of Google Scholar, Scopus, or Web of Science, but also of less frequently analyzed databases. For example, researchers might be surprised how Meta (discontinued), Embase, or Europe PMC are found to cover more records than PubMed in Medicine and other health subjects. These findings should encourage researchers to re-evaluate their go-to databases, also against newly introduced options. Searching with more comprehensive databases can improve finding, particularly when selecting the most fitting databases needs particular thought, such as in systematic reviews and meta-analyses. This comparison can also help librarians and other information experts re-evaluate expensive database procurement strategies. Researchers without institutional access learn which open databases are likely most comprehensive in their disciplines.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.48550/arXiv.2303.17661', '10.1007/s11192-022-04367-w', '10.1007/s11192-022-04289-7', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: I have found two documents that address your question.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661\n",
      "Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w\n",
      "This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "96\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.250\n",
      "Recall: 0.200\n",
      "F1-Score: 0.222\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.3829032, '10.48550/arXiv.2303.17661'), (0.3650723, '10.1007/s11192-022-04367-w'), (0.14534597, '10.1007/s11192-022-04289-7'), (0.10071066, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', \"DOI: 10.48550/arXiv.2404.13948\\n Title: RAG の背中を打ち砕いた ypos: 低レベルの摂動を介して野生のドキュメントをシミュレートすることによる、RAG パイプラインへの遺伝的攻撃 ypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\\nAbstract: 最近の大規模言語モデル (LLM) の適用可能性がさまざまなドメインや現実世界のアプリケーションに拡大するにつれて、その堅牢性がますます重要になってきています。検索拡張生成 (RAG) は、LLM の制限に対処するための有望なソリューションですが、RAG の堅牢性に関する既存の研究では、RAG コンポーネント間の相互接続関係や、軽微なテキスト エラーなど、現実のデータベースに蔓延する潜在的な脅威が見落とされていることがよくあります。この研究では、RAG の堅牢性を評価する際にまだ解明されていない 2 つの側面を調査します。1 つは低レベルの摂動によるノイズの多いドキュメントに対する脆弱性、2 つは RAG の堅牢性の全体的な評価です。さらに、これらの側面をターゲットとした新しい攻撃方法である RAG への遺伝的攻撃を紹介します。具体的には、GARAG は、各コンポーネント内の脆弱性を明らかにし、ノイズの多いドキュメントに対してシステム全体の機能をテストするように設計されています。 \\\\textit{GARAG} を標準 QA データセットに適用し、さまざまな取得者と LLM を組み込むことで、RAG の堅牢性を検証します。実験結果は、GARAG が一貫して高い攻撃成功率を達成していることを示しています。また、各コンポーネントのパフォーマンスとその相乗効果を著しく損なうため、わずかなテキストの不正確さが現実世界の RAG システムを混乱させる大きなリスクを浮き彫りにします。 The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1 vulnerability to noisy documents through low-level perturbations and 2 a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG, which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.48550/arXiv.2406.13213', '10.1145/3637528.3671470', '10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2404.13948']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a promising solution to the limitations of Large Language Models (LLMs), such as hallucinations, outdated knowledge, and non-transparent reasoning processes. RAG enhances the accuracy and credibility of generated content by incorporating knowledge from external databases, allowing for continuous updates and domain-specific information integration.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - This paper provides a comprehensive review of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG. It examines the tripartite foundation of RAG frameworks: retrieval, generation, and augmentation techniques. The study highlights state-of-the-art technologies in each component and introduces an up-to-date evaluation framework and benchmark.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - This research introduces Multi-Meta-RAG, a method to improve RAG for multi-hop queries using database filtering with LLM-extracted metadata. It demonstrates significant improvements in the MultiHop-RAG benchmark.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470 - The study reviews Retrieval-Augmented Large Language Models (RA-LLMs), emphasizing their ability to provide reliable and up-to-date external knowledge. It discusses current limitations and future research directions.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16 - This paper introduces RAGAs, a reference-free evaluation framework for RAG pipelines. It addresses the challenges of evaluating RAG architectures and provides metrics for assessing retrieval, generation, and overall quality.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.13948 - This research investigates the robustness of RAG against noisy documents and minor textual errors. It introduces GARAG, a genetic attack method, to test RAG's vulnerability and overall system functionality.\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions or need additional information, please let me know.\n",
      "96\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.400\n",
      "Recall: 0.400\n",
      "F1-Score: 0.400\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.5488434, '10.48550/arXiv.2312.10997'), (0.52685463, '10.48550/arXiv.2406.13213'), (0.4893821, '10.1145/3637528.3671470'), (0.4833807, '10.18653/v1/2024.eacl-demo.16'), (0.3458787, '10.48550/arXiv.2404.13948')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n', \"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1145/2964909', '10.1109/ADL.1998.670425', '10.1177/09610006241239080', '10.5860/crl.86.1.101', '10.48550/arXiv.2303.17661']\n",
      "\u001b[93mSummary: Metadata quality assessment is a critical area of research, with studies focusing on various aspects such as completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness. However, there is a lack of consensus on the exact definition and measurement of these dimensions, highlighting the need for further investigation.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - This study emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools. It identifies the most commonly measured dimensions of metadata quality, but notes the absence of agreed-upon definitions and measurements.\n",
      "\n",
      "DOI: 10.1145/2964909 - This work aims to develop a generic metadata quality assessment framework for various Open Data portals, addressing concerns related to the quality of data sources and corresponding metadata.\n",
      "\n",
      "DOI: 10.1109/ADL.1998.670425 - This study discusses the application of qualitative and quantitative content analysis techniques to assess metadata records, contributing to a developing dialog about assessing metadata quality.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 - This research explores how metadata quality, consistency, and completeness impact individuals and communities, identifying unique issues and categorizing them to better understand the interaction between metadata and communities.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661 - This study proposes MetaEnhance, a framework utilizing state-of-the-art artificial intelligence methods to improve metadata quality for electronic theses and dissertations, achieving impressive results in error detection and correction.\n",
      "\n",
      "My lady, these studies collectively highlight the challenges and advancements in metadata quality assessment, emphasizing the need for further research and flexible assessment tools to ensure better metadata quality assurance measures.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in metadata quality assessment?']:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.000\n",
      "Accuracy: 0.896\n",
      "Balanced accuracy: 0.473\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.6636041, '10.1145/2964909'), (0.4874788, '10.1109/ADL.1998.670425'), (0.38741276, '10.1177/09610006241239080'), (0.37305376, '10.5860/crl.86.1.101'), (0.2738236, '10.48550/arXiv.2303.17661')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.1080/19386389.2011.570654\\n Title: メタデータ レコードの問題の分析 An Analysis of Problems in Metadata Records\\nAbstract: メタデータはデジタル ライブラリにおいて重要な役割を果たします。しかし、有用であるためには、メタデータ レコードに問題がなくなければなりません。メタデータに問題があると、リソースが正しく表現されず、ユーザーはメタデータのメリットを享受できなくなります。このような問題を排除しないにしても、最小限に抑えるには、メタデータ レコードで発生する可能性のある問題の種類を理解することが不可欠です。この論文では、文献で報告されているメタデータ レコードで見つかった問題を比較および分析します。メタデータの問題の 5 つのカテゴリを特定できることがわかります。これらは、不正な値、不正な要素、情報の欠落、情報損失、および一貫性のない値の表現の問題です。これらの問題がメタデータによって提供できるサービスに悪影響を与えることを考慮すると、メタデータの使用から得られる利点と、メタデータ レコードの作成に費やされるコストと労力のバランスが確保されるように、予防または是正措置を講じる必要があります。 Metadata plays an important role in digital libraries. But to be useful, metadata records must be problem free. When problems are present in the metadata, resources are not correctly represented and users are not able to reap the benefits of metadata. To minimize, if not eliminate, such problems, it is essential to understand the kinds of problems that can occur in metadata records. In this paper, problems found in metadata records as reported in the literature are compared and analyzed. It is found that five categories of metadata problems can be identified. These are the problems of Incorrect Values, Incorrect Elements, Missing Information, Information Loss, and Inconsistent Value Representation. Given that these problems are detrimental to the services that can be provided by metadata, preventive or corrective measures need to be put in place so as to ensure that the benefits derived from using metadata balance the costs and efforts spent in the creation of metadata records.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1007/s11192-023-04923-y\\n Title: OpenAlex に機関が存在しない: 考えられる理由、影響、および解決策 Missing institutions in OpenAlex: possible reasons, implications, and solutions\\nAbstract: オープン サイエンスの到来により、高いデータ品質を備えたオープン データ プラットフォームが必要になります。 2022 年 1 月に開始されたグローバル研究システムの完全にオープンなカタログである OpenAlex は、データへの簡単なアクセスと、量的科学研究で広く使用されている幅広いデータ範囲という 2 つの主な利点を備えています。注目すべきことに、OpenAlex はライデン大学ランキングの重要なデータ ソースとして採用されています。ただし、OpenAlex の雑誌記事メタデータには機関が欠落しているという深刻なデータ品質の問題があります。この研究では、完全な機関情報 (FII)、部分的に欠落している機関情報 (PMII)、および完全に欠落している機関情報 (CMII) という 3 つのタイプの機関情報を定義することにより、問題の考えられる理由とその結果と解決策を調査します。私たちの結果は、OpenAlex のジャーナル記事の 60% 以上で機関の欠落の問題が発生していることを示しています。この問題は、初期のメタデータや社会科学、人文科学で特に蔓延しています。データのサブサンプルを使用して、問題の考えられる理由、歪んだ結果をもたらす可能性のあるリスク、行方不明の機関の問題に対する考えられる解決策をさらに調査します。その目的は、オープン リソースにおけるデータ品質の向上の重要性を高め、それによって量的科学の研究やより広範な文脈においてオープン リソースの責任ある使用をサポートすることです。 The advent of open science calls for open data platforms with high data quality. As a fully open catalog of the global research system launched in January 2022, OpenAlex features two main advantages of easy data accessibility and broad data coverage, which has been widely used in quantitative science studies. Remarkably, OpenAlex is adopted as an important data source for Leiden university ranking. However, there is a severe data quality problem of missing institutions in journal article metadata in OpenAlex. This study investigates the possible reasons for the problem and its consequences and solutions by defining three types of institutional information—full institutional information (FII), partially missing institutional information (PMII) and completely missing institutional information (CMII). Our results show that the problem of missing institutions occurs in more than 60% of the journal articles in OpenAlex. The problem is particularly widespread in metadata from the early years and in the social sciences and humanities. Using sub-samples of the data, we further explore the possible reasons for the problem, the risk it might represent for distorted results, and possible solutions to the problem of missing institutions. The aim is to raise the importance of data quality improvements in open resources, and thus to support the responsible use of open resources in quantitative science studies and also in broader contexts.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.5860/crl.86.1.101', '10.48550/arXiv.2303.17661', '10.1080/19386389.2011.570654', '10.1177/09610006241239080', '10.1007/s11192-023-04923-y']\n",
      "\u001b[93mSummary: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, metadata often exhibits incomplete, inconsistent, and incorrect values. To address this, methods to automatically detect, correct, and canonicalize scholarly metadata are being investigated. Additionally, metadata records must be problem-free to be useful, and preventive or corrective measures are necessary to minimize issues. Metadata quality assessment commonly focuses on dimensions like completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness, but further investigation is needed for less commonly assessed dimensions. Lastly, open data platforms like OpenAlex face severe data quality problems, such as missing institutions in journal article metadata, highlighting the need for data quality improvements in open resources.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661 - Metadata quality is essential for the discovery of digital objects through digital library interfaces. However, metadata often suffers from incomplete, inconsistent, and incorrect values. To tackle this, researchers are exploring methods to automatically detect, correct, and canonicalize scholarly metadata, using electronic theses and dissertations (ETDs) as a case study. They propose MetaEnhance, a framework leveraging state-of-art artificial intelligence methods to improve metadata quality. Evaluation shows promising results in error detection and correction across multiple fields.\n",
      "\n",
      "DOI: 10.1080/19386389.2011.570654 - Metadata plays a vital role in digital libraries, but its usefulness depends on problem-free records. Issues like incorrect values, incorrect elements, missing information, information loss, and inconsistent value representation can hinder metadata's effectiveness. To mitigate these problems, it's essential to understand their types and implement preventive or corrective measures. This ensures that the benefits of metadata outweigh the costs and efforts of creating metadata records.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - This scoping review aims to identify commonly measured dimensions of metadata quality in existing studies. It finds that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are frequently assessed dimensions. However, there is no consensus on their definitions and measurements, indicating a need for further research into less commonly assessed dimensions. The study also highlights the importance of metadata quality research and the need for flexible assessment tools to enhance metadata quality assurance.\n",
      "\n",
      "DOI: 10.1007/s11192-023-04923-y - With the rise of open science, open data platforms like OpenAlex are crucial for easy data access and broad coverage. However, OpenAlex faces a significant data quality issue with missing institutions in journal article metadata. This study investigates the reasons, consequences, and solutions for this problem by defining three types of institutional information. Results show that over 60% of journal articles in OpenAlex have missing institution information, particularly in early metadata and social sciences/humanities. The aim is to improve data quality in open resources, supporting their responsible use in quantitative science and beyond.\n",
      "\n",
      "My lady, I hope this summary addresses your query. If you have any further questions or need additional information, feel free to ask.\n",
      "96\n",
      "For query: ['What are the key challenges and potential solutions for improving metadata quality?']:\n",
      "Precision: 0.800\n",
      "Recall: 0.800\n",
      "F1-Score: 0.800\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.895\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.5573882, '10.5860/crl.86.1.101'), (0.46701634, '10.48550/arXiv.2303.17661'), (0.43467787, '10.1080/19386389.2011.570654'), (0.40952834, '10.1177/09610006241239080'), (0.29672605, '10.1007/s11192-023-04923-y')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 14 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", \"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", \"DOI: 10.48550/arXiv.2505.18247\\n Title: MetaGen Blended RAG: 専門分野の質問応答でゼロショットの精度を解放 MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索拡張生成 (RAG) は、ファイアウォールの背後に隔離されていることが多く、事前トレーニング中に LLM が認識できない複雑で特殊な用語が豊富に含まれる、ドメイン固有のエンタープライズ データセットに苦戦します。医学、ネットワーキング、法律などの分野にわたるセマンティックのばらつきが RAG のコンテキストの精度を妨げる一方、ソリューションを微調整するのはコストがかかり、時間がかかり、新しいデータが出現したときの汎用性が欠けています。微調整を行わずにレトリーバーでゼロショット精度を達成することは依然として重要な課題です。私たちは、メタデータ生成パイプラインと密ベクトルと疎ベクトルを使用したハイブリッド クエリ インデックスを通じてセマンティック リトリーバーを強化する新しいエンタープライズ検索アプローチである「MetaGen Blended RAG」を紹介します。主要な概念、トピック、頭字語を活用することで、私たちのメソッドはメタデータを強化したセマンティック インデックスと強化されたハイブリッド クエリを作成し、微調整することなく堅牢でスケーラブルなパフォーマンスを実現します。生物医学の PubMedQA データセットでは、MetaGen Blended RAG は 82% の検索精度と 77% の RAG 精度を達成し、以前のすべてのゼロショット RAG ベンチマークを上回り、そのデータセット上の微調整モデルに匹敵するだけでなく、SQuAD や NQ などのデータセットでも優れています。このアプローチは、特殊なドメイン全体にわたって比類のない一般化を備えたセマンティック検索ツールを構築する新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\", 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1016/j.caeai.2025.100417', '10.48550/arXiv.2505.18247', '10.1007/s44427-025-00006-3', '10.1145/3637528.3671470']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) systems face challenges such as hallucination, outdated knowledge, and non-transparent reasoning processes, but advancements like MetaGen Blended RAG and Ragas frameworks are enhancing their capabilities.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - This paper discusses the challenges faced by Large Language Models (LLMs), such as hallucination, outdated knowledge, and non-transparent reasoning processes. It highlights RAG as a promising solution by incorporating external knowledge, improving accuracy, and enabling continuous updates. The paper reviews RAG paradigms and provides an evaluation framework.\n",
      "\n",
      "DOI: 10.1016/j.caeai.2025.100417 - Focusing on educational applications, this paper addresses challenges like hallucination and static knowledge in LLMs. It explores RAG's role in improving factual accuracy and dynamic updates, with a focus on practical applications in education.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247 - This study introduces MetaGen Blended RAG, a novel approach to enhance semantic retrievers for domain-specific datasets. It achieves high accuracy without fine-tuning, addressing challenges in specialized domains.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3 - This research evaluates open-source LLMs in RAG systems using the Ragas framework. It highlights the challenges in assessing retrieval and generation components and provides insights for optimizing RAG implementations.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470 - This survey reviews RAG's role in providing reliable external knowledge and its integration with LLMs. It discusses current limitations and future research directions for Retrieval-Augmented Large Language Models (RA-LLMs).\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions or need additional information, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in Retrieval-Augmented Generation (RAG) systems?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9224337, '10.48550/arXiv.2312.10997'), (0.87681407, '10.1016/j.caeai.2025.100417'), (0.7272211, '10.48550/arXiv.2505.18247'), (0.6641271, '10.1007/s44427-025-00006-3'), (0.6102915, '10.1145/3637528.3671470')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 15 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', \"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.1177/09610006241239080', '10.1109/ADL.1998.670425', '10.5281/zenodo.14006424']\n",
      "\u001b[93mSummary: Two studies examined the abstract in metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286\n",
      "This study compared the amount of metadata and the completeness degree of research publications in new academic databases. Seven characteristics were analyzed, including the abstract.\n",
      "\n",
      "DOI: 10.1177/09610006241239080\n",
      "This scoping review identified the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The co-occurrence analysis was made on the title and abstract of selected articles.\n",
      "\n",
      "Can I help you with anything else, my lady?\n",
      "96\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.250\n",
      "Recall: 0.200\n",
      "F1-Score: 0.222\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.63179934, '10.1162/qss_a_00286'), (0.58919704, '10.1177/09610006241239080'), (0.55420566, '10.1109/ADL.1998.670425'), (0.18688062, '10.5281/zenodo.14006424')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1108/JD-10-2022-0234\\n Title: すべての学術分野にわたる引用と参照の習慣の分析: 書誌の参照と引用の実践におけるアプローチと傾向 An analysis of citing and referencing habits across all scholarly disciplines: approaches and trends in bibliographic referencing and citing practices\\nAbstract: この研究で、著者らは、学術文献における引用および参照の誤りについて現在考えられる原因を特定し、スウィートランド氏が1989年の論文で提供したスナップショットから何かが変わったかどうかを比較したいと考えている。,著者らは、27の主題分野にわたる147のジャーナルに掲載された729件の論文から、参考要素、すなわち書誌的参照、言及、引用、およびそれぞれの本文中の参照ポインタを分析した。,分析の結果は、書誌的事項が指摘された。著者らの知る限り、この研究は、Sweetland (1989) 以来、文献における参照および引用の慣行における誤りを分析したものとしては、最近入手可能な最良のものである。 In this study, the authors want to identify current possible causes for citing and referencing errors in scholarly literature to compare if something changed from the snapshot provided by Sweetland in his 1989 paper.,The authors analysed reference elements, i.e. bibliographic references, mentions, quotations and respective in-text reference pointers, from 729 articles published in 147 journals across the 27 subject areas.,The outcomes of the analysis pointed out that bibliographic errors have been perpetuated for decades and that their possible causes have increased, despite the encouraged use of technological facilities, i.e. the reference managers.,As far as the authors know, the study is the best recent available analysis of errors in referencing and citing practices in the literature since Sweetland (1989).\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1108/JD-10-2022-0234', '10.1007/s11192-022-04367-w', '10.1162/qss_a_00112']\n",
      "\u001b[93mSummary: I have found three studies that examined citations.\n",
      "\n",
      "DOI: 10.1108/JD-10-2022-0234 - This study analysed reference elements, i.e. bibliographic references, mentions, quotations and respective in-text reference pointers, from 729 articles published in 147 journals across 27 subject areas.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w - This study analysed open bibliographic metadata available in Crossref, highlighting which publishers were responsible for mistakes and how many of these incorrect DOIs could be corrected through automatic processes.\n",
      "\n",
      "DOI: 10.1162/qss_a_00112 - This study compared five multidisciplinary bibliographic data sources, including Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considered scientific documents from the period 2008–2017 covered by these data sources.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 1.000\n",
      "Recall: 0.600\n",
      "F1-Score: 0.750\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.800\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.3570971, '10.1108/JD-10-2022-0234'), (0.21608633, '10.1007/s11192-022-04367-w'), (0.13627496, '10.1162/qss_a_00112')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1007/s11192-022-04367-w', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: Crossref is a sustainable source of community-owned scholarly metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022 - Crossref is a sustainable source of community-owned scholarly metadata. It contains over 106 million records and expands at an average rate of 11% a year. It has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers.\n",
      "\n",
      "DOI: 10.1162/qss_a_00210 - Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w - This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref.\n",
      "\n",
      "DOI: 10.5281/ZENODO.6188748 - This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.800\n",
      "Recall: 0.800\n",
      "F1-Score: 0.800\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.895\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.7781275, '10.1162/qss_a_00022'), (0.72395444, '10.1162/qss_a_00210'), (0.55623144, '10.31222/osf.io/smxe5'), (0.44883752, '10.1007/s11192-022-04367-w'), (0.17410146, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n', 'DOI: 10.1007/s11192-022-04289-7\\n Title: 最も多く見つかる場所を検索: 56 の書誌データベースの懲戒範囲の比較 Search where you will find most: Comparing the disciplinary coverage of 56 bibliographic databases\\nAbstract: この論文では、新しいサイエントメトリクス手法を紹介し、それを学術界で人気のある英語に焦点を当てた書誌データベースの多くの主題範囲を推定するために適用します。この方法では、クエリ結果を共通の分母として使用して、さまざまな検索エンジン、リポジトリ、デジタル ライブラリ、その他の書誌データベースを比較します。この方法は、データベース カバレッジのより小さいセットを分析する既存のサンプリング ベースのアプローチを拡張します。この調査結果では、56 のデータベースの相対的および絶対的な対象範囲が示されており、これまで入手できなかった情報が示されています。データベースの絶対的な対象範囲を知ることで、特にルックアップ検索や探索的検索に関連する、高い再現率/感度が必要な検索に最も包括的なデータベースを選択できます。データベースの相対的な対象範囲を知ることで、特に体系的な検索に関連する、高い精度と特異性が必要な検索に特化したデータベースを選択できます。この調査結果は、Google Scholar、Scopus、または Web of Science の専門分野の範囲の違いだけでなく、分析頻度が低いデータベースの違いも示しています。たとえば、研究者は、Meta (廃止)、Embase、または Europe PMC が、医学やその他の健康分野の PubMed よりも多くの記録をカバーしていることが判明したことに驚くかもしれません。これらの発見は、研究者が新しく導入されたオプションに対しても頼りになるデータベースを再評価するよう促すはずです。より包括的なデータベースを使用して検索すると、特にシステマティック レビューやメタ分析など、最も適合するデータベースの選択に特別な考慮が必要な場合に、検索結果が向上します。この比較は、図書館員やその他の情報専門家が高価なデータベース調達戦略を再評価するのにも役立ちます。機関にアクセスできない研究者は、どのオープン データベースが自分の専門分野において最も包括的である可能性が高いかを学びます。 This paper introduces a novel scientometrics method and applies it to estimate the subject coverages of many of the popular English-focused bibliographic databases in academia. The method uses query results as a common denominator to compare a wide variety of search engines, repositories, digital libraries, and other bibliographic databases. The method extends existing sampling-based approaches that analyze smaller sets of database coverages. The findings show the relative and absolute subject coverages of 56 databases—information that has often not been available before. Knowing the databases’ absolute subject coverage allows the selection of the most comprehensive databases for searches requiring high recall/sensitivity, particularly relevant in lookup or exploratory searches. Knowing the databases’ relative subject coverage allows the selection of specialized databases for searches requiring high precision/specificity, particularly relevant in systematic searches. The findings illustrate not only differences in the disciplinary coverage of Google Scholar, Scopus, or Web of Science, but also of less frequently analyzed databases. For example, researchers might be surprised how Meta (discontinued), Embase, or Europe PMC are found to cover more records than PubMed in Medicine and other health subjects. These findings should encourage researchers to re-evaluate their go-to databases, also against newly introduced options. Searching with more comprehensive databases can improve finding, particularly when selecting the most fitting databases needs particular thought, such as in systematic reviews and meta-analyses. This comparison can also help librarians and other information experts re-evaluate expensive database procurement strategies. Researchers without institutional access learn which open databases are likely most comprehensive in their disciplines.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.48550/arXiv.2508.18620', '10.1162/qss_a_00286', '10.5281/zenodo.14006424', '10.1007/s11192-022-04289-7']\n",
      "\u001b[93mSummary: OpenAlex is a newly launched bibliographic database that promises comprehensive, inclusive, and open-source research information.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 - This paper evaluates the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 - This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - This study compares the amount of metadata and the completeness degree of research publications in new academic databases, including OpenAlex.\n",
      "\n",
      "DOI: 10.5281/zenodo.14006424 - This paper analyses OpenAlex's coverage and metadata availability of African-based publications, comparing it with Scopus, WoS, and African Journals Online (AJOL).\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need more details, please let me know.\n",
      "96\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.200\n",
      "Recall: 0.200\n",
      "F1-Score: 0.200\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.9108078, '10.48550/arXiv.2409.10633'), (0.42706275, '10.48550/arXiv.2508.18620'), (0.41563448, '10.1162/qss_a_00286'), (0.19475074, '10.5281/zenodo.14006424'), (0.16595806, '10.1007/s11192-022-04289-7')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: COVID-19 研究への資金提供: オープン データ インフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 資金提供機関が提供する資金の成果を分析するには、資金提供によって得られた出版物を追跡できることが不可欠です。私たちは、Crossref での資金調達データのオープンな利用可能性を調査し、新型コロナウイルス感染症 (COVID-19) に関連する研究を報告する出版物の資金調達データに焦点を当てています。また、Scopus と Web of Science という 2 つの独自の書誌データベースで利用可能な資金調達データとの比較も示します。私たちの分析では、Crossref の資金調達データの範囲が限られていることが明らかになりました。また、特に Scopus における資金調達データの品質に関連する問題も示しています。 Crossref での資金調達データのオープンな可用性を向上させるための推奨事項を提供します。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.5281/zenodo.14006424', '10.31222/osf.io/smxe5', '10.1162/qss_a_00286', '10.1162/qss_a_00212']\n",
      "\u001b[93mSummary: Several papers address funding metadata, its availability, or its analysis in scholarly databases.\n",
      "\n",
      "DOI: 10.1162/qss_a_00210 - This paper investigates the availability and completeness of open funder metadata, specifically focusing on publications funded by the Dutch Research Council (NWO). It highlights the challenges in assessing the coverage of funder metadata due to the unknown number of articles resulting from funded research. The study found that only 67% of articles funded by NWO contain funding information in Crossref, with variations in acknowledging NWO as the funder name or linking Funder IDs to NWO.\n",
      "\n",
      "DOI: 10.5281/zenodo.14006424 - This paper analyzes the coverage and metadata availability of African publications in OpenAlex, comparing it with Scopus, Web of Science (WoS), and African Journals Online (AJOL). The study found that OpenAlex offers the most extensive publication coverage but performs worse regarding funder information. However, metadata availability in OpenAlex is better for publications also indexed in Scopus or WoS.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This paper presents an overview of the availability of six metadata elements in Crossref, including funding information. The analysis shows that the availability of these metadata elements has improved over time, but many publishers need to make additional efforts to realize full openness of bibliographic metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - This study compares the amount of metadata and completeness degree of research publications in eight free-access scholarly databases, including Dimensions, Google Scholar, and OpenAlex. The results show that third-party databases have more metadata quality and a higher completeness rate compared to academic search engines.\n",
      "\n",
      "DOI: 10.1162/qss_a_00212 - This paper focuses on the open availability of funding data in Crossref, specifically for publications related to COVID-19 research. It compares funding data in Crossref with two proprietary bibliometric databases, Scopus and Web of Science, and provides recommendations for improving the open availability of funding data in Crossref.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need more details, please let me know.\n",
      "96\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.800\n",
      "Recall: 1.000\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.990\n",
      "Balanced accuracy: 0.995\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.68798697, '10.1162/qss_a_00210'), (0.48616135, '10.5281/zenodo.14006424'), (0.4024631, '10.31222/osf.io/smxe5'), (0.31545174, '10.1162/qss_a_00286'), (0.30762884, '10.1162/qss_a_00212')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", \"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.48550/arXiv.2312.10997', '10.1145/3637528.3671470', '10.6109/jkiice.2023.27.12.1489', '10.1007/s44427-025-00006-3']\n",
      "\u001b[93mI'm sorry, my lady, I am unable to answer your question.\n",
      "\n",
      "I have found several papers that discuss Retrieval-Augmented Generation (RAG) but none of them mention applications.\n",
      "\n",
      "Would you like me to answer a different question?\n",
      "96\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) when used in applications?']:\n",
      "Precision: 0.200\n",
      "Recall: 0.200\n",
      "F1-Score: 0.200\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 0\n",
      "Documents score: [(0.8958969, '10.1016/j.caeai.2025.100417'), (0.81418616, '10.48550/arXiv.2312.10997'), (0.7173491, '10.1145/3637528.3671470'), (0.69672287, '10.6109/jkiice.2023.27.12.1489'), (0.6675167, '10.1007/s44427-025-00006-3')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1007/s11192-022-04367-w', '10.1162/qss_a_00112']\n",
      "\u001b[93mSummary: Crossref is a community-owned source of scholarly metadata that is openly available and contains over 106 million records.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.95628047, '10.1162/qss_a_00022'), (0.47548413, '10.1162/qss_a_00210'), (0.44680908, '10.31222/osf.io/smxe5'), (0.21246108, '10.1007/s11192-022-04367-w'), (0.19260046, '10.1162/qss_a_00112')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.29173/cais1943\\n Title: OpenAlex と Web of Science の間のドキュメント タイプの不一致を調査する Investigating Document Type Discrepancies between OpenAlex and the Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された OpenAlex は、そのオープン性と広範なカバレッジで際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この研究は、ドキュメント タイプの精度に焦点を当て、OpenAlex と WoS における作品のメタデータの品質を評価することを目的としています。 OpenAlex と WoS の両方でインデックス付けされている出版物の 4% 以上が研究論文またはレビューとして誤って分類されているようであり、これらのエラーの大部分 (約 97%) が OpenAlex で発生していることが観察されています。この研究は、文書タイプの不一致や誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, launched in 2022, stands out for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinder its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess the metadata quality of works in OpenAlex and WoS, focusing on document type accuracy. We observe that over 4% of the publications indexed in both OpenAlex and WoS appear to be misclassified as research articles or reviews, and that the vast majority (about 97%) of these errors occur in OpenAlex. By addressing discrepancies and misattributions in document types this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', \"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.3145/epi.2023.mar.09\\n Title: Microsoft Academic Graph から OpenAlex に切り替える場合、書誌情報学に関連するメタデータはどれが同じで、どれが異なりますか? Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\\nAbstract: Microsoft Academic Graph (MAG) の廃止の発表に伴い、非営利団体 OurResearch は OpenAlex という名前で同様のリソースを提供すると発表しました。したがって、最新の MAG スナップショットと初期の OpenAlex スナップショットの書誌学的分析に関連するメタデータを比較します。 MAG の実質的にすべての著作物は、書誌データの出版年、巻数、最初と最後のページ、DOI、および引用分析の重要な要素である参考文献の数を保存しながら OpenAlex に転送されました。 MAG ドキュメントの 90% 以上が OpenAlex に同等のドキュメント タイプを持っています。残りのうち、特に OpenAlex 文書タイプの Journal-article および Book-chapter への再分類は正しいようで、その割合は 7% 以上に達しており、文書タイプの仕様は MAG から OpenAlex に大幅に改善されました。書誌学的関連メタデータの別の項目として、MAG と OpenAlex における紙ベースの主題分類を調べました。 OpenAlex では、MAG よりもはるかに多くの主題分類が割り当てられているドキュメントが見つかりました。第 1 レベルと第 2 レベルでは、分類構造はほぼ同じです。主題の再分類に関するデータを表とグラフの両方のレベルで表示します。分野で正規化された書誌学的評価に対する豊富な主題の再分類の影響の評価は、この論文の範囲には含まれません。この未解決の質問とは別に、OpenAlex は全体的に、文書タイプの割り当てがより広範囲にカバーされているため、2021 年より前の出版物については少なくとも MAG と同等かそれ以上に書誌学的分析に適しているようです。 With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2508.18620', '10.29173/cais1943', '10.48550/arXiv.2409.10633', '10.3145/epi.2023.mar.09']\n",
      "\u001b[93mSummary: OpenAlex is a bibliographic database that launched in 2022, and is distinguished by its openness and extensive coverage.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. However, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 - OpenAlex may reduce or eliminate barriers to accessing bibliometric data, but one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata.\n",
      "\n",
      "DOI: 10.29173/cais1943 - OpenAlex stands out for its openness and extensive coverage. However, over 4% of the publications indexed in both OpenAlex and WoS appear to be misclassified as research articles or reviews, and the vast majority (about 97%) of these errors occur in OpenAlex.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 - OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages.\n",
      "\n",
      "DOI: 10.3145/epi.2023.mar.09 - OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.8455478, '10.48550/arXiv.2404.17663'), (0.7178241, '10.48550/arXiv.2508.18620'), (0.69024646, '10.29173/cais1943'), (0.64344186, '10.48550/arXiv.2409.10633'), (0.6353361, '10.3145/epi.2023.mar.09')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1007/s11192-015-1765-5\\n Title: Web of Science と Scopus のジャーナルの報道: 比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌学的手法は、研究の評価など、さまざまな目的で複数の分野で使用されています。ほとんどの書誌学的分析には、トムソン・ロイターの Web of Science (WoS) とエルゼビアの Scopus というデータ ソースが共通しています。この調査の目的は、これら 2 つのデータベースのジャーナルの対象範囲を説明し、特定の分野、出版国、言語が過小評価されているかどうかを評価することです。これを行うために、WoS (13,605 ジャーナル) および Scopus (20,346 ジャーナル) のアクティブな学術ジャーナルの範囲を、ウルリッヒの広範な定期刊行物ディレクトリ (63,013 ジャーナル) と比較しました。結果は、WoS または Scopus を研究評価に使用すると、社会科学、芸術、人文科学に不利益をもたらす、自然科学と工学、生物医学研究に有利なバイアスを導入する可能性があることを示しています。同様に、英語のジャーナルが過大評価され、他の言語に損害を与えています。どちらのデータベースもこれらのバイアスを共有していますが、その範囲は大きく異なります。そのため、書誌情報分析の結果は使用するデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈において、特に異なる分野、機関、国、または言語を比較する場合には、WoS と Scopus を慎重に使用する必要があることを示唆しています。書誌学コミュニティは、分野固有の引用インデックスや全国的な引用インデックスなど、WoS や Scopus ではカバーされていない科学的成果を含む手法や指標を開発する努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.29173/cais1943\\n Title: OpenAlex と Web of Science の間のドキュメント タイプの不一致を調査する Investigating Document Type Discrepancies between OpenAlex and the Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された OpenAlex は、そのオープン性と広範なカバレッジで際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この研究は、ドキュメント タイプの精度に焦点を当て、OpenAlex と WoS における作品のメタデータの品質を評価することを目的としています。 OpenAlex と WoS の両方でインデックス付けされている出版物の 4% 以上が研究論文またはレビューとして誤って分類されているようであり、これらのエラーの大部分 (約 97%) が OpenAlex で発生していることが観察されています。この研究は、文書タイプの不一致や誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, launched in 2022, stands out for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinder its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess the metadata quality of works in OpenAlex and WoS, focusing on document type accuracy. We observe that over 4% of the publications indexed in both OpenAlex and WoS appear to be misclassified as research articles or reviews, and that the vast majority (about 97%) of these errors occur in OpenAlex. By addressing discrepancies and misattributions in document types this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.1162/qss_a_00211\\n Title: 国際的な研究協力を測定するための書誌データソースの品質を評価する Assessing the quality of bibliographic data sources for measuring international research collaboration\\nAbstract: 国際研究協力 (IRC) の測定は、さまざまな研究評価タスクに不可欠ですが、どのデータソースを使用するかなど、さまざまな測定決定の影響については十分に研究されていません。データ ソースの選択が IRC 測定に及ぼす影響をより深く理解するために、利用可能なディメンションを確認して選択し、適切な計算可能なメトリクスを設計することにより、書誌データに特化したデータ品質評価フレームワークを設計および実装し、次にそれを書誌データの 4 つの一般的なソース (Microsoft Academic Graph、Web of Science (WoS)、Dimensions、ACM Digital Library) に適用してフレームワークを検証します。このフレームワークの検証が成功した場合、それが Wang と Strong (1996) によって提案された情報品質の一般的な概念フレームワークと一致しており、調査された情報源の品質の違いが適切に特定されることが示唆されます。フレームワークを適用すると、検討したセットの中で WS が全体的な品質が最も高いことがわかります。そして、品質の違いは主にデータ ソースの編成方法によって説明できると考えられます。私たちの研究は、研究者がこの IRC 測定ツールを研究に適用できるようにする方法論的な貢献で構成されており、書誌データの 4 つの一般的な情報源とその IRC 測定への影響をさらに特徴付けることによって実証的な貢献をしています。 Measuring international research collaboration (IRC) is essential to various research assessment tasks but the effect of various measurement decisions, including which data sources to use, has not been thoroughly studied. To better understand the effect of data source choice on IRC measurement, we design and implement a data quality assessment framework specifically for bibliographic data by reviewing and selecting available dimensions and designing appropriate computable metrics, and then validate the framework by applying it to four popular sources of bibliographic data: Microsoft Academic Graph, Web of Science (WoS), Dimensions, and the ACM Digital Library. Successful validation of the framework suggests it is consistent with the popular conceptual framework of information quality proposed by Wang and Strong (1996) and adequately identifies the differences in quality in the sources examined. Application of the framework reveals that WoS has the highest overall quality among the sets considered; and that the differences in quality can be explained primarily by how the data sources are organized. Our study comprises a methodological contribution that enables researchers to apply this IRC measurement tool in their studies and makes an empirical contribution by further characterizing four popular sources of bibliographic data and their impact on IRC measurement.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1007/s11192-015-1765-5', '10.48550/arXiv.2508.18620', '10.29173/cais1943', '10.1162/qss_a_00112', '10.1162/qss_a_00211']\n",
      "\u001b[93mSummary: I'm sorry, my lady, I could not find any information about the strengths and weaknesses of Web of Science (WoS) as a bibliometric database.\n",
      "\n",
      "DOI: 10.1007/s11192-015-1765-5 - WoS and Scopus are the most common data sources for bibliometric analyses. This study compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favour Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 - WoS was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors.\n",
      "\n",
      "DOI: 10.29173/cais1943 - WoS was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, launched in 2022, stands out for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess the metadata quality of works in OpenAlex and WoS, focusing on document type accuracy. We observe that over 4% of the publications indexed in both OpenAlex and WoS appear to be misclassified as research articles or reviews, and that the vast majority (about 97%) of these errors occur in OpenAlex.\n",
      "\n",
      "DOI: 10.1162/qss_a_00112 - This study presents a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\n",
      "\n",
      "DOI: 10.1162/qss_a_00211 - To better understand the effect of data source choice on IRC measurement, we design and implement a data quality assessment framework specifically for bibliographic data by reviewing and selecting available dimensions and designing appropriate computable metrics, and then validate the framework by applying it to four popular sources of bibliographic data: Microsoft Academic Graph, Web of Science (WoS), Dimensions, and the ACM Digital Library. Successful validation of the framework suggests it is consistent with the popular conceptual framework of information quality proposed by Wang and Strong (1996) and adequately identifies the differences in quality in the sources examined. Application of the framework reveals that WoS has the highest overall quality among the sets considered; and that the differences in quality can be explained primarily by how the data sources are organized.\n",
      "96\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.400\n",
      "Recall: 0.400\n",
      "F1-Score: 0.400\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.6334335, '10.1007/s11192-015-1765-5'), (0.56431514, '10.48550/arXiv.2508.18620'), (0.54274267, '10.29173/cais1943'), (0.45914948, '10.1162/qss_a_00112'), (0.2705689, '10.1162/qss_a_00211')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.17821/srels/2024/v61i5/171583\\n Title: 図書館における質問回答ベースの検索システムの設計: オープンソースの検索拡張生成 (RAG) パイプラインのアプリケーション Designing Question-Answer Based Search System in Libraries: Application of Open Source Retrieval Augmented Generation (RAG) Pipeline\\nAbstract: この研究の主な目的は、プロトタイプを準備し、図書館が検索拡張生成 (RAG) フレームワークを通じてオープンソース ソフトウェア ツールと大規模言語モデル (LLM) を使用して低コストの会話型検索システムを開発できることを実証することです。 LLM は幻覚を起こし、時代遅れで文脈を理解していない応答を返すことがよくあります。ただし、この実験は、LLM が一連の関連文書で強化された場合、文脈に応じた適切な応答を提供できることを示しています。回答を生成する前に関連ドキュメントで LLM を拡張することは、検索拡張生成として知られています。この方法論には、LangChain などのツール、ChromaDB などのベクトル データベース、Llama3 (700 億のパラメーター ベースのモデル) などのオープンソース LLM を使用して RAG パイプラインを作成することが含まれていました。開発されたプロトタイプには、収集、処理され、パイプラインに取り込まれたチャンドラヤーン 3 ミッションに関する 250 以上の関連文書のデータセットが含まれています。最後に、研究では標準的な LLM と RAG 拡張を備えた LLM からの応答を比較しました。主な調査結果から、標準的な LLM (RAG なし) は、チャンドラヤーン 3 に関連するクエリに対して自信を持って不正確で幻覚のような応答を生成するのに対し、RAG を使用する LLM は、応答を生成する前に関連文書のセットが提供されると、一貫して正確で有益な、文脈に沿った応答を提供することが明らかになりました。この調査では、オープンソースの RAG ベースのシステムは、情報検索を強化し、図書館を動的な情報サービスに変えるための費用対効果の高いソリューションを図書館に提供すると結論付けています。 This study primarily aims to prepare a prototype and demonstrate that libraries can develop a low-cost conversational search system using open-source software tools and Large Language Models (LLMs) through a Retrieval-Augmented Generation (RAG) framework. LLMs often hallucinate and provide outdated and non-contextualized responses. However, this experiment shows that LLMs can deliver contextualized, relevant responses when augmented with a set of relevant documents. Augmenting LLMs with relevant documents before generating answers is known as retrieval-augmented generation. The methodology involved creating a RAG pipeline using tools like LangChain, vector databases like ChromaDB, and open-source LLMs like Llama3 (a 70-billion parameter-based model). The prototype developed includes a dataset of 250+ relevant documents on the Chandrayaan-3 mission that was collected, processed, and ingested into the pipeline. Finally, the study compared responses from standard LLMs and LLMs with RAG augmentation. Key findings revealed that standard LLMs (without RAG) produced confidently incorrect, hallucinated responses against queries related to Chandrayaan-3, while LLMs with RAG consistently provided accurate, informative, and contextualized answers when supplied with a set of relevant documents before generating the response. The study concluded that open-source RAG-based systems offer a cost-effective solution for libraries to enhance information retrieval and transform libraries into dynamic information services.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.48550/arXiv.2505.13557\\n Title: AMAQA: RAG システム用のメタデータベースの QA データセット AMAQA: A Metadata-based QA Dataset for RAG Systems\\nAbstract: 検索拡張生成 (RAG) システムは、質問応答 (QA) タスクで広く使用されていますが、現在のベンチマークにはメタデータの統合が不足しており、テキスト データと外部情報の両方を必要とするシナリオでの評価が妨げられています。これに対処するために、テキストとメタデータを組み合わせたタスクを評価するように設計された新しいオープンアクセス QA データセットである AMAQA を紹介します。メタデータの統合は、サイバーセキュリティやインテリジェンスなど、関連情報へのタイムリーなアクセスが重要な、大量のデータの迅速な分析が必要な分野で特に重要です。 AMAQA には、26 のパブリック Telegram グループから収集された約 110 万件の英語メッセージが含まれており、タイムスタンプ、トピック、感情の調子、毒性指標などのメタデータが充実しており、特定の基準に基づいてドキュメントをフィルタリングすることで、正確で文脈に応じたクエリを実行できます。また、450 の高品質 QA ペアも含まれており、メタデータ主導の QA および RAG システムの研究を進めるための貴重なリソースとなります。私たちの知る限り、AMAQA は、メッセージで扱われるトピックなどのメタデータとラベルを組み込んだ最初のシングルホップ QA ベンチマークです。私たちはベンチマークで広範なテストを実施し、将来の研究のための新しい基準を確立します。メタデータを活用すると精度が 0.12 から 0.61 に向上し、構造化コンテキストの価値が強調されることがわかりました。これに基づいて、提供されたコンテキストを反復処理し、ノイズの多いドキュメントで強化することで LLM 入力を洗練するためのいくつかの戦略を検討し、最良のベースラインよりもさらに 3 ポイントの向上を達成し、単純なメタデータ フィルタリングよりも 14 ポイントの改善を達成しました。 Retrieval-augmented generation (RAG) systems are widely used in question-answering (QA) tasks, but current benchmarks lack metadata integration, hindering evaluation in scenarios requiring both textual data and external information. To address this, we present AMAQA, a new open-access QA dataset designed to evaluate tasks combining text and metadata. The integration of metadata is especially important in fields that require rapid analysis of large volumes of data, such as cybersecurity and intelligence, where timely access to relevant information is critical. AMAQA includes about 1.1 million English messages collected from 26 public Telegram groups, enriched with metadata such as timestamps, topics, emotional tones, and toxicity indicators, which enable precise and contextualized queries by filtering documents based on specific criteria. It also includes 450 high-quality QA pairs, making it a valuable resource for advancing research on metadata-driven QA and RAG systems. To the best of our knowledge, AMAQA is the first single-hop QA benchmark to incorporate metadata and labels such as topics covered in the messages. We conduct extensive tests on the benchmark, establishing a new standard for future research. We show that leveraging metadata boosts accuracy from 0.12 to 0.61, highlighting the value of structured context. Building on this, we explore several strategies to refine the LLM input by iterating over provided context and enriching it with noisy documents, achieving a further 3-point gain over the best baseline and a 14-point improvement over simple metadata filtering.\\n', \"DOI: 10.48550/arXiv.2505.18247\\n Title: MetaGen Blended RAG: 専門分野の質問応答でゼロショットの精度を解放 MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索拡張生成 (RAG) は、ファイアウォールの背後に隔離されていることが多く、事前トレーニング中に LLM が認識できない複雑で特殊な用語が豊富に含まれる、ドメイン固有のエンタープライズ データセットに苦戦します。医学、ネットワーキング、法律などの分野にわたるセマンティックのばらつきが RAG のコンテキストの精度を妨げる一方、ソリューションを微調整するのはコストがかかり、時間がかかり、新しいデータが出現したときの汎用性が欠けています。微調整を行わずにレトリーバーでゼロショット精度を達成することは依然として重要な課題です。私たちは、メタデータ生成パイプラインと密ベクトルと疎ベクトルを使用したハイブリッド クエリ インデックスを通じてセマンティック リトリーバーを強化する新しいエンタープライズ検索アプローチである「MetaGen Blended RAG」を紹介します。主要な概念、トピック、頭字語を活用することで、私たちのメソッドはメタデータを強化したセマンティック インデックスと強化されたハイブリッド クエリを作成し、微調整することなく堅牢でスケーラブルなパフォーマンスを実現します。生物医学の PubMedQA データセットでは、MetaGen Blended RAG は 82% の検索精度と 77% の RAG 精度を達成し、以前のすべてのゼロショット RAG ベンチマークを上回り、そのデータセット上の微調整モデルに匹敵するだけでなく、SQuAD や NQ などのデータセットでも優れています。このアプローチは、特殊なドメイン全体にわたって比類のない一般化を備えたセマンティック検索ツールを構築する新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.17821/srels/2024/v61i5/171583', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2505.13557', '10.48550/arXiv.2505.18247']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a technique used to improve question answering and information retrieval systems by providing relevant documents to Large Language Models (LLMs) before generating responses. This approach enhances the accuracy, context, and relevance of the answers provided by LLMs.\n",
      "\n",
      "DOI: 10.17821/srels/2024/v61i5/171583 - This study demonstrates that libraries can develop low-cost conversational search systems using open-source software tools and LLMs through a RAG framework. The experiment shows that LLMs, when augmented with relevant documents, can provide contextualized and accurate responses, outperforming standard LLMs without RAG.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - This paper introduces Multi-Meta-RAG, a method that improves RAG for multi-hop queries by using database filtering with LLM-extracted metadata. This approach enhances the selection of relevant documents, leading to better performance on the MultiHop-RAG benchmark.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - The study proposes a Metadata Retrieval-Augmented Generation (M-RAG) method that enhances open-domain question answering systems by utilizing embeddings with metadata and generative models. This method improves performance by up to 46% compared to external inference ODQA systems and 6% over conventional RAG methods.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.13557 - This research presents AMAQA, a metadata-based QA dataset for RAG systems, which addresses the lack of metadata integration in current benchmarks. Leveraging metadata boosts accuracy and enables precise, contextualized queries, particularly in fields requiring rapid data analysis.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247 - The paper introduces MetaGen Blended RAG, a novel approach that enhances semantic retrievers for specialized domain question answering without fine-tuning. This method achieves high retrieval and RAG accuracy, surpassing prior zero-shot RAG benchmarks and rivaling fine-tuned models.\n",
      "\n",
      "My lady, RAG is a powerful technique that significantly improves the performance of question answering and information retrieval systems by providing LLMs with relevant context, thereby enhancing accuracy and relevance.\n",
      "96\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.81133276, '10.17821/srels/2024/v61i5/171583'), (0.72348577, '10.48550/arXiv.2406.13213'), (0.7211353, '10.6109/jkiice.2023.27.12.1489'), (0.72019166, '10.48550/arXiv.2505.13557'), (0.5614318, '10.48550/arXiv.2505.18247')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1045/september2016‐meschenmoser\\n Title: 科学 Web リポジトリのスクレイピング: 自動コンテンツ抽出の課題と解決策 Scraping Scientific Web Repositories: Challenges and Solutions for Automated Content Extraction\\nAbstract: 多くの科学 Web リポジトリは、科学出版物の可視性とアクセシビリティを向上させるだけでなく、h インデックスなどの指標を表示することによって、研究者の定量的および定性的な出版パフォーマンスも評価しています。これらの指標は、研究機関やその他の関係者にとって、採用や資金調達の決定など、影響力のある意思決定プロセスをサポートするために重要になっています。ただし、科学 Web リポジトリは通常、単純なパフォーマンス メトリクスと限られた分析オプションのみを提供します。さらに、パフォーマンス指標を計算するためのデータとアルゴリズムは通常公開されていません。したがって、システムがどの出版物を計算に含めるか、またシステムが結果をどのようにランク付けするかは透明性がなく、検証可能ではありません。多くの研究者は、これらのシステムの透明性を高めるために、基礎となるサイエントメトリーの生データにアクセスすることに興味を持っています。このペーパーでは、課題について説明し、科学 Web リポジトリ内のそのようなデータにプログラムでアクセスするための戦略を示します。 Google Scholar データに基づいて研究パフォーマンスを比較できるオープンソース ツール (MIT ライセンス) の一部として戦略を示します。ツールに含まれるスクレイパーは、リポジトリの運営者から同意が得られた場合にのみ使用する必要があることを強調したいと思います。私たちの経験では、研究目標が明確に説明され、プロジェクトが非営利的な性質のものであれば、同意が得られることがよくあります。 ide from improving the visibility and accessibility of scientific publications, many scientific Web repositories also assess researchers' quantitative and qualitative publication performance, e.g., by displaying metrics such as the h‐index. These metrics have become important for research institutions and other stakeholders to support impactful decision making processes such as hiring or funding decisions. However, scientific Web repositories typically offer only simple performance metrics and limited analysis options. Moreover, the data and algorithms to compute performance metrics are usually not published. Hence, it is not transparent or verifiable which publications the systems include in the computation and how the systems rank the results. Many researchers are interested in accessing the underlying scientometric raw data to increase the transparency of these systems. In this paper, we discuss the challenges and present strategies to programmatically access such data in scientific Web repositories. We demonstrate the strategies as part of an open source tool (MIT license) that allows research performance comparisons based on Google Scholar data. We would like to emphasize that the scraper included in the tool should only be used if consent was given by the operator of a repository. In our experience, consent is often given if the research goals are clearly explained and the project is of a non‐commercial nature.\\n\"]\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1045/september2016‐meschenmoser']\n",
      "\u001b[93mSummary: I'm sorry, my lady, I couldn't find any information about the main challenges in normalising citation metrics across scientific fields.\n",
      "\n",
      "DOI: 10.1045/september2016‐meschenmoser\n",
      "96\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.000\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.495\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.14296049, '10.1045/september2016‐meschenmoser')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1007/s11192-022-04289-7\\n Title: 最も多く見つかる場所を検索: 56 の書誌データベースの懲戒範囲の比較 Search where you will find most: Comparing the disciplinary coverage of 56 bibliographic databases\\nAbstract: この論文では、新しいサイエントメトリクス手法を紹介し、それを学術界で人気のある英語に焦点を当てた書誌データベースの多くの主題範囲を推定するために適用します。この方法では、クエリ結果を共通の分母として使用して、さまざまな検索エンジン、リポジトリ、デジタル ライブラリ、その他の書誌データベースを比較します。この方法は、データベース カバレッジのより小さいセットを分析する既存のサンプリング ベースのアプローチを拡張します。この調査結果では、56 のデータベースの相対的および絶対的な対象範囲が示されており、これまで入手できなかった情報が示されています。データベースの絶対的な対象範囲を知ることで、特にルックアップ検索や探索的検索に関連する、高い再現率/感度が必要な検索に最も包括的なデータベースを選択できます。データベースの相対的な対象範囲を知ることで、特に体系的な検索に関連する、高い精度と特異性が必要な検索に特化したデータベースを選択できます。この調査結果は、Google Scholar、Scopus、または Web of Science の専門分野の範囲の違いだけでなく、分析頻度が低いデータベースの違いも示しています。たとえば、研究者は、Meta (廃止)、Embase、または Europe PMC が、医学やその他の健康分野の PubMed よりも多くの記録をカバーしていることが判明したことに驚くかもしれません。これらの発見は、研究者が新しく導入されたオプションに対しても頼りになるデータベースを再評価するよう促すはずです。より包括的なデータベースを使用して検索すると、特にシステマティック レビューやメタ分析など、最も適合するデータベースの選択に特別な考慮が必要な場合に、検索結果が向上します。この比較は、図書館員やその他の情報専門家が高価なデータベース調達戦略を再評価するのにも役立ちます。機関にアクセスできない研究者は、どのオープン データベースが自分の専門分野において最も包括的である可能性が高いかを学びます。 This paper introduces a novel scientometrics method and applies it to estimate the subject coverages of many of the popular English-focused bibliographic databases in academia. The method uses query results as a common denominator to compare a wide variety of search engines, repositories, digital libraries, and other bibliographic databases. The method extends existing sampling-based approaches that analyze smaller sets of database coverages. The findings show the relative and absolute subject coverages of 56 databases—information that has often not been available before. Knowing the databases’ absolute subject coverage allows the selection of the most comprehensive databases for searches requiring high recall/sensitivity, particularly relevant in lookup or exploratory searches. Knowing the databases’ relative subject coverage allows the selection of specialized databases for searches requiring high precision/specificity, particularly relevant in systematic searches. The findings illustrate not only differences in the disciplinary coverage of Google Scholar, Scopus, or Web of Science, but also of less frequently analyzed databases. For example, researchers might be surprised how Meta (discontinued), Embase, or Europe PMC are found to cover more records than PubMed in Medicine and other health subjects. These findings should encourage researchers to re-evaluate their go-to databases, also against newly introduced options. Searching with more comprehensive databases can improve finding, particularly when selecting the most fitting databases needs particular thought, such as in systematic reviews and meta-analyses. This comparison can also help librarians and other information experts re-evaluate expensive database procurement strategies. Researchers without institutional access learn which open databases are likely most comprehensive in their disciplines.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.48550/arXiv.2303.17661', '10.1007/s11192-022-04367-w', '10.1007/s11192-022-04289-7', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: I have found two documents that address your needs, my lady.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661\n",
      "Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w\n",
      "This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\n",
      "96\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.250\n",
      "Recall: 0.200\n",
      "F1-Score: 0.222\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.3829032, '10.48550/arXiv.2303.17661'), (0.3650723, '10.1007/s11192-022-04367-w'), (0.14534597, '10.1007/s11192-022-04289-7'), (0.10071066, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', \"DOI: 10.48550/arXiv.2404.13948\\n Title: RAG の背中を打ち砕いた ypos: 低レベルの摂動を介して野生のドキュメントをシミュレートすることによる、RAG パイプラインへの遺伝的攻撃 ypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\\nAbstract: 最近の大規模言語モデル (LLM) の適用可能性がさまざまなドメインや現実世界のアプリケーションに拡大するにつれて、その堅牢性がますます重要になってきています。検索拡張生成 (RAG) は、LLM の制限に対処するための有望なソリューションですが、RAG の堅牢性に関する既存の研究では、RAG コンポーネント間の相互接続関係や、軽微なテキスト エラーなど、現実のデータベースに蔓延する潜在的な脅威が見落とされていることがよくあります。この研究では、RAG の堅牢性を評価する際にまだ解明されていない 2 つの側面を調査します。1 つは低レベルの摂動によるノイズの多いドキュメントに対する脆弱性、2 つは RAG の堅牢性の全体的な評価です。さらに、これらの側面をターゲットとした新しい攻撃方法である RAG への遺伝的攻撃を紹介します。具体的には、GARAG は、各コンポーネント内の脆弱性を明らかにし、ノイズの多いドキュメントに対してシステム全体の機能をテストするように設計されています。 \\\\textit{GARAG} を標準 QA データセットに適用し、さまざまな取得者と LLM を組み込むことで、RAG の堅牢性を検証します。実験結果は、GARAG が一貫して高い攻撃成功率を達成していることを示しています。また、各コンポーネントのパフォーマンスとその相乗効果を著しく損なうため、わずかなテキストの不正確さが現実世界の RAG システムを混乱させる大きなリスクを浮き彫りにします。 The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1 vulnerability to noisy documents through low-level perturbations and 2 a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG, which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.48550/arXiv.2406.13213', '10.1145/3637528.3671470', '10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2404.13948']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a promising solution to the limitations of Large Language Models (LLMs), such as hallucination, outdated knowledge, and non-transparent reasoning processes. RAG enhances the accuracy and credibility of generated content by incorporating knowledge from external databases. This approach allows for continuous knowledge updates and integration of domain-specific information. RAG systems consist of a retrieval module and an LLM-based generation module, working together to reduce the risk of hallucinations and improve output quality.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997: This paper provides a comprehensive review of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG. It examines the tripartite foundation of RAG frameworks: retrieval, generation, and augmentation techniques. The study highlights state-of-the-art technologies in each component and introduces an up-to-date evaluation framework and benchmark. It also discusses current challenges and future research directions.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213: This research introduces Multi-Meta-RAG, a method to improve RAG for multi-hop queries by using LLM-extracted metadata for database filtering. The approach enhances the selection of relevant documents from various sources, demonstrating significant improvements on the MultiHop-RAG benchmark.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470: This survey focuses on Retrieval-Augmented Large Language Models (RA-LLMs), emphasizing their ability to leverage external knowledge bases for high-quality content generation. It reviews existing research, covering three primary technical perspectives, and discusses limitations and future research directions.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16: This paper introduces RAGAs, a framework for reference-free evaluation of RAG pipelines. RAGAs provides a suite of metrics to assess the retrieval system's ability to identify relevant context, the LLM's ability to utilize this context faithfully, and the quality of the generated content.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.13948: This study investigates the robustness of RAG systems, focusing on vulnerability to noisy documents and holistic evaluation. It introduces GARAG, a genetic attack method to test RAG components and overall system functionality. Experimental results highlight the risk of minor textual inaccuracies in real-world RAG systems.\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions or need additional details, please let me know.\n",
      "96\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.400\n",
      "Recall: 0.400\n",
      "F1-Score: 0.400\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.5488434, '10.48550/arXiv.2312.10997'), (0.52685463, '10.48550/arXiv.2406.13213'), (0.4893821, '10.1145/3637528.3671470'), (0.4833807, '10.18653/v1/2024.eacl-demo.16'), (0.3458787, '10.48550/arXiv.2404.13948')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n', \"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1145/2964909', '10.1109/ADL.1998.670425', '10.1177/09610006241239080', '10.5860/crl.86.1.101', '10.48550/arXiv.2303.17661']\n",
      "\u001b[93mSummary: The key challenges in metadata quality assessment include defining and measuring quality dimensions, handling diverse metadata types, and addressing cultural and contextual influences. Advancements include automated frameworks, standardized schemas, and AI-driven tools for improved assessment and enhancement.\n",
      "\n",
      "DOI: 10.1145/2964909 - This study focuses on developing a generic metadata quality assessment framework for open data portals, addressing challenges like searchability and usability by mapping specific metadata to standardized schemas and defining quality metrics for automated evaluation.\n",
      "\n",
      "DOI: 10.1109/ADL.1998.670425 - This research highlights the need for diverse criteria and procedures to evaluate different types of metadata, contributing to the methodological development of metadata quality assessment.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - A scoping review identifies commonly used dimensions in metadata quality assessment but notes a lack of consensus on definitions and measurements, emphasizing the need for further research.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 - This study explores how metadata quality interacts with cultural meanings, identifying unique issues and categorizing them to understand their impact on individuals and communities.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661 - Introducing MetaEnhance, an AI-driven framework for improving metadata quality in academic repositories, demonstrating high effectiveness in error detection and correction.\n",
      "\n",
      "My lady, these findings collectively underscore the complexity of metadata quality assessment and the ongoing efforts to enhance its methodologies and tools.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in metadata quality assessment?']:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.000\n",
      "Accuracy: 0.896\n",
      "Balanced accuracy: 0.473\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.6636041, '10.1145/2964909'), (0.4874788, '10.1109/ADL.1998.670425'), (0.38741276, '10.1177/09610006241239080'), (0.37305376, '10.5860/crl.86.1.101'), (0.2738236, '10.48550/arXiv.2303.17661')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.1080/19386389.2011.570654\\n Title: メタデータ レコードの問題の分析 An Analysis of Problems in Metadata Records\\nAbstract: メタデータはデジタル ライブラリにおいて重要な役割を果たします。しかし、有用であるためには、メタデータ レコードに問題がなくなければなりません。メタデータに問題があると、リソースが正しく表現されず、ユーザーはメタデータのメリットを享受できなくなります。このような問題を排除しないにしても、最小限に抑えるには、メタデータ レコードで発生する可能性のある問題の種類を理解することが不可欠です。この論文では、文献で報告されているメタデータ レコードで見つかった問題を比較および分析します。メタデータの問題の 5 つのカテゴリを特定できることがわかります。これらは、不正な値、不正な要素、情報の欠落、情報損失、および一貫性のない値の表現の問題です。これらの問題がメタデータによって提供できるサービスに悪影響を与えることを考慮すると、メタデータの使用から得られる利点と、メタデータ レコードの作成に費やされるコストと労力のバランスが確保されるように、予防または是正措置を講じる必要があります。 Metadata plays an important role in digital libraries. But to be useful, metadata records must be problem free. When problems are present in the metadata, resources are not correctly represented and users are not able to reap the benefits of metadata. To minimize, if not eliminate, such problems, it is essential to understand the kinds of problems that can occur in metadata records. In this paper, problems found in metadata records as reported in the literature are compared and analyzed. It is found that five categories of metadata problems can be identified. These are the problems of Incorrect Values, Incorrect Elements, Missing Information, Information Loss, and Inconsistent Value Representation. Given that these problems are detrimental to the services that can be provided by metadata, preventive or corrective measures need to be put in place so as to ensure that the benefits derived from using metadata balance the costs and efforts spent in the creation of metadata records.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1007/s11192-023-04923-y\\n Title: OpenAlex に機関が存在しない: 考えられる理由、影響、および解決策 Missing institutions in OpenAlex: possible reasons, implications, and solutions\\nAbstract: オープン サイエンスの到来により、高いデータ品質を備えたオープン データ プラットフォームが必要になります。 2022 年 1 月に開始されたグローバル研究システムの完全にオープンなカタログである OpenAlex は、データへの簡単なアクセスと、量的科学研究で広く使用されている幅広いデータ範囲という 2 つの主な利点を備えています。注目すべきことに、OpenAlex はライデン大学ランキングの重要なデータ ソースとして採用されています。ただし、OpenAlex の雑誌記事メタデータには機関が欠落しているという深刻なデータ品質の問題があります。この研究では、完全な機関情報 (FII)、部分的に欠落している機関情報 (PMII)、および完全に欠落している機関情報 (CMII) という 3 つのタイプの機関情報を定義することにより、問題の考えられる理由とその結果と解決策を調査します。私たちの結果は、OpenAlex のジャーナル記事の 60% 以上で機関の欠落の問題が発生していることを示しています。この問題は、初期のメタデータや社会科学、人文科学で特に蔓延しています。データのサブサンプルを使用して、問題の考えられる理由、歪んだ結果をもたらす可能性のあるリスク、行方不明の機関の問題に対する考えられる解決策をさらに調査します。その目的は、オープン リソースにおけるデータ品質の向上の重要性を高め、それによって量的科学の研究やより広範な文脈においてオープン リソースの責任ある使用をサポートすることです。 The advent of open science calls for open data platforms with high data quality. As a fully open catalog of the global research system launched in January 2022, OpenAlex features two main advantages of easy data accessibility and broad data coverage, which has been widely used in quantitative science studies. Remarkably, OpenAlex is adopted as an important data source for Leiden university ranking. However, there is a severe data quality problem of missing institutions in journal article metadata in OpenAlex. This study investigates the possible reasons for the problem and its consequences and solutions by defining three types of institutional information—full institutional information (FII), partially missing institutional information (PMII) and completely missing institutional information (CMII). Our results show that the problem of missing institutions occurs in more than 60% of the journal articles in OpenAlex. The problem is particularly widespread in metadata from the early years and in the social sciences and humanities. Using sub-samples of the data, we further explore the possible reasons for the problem, the risk it might represent for distorted results, and possible solutions to the problem of missing institutions. The aim is to raise the importance of data quality improvements in open resources, and thus to support the responsible use of open resources in quantitative science studies and also in broader contexts.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.5860/crl.86.1.101', '10.48550/arXiv.2303.17661', '10.1080/19386389.2011.570654', '10.1177/09610006241239080', '10.1007/s11192-023-04923-y']\n",
      "\u001b[93mSummary: Metadata quality is crucial for digital object discovery, data retrieval, and user experience, but faces challenges like incompleteness, inconsistency, and inaccuracy. Solutions include automated detection and correction, standardization, and improved assessment tools.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 - Metadata serves as a standardized tool for discovery and access, but faces tensions between sociocultural representations, resource constraints, and standardized systems. Interventions can be seen as quality issues, political acts, or strategic choices. A study identified 32 unique issues across 427 records, categorized into 5 groups, highlighting the interaction between metadata and cultural meanings.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661 - Metadata quality is essential for digital library interfaces, but often suffers from incomplete, inconsistent, and incorrect values. The MetaEnhance framework uses AI to detect, correct, and canonicalize scholarly metadata, achieving high F1-scores in error detection and correction for electronic theses and dissertations.\n",
      "\n",
      "DOI: 10.1080/19386389.2011.570654 - Metadata records must be problem-free to ensure accurate resource representation. Common issues include incorrect values, elements, missing information, information loss, and inconsistent value representation. Preventive and corrective measures are needed to balance the benefits of metadata with its creation costs.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - Metadata plays a pivotal role in data retrieval, organization, and user experience. A scoping review identified completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness as key dimensions in metadata quality assessment. However, there is no consensus on their definitions and measurements, necessitating further research and flexible assessment tools.\n",
      "\n",
      "DOI: 10.1007/s11192-023-04923-y - Open data platforms like OpenAlex face data quality issues, particularly missing institutions in journal article metadata. The study defines three types of institutional information and finds the problem in over 60% of articles, especially in early metadata and social sciences/humanities. Solutions aim to improve data quality in open resources.\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and potential solutions for improving metadata quality?']:\n",
      "Precision: 0.800\n",
      "Recall: 0.800\n",
      "F1-Score: 0.800\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.895\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.5573882, '10.5860/crl.86.1.101'), (0.46701634, '10.48550/arXiv.2303.17661'), (0.43467787, '10.1080/19386389.2011.570654'), (0.40952834, '10.1177/09610006241239080'), (0.29672605, '10.1007/s11192-023-04923-y')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 14 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", \"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", \"DOI: 10.48550/arXiv.2505.18247\\n Title: MetaGen Blended RAG: 専門分野の質問応答でゼロショットの精度を解放 MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索拡張生成 (RAG) は、ファイアウォールの背後に隔離されていることが多く、事前トレーニング中に LLM が認識できない複雑で特殊な用語が豊富に含まれる、ドメイン固有のエンタープライズ データセットに苦戦します。医学、ネットワーキング、法律などの分野にわたるセマンティックのばらつきが RAG のコンテキストの精度を妨げる一方、ソリューションを微調整するのはコストがかかり、時間がかかり、新しいデータが出現したときの汎用性が欠けています。微調整を行わずにレトリーバーでゼロショット精度を達成することは依然として重要な課題です。私たちは、メタデータ生成パイプラインと密ベクトルと疎ベクトルを使用したハイブリッド クエリ インデックスを通じてセマンティック リトリーバーを強化する新しいエンタープライズ検索アプローチである「MetaGen Blended RAG」を紹介します。主要な概念、トピック、頭字語を活用することで、私たちのメソッドはメタデータを強化したセマンティック インデックスと強化されたハイブリッド クエリを作成し、微調整することなく堅牢でスケーラブルなパフォーマンスを実現します。生物医学の PubMedQA データセットでは、MetaGen Blended RAG は 82% の検索精度と 77% の RAG 精度を達成し、以前のすべてのゼロショット RAG ベンチマークを上回り、そのデータセット上の微調整モデルに匹敵するだけでなく、SQuAD や NQ などのデータセットでも優れています。このアプローチは、特殊なドメイン全体にわたって比類のない一般化を備えたセマンティック検索ツールを構築する新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\", 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1016/j.caeai.2025.100417', '10.48550/arXiv.2505.18247', '10.1007/s44427-025-00006-3', '10.1145/3637528.3671470']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) systems face challenges such as hallucination, outdated knowledge, non-transparent reasoning processes, and computational costs. However, advancements include improved factual accuracy, dynamic knowledge updates, and enhanced multimodal support.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - This paper discusses the progression of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG, and highlights the state-of-the-art technologies in retrieval, generation, and augmentation techniques.\n",
      "\n",
      "DOI: 10.1016/j.caeai.2025.100417 - This study focuses on RAG in educational applications, addressing challenges like hallucination mitigation, knowledge completeness, and computational cost reduction.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247 - Introduces MetaGen Blended RAG, a novel approach to enhance semantic retrievers for specialized domain question-answering, achieving high zero-shot precision.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3 - Evaluates open-source LLMs in RAG systems using the Ragas framework, highlighting the strengths and weaknesses of different models for academic tasks.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470 - Surveys Retrieval-Augmented Large Language Models (RA-LLMs), discussing their limitations and future research directions.\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions or need more details, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in Retrieval-Augmented Generation (RAG) systems?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9224337, '10.48550/arXiv.2312.10997'), (0.87681407, '10.1016/j.caeai.2025.100417'), (0.7272211, '10.48550/arXiv.2505.18247'), (0.6641271, '10.1007/s44427-025-00006-3'), (0.6102915, '10.1145/3637528.3671470')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 15 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', \"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.1177/09610006241239080', '10.1109/ADL.1998.670425', '10.5281/zenodo.14006424']\n",
      "\u001b[93mSummary: Two studies examined the abstract in metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - This study compared the amount of metadata and the completeness degree of research publications in new academic databases. Seven characteristics were analyzed, including the abstract.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - This study investigated the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The co-occurrence analysis was made on the title and abstract of selected articles.\n",
      "\n",
      "My lady, I hope this information is helpful. Please let me know if you have any further questions.\n",
      "96\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.250\n",
      "Recall: 0.200\n",
      "F1-Score: 0.222\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.6320719, '10.1162/qss_a_00286'), (0.58919704, '10.1177/09610006241239080'), (0.55420566, '10.1109/ADL.1998.670425'), (0.18688062, '10.5281/zenodo.14006424')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1108/JD-10-2022-0234\\n Title: すべての学術分野にわたる引用と参照の習慣の分析: 書誌の参照と引用の実践におけるアプローチと傾向 An analysis of citing and referencing habits across all scholarly disciplines: approaches and trends in bibliographic referencing and citing practices\\nAbstract: この研究で、著者らは、学術文献における引用および参照の誤りについて現在考えられる原因を特定し、スウィートランド氏が1989年の論文で提供したスナップショットから何かが変わったかどうかを比較したいと考えている。,著者らは、27の主題分野にわたる147のジャーナルに掲載された729件の論文から、参考要素、すなわち書誌的参照、言及、引用、およびそれぞれの本文中の参照ポインタを分析した。,分析の結果は、書誌的事項が指摘された。著者らの知る限り、この研究は、Sweetland (1989) 以来、文献における参照および引用の慣行における誤りを分析したものとしては、最近入手可能な最良のものである。 In this study, the authors want to identify current possible causes for citing and referencing errors in scholarly literature to compare if something changed from the snapshot provided by Sweetland in his 1989 paper.,The authors analysed reference elements, i.e. bibliographic references, mentions, quotations and respective in-text reference pointers, from 729 articles published in 147 journals across the 27 subject areas.,The outcomes of the analysis pointed out that bibliographic errors have been perpetuated for decades and that their possible causes have increased, despite the encouraged use of technological facilities, i.e. the reference managers.,As far as the authors know, the study is the best recent available analysis of errors in referencing and citing practices in the literature since Sweetland (1989).\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1108/JD-10-2022-0234', '10.1007/s11192-022-04367-w', '10.1162/qss_a_00112']\n",
      "\u001b[93mSummary: I have found three studies that examined citations.\n",
      "\n",
      "DOI: 10.1108/JD-10-2022-0234\n",
      "This study examined citing and referencing habits across all scholarly disciplines.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w\n",
      "This study examined invalid citations due to DOI errors in Crossref data.\n",
      "\n",
      "DOI: 10.1162/qss_a_00112\n",
      "This study examined the completeness and accuracy of citation links in five multidisciplinary bibliographic data sources.\n",
      "\n",
      "My lady, I hope this information is helpful.\n",
      "96\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 1.000\n",
      "Recall: 0.600\n",
      "F1-Score: 0.750\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.800\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.3569626, '10.1108/JD-10-2022-0234'), (0.21608633, '10.1007/s11192-022-04367-w'), (0.13634396, '10.1162/qss_a_00112')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1007/s11192-022-04367-w', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: Crossref is a sustainable source of community-owned scholarly metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022 - Crossref is a sustainable source of community-owned scholarly metadata. It is one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications.\n",
      "\n",
      "DOI: 10.1162/qss_a_00210 - Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w - This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes.\n",
      "\n",
      "DOI: 10.5281/ZENODO.6188748 - This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data.\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "96\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.800\n",
      "Recall: 0.800\n",
      "F1-Score: 0.800\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.895\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.7781275, '10.1162/qss_a_00022'), (0.72395444, '10.1162/qss_a_00210'), (0.55623144, '10.31222/osf.io/smxe5'), (0.44883752, '10.1007/s11192-022-04367-w'), (0.17410146, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n', 'DOI: 10.1007/s11192-022-04289-7\\n Title: 最も多く見つかる場所を検索: 56 の書誌データベースの懲戒範囲の比較 Search where you will find most: Comparing the disciplinary coverage of 56 bibliographic databases\\nAbstract: この論文では、新しいサイエントメトリクス手法を紹介し、それを学術界で人気のある英語に焦点を当てた書誌データベースの多くの主題範囲を推定するために適用します。この方法では、クエリ結果を共通の分母として使用して、さまざまな検索エンジン、リポジトリ、デジタル ライブラリ、その他の書誌データベースを比較します。この方法は、データベース カバレッジのより小さいセットを分析する既存のサンプリング ベースのアプローチを拡張します。この調査結果では、56 のデータベースの相対的および絶対的な対象範囲が示されており、これまで入手できなかった情報が示されています。データベースの絶対的な対象範囲を知ることで、特にルックアップ検索や探索的検索に関連する、高い再現率/感度が必要な検索に最も包括的なデータベースを選択できます。データベースの相対的な対象範囲を知ることで、特に体系的な検索に関連する、高い精度と特異性が必要な検索に特化したデータベースを選択できます。この調査結果は、Google Scholar、Scopus、または Web of Science の専門分野の範囲の違いだけでなく、分析頻度が低いデータベースの違いも示しています。たとえば、研究者は、Meta (廃止)、Embase、または Europe PMC が、医学やその他の健康分野の PubMed よりも多くの記録をカバーしていることが判明したことに驚くかもしれません。これらの発見は、研究者が新しく導入されたオプションに対しても頼りになるデータベースを再評価するよう促すはずです。より包括的なデータベースを使用して検索すると、特にシステマティック レビューやメタ分析など、最も適合するデータベースの選択に特別な考慮が必要な場合に、検索結果が向上します。この比較は、図書館員やその他の情報専門家が高価なデータベース調達戦略を再評価するのにも役立ちます。機関にアクセスできない研究者は、どのオープン データベースが自分の専門分野において最も包括的である可能性が高いかを学びます。 This paper introduces a novel scientometrics method and applies it to estimate the subject coverages of many of the popular English-focused bibliographic databases in academia. The method uses query results as a common denominator to compare a wide variety of search engines, repositories, digital libraries, and other bibliographic databases. The method extends existing sampling-based approaches that analyze smaller sets of database coverages. The findings show the relative and absolute subject coverages of 56 databases—information that has often not been available before. Knowing the databases’ absolute subject coverage allows the selection of the most comprehensive databases for searches requiring high recall/sensitivity, particularly relevant in lookup or exploratory searches. Knowing the databases’ relative subject coverage allows the selection of specialized databases for searches requiring high precision/specificity, particularly relevant in systematic searches. The findings illustrate not only differences in the disciplinary coverage of Google Scholar, Scopus, or Web of Science, but also of less frequently analyzed databases. For example, researchers might be surprised how Meta (discontinued), Embase, or Europe PMC are found to cover more records than PubMed in Medicine and other health subjects. These findings should encourage researchers to re-evaluate their go-to databases, also against newly introduced options. Searching with more comprehensive databases can improve finding, particularly when selecting the most fitting databases needs particular thought, such as in systematic reviews and meta-analyses. This comparison can also help librarians and other information experts re-evaluate expensive database procurement strategies. Researchers without institutional access learn which open databases are likely most comprehensive in their disciplines.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.48550/arXiv.2508.18620', '10.1162/qss_a_00286', '10.5281/zenodo.14006424', '10.1007/s11192-022-04289-7']\n",
      "\u001b[93mSummary: I found four papers that evaluate the linguistic coverage or language-related metadata in scholarly databases.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633\n",
      "Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620\n",
      "Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\n",
      "\n",
      "DOI: 10.1162/qss_a_00286\n",
      "Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "\n",
      "DOI: 10.5281/zenodo.14006424\n",
      "Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\n",
      "\n",
      "My lady, I hope this helps.\n",
      "96\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.200\n",
      "Recall: 0.200\n",
      "F1-Score: 0.200\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.9108078, '10.48550/arXiv.2409.10633'), (0.42706275, '10.48550/arXiv.2508.18620'), (0.41563448, '10.1162/qss_a_00286'), (0.19475074, '10.5281/zenodo.14006424'), (0.16595806, '10.1007/s11192-022-04289-7')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.5281/zenodo.14006424\\n Title: OpenAlex におけるアフリカ出版物の報道範囲とメタデータの利用可能性: 比較分析 Coverage and metadata availability of African publications in OpenAlex: A comparative analysis\\nAbstract: Scopus や Web of Science (WoS) などの従来の独自データ ソースとは異なり、OpenAlex は包括的なカバレッジを重視しており、特に人文科学、英語以外の言語、グローバル サウスの研究が含まれていることを強調しています。科学における多様性と包括性を強化することは、倫理的および実際的な理由から非常に重要です。このペーパーでは、アフリカを拠点とする出版物の OpenAlex の対象範囲とメタデータの可用性を分析します。この目的のために、OpenAlex を Scopus、WoS、および African Journals Online (AJOL) と比較します。まず、OpenAlex におけるアフリカの研究出版物の報道範囲を、WoS、Scopus、および AJOL の報道範囲と比較します。次に、OpenAlex、Scopus、および WoS 出版物の利用可能なメタデータを評価し、比較します。私たちの分析では、OpenAlex が最も広範な出版物をカバーしていることがわかりました。メタデータの点では、OpenAlex は出版物と著者の情報を幅広くカバーしています。所属、参照、資金提供者情報に関してはパフォーマンスが低下します。重要なことに、この結果は、Scopus または WoS でもインデックス付けされている出版物では、OpenAlex でのメタデータの可用性が優れていることも示しています。 Unlike traditional proprietary data sources like Scopus and Web of Science (WoS), OpenAlex emphasizes its comprehensive coverage, particularly highlighting its inclusion of the humanities, non-English languages, and research from the Global South. Strengthening diversity and inclusivity in science is crucial for ethical and practical reasons. This paper analyses OpenAlex’s coverage and metadata availability of African-based publications. For this purpose, we compare OpenAlex with Scopus, WoS, and African Journals Online (AJOL). We first compare the coverage of African research publications in OpenAlex against that of WoS, Scopus, and AJOL. We then assess and compare the available metadata for OpenAlex, Scopus, and WoS publications. Our analysis shows that OpenAlex offers the most extensive publication coverage. In terms of metadata, OpenAlex offers a high coverage of publication and author information. It performs worse regarding affiliations, references, and funder information. Importantly, our results also show that metadata availability in OpenAlex is better for publications that are also indexed in Scopus or WoS.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: COVID-19 研究への資金提供: オープン データ インフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 資金提供機関が提供する資金の成果を分析するには、資金提供によって得られた出版物を追跡できることが不可欠です。私たちは、Crossref での資金調達データのオープンな利用可能性を調査し、新型コロナウイルス感染症 (COVID-19) に関連する研究を報告する出版物の資金調達データに焦点を当てています。また、Scopus と Web of Science という 2 つの独自の書誌データベースで利用可能な資金調達データとの比較も示します。私たちの分析では、Crossref の資金調達データの範囲が限られていることが明らかになりました。また、特に Scopus における資金調達データの品質に関連する問題も示しています。 Crossref での資金調達データのオープンな可用性を向上させるための推奨事項を提供します。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.5281/zenodo.14006424', '10.31222/osf.io/smxe5', '10.1162/qss_a_00286', '10.1162/qss_a_00212']\n",
      "\u001b[93mSummary: There are four papers that address funding metadata, its availability, or its analysis in scholarly databases.\n",
      "\n",
      "DOI: 10.1162/qss_a_00210 - This paper addresses the availability and completeness of open funder metadata by looking at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO.\n",
      "\n",
      "DOI: 10.5281/zenodo.14006424 - This paper analyses OpenAlex’s coverage and metadata availability of African-based publications.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This paper presents an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information.\n",
      "\n",
      "DOI: 10.1162/qss_a_00212 - This paper studies the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19.\n",
      "\n",
      "My lady, I hope this information is helpful.\n",
      "96\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.800\n",
      "Recall: 1.000\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.990\n",
      "Balanced accuracy: 0.995\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.68798697, '10.1162/qss_a_00210'), (0.48616135, '10.5281/zenodo.14006424'), (0.4024631, '10.31222/osf.io/smxe5'), (0.31545174, '10.1162/qss_a_00286'), (0.30762884, '10.1162/qss_a_00212')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", \"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.48550/arXiv.2312.10997', '10.1145/3637528.3671470', '10.6109/jkiice.2023.27.12.1489', '10.1007/s44427-025-00006-3']\n",
      "\u001b[93mI'm sorry, my lady, I cannot find any papers that discuss the use of Retrieval-Augmented Generation (RAG) when used in applications.\n",
      "96\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) when used in applications?']:\n",
      "Precision: 0.200\n",
      "Recall: 0.200\n",
      "F1-Score: 0.200\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 0\n",
      "Documents score: [(0.8958969, '10.1016/j.caeai.2025.100417'), (0.81418616, '10.48550/arXiv.2312.10997'), (0.7173491, '10.1145/3637528.3671470'), (0.69672287, '10.6109/jkiice.2023.27.12.1489'), (0.6675167, '10.1007/s44427-025-00006-3')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1007/s11192-022-04367-w', '10.1162/qss_a_00112']\n",
      "\u001b[93mSummary: Crossref is a sustainable source of community-owned scholarly metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "\n",
      "Crossref is a sustainable source of community-owned scholarly metadata. It collects and makes available academic metadata and is important in the scholarly research ecosystem. It contains over 106 million records and expands at an average rate of 11% a year. It has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH.\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "96\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.95628047, '10.1162/qss_a_00022'), (0.47548413, '10.1162/qss_a_00210'), (0.44680908, '10.31222/osf.io/smxe5'), (0.21246108, '10.1007/s11192-022-04367-w'), (0.19260046, '10.1162/qss_a_00112')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.29173/cais1943\\n Title: OpenAlex と Web of Science の間のドキュメント タイプの不一致を調査する Investigating Document Type Discrepancies between OpenAlex and the Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された OpenAlex は、そのオープン性と広範なカバレッジで際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この研究は、ドキュメント タイプの精度に焦点を当て、OpenAlex と WoS における作品のメタデータの品質を評価することを目的としています。 OpenAlex と WoS の両方でインデックス付けされている出版物の 4% 以上が研究論文またはレビューとして誤って分類されているようであり、これらのエラーの大部分 (約 97%) が OpenAlex で発生していることが観察されています。この研究は、文書タイプの不一致や誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, launched in 2022, stands out for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinder its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess the metadata quality of works in OpenAlex and WoS, focusing on document type accuracy. We observe that over 4% of the publications indexed in both OpenAlex and WoS appear to be misclassified as research articles or reviews, and that the vast majority (about 97%) of these errors occur in OpenAlex. By addressing discrepancies and misattributions in document types this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', \"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.3145/epi.2023.mar.09\\n Title: Microsoft Academic Graph から OpenAlex に切り替える場合、書誌情報学に関連するメタデータはどれが同じで、どれが異なりますか? Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\\nAbstract: Microsoft Academic Graph (MAG) の廃止の発表に伴い、非営利団体 OurResearch は OpenAlex という名前で同様のリソースを提供すると発表しました。したがって、最新の MAG スナップショットと初期の OpenAlex スナップショットの書誌学的分析に関連するメタデータを比較します。 MAG の実質的にすべての著作物は、書誌データの出版年、巻数、最初と最後のページ、DOI、および引用分析の重要な要素である参考文献の数を保存しながら OpenAlex に転送されました。 MAG ドキュメントの 90% 以上が OpenAlex に同等のドキュメント タイプを持っています。残りのうち、特に OpenAlex 文書タイプの Journal-article および Book-chapter への再分類は正しいようで、その割合は 7% 以上に達しており、文書タイプの仕様は MAG から OpenAlex に大幅に改善されました。書誌学的関連メタデータの別の項目として、MAG と OpenAlex における紙ベースの主題分類を調べました。 OpenAlex では、MAG よりもはるかに多くの主題分類が割り当てられているドキュメントが見つかりました。第 1 レベルと第 2 レベルでは、分類構造はほぼ同じです。主題の再分類に関するデータを表とグラフの両方のレベルで表示します。分野で正規化された書誌学的評価に対する豊富な主題の再分類の影響の評価は、この論文の範囲には含まれません。この未解決の質問とは別に、OpenAlex は全体的に、文書タイプの割り当てがより広範囲にカバーされているため、2021 年より前の出版物については少なくとも MAG と同等かそれ以上に書誌学的分析に適しているようです。 With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2508.18620', '10.29173/cais1943', '10.48550/arXiv.2409.10633', '10.3145/epi.2023.mar.09']\n",
      "\u001b[93mSummary: OpenAlex is a bibliographic database that was launched in 2022. It is open-source and inclusive, and promises comprehensive research information.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. However, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 - OpenAlex may reduce or eliminate barriers to accessing bibliometric data, but one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata.\n",
      "\n",
      "DOI: 10.29173/cais1943 - OpenAlex stands out for its openness and extensive coverage. However, one of the concerns that hinder its broader adoption for research and research evaluation is the quality of its metadata.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 - OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages.\n",
      "\n",
      "DOI: 10.3145/epi.2023.mar.09 - OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.8455478, '10.48550/arXiv.2404.17663'), (0.7178241, '10.48550/arXiv.2508.18620'), (0.69024646, '10.29173/cais1943'), (0.64344186, '10.48550/arXiv.2409.10633'), (0.6353361, '10.3145/epi.2023.mar.09')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1007/s11192-015-1765-5\\n Title: Web of Science と Scopus のジャーナルの報道: 比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌学的手法は、研究の評価など、さまざまな目的で複数の分野で使用されています。ほとんどの書誌学的分析には、トムソン・ロイターの Web of Science (WoS) とエルゼビアの Scopus というデータ ソースが共通しています。この調査の目的は、これら 2 つのデータベースのジャーナルの対象範囲を説明し、特定の分野、出版国、言語が過小評価されているかどうかを評価することです。これを行うために、WoS (13,605 ジャーナル) および Scopus (20,346 ジャーナル) のアクティブな学術ジャーナルの範囲を、ウルリッヒの広範な定期刊行物ディレクトリ (63,013 ジャーナル) と比較しました。結果は、WoS または Scopus を研究評価に使用すると、社会科学、芸術、人文科学に不利益をもたらす、自然科学と工学、生物医学研究に有利なバイアスを導入する可能性があることを示しています。同様に、英語のジャーナルが過大評価され、他の言語に損害を与えています。どちらのデータベースもこれらのバイアスを共有していますが、その範囲は大きく異なります。そのため、書誌情報分析の結果は使用するデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈において、特に異なる分野、機関、国、または言語を比較する場合には、WoS と Scopus を慎重に使用する必要があることを示唆しています。書誌学コミュニティは、分野固有の引用インデックスや全国的な引用インデックスなど、WoS や Scopus ではカバーされていない科学的成果を含む手法や指標を開発する努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.29173/cais1943\\n Title: OpenAlex と Web of Science の間のドキュメント タイプの不一致を調査する Investigating Document Type Discrepancies between OpenAlex and the Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された OpenAlex は、そのオープン性と広範なカバレッジで際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この研究は、ドキュメント タイプの精度に焦点を当て、OpenAlex と WoS における作品のメタデータの品質を評価することを目的としています。 OpenAlex と WoS の両方でインデックス付けされている出版物の 4% 以上が研究論文またはレビューとして誤って分類されているようであり、これらのエラーの大部分 (約 97%) が OpenAlex で発生していることが観察されています。この研究は、文書タイプの不一致や誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, launched in 2022, stands out for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinder its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess the metadata quality of works in OpenAlex and WoS, focusing on document type accuracy. We observe that over 4% of the publications indexed in both OpenAlex and WoS appear to be misclassified as research articles or reviews, and that the vast majority (about 97%) of these errors occur in OpenAlex. By addressing discrepancies and misattributions in document types this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.1162/qss_a_00211\\n Title: 国際的な研究協力を測定するための書誌データソースの品質を評価する Assessing the quality of bibliographic data sources for measuring international research collaboration\\nAbstract: 国際研究協力 (IRC) の測定は、さまざまな研究評価タスクに不可欠ですが、どのデータソースを使用するかなど、さまざまな測定決定の影響については十分に研究されていません。データ ソースの選択が IRC 測定に及ぼす影響をより深く理解するために、利用可能なディメンションを確認して選択し、適切な計算可能なメトリクスを設計することにより、書誌データに特化したデータ品質評価フレームワークを設計および実装し、次にそれを書誌データの 4 つの一般的なソース (Microsoft Academic Graph、Web of Science (WoS)、Dimensions、ACM Digital Library) に適用してフレームワークを検証します。このフレームワークの検証が成功した場合、それが Wang と Strong (1996) によって提案された情報品質の一般的な概念フレームワークと一致しており、調査された情報源の品質の違いが適切に特定されることが示唆されます。フレームワークを適用すると、検討したセットの中で WS が全体的な品質が最も高いことがわかります。そして、品質の違いは主にデータ ソースの編成方法によって説明できると考えられます。私たちの研究は、研究者がこの IRC 測定ツールを研究に適用できるようにする方法論的な貢献で構成されており、書誌データの 4 つの一般的な情報源とその IRC 測定への影響をさらに特徴付けることによって実証的な貢献をしています。 Measuring international research collaboration (IRC) is essential to various research assessment tasks but the effect of various measurement decisions, including which data sources to use, has not been thoroughly studied. To better understand the effect of data source choice on IRC measurement, we design and implement a data quality assessment framework specifically for bibliographic data by reviewing and selecting available dimensions and designing appropriate computable metrics, and then validate the framework by applying it to four popular sources of bibliographic data: Microsoft Academic Graph, Web of Science (WoS), Dimensions, and the ACM Digital Library. Successful validation of the framework suggests it is consistent with the popular conceptual framework of information quality proposed by Wang and Strong (1996) and adequately identifies the differences in quality in the sources examined. Application of the framework reveals that WoS has the highest overall quality among the sets considered; and that the differences in quality can be explained primarily by how the data sources are organized. Our study comprises a methodological contribution that enables researchers to apply this IRC measurement tool in their studies and makes an empirical contribution by further characterizing four popular sources of bibliographic data and their impact on IRC measurement.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1007/s11192-015-1765-5', '10.48550/arXiv.2508.18620', '10.29173/cais1943', '10.1162/qss_a_00112', '10.1162/qss_a_00211']\n",
      "\u001b[93mSummary: Web of Science (WoS) is a bibliometric database that has been a major supporting infrastructure for over 30 years.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 and 10.29173/cais1943 both state that WoS has been a major supporting infrastructure for over 30 years.\n",
      "\n",
      "My lady, I am unable to find information about the strengths and weaknesses of WoS. Would you like me to answer a different question?\n",
      "96\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.400\n",
      "Recall: 0.400\n",
      "F1-Score: 0.400\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.6334335, '10.1007/s11192-015-1765-5'), (0.56431514, '10.48550/arXiv.2508.18620'), (0.54274267, '10.29173/cais1943'), (0.45914948, '10.1162/qss_a_00112'), (0.2705689, '10.1162/qss_a_00211')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.17821/srels/2024/v61i5/171583\\n Title: 図書館における質問回答ベースの検索システムの設計: オープンソースの検索拡張生成 (RAG) パイプラインのアプリケーション Designing Question-Answer Based Search System in Libraries: Application of Open Source Retrieval Augmented Generation (RAG) Pipeline\\nAbstract: この研究の主な目的は、プロトタイプを準備し、図書館が検索拡張生成 (RAG) フレームワークを通じてオープンソース ソフトウェア ツールと大規模言語モデル (LLM) を使用して低コストの会話型検索システムを開発できることを実証することです。 LLM は幻覚を起こし、時代遅れで文脈を理解していない応答を返すことがよくあります。ただし、この実験は、LLM が一連の関連文書で強化された場合、文脈に応じた適切な応答を提供できることを示しています。回答を生成する前に関連ドキュメントで LLM を拡張することは、検索拡張生成として知られています。この方法論には、LangChain などのツール、ChromaDB などのベクトル データベース、Llama3 (700 億のパラメーター ベースのモデル) などのオープンソース LLM を使用して RAG パイプラインを作成することが含まれていました。開発されたプロトタイプには、収集、処理され、パイプラインに取り込まれたチャンドラヤーン 3 ミッションに関する 250 以上の関連文書のデータセットが含まれています。最後に、研究では標準的な LLM と RAG 拡張を備えた LLM からの応答を比較しました。主な調査結果から、標準的な LLM (RAG なし) は、チャンドラヤーン 3 に関連するクエリに対して自信を持って不正確で幻覚のような応答を生成するのに対し、RAG を使用する LLM は、応答を生成する前に関連文書のセットが提供されると、一貫して正確で有益な、文脈に沿った応答を提供することが明らかになりました。この調査では、オープンソースの RAG ベースのシステムは、情報検索を強化し、図書館を動的な情報サービスに変えるための費用対効果の高いソリューションを図書館に提供すると結論付けています。 This study primarily aims to prepare a prototype and demonstrate that libraries can develop a low-cost conversational search system using open-source software tools and Large Language Models (LLMs) through a Retrieval-Augmented Generation (RAG) framework. LLMs often hallucinate and provide outdated and non-contextualized responses. However, this experiment shows that LLMs can deliver contextualized, relevant responses when augmented with a set of relevant documents. Augmenting LLMs with relevant documents before generating answers is known as retrieval-augmented generation. The methodology involved creating a RAG pipeline using tools like LangChain, vector databases like ChromaDB, and open-source LLMs like Llama3 (a 70-billion parameter-based model). The prototype developed includes a dataset of 250+ relevant documents on the Chandrayaan-3 mission that was collected, processed, and ingested into the pipeline. Finally, the study compared responses from standard LLMs and LLMs with RAG augmentation. Key findings revealed that standard LLMs (without RAG) produced confidently incorrect, hallucinated responses against queries related to Chandrayaan-3, while LLMs with RAG consistently provided accurate, informative, and contextualized answers when supplied with a set of relevant documents before generating the response. The study concluded that open-source RAG-based systems offer a cost-effective solution for libraries to enhance information retrieval and transform libraries into dynamic information services.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.48550/arXiv.2505.13557\\n Title: AMAQA: RAG システム用のメタデータベースの QA データセット AMAQA: A Metadata-based QA Dataset for RAG Systems\\nAbstract: 検索拡張生成 (RAG) システムは、質問応答 (QA) タスクで広く使用されていますが、現在のベンチマークにはメタデータの統合が不足しており、テキスト データと外部情報の両方を必要とするシナリオでの評価が妨げられています。これに対処するために、テキストとメタデータを組み合わせたタスクを評価するように設計された新しいオープンアクセス QA データセットである AMAQA を紹介します。メタデータの統合は、サイバーセキュリティやインテリジェンスなど、関連情報へのタイムリーなアクセスが重要な、大量のデータの迅速な分析が必要な分野で特に重要です。 AMAQA には、26 のパブリック Telegram グループから収集された約 110 万件の英語メッセージが含まれており、タイムスタンプ、トピック、感情の調子、毒性指標などのメタデータが充実しており、特定の基準に基づいてドキュメントをフィルタリングすることで、正確で文脈に応じたクエリを実行できます。また、450 の高品質 QA ペアも含まれており、メタデータ主導の QA および RAG システムの研究を進めるための貴重なリソースとなります。私たちの知る限り、AMAQA は、メッセージで扱われるトピックなどのメタデータとラベルを組み込んだ最初のシングルホップ QA ベンチマークです。私たちはベンチマークで広範なテストを実施し、将来の研究のための新しい基準を確立します。メタデータを活用すると精度が 0.12 から 0.61 に向上し、構造化コンテキストの価値が強調されることがわかりました。これに基づいて、提供されたコンテキストを反復処理し、ノイズの多いドキュメントで強化することで LLM 入力を洗練するためのいくつかの戦略を検討し、最良のベースラインよりもさらに 3 ポイントの向上を達成し、単純なメタデータ フィルタリングよりも 14 ポイントの改善を達成しました。 Retrieval-augmented generation (RAG) systems are widely used in question-answering (QA) tasks, but current benchmarks lack metadata integration, hindering evaluation in scenarios requiring both textual data and external information. To address this, we present AMAQA, a new open-access QA dataset designed to evaluate tasks combining text and metadata. The integration of metadata is especially important in fields that require rapid analysis of large volumes of data, such as cybersecurity and intelligence, where timely access to relevant information is critical. AMAQA includes about 1.1 million English messages collected from 26 public Telegram groups, enriched with metadata such as timestamps, topics, emotional tones, and toxicity indicators, which enable precise and contextualized queries by filtering documents based on specific criteria. It also includes 450 high-quality QA pairs, making it a valuable resource for advancing research on metadata-driven QA and RAG systems. To the best of our knowledge, AMAQA is the first single-hop QA benchmark to incorporate metadata and labels such as topics covered in the messages. We conduct extensive tests on the benchmark, establishing a new standard for future research. We show that leveraging metadata boosts accuracy from 0.12 to 0.61, highlighting the value of structured context. Building on this, we explore several strategies to refine the LLM input by iterating over provided context and enriching it with noisy documents, achieving a further 3-point gain over the best baseline and a 14-point improvement over simple metadata filtering.\\n', \"DOI: 10.48550/arXiv.2505.18247\\n Title: MetaGen Blended RAG: 専門分野の質問応答でゼロショットの精度を解放 MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索拡張生成 (RAG) は、ファイアウォールの背後に隔離されていることが多く、事前トレーニング中に LLM が認識できない複雑で特殊な用語が豊富に含まれる、ドメイン固有のエンタープライズ データセットに苦戦します。医学、ネットワーキング、法律などの分野にわたるセマンティックのばらつきが RAG のコンテキストの精度を妨げる一方、ソリューションを微調整するのはコストがかかり、時間がかかり、新しいデータが出現したときの汎用性が欠けています。微調整を行わずにレトリーバーでゼロショット精度を達成することは依然として重要な課題です。私たちは、メタデータ生成パイプラインと密ベクトルと疎ベクトルを使用したハイブリッド クエリ インデックスを通じてセマンティック リトリーバーを強化する新しいエンタープライズ検索アプローチである「MetaGen Blended RAG」を紹介します。主要な概念、トピック、頭字語を活用することで、私たちのメソッドはメタデータを強化したセマンティック インデックスと強化されたハイブリッド クエリを作成し、微調整することなく堅牢でスケーラブルなパフォーマンスを実現します。生物医学の PubMedQA データセットでは、MetaGen Blended RAG は 82% の検索精度と 77% の RAG 精度を達成し、以前のすべてのゼロショット RAG ベンチマークを上回り、そのデータセット上の微調整モデルに匹敵するだけでなく、SQuAD や NQ などのデータセットでも優れています。このアプローチは、特殊なドメイン全体にわたって比類のない一般化を備えたセマンティック検索ツールを構築する新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.17821/srels/2024/v61i5/171583', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2505.13557', '10.48550/arXiv.2505.18247']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a technique used to improve the accuracy and context-awareness of responses in question-answering and information retrieval systems, particularly when dealing with large language models (LLMs).\n",
      "\n",
      "DOI: 10.17821/srels/2024/v61i5/171583 discusses how RAG can be used to develop low-cost conversational search systems in libraries. By augmenting LLMs with relevant documents before generating answers, RAG helps provide accurate, informative, and contextualized responses, as demonstrated in a study comparing standard LLMs with RAG-augmented LLMs.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 introduces Multi-Meta-RAG, a method that improves RAG for multi-hop queries by using database filtering with LLM-extracted metadata. This approach enhances the selection of relevant documents, thereby improving the performance of RAG in complex question-answering tasks.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 proposes a Metadata Retrieval-Augmented Generation (M-RAG) method for open-domain question answering. By utilizing embeddings with metadata and generative models, M-RAG improves the understanding of document order and context, leading to more accurate and sourced responses.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.13557 presents AMAQA, a metadata-based QA dataset designed to evaluate RAG systems in scenarios requiring both textual data and external information. The integration of metadata significantly boosts accuracy, highlighting its importance in fields like cybersecurity and intelligence.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247 introduces MetaGen Blended RAG, a novel approach that enhances semantic retrievers for specialized domain question-answering. By leveraging metadata and hybrid query indexes, this method achieves robust zero-shot precision without fine-tuning, redefining enterprise search capabilities.\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions or need additional details, please let me know.\n",
      "96\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.81133276, '10.17821/srels/2024/v61i5/171583'), (0.72348577, '10.48550/arXiv.2406.13213'), (0.7211353, '10.6109/jkiice.2023.27.12.1489'), (0.72019166, '10.48550/arXiv.2505.13557'), (0.5614318, '10.48550/arXiv.2505.18247')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1045/september2016‐meschenmoser\\n Title: 科学 Web リポジトリのスクレイピング: 自動コンテンツ抽出の課題と解決策 Scraping Scientific Web Repositories: Challenges and Solutions for Automated Content Extraction\\nAbstract: 多くの科学 Web リポジトリは、科学出版物の可視性とアクセシビリティを向上させるだけでなく、h インデックスなどの指標を表示することによって、研究者の定量的および定性的な出版パフォーマンスも評価しています。これらの指標は、研究機関やその他の関係者にとって、採用や資金調達の決定など、影響力のある意思決定プロセスをサポートするために重要になっています。ただし、科学 Web リポジトリは通常、単純なパフォーマンス メトリクスと限られた分析オプションのみを提供します。さらに、パフォーマンス指標を計算するためのデータとアルゴリズムは通常公開されていません。したがって、システムがどの出版物を計算に含めるか、またシステムが結果をどのようにランク付けするかは透明性がなく、検証可能ではありません。多くの研究者は、これらのシステムの透明性を高めるために、基礎となるサイエントメトリーの生データにアクセスすることに興味を持っています。このペーパーでは、課題について説明し、科学 Web リポジトリ内のそのようなデータにプログラムでアクセスするための戦略を示します。 Google Scholar データに基づいて研究パフォーマンスを比較できるオープンソース ツール (MIT ライセンス) の一部として戦略を示します。ツールに含まれるスクレイパーは、リポジトリの運営者から同意が得られた場合にのみ使用する必要があることを強調したいと思います。私たちの経験では、研究目標が明確に説明され、プロジェクトが非営利的な性質のものであれば、同意が得られることがよくあります。 ide from improving the visibility and accessibility of scientific publications, many scientific Web repositories also assess researchers' quantitative and qualitative publication performance, e.g., by displaying metrics such as the h‐index. These metrics have become important for research institutions and other stakeholders to support impactful decision making processes such as hiring or funding decisions. However, scientific Web repositories typically offer only simple performance metrics and limited analysis options. Moreover, the data and algorithms to compute performance metrics are usually not published. Hence, it is not transparent or verifiable which publications the systems include in the computation and how the systems rank the results. Many researchers are interested in accessing the underlying scientometric raw data to increase the transparency of these systems. In this paper, we discuss the challenges and present strategies to programmatically access such data in scientific Web repositories. We demonstrate the strategies as part of an open source tool (MIT license) that allows research performance comparisons based on Google Scholar data. We would like to emphasize that the scraper included in the tool should only be used if consent was given by the operator of a repository. In our experience, consent is often given if the research goals are clearly explained and the project is of a non‐commercial nature.\\n\"]\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1045/september2016‐meschenmoser']\n",
      "\u001b[93mSummary: I'm sorry, my lady, I couldn't find any information about the main challenges in normalising citation metrics across scientific fields.\n",
      "\n",
      "DOI: 10.1045/september2016‐meschenmoser discusses the challenges and solutions for automated content extraction from scientific web repositories.\n",
      "96\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.000\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.495\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.14296049, '10.1045/september2016‐meschenmoser')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1007/s11192-022-04289-7\\n Title: 最も多く見つかる場所を検索: 56 の書誌データベースの懲戒範囲の比較 Search where you will find most: Comparing the disciplinary coverage of 56 bibliographic databases\\nAbstract: この論文では、新しいサイエントメトリクス手法を紹介し、それを学術界で人気のある英語に焦点を当てた書誌データベースの多くの主題範囲を推定するために適用します。この方法では、クエリ結果を共通の分母として使用して、さまざまな検索エンジン、リポジトリ、デジタル ライブラリ、その他の書誌データベースを比較します。この方法は、データベース カバレッジのより小さいセットを分析する既存のサンプリング ベースのアプローチを拡張します。この調査結果では、56 のデータベースの相対的および絶対的な対象範囲が示されており、これまで入手できなかった情報が示されています。データベースの絶対的な対象範囲を知ることで、特にルックアップ検索や探索的検索に関連する、高い再現率/感度が必要な検索に最も包括的なデータベースを選択できます。データベースの相対的な対象範囲を知ることで、特に体系的な検索に関連する、高い精度と特異性が必要な検索に特化したデータベースを選択できます。この調査結果は、Google Scholar、Scopus、または Web of Science の専門分野の範囲の違いだけでなく、分析頻度が低いデータベースの違いも示しています。たとえば、研究者は、Meta (廃止)、Embase、または Europe PMC が、医学やその他の健康分野の PubMed よりも多くの記録をカバーしていることが判明したことに驚くかもしれません。これらの発見は、研究者が新しく導入されたオプションに対しても頼りになるデータベースを再評価するよう促すはずです。より包括的なデータベースを使用して検索すると、特にシステマティック レビューやメタ分析など、最も適合するデータベースの選択に特別な考慮が必要な場合に、検索結果が向上します。この比較は、図書館員やその他の情報専門家が高価なデータベース調達戦略を再評価するのにも役立ちます。機関にアクセスできない研究者は、どのオープン データベースが自分の専門分野において最も包括的である可能性が高いかを学びます。 This paper introduces a novel scientometrics method and applies it to estimate the subject coverages of many of the popular English-focused bibliographic databases in academia. The method uses query results as a common denominator to compare a wide variety of search engines, repositories, digital libraries, and other bibliographic databases. The method extends existing sampling-based approaches that analyze smaller sets of database coverages. The findings show the relative and absolute subject coverages of 56 databases—information that has often not been available before. Knowing the databases’ absolute subject coverage allows the selection of the most comprehensive databases for searches requiring high recall/sensitivity, particularly relevant in lookup or exploratory searches. Knowing the databases’ relative subject coverage allows the selection of specialized databases for searches requiring high precision/specificity, particularly relevant in systematic searches. The findings illustrate not only differences in the disciplinary coverage of Google Scholar, Scopus, or Web of Science, but also of less frequently analyzed databases. For example, researchers might be surprised how Meta (discontinued), Embase, or Europe PMC are found to cover more records than PubMed in Medicine and other health subjects. These findings should encourage researchers to re-evaluate their go-to databases, also against newly introduced options. Searching with more comprehensive databases can improve finding, particularly when selecting the most fitting databases needs particular thought, such as in systematic reviews and meta-analyses. This comparison can also help librarians and other information experts re-evaluate expensive database procurement strategies. Researchers without institutional access learn which open databases are likely most comprehensive in their disciplines.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.48550/arXiv.2303.17661', '10.1007/s11192-022-04367-w', '10.1007/s11192-022-04289-7', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: I have found two documents that address your needs, my lady.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w\n",
      "Identifying and correcting invalid citations due to DOI errors in Crossref data\n",
      "This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661\n",
      "MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\n",
      "Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields.\n",
      "96\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.250\n",
      "Recall: 0.200\n",
      "F1-Score: 0.222\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.3829032, '10.48550/arXiv.2303.17661'), (0.3650723, '10.1007/s11192-022-04367-w'), (0.14534597, '10.1007/s11192-022-04289-7'), (0.10071066, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', \"DOI: 10.48550/arXiv.2404.13948\\n Title: RAG の背中を打ち砕いた ypos: 低レベルの摂動を介して野生のドキュメントをシミュレートすることによる、RAG パイプラインへの遺伝的攻撃 ypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\\nAbstract: 最近の大規模言語モデル (LLM) の適用可能性がさまざまなドメインや現実世界のアプリケーションに拡大するにつれて、その堅牢性がますます重要になってきています。検索拡張生成 (RAG) は、LLM の制限に対処するための有望なソリューションですが、RAG の堅牢性に関する既存の研究では、RAG コンポーネント間の相互接続関係や、軽微なテキスト エラーなど、現実のデータベースに蔓延する潜在的な脅威が見落とされていることがよくあります。この研究では、RAG の堅牢性を評価する際にまだ解明されていない 2 つの側面を調査します。1 つは低レベルの摂動によるノイズの多いドキュメントに対する脆弱性、2 つは RAG の堅牢性の全体的な評価です。さらに、これらの側面をターゲットとした新しい攻撃方法である RAG への遺伝的攻撃を紹介します。具体的には、GARAG は、各コンポーネント内の脆弱性を明らかにし、ノイズの多いドキュメントに対してシステム全体の機能をテストするように設計されています。 \\\\textit{GARAG} を標準 QA データセットに適用し、さまざまな取得者と LLM を組み込むことで、RAG の堅牢性を検証します。実験結果は、GARAG が一貫して高い攻撃成功率を達成していることを示しています。また、各コンポーネントのパフォーマンスとその相乗効果を著しく損なうため、わずかなテキストの不正確さが現実世界の RAG システムを混乱させる大きなリスクを浮き彫りにします。 The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1 vulnerability to noisy documents through low-level perturbations and 2 a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG, which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.48550/arXiv.2406.13213', '10.1145/3637528.3671470', '10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2404.13948']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a promising solution to the limitations of Large Language Models (LLMs), enhancing their accuracy and credibility by incorporating external knowledge.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - RAG combines LLMs' intrinsic knowledge with external databases, improving performance on knowledge-intensive tasks and enabling continuous knowledge updates.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - RAG enables LLMs to answer queries over unseen document collections, but struggles with multi-hop questions. Multi-Meta-RAG improves this by using LLM-extracted metadata for database filtering.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470 - RAG provides reliable, up-to-date external knowledge, aiding AI-generated content (AIGC) and addressing LLMs' limitations like hallucinations.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16 - RAG systems consist of retrieval and generation modules, reducing hallucinations by providing LLMs with external knowledge. RAGAs offers a reference-free evaluation framework.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.13948 - RAG's robustness is crucial, but existing studies overlook interconnected relationships and real-world threats. GARAG is introduced to test RAG's vulnerability to noisy documents.\n",
      "\n",
      "My lady, is there anything else I can assist you with?\n",
      "96\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.400\n",
      "Recall: 0.400\n",
      "F1-Score: 0.400\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.5488434, '10.48550/arXiv.2312.10997'), (0.52685463, '10.48550/arXiv.2406.13213'), (0.4893821, '10.1145/3637528.3671470'), (0.4833807, '10.18653/v1/2024.eacl-demo.16'), (0.3458787, '10.48550/arXiv.2404.13948')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n', \"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1145/2964909', '10.1109/ADL.1998.670425', '10.1177/09610006241239080', '10.5860/crl.86.1.101', '10.48550/arXiv.2303.17661']\n",
      "\u001b[93mSummary: Metadata quality assessment is crucial for data retrieval, organization, interoperability, preservation, and user experience. Key challenges include the lack of consensus on definitions and measurements of quality dimensions, cultural and resource-related tensions affecting metadata consistency and completeness, and the need for automated, efficient evaluation methods. Advancements include the development of frameworks like MetaEnhance, which uses AI to improve metadata quality, and efforts to standardize metadata schemas across different portal software frameworks.\n",
      "\n",
      "DOI: 10.1145/2964909 - This study focuses on developing a generic metadata quality assessment framework for open data portals, addressing challenges in searchability, discoverability, and usability. It maps metadata from CKAN, Socrata, and OpenDataSoft to a standardized schema and defines quality metrics for automated evaluation.\n",
      "\n",
      "DOI: 10.1109/ADL.1998.670425 - This research discusses qualitative and quantitative techniques for assessing metadata records, particularly in the context of the US Government Information Locator Service (GILS). It highlights the need for diverse criteria to evaluate different types of metadata.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - A scoping review identifying commonly measured dimensions of metadata quality, such as completeness, accuracy, and consistency. It emphasizes the lack of consensus on definitions and the need for further research into less commonly assessed dimensions.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 - This study explores how metadata quality issues arise from sociocultural representations, resource constraints, and standardized systems. It identifies 32 unique issues and categorizes them to understand their impact on individuals and communities.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661 - Introduces MetaEnhance, a framework using AI to automatically detect, correct, and normalize metadata for electronic theses and dissertations. It demonstrates high effectiveness in error detection and correction across multiple fields.\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions or need additional details, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in metadata quality assessment?']:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.000\n",
      "Accuracy: 0.896\n",
      "Balanced accuracy: 0.473\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.6636041, '10.1145/2964909'), (0.4874788, '10.1109/ADL.1998.670425'), (0.38741276, '10.1177/09610006241239080'), (0.37305376, '10.5860/crl.86.1.101'), (0.2738236, '10.48550/arXiv.2303.17661')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.1080/19386389.2011.570654\\n Title: メタデータ レコードの問題の分析 An Analysis of Problems in Metadata Records\\nAbstract: メタデータはデジタル ライブラリにおいて重要な役割を果たします。しかし、有用であるためには、メタデータ レコードに問題がなくなければなりません。メタデータに問題があると、リソースが正しく表現されず、ユーザーはメタデータのメリットを享受できなくなります。このような問題を排除しないにしても、最小限に抑えるには、メタデータ レコードで発生する可能性のある問題の種類を理解することが不可欠です。この論文では、文献で報告されているメタデータ レコードで見つかった問題を比較および分析します。メタデータの問題の 5 つのカテゴリを特定できることがわかります。これらは、不正な値、不正な要素、情報の欠落、情報損失、および一貫性のない値の表現の問題です。これらの問題がメタデータによって提供できるサービスに悪影響を与えることを考慮すると、メタデータの使用から得られる利点と、メタデータ レコードの作成に費やされるコストと労力のバランスが確保されるように、予防または是正措置を講じる必要があります。 Metadata plays an important role in digital libraries. But to be useful, metadata records must be problem free. When problems are present in the metadata, resources are not correctly represented and users are not able to reap the benefits of metadata. To minimize, if not eliminate, such problems, it is essential to understand the kinds of problems that can occur in metadata records. In this paper, problems found in metadata records as reported in the literature are compared and analyzed. It is found that five categories of metadata problems can be identified. These are the problems of Incorrect Values, Incorrect Elements, Missing Information, Information Loss, and Inconsistent Value Representation. Given that these problems are detrimental to the services that can be provided by metadata, preventive or corrective measures need to be put in place so as to ensure that the benefits derived from using metadata balance the costs and efforts spent in the creation of metadata records.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1007/s11192-023-04923-y\\n Title: OpenAlex に機関が存在しない: 考えられる理由、影響、および解決策 Missing institutions in OpenAlex: possible reasons, implications, and solutions\\nAbstract: オープン サイエンスの到来により、高いデータ品質を備えたオープン データ プラットフォームが必要になります。 2022 年 1 月に開始されたグローバル研究システムの完全にオープンなカタログである OpenAlex は、データへの簡単なアクセスと、量的科学研究で広く使用されている幅広いデータ範囲という 2 つの主な利点を備えています。注目すべきことに、OpenAlex はライデン大学ランキングの重要なデータ ソースとして採用されています。ただし、OpenAlex の雑誌記事メタデータには機関が欠落しているという深刻なデータ品質の問題があります。この研究では、完全な機関情報 (FII)、部分的に欠落している機関情報 (PMII)、および完全に欠落している機関情報 (CMII) という 3 つのタイプの機関情報を定義することにより、問題の考えられる理由とその結果と解決策を調査します。私たちの結果は、OpenAlex のジャーナル記事の 60% 以上で機関の欠落の問題が発生していることを示しています。この問題は、初期のメタデータや社会科学、人文科学で特に蔓延しています。データのサブサンプルを使用して、問題の考えられる理由、歪んだ結果をもたらす可能性のあるリスク、行方不明の機関の問題に対する考えられる解決策をさらに調査します。その目的は、オープン リソースにおけるデータ品質の向上の重要性を高め、それによって量的科学の研究やより広範な文脈においてオープン リソースの責任ある使用をサポートすることです。 The advent of open science calls for open data platforms with high data quality. As a fully open catalog of the global research system launched in January 2022, OpenAlex features two main advantages of easy data accessibility and broad data coverage, which has been widely used in quantitative science studies. Remarkably, OpenAlex is adopted as an important data source for Leiden university ranking. However, there is a severe data quality problem of missing institutions in journal article metadata in OpenAlex. This study investigates the possible reasons for the problem and its consequences and solutions by defining three types of institutional information—full institutional information (FII), partially missing institutional information (PMII) and completely missing institutional information (CMII). Our results show that the problem of missing institutions occurs in more than 60% of the journal articles in OpenAlex. The problem is particularly widespread in metadata from the early years and in the social sciences and humanities. Using sub-samples of the data, we further explore the possible reasons for the problem, the risk it might represent for distorted results, and possible solutions to the problem of missing institutions. The aim is to raise the importance of data quality improvements in open resources, and thus to support the responsible use of open resources in quantitative science studies and also in broader contexts.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.5860/crl.86.1.101', '10.48550/arXiv.2303.17661', '10.1080/19386389.2011.570654', '10.1177/09610006241239080', '10.1007/s11192-023-04923-y']\n",
      "\u001b[93mSummary: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, metadata often exhibits incomplete, inconsistent, and incorrect values. To address these challenges, methods for automatic detection, correction, and canonicalization of scholarly metadata are being investigated.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661 - This study investigates methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. The proposed framework, MetaEnhance, utilizes state-of-the-art artificial intelligence methods to improve metadata quality.\n",
      "\n",
      "My lady, I hope this response meets your requirements. If you have any further questions or need additional information, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and potential solutions for improving metadata quality?']:\n",
      "Precision: 0.800\n",
      "Recall: 0.800\n",
      "F1-Score: 0.800\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.895\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.5573882, '10.5860/crl.86.1.101'), (0.46701634, '10.48550/arXiv.2303.17661'), (0.43467787, '10.1080/19386389.2011.570654'), (0.40952834, '10.1177/09610006241239080'), (0.29672605, '10.1007/s11192-023-04923-y')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 14 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模言語モデルの検索拡張生成: 調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模言語モデル (LLM) は優れた機能を備えていますが、幻覚、古い知識、不透明で追跡できない推論プロセスなどの課題に直面しています。検索拡張生成 (RAG) は、外部データベースからの知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の精度と信頼性が向上し、継続的な知識の更新とドメイン固有の情報の統合が可能になります。 RAG は、LLM の固有の知識を外部データベースの広大で動的なリポジトリと相乗的に結合します。この包括的なレビュー ペーパーでは、Naive RAG、Advanced RAG、および Modular RAG を含む、RAG パラダイムの進歩の詳細な調査を提供します。これは、取得、生成、拡張技術を含む RAG フレームワークの 3 つの要素からなる基盤を細心の注意を払って精査します。この文書では、これらの重要なコンポーネントのそれぞれに組み込まれた最先端のテクノロジーに焦点を当て、RAG システムの進歩についての深い理解を提供します。さらに、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を概説し、研究開発の予想される道筋を指摘します。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", \"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", \"DOI: 10.48550/arXiv.2505.18247\\n Title: MetaGen Blended RAG: 専門分野の質問応答でゼロショットの精度を解放 MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索拡張生成 (RAG) は、ファイアウォールの背後に隔離されていることが多く、事前トレーニング中に LLM が認識できない複雑で特殊な用語が豊富に含まれる、ドメイン固有のエンタープライズ データセットに苦戦します。医学、ネットワーキング、法律などの分野にわたるセマンティックのばらつきが RAG のコンテキストの精度を妨げる一方、ソリューションを微調整するのはコストがかかり、時間がかかり、新しいデータが出現したときの汎用性が欠けています。微調整を行わずにレトリーバーでゼロショット精度を達成することは依然として重要な課題です。私たちは、メタデータ生成パイプラインと密ベクトルと疎ベクトルを使用したハイブリッド クエリ インデックスを通じてセマンティック リトリーバーを強化する新しいエンタープライズ検索アプローチである「MetaGen Blended RAG」を紹介します。主要な概念、トピック、頭字語を活用することで、私たちのメソッドはメタデータを強化したセマンティック インデックスと強化されたハイブリッド クエリを作成し、微調整することなく堅牢でスケーラブルなパフォーマンスを実現します。生物医学の PubMedQA データセットでは、MetaGen Blended RAG は 82% の検索精度と 77% の RAG 精度を達成し、以前のすべてのゼロショット RAG ベンチマークを上回り、そのデータセット上の微調整モデルに匹敵するだけでなく、SQuAD や NQ などのデータセットでも優れています。このアプローチは、特殊なドメイン全体にわたって比類のない一般化を備えたセマンティック検索ツールを構築する新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\", 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1016/j.caeai.2025.100417', '10.48550/arXiv.2505.18247', '10.1007/s44427-025-00006-3', '10.1145/3637528.3671470']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) systems face challenges such as hallucinations, outdated knowledge, and non-transparent reasoning processes. However, advancements in RAG, such as MetaGen Blended RAG, have shown promise in improving accuracy and scalability across specialized domains.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - This paper discusses the challenges faced by Large Language Models (LLMs), including hallucinations, outdated knowledge, and non-transparent reasoning processes. It highlights RAG as a promising solution by incorporating external knowledge, enhancing accuracy and credibility, and enabling continuous knowledge updates.\n",
      "\n",
      "DOI: 10.1016/j.caeai.2025.100417 - Focusing on educational applications, this paper addresses challenges like hallucinations and static internal knowledge in LLMs. It emphasizes RAG's role in improving factual accuracy and enabling dynamic knowledge updates, making it suitable for educational scenarios.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247 - This study introduces MetaGen Blended RAG, a novel approach that enhances semantic retrievers and achieves high zero-shot precision in specialized domains like medicine and law, addressing the challenge of domain-specific data.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3 - This research evaluates open-source LLMs in RAG systems using the Ragas framework, highlighting the challenges in assessing retrieval and generation components and providing insights into optimizing RAG implementations.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470 - This survey reviews Retrieval-Augmented Large Language Models (RA-LLMs), discussing their ability to provide reliable external knowledge and enhance AI-generated content quality, while also addressing current limitations and future research directions.\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions or need additional information, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in Retrieval-Augmented Generation (RAG) systems?']:\n",
      "Precision: 0.600\n",
      "Recall: 0.600\n",
      "F1-Score: 0.600\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.789\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9224337, '10.48550/arXiv.2312.10997'), (0.87681407, '10.1016/j.caeai.2025.100417'), (0.7272211, '10.48550/arXiv.2505.18247'), (0.6641271, '10.1007/s44427-025-00006-3'), (0.6102915, '10.1145/3637528.3671470')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 15 in loop: 1\n",
      "\u001b[95m!!!!! All Done!!!!!\n"
     ]
    }
   ],
   "source": [
    "#run the test from here\n",
    "\n",
    "\n",
    "#***** Begin chat session *****\n",
    "# set directory path\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_jats\"\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_multi_lang_ja\"\n",
    "\n",
    "# read documents and dois from the directory path\n",
    "documents_with_doi = read_documents_with_doi(directory_path)\n",
    "documents = [doc[0].split('\\n')[1:] for doc in documents_with_doi]\n",
    "print(f\"Length of documents: {len(documents)}\")\n",
    "print(f\"Length of corpus: {len(documents_with_doi)}\")\n",
    "\n",
    "# Countdown function\n",
    "def countdown(seconds:int)->None:\n",
    "    # Loop until seconds is 0\n",
    "    while seconds > 0:\n",
    "        print(Fore.LIGHTMAGENTA_EX + f\"{seconds}\", end='      \\r')  # Print current countdown value\n",
    "        time.sleep(1)  # Wait for 1 second\n",
    "        seconds -= 1  # Decrease seconds by 1\n",
    "    print(\"The time has come!\")  # Countdown finished message\n",
    "\n",
    "def evaluate_retrieval(retrieved_dois, ground_truth, response, query:str,reranked_DOIs_with_score_end)->Dict:\n",
    "    corpus_doi_list = []\n",
    "    #corpus_list is a global variable in rag_pipeline()\n",
    "    for each in range(len(documents_with_doi)):\n",
    "        #a = documents_with_doi[each].get('doi',\"\")\n",
    "        a = documents_with_doi[each].split(\"\\n\")[0].lstrip(\"DOI: \")\n",
    "        corpus_doi_list.append(a)\n",
    "    print(len(corpus_doi_list))\n",
    "\n",
    "    def compare_lists(list1, list2, list3):\n",
    "        for val in list1:\n",
    "            if val in list2:\n",
    "                list3.append(1)\n",
    "            else:\n",
    "                list3.append(0)\n",
    "\n",
    "    #set y_true so that len(y_true)==len(corpus_doi_list)\n",
    "    y_true = []\n",
    "    compare_lists(corpus_doi_list,ground_truth,y_true)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = []\n",
    "    compare_lists(corpus_doi_list,retrieved_dois,y_pred)\n",
    "\n",
    "\n",
    "    # calculate metrics - could also use sklearn.metrics functions such as precision_score, but this is easier to read\n",
    "    precision = precision_score(y_true, y_pred,)\n",
    "    recall = recall_score(y_true, y_pred,)\n",
    "    f1 = f1_score(y_true, y_pred,)\n",
    "    accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    faithfulness_score = 0\n",
    "    for each in retrieved_dois:\n",
    "        if each in response.message.content[0].text:\n",
    "            faithfulness_score+=1\n",
    "        else:\n",
    "            faithfulness_score+=0\n",
    "\n",
    "        \n",
    "    return {\n",
    "        'Query':f\"{query}\",\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\":accuracy,\n",
    "        \"Balanced accuracy\":balanced_accuracy,\n",
    "        \"Faithfulness score\":faithfulness_score,\n",
    "        \"Documents score\":str(reranked_DOIs_with_score_end),\n",
    "        \"Response\":response.message.content[0].text\n",
    "    }\n",
    "\n",
    "def print_results(retrieved_dois, ground_truth, response, query:str, reranked_DOIs_with_score_end)->Dict:\n",
    "    \"\"\"\n",
    "    Prints a nicely ordered set of results from evalaute_retrieval()\n",
    "    \"\"\"\n",
    "\n",
    "    results = evaluate_retrieval(retrieved_dois, ground_truth, response, query, reranked_DOIs_with_score_end)\n",
    "    print(f\"For query: {results['Query']}:\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"F1-Score: {results['F1-Score']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    print(f\"Balanced accuracy: {results['Balanced accuracy']:.3f}\")\n",
    "    print(f\"Faithfulness score: {results['Faithfulness score']}\")\n",
    "    print(f\"Documents score: {results['Documents score']}\")\n",
    "    return results\n",
    "\n",
    "#print_results()\n",
    "\n",
    "def cohere_test_loop(query:str,ground_truth:List[str]):\n",
    "\n",
    "    # set top_k\n",
    "    top_k = 5\n",
    "    #set threshold \n",
    "    threshold = 0.10\n",
    "\n",
    "    response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,query,top_k,threshold)\n",
    "    \n",
    "    # Extract DOIs from retrieved documents\n",
    "    retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "    print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "    # Display the response\n",
    "    print(Fore.LIGHTYELLOW_EX + f\"{response.message.content[0].text}\")\n",
    "\n",
    "    new_result = print_results(retrieved_dois, ground_truth, response, query, reranked_DOIs_with_score_end)\n",
    "    # add the new result to the df\n",
    "    results_df.loc[len(results_df)] = new_result\n",
    "\n",
    "    #save the queries and responses to separate dataframe to be manually annontated\n",
    "    answer_relevance_df = results_df[['Query','Response']].copy(deep=True)\n",
    "\n",
    "    # save out answer_relevance_df\n",
    "    filename=\"analysis/Round1/results/multi_lang_answer_relevance_results.xlsx\"\n",
    "    answer_relevance_df.to_excel(filename)\n",
    "\n",
    "    filename = \"analysis/Round1/results/multi_lang_analysis_results.xlsx\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    results_df.to_excel(filename)\n",
    "\n",
    "    # rate limit functions\n",
    "    seconds = 10\n",
    "    print(Fore.LIGHTRED_EX + f\"Waiting for {seconds} seconds...\")\n",
    "    countdown(seconds)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "#golden_set_df_test['Response\\nDense'] = golden_set_df_test.apply(lambda x: test_loop(x.query,x.ground_truth), axis=1)\n",
    "golden_set_df_query = golden_set_df['query'].to_list()\n",
    "golden_set_df_ground_truth = golden_set_df['ground_truth'].to_list()\n",
    "\n",
    "loop_length = 5\n",
    "while loop_length:\n",
    "    for i in range(len(golden_set_df_query)):\n",
    "        \n",
    "        cohere_test_loop([golden_set_df_query[i]],golden_set_df_ground_truth[i])\n",
    "        print(Fore.LIGHTCYAN_EX + f\"Working on row: {i} in loop: {loop_length}\")\n",
    "    loop_length = loop_length-1\n",
    "\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"!!!!! All Done!!!!!\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
