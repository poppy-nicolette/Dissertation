{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test of BM25s\n",
    "BM25 Sparse\n",
    "https://bm25s.github.io/\n",
    "\n",
    "[ ] installed in venv bm25s\n",
    "\n",
    "# Install with all extra dependencies\n",
    "pip install bm25s[full]\n",
    "\n",
    "# installed nltk for porter stemmer\n",
    "pip install nltk\n",
    "\n",
    "\n",
    "## documentation\n",
    "https://github.com/xhluca/bm25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import bm25s\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "# Create your corpus here\n",
    "## ********** change this to retrieve the data/documents.txt file for short term\n",
    "## ********** long term: chanage this to query via SQL to DBeaver ofi database\n",
    "input_dir = '/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data'\n",
    "\n",
    "#create list\n",
    "corpus = []\n",
    "\n",
    "url = []\n",
    "\n",
    "#read each file in input_dir\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(input_dir,file_name)\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "\n",
    "            #extract title and abstract - I think we also need 'landing_page_url' for retrieval\n",
    "            title = None\n",
    "            abstract = None\n",
    "            keyword = None\n",
    "            doi = None\n",
    "\n",
    "            for line in content.split('\\n'):\n",
    "                if line.startswith('DOI: '):\n",
    "                    doi = line[len('DOI: '):].strip()\n",
    "                if line.startswith('Title:'):\n",
    "                    title = line[len('Title:'):].strip()\n",
    "                elif line.startswith('Abstract:'):\n",
    "                    abstract = line[len('Abstract:'):].strip()\n",
    "                #elif line.startswith('keyword:'):\n",
    "                #    keyword = line[len('keyword:'):].strip()\n",
    "            \n",
    "            if title and abstract:\n",
    "                if keyword is not None:\n",
    "                    #combine into single entry\n",
    "                    document = f\"{title} {abstract} {keyword}\"\n",
    "                    corpus.append(document)\n",
    "                else:\n",
    "                    document = f\"{title} {abstract}\"\n",
    "                    corpus.append(document)\n",
    "\n",
    "            #extract landing_page_url and combine with title\n",
    "            #for line in content.split('\\n'):\n",
    "            #    if line.startswith('DOI'):\n",
    "            #        doi = line[len('DOI'):].strip()\n",
    "\n",
    "            if title and doi:\n",
    "                resolver_doi = f\"https://doi.org/{doi}\"\n",
    "                link = f\"URL: {resolver_doi} for title: {title}\"\n",
    "                url.append(link)\n",
    "                    \n",
    "\n",
    "\n",
    "#export corpus and url lists for import later\n",
    "with open('corpus.pkl', 'wb') as file:\n",
    "    pickle.dump(corpus, file)\n",
    "\n",
    "with open('url.pkl','wb') as file:\n",
    "    pickle.dump(url, file)\n",
    "\n",
    "print('complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    }
   ],
   "source": [
    "#optional stemmer\n",
    "#stemmer = Stemmer.Stemmer(\"english\")\n",
    "\n",
    "#Tokenize the corpus and index it - removes stopwords \n",
    "#you can also add a stemmer here as an arg: stemmer=stemmer\n",
    "corpus_tokens = bm25s.tokenize(corpus, stopwords=\"en\", show_progress=True)\n",
    "\n",
    "# Create the BM25 model and index the corpus\n",
    "retriever = bm25s.BM25(corpus=corpus) # this arg returns docs, not ids. to have id's remove this arg\n",
    "retriever.index(corpus_tokens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding newlines for mmindex: 100%|██████████| 59.0k/59.0k [00:00<00:00, 70.8MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# you can save the index\n",
    "\n",
    "# You can save the corpus along with the model\n",
    "retriever.save(\"data/bm25\", corpus=corpus)\n",
    "\n",
    "# ...and load them when you need them\n",
    "retriever = bm25s.BM25.load(\"data/bm25\", load_corpus=True)\n",
    "# set load_corpus=False if you don't need the corpus\n",
    "\n",
    "# reimport corpus and url lists\n",
    "with open('corpus.pkl', 'rb') as file:\n",
    "    corpus_list = pickle.load(file)\n",
    "print(len(corpus_list))\n",
    "with open('url.pkl', 'rb') as file:\n",
    "    url_list = pickle.load(file)\n",
    "print(len(url_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 (score: 0.62): URL: https://doi.org/10.1162/qss_a_00022 for title: Crossref: The sustainable source of community-owned scholarly metadata\n",
      "Rank 2 (score: 0.60): URL: https://doi.org/10.31222/osf.io/smxe5 for title: Crossref as a source of open bibliographic metadata\n",
      "Rank 3 (score: 0.60): URL: https://doi.org/10.5860/crl.86.1.101 for title: Identifying Metadata Quality Issues Across Cultures\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#You can now search the corpus with a query\n",
    "query = input(\"what is your query\")\n",
    "#you can also add a stemmer here as an arg: stemmer=stemmer\n",
    "query_tokens = bm25s.tokenize(query,\n",
    "                            stopwords=True,\n",
    "                            lower=True)\n",
    "results, scores = retriever.retrieve(query_tokens, k=3)\n",
    "\n",
    "#note: if you pass a new corpus here, it must have the same length as your indexed corpus\n",
    "#in this case, I am passing the new list 'url_list' - it contains just the DOI and title\n",
    "results = retriever.retrieve(query_tokens, corpus=url_list, k=3, return_as=\"documents\")\n",
    "#loop through results\n",
    "if all(score ==0.00 for score in scores[0]):\n",
    "    print(\"Nothing found, please try another query.\")\n",
    "else:\n",
    "    for i in range(results.shape[1]):\n",
    "        doc, score = results[0, i], scores[0, i]\n",
    "        print(f\"Rank {i+1} (score: {score:.2f}): {doc}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is working using a dedicated virtual env.\n",
    "- [ ] get this working in the same environment as app.py for the LNS project, 3.12 Python\n",
    "- [ ] set up requirements\n",
    "- [ ] turn into function module to import into app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import bm25s\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "# ...and load them when you need them\n",
    "retriever = bm25s.BM25.load(\"lns_bm25\", load_corpus=True)\n",
    "# set load_corpus=False if you don't need the corpus\n",
    "\n",
    "#load the url_list\n",
    "with open('url.pkl', 'rb') as file:\n",
    "    url_list = pickle.load(file)\n",
    "print(len(url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 (score: 1.25): URL: : https://doi.org/10.1080/17408989.2021.2014438 for title: PLitPE: an intervention for physical literacy enriched pedagogy in Canadian elementary school physical education classes\n",
      "Rank 2 (score: 1.24): URL: : https://doi.org/10.1186/s12889-018-5897-4 for title: The relationship between physical literacy scores and adherence to Canadian physical activity and sedentary behaviour guidelines\n",
      "Rank 3 (score: 1.23): URL: : https://doi.org/10.1007/s11125-020-09519-5 for title: Increasing physical literacy in youth: A two-week Sport for Development program for children aged 6-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#You can now search the corpus with a query\n",
    "query = input(\"what is your query\")\n",
    "#you can also add a stemmer here as an arg: stemmer=stemmer\n",
    "query_tokens = bm25s.tokenize(query)\n",
    "results, scores = retriever.retrieve(query_tokens, k=3)\n",
    "\n",
    "#note: if you pass a new corpus here, it must have the same length as your indexed corpus\n",
    "#in this case, I am passing the new list 'url_list'\n",
    "results = retriever.retrieve(query_tokens, corpus=url_list, k=3, return_as=\"documents\")\n",
    "#loop through results\n",
    "if all(score ==0.00 for score in scores[0]):\n",
    "    print(\"Nothing found, please try another query.\")\n",
    "else:\n",
    "    for i in range(results.shape[1]):\n",
    "        doc, score = results[0, i], scores[0, i]\n",
    "        print(f\"Rank {i+1} (score: {score:.2f}): {doc}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
