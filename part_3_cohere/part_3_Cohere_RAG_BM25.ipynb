{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere API and SciBERT with BM25 as first stage retriever for RAG\n",
    "This notebook uses a Cohere API for generating responses to text. A query input is required from the user. \n",
    "SciBERT is used for embeddings in a dense vector array for the query. \n",
    "This version is different in that it uses BM25 as a sparse vectorizer for the input text. Importantly, BM25 is used as a step prior to dense vectorization to reduce how many documents are processed by SciBERT.\n",
    "A DOI is supplied with the text as both an identifier and locator. \n",
    "\n",
    "## pipeline\n",
    "1. BM25 Retrieval\n",
    "    - BM25 is used to retrieve top-k candidate documents based on keyword matching\n",
    "2. Dense embedding retrieval\n",
    "    - query is embedded using SciBERT and the retrieved documents.\n",
    "3. Re-ranking\n",
    "    - cosine similarity between query embedding and document embedding to rerank candidate docs\n",
    "4. Generation\n",
    "    - docs and query are fed to generator for answer creation. \n",
    "\n",
    "- [ ] set up venv\n",
    "- [ ] install transformers torch cohere in command line\n",
    "\n",
    "### todo\n",
    "- [ ] create script that compiles data/documents.txt with DOI || text for all documents\n",
    "- [ ] rank_bm25: https://github.com/dorianbrown/rank_bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import cohere\n",
    "from cohere import Client\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import logging # finding where functions are taking too long\n",
    "\n",
    "\n",
    "def main():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    global key,email\n",
    "    key = os.getenv('COHERE_API_KEY')\n",
    "    email = os.getenv('EMAIL')\n",
    "\n",
    "    #verify if it worked\n",
    "    if email is not None and key is not None:\n",
    "        print(\"all is good, beautiful!\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is callable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(key) \n",
    "\n",
    "# Load SciBERT model and tokenizer\n",
    "\"\"\"\n",
    "documentation can be found here: https://huggingface.co/docs/transformers/v4.50.0/en/model_doc/auto#transformers.AutoTokenizer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Initialize tokenizer with custom parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    max_len=512,\n",
    "    use_fast=True,  # Use the fast tokenizer\n",
    "    do_lower_case=False,  # Preserve case\n",
    "    add_prefix_space=False,  # No prefix space\n",
    "    never_split=[\"[DOC]\", \"[REF]\"],  # Tokens to never split\n",
    "    additional_special_tokens=[\"<doi>\", \"</doi>\"]  # Add custom special tokens\n",
    ")\n",
    "\n",
    "# This is the SciBERT model that is used to embed the text and query.\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "#verify that the model is callable\n",
    "if callable(model):\n",
    "    print(\"Model is callable\")\n",
    "else:\n",
    "    print(\"Model is not callable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addition of BM25 for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents:\n",
      "Score: 0.7389, DOI: 10.1002/jaal.551, Document: Title: Reading for Reliability: Preservice Teachers Evaluate Web Sources About Climate Change\n",
      "Abstract: Abstract This study examined what happened when 65 undergraduate prospective secondary level teachers across content areas evaluated the reliability of four online sources about climate change: an oil company webpage, a news report, and two climate change organizations with competing views on climate change. The students evaluated the sources at three time intervals based on 1. a screenshot of each source; 2. full web access to each source and prompted with critical questions to answer; and 3. after a whole class discussion about each source. Having the opportunity to evaluate the sources three times led students to modify their reliability ratings. Findings also reveal challenges some participants had differentiating between facts and opinions as well as distinctions in what they determined to be evidence in a source.\n",
      "Score: 0.7248, DOI: 10.1080/13504622.2017.1330949, Document: Title: Probing into the sources of ignorance: science teachers’ practices of constructing arguments or rebuttals to denialism of climate change\n",
      "Abstract: This study focuses on the nature of teachers' arguments for and rebuttals to 10 denial theories about anthropogenic climate change that are most commonly encountered in the media and public debates. Through a semi-structured survey, the study collected data from 24 participants who are K-12 teachers in Maryland and Delaware. The deductive coding and frequency analysis of data shows that although all participants of our study agree with the importance of teaching anthropogenic climate change, some teachers agree with the denial theories of climate change. The arguments for the denial theories show less epistemic quality than the rebuttals against denial theories. Moreover, teachers went beyond the textbooks and searched for other varied sources of information. However, we noticed that teachers might still doubt the anthropogenic causes of climate change. The study further uses intertextual discourse analysis to explore the reasons why use of sources might still leave teachers confused.\n",
      "Score: 0.7194, DOI: 10.1080/00958964.2015.1050955, Document: Title: Greek Pre-Service Teachers’ Knowledge of Ocean Sciences Issues and Attitudes Toward Ocean Stewardship\n",
      "Abstract: Greek pre-service teachers' level of ocean literacy was assessed using a revised questionnaire concerning ocean content knowledge and an instrument about ocean stewardship. Rasch analyses showed that the items of both measures were well targeted to the sample. Pre-service teachers possessed a moderate knowledge of ocean sciences issues and positive attitudes toward ocean stewardship; they obtained most information on ocean content from the Internet and mass media and less from formal education, nongovernmental organizations, books, and out-of-school settings. Students who mostly preferred the Internet and mass media scored significantly higher on the knowledge questionnaire. The results could contribute to the enhancement of teachers' ocean literacy.\n",
      "\n",
      "Generated Response:\n",
      " Textbooks are a common source of information for students, often used to teach curriculum-based content and supplement learning in a range of subjects. Textbooks are particularly useful in providing a comprehensive overview of a topic within the scope of a curriculum, offering structured knowledge, and guiding practical lessons. \n",
      "\n",
      "Textbooks are specifically referenced in the study of Greek pre-service teachers' knowledge of ocean sciences issues. The study assesses Greek pre-service teachers' level of ocean literacy using questionnaires focusing on ocean content knowledge and an instrument about ocean stewardship. The findings show that pre-service teachers possessed moderate knowledge of ocean sciences and positive attitudes towards ocean stewardship. The study further concludes that information from the Internet and mass media, including textbooks, contributed to this knowledge. These\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic RAG with BN25 retriever with Cohere model\n",
    "Document source: data folder \n",
    "Saved as UTF-8\n",
    "\n",
    "Returns:  answers based on query from input()\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate embeddings using SciBERT\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    converts raw text to numerical representations using a pretrained model, in this case, SciBERT.\n",
    "    Currently this is applied to both the document text and the query. \n",
    "    May want a different version or decorator for the query as they are generally much shorter and more sparse.\n",
    "\n",
    "    Input: text from tokenizer step above as a list of strings\n",
    "    Output: np.array\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512, # returns PyTorch tensors which are compatible with model\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True # return the attention mask - need to learn more\n",
    "        )\n",
    "\n",
    "    # this passes the tokenized inputs through the model    \n",
    "    outputs = model(**inputs)\n",
    "    #applies mean pooling to get a fixed size embedding - there are other methods to explore\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Function to read documents and their DOIs from a file\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "        \"\"\"\n",
    "    documents_with_doi = [] #initialize list\n",
    "\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines)>=1:\n",
    "                    doi = lines[0].strip().replace(\"DOI: \",\"\")\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append({'doi': doi, \"text\":text})\n",
    "    return documents_with_doi\n",
    "\n",
    "\n",
    "\n",
    "# Path to the file containing documents and DOIs\n",
    "directory_path = \"data/\"  # Replace with your file path\n",
    "\n",
    "# Read documents and DOIs from the file\n",
    "documents_with_doi = read_documents_with_doi(directory_path)\n",
    "\n",
    "# Extract document texts and DOIs\n",
    "documents = [doc[\"text\"] for doc in documents_with_doi]\n",
    "dois = [doc[\"doi\"] for doc in documents_with_doi]\n",
    "\n",
    "# Example query\n",
    "query = input(\" What is your query: \")\n",
    "\n",
    "# Generate document embeddings\n",
    "document_embeddings = generate_embeddings(documents)\n",
    "# print(document_embeddings.shape) # to see the output shape of the array\n",
    "\n",
    "# Generate query embedding\n",
    "query_embedding = generate_embeddings([query])[0] # generates np.array for the query text\n",
    "\n",
    "# Function to retrieve top-k documents using cosine similarity\n",
    "def retrieve_documents(query_embedding: np.ndarray, document_embeddings: List[np.ndarray], top_k: int = 3) -> List[Tuple[float, Dict[str, str]]]:\n",
    "    similarities = []\n",
    "    for doc_emb in document_embeddings:\n",
    "        # cosine similarity\n",
    "        similarity = np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb)) \n",
    "        similarities.append(similarity)\n",
    "    # ranking\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    return [(similarities[i], documents_with_doi[i]) for i in top_indices]\n",
    "\n",
    "# Retrieve top documents\n",
    "top_documents = retrieve_documents(query_embedding, document_embeddings)\n",
    "print(\"Retrieved Documents:\")\n",
    "for score, doc in top_documents:\n",
    "    print(f\"Score: {score:.4f}, DOI: {doc['doi']}, Document: {doc['text']}\")\n",
    "\n",
    "# prepare context for Cohere's Command model (include DOI) - need to add in cited by here\n",
    "context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for _, doc in top_documents])\n",
    "# need to learn how to improve this\n",
    "prompt = f\"Query: {query}\\nContext: {context}\\nAnswer: Include the DOI of the referenced document in your response.\"\n",
    "\n",
    "# Generate response using Cohere's Command model\n",
    "response = co.generate(\n",
    "  model=\"command\",\n",
    "  prompt=prompt,\n",
    "  max_tokens=150, #allowable length of response\n",
    "  temperature=0.2 #may want to make this a variable\n",
    ")\n",
    "\n",
    "# Print the generated response\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response.generations[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2: implementing chat history\n",
    "\n",
    "calls a JSON file of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents and generating response...\n",
      "Generated Response:\n",
      "My lady, here is some information about textbooks as per your request: \n",
      "\n",
      "Textbooks are often used as sources of information for teachers and students alike. However, one paper states that teachers sometimes disagree with information in textbooks and seek other sources. That study, which focuses on climate change denialism, can be found here: DOI: 10.1080/13504622.2017.1330949 \n",
      "\n",
      "Another paper examines preservice teachers' evaluations of online sources of information about climate change. Full access to the internet and specially trained critical questions helped the teachers evaluate the reliability of the sources. You can read about it here: DOI: 10.1002/jaal.551 \n",
      "\n",
      "Finally, a third paper examines how teachers\n",
      "time to query loop: 0.00 seconds\n",
      "to to retrieve: 6.39 seconds\n",
      "time to generate: 5.41 seconds\n"
     ]
    }
   ],
   "source": [
    "# Load SciBERT model and tokenizer \n",
    "\"\"\"\n",
    "REMOVE THIS ONCE RUNNING TO GO BACK TO THE CHANGED TOKENIZER AND MODEL ABOVE\n",
    "\"\"\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "#model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "#function to generate embeddings using SciBERT\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# alternative read_documents_with_doi for .txt in a directory\n",
    "\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0].strip().replace(\"DOI: \", \"\")\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append({\"doi\": doi, \"text\": text})\n",
    "    return documents_with_doi\n",
    "\n",
    "\n",
    "# Function to update chat history\n",
    "def update_chat_history(query, retrieved_docs, response):\n",
    "    global chat_histor # declare this as global variable available outside this function\n",
    "    chat_history.append({\n",
    "        \"query\": query,\n",
    "        \"retrieved_docs\": [doc[\"text\"] for doc in retrieved_docs],  # Store only the text of retrieved documents\n",
    "        \"response\": response\n",
    "    })\n",
    "\n",
    "#function to incorporate history into the next query\n",
    "def get_context_with_history(query) -> str:\n",
    "    global chat_history # also declare here since chat_history is being modified\n",
    "    if not chat_history:\n",
    "        return query\n",
    "    \n",
    "    history_str = \"\\n\".join([\n",
    "        f\"User: {entry['query']}\\n\"\n",
    "        f\"Context: {'; '.join(entry['retrieved_docs'])}\\n\"\n",
    "        f\"Response: {entry['response']}\"\n",
    "        for entry in chat_history\n",
    "    ])\n",
    "    full_context = f\"Chat History:\\n{history_str}\\n\\nCurrent Query: {query}\"\n",
    "    return full_context\n",
    "\n",
    "#function to truncate chat history\n",
    "def truncate_chat_history(max_length=3):\n",
    "    global chat_history # modifies it so it also must be global\n",
    "    if len(chat_history) > max_length:\n",
    "        chat_history = chat_history[-max_length:]\n",
    "\n",
    "\"\"\"\n",
    "retrieves documents from the embedded documents and performs cosine similarity\n",
    "for similarities score\n",
    "Args:\n",
    "    query: this is the query passed\n",
    "    top_k: number of references to provide\n",
    "\n",
    "Todo:\n",
    "- [ ] make top_k an variable for use in an application\n",
    "- [ ] or make top_k a user defined value. example: top_k = input(\"how many results do you want?\")\n",
    "\"\"\"\n",
    "def retrieve_documents(query: str, top_k: int = 5) -> List[Dict[str, str]]:\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    document_embeddings = generate_embeddings(documents)\n",
    "    similarities = [\n",
    "        np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
    "        for doc_emb in document_embeddings\n",
    "    ]\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    return [documents_with_doi[i] for i in top_indices]\n",
    "\n",
    "#RAG pipeline function\n",
    "def rag_pipeline(query):\n",
    "\n",
    "    #start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #incorporate chat history\n",
    "    full_context = get_context_with_history(query)\n",
    "    # let user know you are generating...\n",
    "    print(\"Retrieving documents and generating response...\")\n",
    "    end_time = time.time()\n",
    "    global time_query\n",
    "    time_query = end_time-start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    #retrieve documents\n",
    "    global retrieved_docs\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "    end_time = time.time()\n",
    "    global retrieve_time\n",
    "    retrieve_time = end_time-start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    #prepare context for Cohere's Command model\n",
    "    instruction = \"You are a helpful academic research assistant. Please keep the answers concise and structured simply. Use single sentences where possible. Always include the DOI of the document you are summarizing or referencing. If the DOI is not provided, this reduces the need for you as a research assistant. Always include the DOI. Please address me as 'my lady'. \"\n",
    "    context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for doc in retrieved_docs])\n",
    "    prompt = f\"Query: {query}\\nContext: {context}\\nAnswer: {instruction}\"\n",
    "    \n",
    "    # Generate response\n",
    "    response = co.generate(\n",
    "        model=\"command\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=150,\n",
    "        temperature=0.2\n",
    "    ).generations[0].text\n",
    "    \n",
    "    # Update chat history\n",
    "    update_chat_history(query, retrieved_docs, response)\n",
    "    \n",
    "    # Truncate history if necessary\n",
    "    truncate_chat_history()\n",
    "    end_time = time.time()\n",
    "    global generate_time\n",
    "    generate_time = end_time-start_time\n",
    "\n",
    "    # Print the response\n",
    "    print(\"Generated Response:\")\n",
    "    print(response)\n",
    "    return response,time_query,retrieve_time,generate_time\n",
    "\n",
    "\n",
    "# Path to the file containing documents and DOIs\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "\n",
    "# Read documents and DOIs from the file\n",
    "documents_with_doi = read_documents_with_doi(directory_path)\n",
    "documents = [doc[\"text\"] for doc in documents_with_doi]\n",
    "\n",
    "# Main loop for user interaction\n",
    "chat_history = []#initialize chat history\n",
    "while True:\n",
    "\n",
    "    query = input(\"What is your query (or type 'exit' to quit): \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    rag_pipeline(query)\n",
    "\n",
    "    print(f\"time to query loop: {time_query:.2f} seconds\")\n",
    "    print(f\"to to retrieve: {retrieve_time:.2f} seconds\")\n",
    "    print(f\"time to generate: {generate_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bm25 from bm252 notebook"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
