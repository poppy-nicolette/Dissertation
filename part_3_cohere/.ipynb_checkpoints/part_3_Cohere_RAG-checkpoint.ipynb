{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere API and SciBERT for RAG\n",
    "This notebook uses a Cohere API for generating responses to text. A query input is required from the user. \n",
    "SciBERT is used for embeddings in a dense vector array both the text and the query. \n",
    "A DOI is supplied with the text as both an identifier and locator. \n",
    "\n",
    "- [ ] set up venv\n",
    "- [ ] install transformers torch cohere in command line\n",
    "\n",
    "### todo\n",
    "- [ ] create script that compiles data/documents.txt with DOI || text for all documents\n",
    "- [ ] store vectorized documents in a db\n",
    "    - https://huggingface.co/learn/cookbook/rag_with_hugging_face_gemma_mongodb\n",
    "\n",
    "### options\n",
    "- Batch Processing:\n",
    "    If large number of texts, process them in batches to avoid memory issues.\n",
    "    Example: Use a loop or torch.utils.data.DataLoader.\n",
    "\n",
    "- Change model size: smaller models require less processing\n",
    "\n",
    "- fine tune model on corpus\n",
    "\n",
    "- look into pooling strategies\n",
    "\n",
    "- Tokenizer\n",
    "    - put cleaning process distincly prior to the tokenizer, using the default values as much as possible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cohere\n",
    "from cohere import Client\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(\"i4WfLKa1zNNKsPU3n4ZEVuzpaTCBwztx6p6hebpO\")\n",
    "\n",
    "# Load SciBERT model and tokenizer\n",
    "\"\"\"\n",
    "Autotokenizer documentation can be found here: https://huggingface.co/docs/transformers/v4.50.0/en/model_doc/auto#transformers.AutoTokenizer\n",
    "\n",
    "Model documentation can be found here: https://huggingface.co/allenai/scibert_scivocab_uncased\n",
    "Citation for SciBERT:\n",
    "@inproceedings{beltagy-etal-2019-scibert,\n",
    "    title = \"SciBERT: A Pretrained Language Model for Scientific Text\",\n",
    "    author = \"Beltagy, Iz  and Lo, Kyle  and Cohan, Arman\",\n",
    "    booktitle = \"EMNLP\",\n",
    "    year = \"2019\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://www.aclweb.org/anthology/D19-1371\"\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Initialize tokenizer with custom parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    max_len=512,\n",
    "    use_fast=True,  # Use the fast tokenizer\n",
    "    do_lower_case=False,  # Preserve case\n",
    "    add_prefix_space=False,  # No prefix space\n",
    "    never_split=[\"[DOC]\", \"[REF]\"],  # Tokens to never split\n",
    "    additional_special_tokens=[\"<doi>\", \"</doi>\"]  # Add custom special tokens\n",
    ")\n",
    "\n",
    "# This is the SciBERT model that is used to embed the text and query.\n",
    "# other models: 'allenai-specter', \n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents:\n",
      "Score: 0.7844, DOI: https://doi.org/10.1162/qss_a_00286, Document: ABSTRACT  The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\n",
      "Score: 0.7556, DOI: https://doi.org/10.1007/s11192-022-04367-w, Document: Abstract  This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 88\u001b[39m\n",
      "\u001b[32m     85\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mContext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAnswer: Include the DOI of the referenced document in your response.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m     87\u001b[39m \u001b[38;5;66;03m# Generate response using Cohere's Command model\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m response = \u001b[43mco\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m     89\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcommand\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# there are other models to consider within command\u001b[39;49;00m\n",
      "\u001b[32m     90\u001b[39m \u001b[43m  \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     91\u001b[39m \u001b[43m  \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# allowable length of response\u001b[39;49;00m\n",
      "\u001b[32m     92\u001b[39m \u001b[43m  \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# lower for less creativity, more for more creativity\u001b[39;49;00m\n",
      "\u001b[32m     93\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Print the generated response\u001b[39;00m\n",
      "\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mGenerated Response:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/cohere/base_client.py:1568\u001b[39m, in \u001b[36mBaseCohere.generate\u001b[39m\u001b[34m(self, prompt, model, num_generations, max_tokens, truncate, temperature, seed, preset, end_sequences, stop_sequences, k, p, frequency_penalty, presence_penalty, return_likelihoods, raw_prompting, request_options)\u001b[39m\n",
      "\u001b[32m   1433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n",
      "\u001b[32m   1434\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[32m   1435\u001b[39m     *,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m   1452\u001b[39m     request_options: typing.Optional[RequestOptions] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[32m   1453\u001b[39m ) -> Generation:\n",
      "\u001b[32m   1454\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[32m   1455\u001b[39m \u001b[33;03m    <Warning>\u001b[39;00m\n",
      "\u001b[32m   1456\u001b[39m \u001b[33;03m    This API is marked as \"Legacy\" and is no longer maintained. Follow the [migration guide](https://docs.cohere.com/docs/migrating-from-cogenerate-to-cochat) to start using the Chat API.\u001b[39;00m\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m   1566\u001b[39m \u001b[33;03m    )\u001b[39;00m\n",
      "\u001b[32m   1567\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1568\u001b[39m     _response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_wrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhttpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   1569\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mv1/generate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1570\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1571\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[32m   1572\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1573\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1574\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_generations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_generations\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1575\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1576\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1577\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1578\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1579\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpreset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1580\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mend_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_sequences\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1581\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1582\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mk\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1583\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1584\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1585\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1586\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreturn_likelihoods\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_likelihoods\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1587\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw_prompting\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_prompting\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1588\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1589\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1590\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[32m   1591\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent-type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1592\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1594\u001b[39m \u001b[43m        \u001b[49m\u001b[43momit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOMIT\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1595\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m   1597\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m200\u001b[39m <= _response.status_code < \u001b[32m300\u001b[39m:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/cohere/core/http_client.py:198\u001b[39m, in \u001b[36mHttpClient.request\u001b[39m\u001b[34m(self, path, method, base_url, params, json, data, content, files, headers, request_options, retries, omit)\u001b[39m\n",
      "\u001b[32m    190\u001b[39m timeout = (\n",
      "\u001b[32m    191\u001b[39m     request_options.get(\u001b[33m\"\u001b[39m\u001b[33mtimeout_in_seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m request_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m request_options.get(\u001b[33m\"\u001b[39m\u001b[33mtimeout_in_seconds\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    193\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.base_timeout()\n",
      "\u001b[32m    194\u001b[39m )\n",
      "\u001b[32m    196\u001b[39m json_body, data_body = get_request_body(json=json, data=data, request_options=request_options, omit=omit)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhttpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m.\u001b[49m\u001b[43murljoin\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjsonable_encoder\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[32m    204\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    205\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    206\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madditional_headers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_query\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjsonable_encoder\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    213\u001b[39m \u001b[43m                \u001b[49m\u001b[43mremove_omit_from_dict\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    214\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[32m    215\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    216\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    217\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madditional_query_parameters\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[32m    218\u001b[39m \u001b[43m                            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n",
      "\u001b[32m    219\u001b[39m \u001b[43m                            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[32m    220\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    221\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    222\u001b[39m \u001b[43m                    \u001b[49m\u001b[43momit\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    223\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    224\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_body\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_file_dict_to_httpx_tuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_omit_from_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43momit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n",
      "\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    236\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    238\u001b[39m max_retries: \u001b[38;5;28mint\u001b[39m = request_options.get(\u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m request_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _should_retry(response=response):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n",
      "\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n",
      "\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n",
      "\u001b[32m    813\u001b[39m     method=method,\n",
      "\u001b[32m    814\u001b[39m     url=url,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n",
      "\u001b[32m    824\u001b[39m )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n",
      "\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n",
      "\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n",
      "\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n",
      "\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n",
      "\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[32m    977\u001b[39m     hook(request)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   1011\u001b[39m     )\n",
      "\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[32m   1018\u001b[39m response.request = request\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    237\u001b[39m req = httpcore.Request(\n",
      "\u001b[32m    238\u001b[39m     method=request.method,\n",
      "\u001b[32m    239\u001b[39m     url=httpcore.URL(\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n",
      "\u001b[32m    248\u001b[39m )\n",
      "\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n",
      "\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n",
      "\u001b[32m    255\u001b[39m     status_code=resp.status,\n",
      "\u001b[32m    256\u001b[39m     headers=resp.headers,\n",
      "\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n",
      "\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n",
      "\u001b[32m    259\u001b[39m )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n",
      "\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n",
      "\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n",
      "\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n",
      "\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n",
      "\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n",
      "\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n",
      "\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n",
      "\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n",
      "\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n",
      "\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n",
      "\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n",
      "\u001b[32m    100\u001b[39m     (\n",
      "\u001b[32m    101\u001b[39m         http_version,\n",
      "\u001b[32m    102\u001b[39m         status,\n",
      "\u001b[32m    103\u001b[39m         reason_phrase,\n",
      "\u001b[32m    104\u001b[39m         headers,\n",
      "\u001b[32m    105\u001b[39m         trailing_data,\n",
      "\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    107\u001b[39m     trace.return_value = (\n",
      "\u001b[32m    108\u001b[39m         http_version,\n",
      "\u001b[32m    109\u001b[39m         status,\n",
      "\u001b[32m    110\u001b[39m         reason_phrase,\n",
      "\u001b[32m    111\u001b[39m         headers,\n",
      "\u001b[32m    112\u001b[39m     )\n",
      "\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n",
      "\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n",
      "\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n",
      "\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n",
      "\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n",
      "\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n",
      "\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n",
      "\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n",
      "\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n",
      "\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n",
      "\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n",
      "\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n",
      "\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n",
      "\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic RAG with Cohere model\n",
    "Document source: data/documents.txt where the DOI with resolver is separated from the abstract by ||. One record per line. \n",
    "Saved as UTF-8\n",
    "\n",
    "Returns:  answers based on query from input()\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate embeddings using SciBERT\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    converts raw text to numerical representations using a pretrained model, in this case, SciBERT.\n",
    "    Currently this is applied to both the document text and the query. \n",
    "    May want a different version or decorator for the query as they are generally much shorter and more sparse.\n",
    "\n",
    "    Input: text from tokenizer step above as a list of strings\n",
    "    Output: np.array\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512, # returns PyTorch tensors which are compatible with model\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True # return the attention mask - need to learn more\n",
    "        )\n",
    "    # this passes the tokenized inputs through the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # applies mean pooling to get a fixed size embedding\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Function to read documents and their DOIs from a file\n",
    "def read_documents_with_doi(file_path: str) -> List[Dict[str, str]]:\n",
    "    documents_with_doi = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\"||\")  # Assuming DOI and document are separated by \"||\"\n",
    "            if len(parts) == 2:\n",
    "                doi, document = parts\n",
    "                documents_with_doi.append({\"doi\": doi.strip(), \"text\": document.strip()})\n",
    "    return documents_with_doi\n",
    "\n",
    "# Path to the file containing documents and DOIs\n",
    "file_path = \"data/documents.txt\"  # Replace with your file path\n",
    "\n",
    "# Read documents and DOIs from the file\n",
    "documents_with_doi = read_documents_with_doi(file_path)\n",
    "\n",
    "# Extract document texts and DOIs\n",
    "documents = [doc[\"text\"] for doc in documents_with_doi]\n",
    "dois = [doc[\"doi\"] for doc in documents_with_doi]\n",
    "\n",
    "# Example query\n",
    "query = input(\" What is your query: \")\n",
    "\n",
    "# Generate document embeddings\n",
    "document_embeddings = generate_embeddings(documents)\n",
    "# print(document_embeddings.shape) # to see the output shape of the array\n",
    "\n",
    "# Generate query embedding\n",
    "query_embedding = generate_embeddings([query])[0] # generates np.array for the query text\n",
    "\n",
    "# Function to retrieve top-k documents using cosine similarity\n",
    "def retrieve_documents(query_embedding: np.ndarray, document_embeddings: List[np.ndarray], top_k: int = 2) -> List[Tuple[float, Dict[str, str]]]:\n",
    "    similarities = []\n",
    "    for doc_emb in document_embeddings:\n",
    "        # cosine similarity\n",
    "        similarity = np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb)) \n",
    "        similarities.append(similarity)\n",
    "    # ranking\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    return [(similarities[i], documents_with_doi[i]) for i in top_indices]\n",
    "\n",
    "# Retrieve top documents\n",
    "top_documents = retrieve_documents(query_embedding, document_embeddings)\n",
    "print(\"Retrieved Documents:\")\n",
    "for score, doc in top_documents:\n",
    "    print(f\"Score: {score:.4f}, DOI: {doc['doi']}, Document: {doc['text']}\")\n",
    "\n",
    "# Prepare context for Cohere's Command model (include DOI) - need to add in cited by here\n",
    "context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for _, doc in top_documents])\n",
    "# need to learn how to improve this\n",
    "prompt = f\"Query: {query}\\nContext: {context}\\nAnswer: Include the DOI of the referenced document in your response.\"\n",
    "\n",
    "# Generate response using Cohere's Command model\n",
    "response = co.generate(\n",
    "  model=\"command\", # there are other models to consider within command\n",
    "  prompt=prompt,\n",
    "  max_tokens=150, # allowable length of response\n",
    "  temperature=0.5 # lower for less creativity, more for more creativity\n",
    ")\n",
    "\n",
    "# Print the generated response\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response.generations[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2: implementing chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      " Sure, I can help analyze this DOI if you provide me with the specific information you would like to know, such as the main themes, key articles, or relevant information. \n",
      "Generated Response:\n",
      " https://doi.org/10.48550/arXiv.2401.16359\n",
      "Generated Response:\n",
      " Thank you for the context provided with the query, this has helped me to understand and complete your request. \n",
      "\n",
      "Here is a response based on the information provided: \n",
      "\n",
      "The DOI you provided points to an academic paper that conducts a study with the aim of comparing the amount of metadata and the completeness of research publications across different academic databases. It assesses seven characteristics (abstract, access, bibliographic info, document type, publication date, language, and identifiers) across seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). The study finds that third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate compared to\n",
      "Generated Response:\n",
      " DOI: https://doi.org/10.1162/qss_a_00286\n",
      "Generated Response:\n",
      " Thank you for the context provided with the query, this has helped me to understand and complete the request. \n",
      "\n",
      "The two DOI numbers you provided correspond to different studies comparing the coverage and metadata quality of different databases. These studies appear to be contrasting in their findings, with the first study concluding that OpenAlex provides data on par with proprietary alternatives in terms of metadata quality, while the second suggests that third-party databases like OpenAlex have better metadata quality than academic search engines. \n",
      "\n",
      "It seems that the first study you referenced is specifically examining the characteristics of OpenAlex in comparison to other databases, while the study in the second DOI is making a broader comparison across multiple databases, including OpenAlex. \n",
      "\n",
      "It's important to note that the\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 119\u001b[39m\n",
      "\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# Main loop for user interaction\u001b[39;00m\n",
      "\u001b[32m    118\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     query = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is your query (or type \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mexit\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m to quit): \u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query.lower() == \u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[32m    121\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/ipykernel/kernelbase.py:1282\u001b[39m, in \u001b[36mKernel.raw_input\u001b[39m\u001b[34m(self, prompt)\u001b[39m\n",
      "\u001b[32m   1280\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   1283\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1284\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1285\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mshell\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1286\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1287\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/ipykernel/kernelbase.py:1325\u001b[39m, in \u001b[36mKernel._input_request\u001b[39m\u001b[34m(self, prompt, ident, parent, password)\u001b[39m\n",
      "\u001b[32m   1322\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n",
      "\u001b[32m   1323\u001b[39m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n",
      "\u001b[32m   1324\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mInterrupted by user\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1325\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m   1326\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "\u001b[32m   1327\u001b[39m     \u001b[38;5;28mself\u001b[39m.log.warning(\u001b[33m\"\u001b[39m\u001b[33mInvalid Message:\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# Load SciBERT model and tokenizer \n",
    "\"\"\"\n",
    "REMOVE THIS ONCE RUNNING TO GO BACK TO THE CHANGED TOKENIZER AND MODEL ABOVE\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "# Initialize chat history\n",
    "chat_history = []\n",
    "\n",
    "# Function to generate embeddings using SciBERT\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Function to read documents and their DOIs from a file\n",
    "def read_documents_with_doi(file_path: str) -> List[Dict[str, str]]:\n",
    "    documents_with_doi = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\"||\")\n",
    "            if len(parts) == 2:\n",
    "                doi, document = parts\n",
    "                documents_with_doi.append({\"doi\": doi.strip(), \"text\": document.strip()})\n",
    "    return documents_with_doi\n",
    "\n",
    "# Path to the file containing documents and DOIs\n",
    "# file_path = \"data/documents.txt\"\n",
    "file_path = \"data/works_202504300942.json\"\n",
    "\n",
    "# Read documents and DOIs from the file\n",
    "documents_with_doi = read_documents_with_doi(file_path)\n",
    "documents = [doc[\"text\"] for doc in documents_with_doi]\n",
    "\n",
    "\n",
    "# Function to update chat history\n",
    "def update_chat_history(query, retrieved_docs, response):\n",
    "    global chat_histor # declare this as global variable available outside this function\n",
    "    chat_history.append({\n",
    "        \"query\": query,\n",
    "        \"retrieved_docs\": [doc[\"text\"] for doc in retrieved_docs],  # Store only the text of retrieved documents\n",
    "        \"response\": response\n",
    "    })\n",
    "\n",
    "# Function to incorporate history into the next query\n",
    "def get_context_with_history(query) -> str:\n",
    "    global chat_history # also declare here since chat_history is being modified\n",
    "    if not chat_history:\n",
    "        return query\n",
    "    \n",
    "    history_str = \"\\n\".join([\n",
    "        f\"User: {entry['query']}\\n\"\n",
    "        f\"Context: {'; '.join(entry['retrieved_docs'])}\\n\"\n",
    "        f\"Response: {entry['response']}\"\n",
    "        for entry in chat_history\n",
    "    ])\n",
    "    full_context = f\"Chat History:\\n{history_str}\\n\\nCurrent Query: {query}\"\n",
    "    return full_context\n",
    "\n",
    "# Function to truncate chat history\n",
    "def truncate_chat_history(max_length=3):\n",
    "    global chat_history # modifies it so it also must be global\n",
    "    if len(chat_history) > max_length:\n",
    "        chat_history = chat_history[-max_length:]\n",
    "\n",
    "# Function to retrieve top-k documents using cosine similarity\n",
    "def retrieve_documents(query: str, top_k: int = 2) -> List[Dict[str, str]]:\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    document_embeddings = generate_embeddings(documents)\n",
    "    similarities = [\n",
    "        np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
    "        for doc_emb in document_embeddings\n",
    "    ]\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    return [documents_with_doi[i] for i in top_indices]\n",
    "\n",
    "# RAG pipeline function\n",
    "def rag_pipeline(query):\n",
    "    # Incorporate chat history\n",
    "    full_context = get_context_with_history(query)\n",
    "    \n",
    "    # Retrieve documents\n",
    "    global retrieved_docs\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "    \n",
    "\n",
    "    # Prepare context for Cohere's Command model\n",
    "    context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for doc in retrieved_docs])\n",
    "    prompt = f\"Query: {query}\\nContext: {context}\\nAnswer: Include the DOI of the referenced document in your response.\"\n",
    "    \n",
    "    # Generate response\n",
    "    response = co.generate(\n",
    "        model=\"command\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=150,\n",
    "        temperature=0.5\n",
    "    ).generations[0].text\n",
    "    \n",
    "    # Update chat history\n",
    "    update_chat_history(query, retrieved_docs, response)\n",
    "    \n",
    "    # Truncate history if necessary\n",
    "    truncate_chat_history()\n",
    "    \n",
    "    # Print the response\n",
    "    print(\"Generated Response:\")\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "# Main loop for user interaction\n",
    "while True:\n",
    "    query = input(\"What is your query (or type 'exit' to quit): \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    rag_pipeline(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "## Test One: \n",
    "- [ ] count specific terms found in responses and in post-tokenized text. \n",
    "- [ ] compare post-tokenized and pre-tokenized text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doi': 'https://doi.org/10.1162/qss_a_00286',\n",
       "  'text': 'ABSTRACT  The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.'},\n",
       " {'doi': 'https://doi.org/10.48550/arXiv.2401.16359',\n",
       "  'text': 'Abstract  OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our largescale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code is from the huggingface notebook \n",
    "    - [ ] track down source\n",
    "\n",
    "todo:\n",
    "- [ ] rewrite queuries \n",
    "- [ ] reqrite retrieved_docs - the dictionary is not quite what is needed\n",
    "- [ ] create ground truth from dataset\n",
    "\"\"\"\n",
    "\n",
    "# Queries go here - change this to an input or a list from a file\n",
    "queries = [\n",
    "    \"What is SciBERT used for?\",\n",
    "    \"How does BERT differ from SciBERT?\"\n",
    "]\n",
    "\n",
    "# This needs to come from the RAG above \n",
    "# REWRITE this as the two dictionaries are not the same. Need to take kv pair for 'doi' in retrieved_docs and\n",
    "# create a new dictionary in a list\n",
    "retrieved_docs = [\n",
    "    {\"DOI\": \"10.1234/scibert.2021\"},  # For query 1\n",
    "    {\"DOI\": \"10.5678/bert.2019\"}       # For query 2\n",
    "]\n",
    "\n",
    "# Ground truth relevant documents (DOIs) for each query\n",
    "ground_truth = [\n",
    "    {\"DOI\": \"10.1234/scibert.2021\"},  # For query 1\n",
    "    {\"DOI\": \"10.5678/bert.2019\", \"DOI\": \"10.9101/transformers.2020\"}  # For query 2\n",
    "]\n",
    "\n",
    "# Function to compute precision, recall, and F1-score\n",
    "def evaluate_retrieval(\n",
    "    queries: List[str],\n",
    "    retrieved_docs: List[List[Dict[str, str]]],\n",
    "    ground_truth: List[List[Dict[str, str]]]\n",
    ") -> Dict[str, float]:\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for query, retrieved, gt in zip(queries, retrieved_docs, ground_truth):\n",
    "        # Convert DOIs to sets for easy comparison\n",
    "        retrieved_dois = set([doc[\"DOI\"] for doc in retrieved])\n",
    "        gt_dois = set([doc[\"DOI\"] for doc in gt])\n",
    "        \n",
    "        # Binary labels: 1 if document is relevant, 0 otherwise\n",
    "        labels = [1 if doi in gt_dois else 0 for doi in documents_with_doi]\n",
    "        predictions = [1 if doi in retrieved_dois else 0 for doi in documents_with_doi]\n",
    "        \n",
    "        all_labels.extend(labels)\n",
    "        all_predictions.extend(predictions)\n",
    "\n",
    "    # Compute metrics\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "results = evaluate_retrieval(queries, retrieved_docs, ground_truth)\n",
    "print(f\"Precision: {results['Precision']:.4f}\")\n",
    "print(f\"Recall: {results['Recall']:.4f}\")\n",
    "print(f\"F1-Score: {results['F1-Score']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
