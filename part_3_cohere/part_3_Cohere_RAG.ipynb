{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere API and SciBERT for RAG\n",
    "This notebook uses a Cohere API for generating responses to text. A query input is required from the user. \n",
    "SciBERT is used for embeddings in a dense vector array both the text and the query. \n",
    "A DOI is supplied with the text as both an identifier and locator. \n",
    "\n",
    "- [x] set up venv\n",
    "- [x] install transformers torch cohere in command line\n",
    "\n",
    "### todo\n",
    "- [ ] create script that compiles data/documents.txt with DOI || text for all documents\n",
    "- [ ] reduce code by refactoring into modules\n",
    "- [ ] store vectorized documents in a db\n",
    "    - https://huggingface.co/learn/cookbook/rag_with_hugging_face_gemma_mongodb\n",
    "\n",
    "### options\n",
    "- Batch Processing:\n",
    "    If large number of texts, process them in batches to avoid memory issues.\n",
    "    Example: Use a loop or torch.utils.data.DataLoader.\n",
    "\n",
    "- Change model size: smaller models require less processing\n",
    "\n",
    "- fine tune model on corpus - i don't think this is an option\n",
    "\n",
    "- look into pooling strategies\n",
    "- concurrency? \n",
    "\n",
    "- Tokenizer\n",
    "    - put cleaning process distincly prior to the tokenizer, using the default values as much as possible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cohere\n",
    "from cohere import Client\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import logging # finding where functions are taking too long\n",
    "import sys\n",
    "\n",
    "# load secret from local .env file\n",
    "def get_key():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    _key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "    #verify if it worked\n",
    "    if _key is not None:\n",
    "        print(\"all is good, beautiful!\")\n",
    "        return _key\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all is good, beautiful!\n",
      "Model is callable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize Cohere client with key from secrets\n",
    "co = cohere.Client(get_key())\n",
    "\n",
    "# load SciBERT model and tokenizer\n",
    "\"\"\"\n",
    "Autotokenizer documentation can be found here: https://huggingface.co/docs/transformers/v4.50.0/en/model_doc/auto#transformers.AutoTokenizer\n",
    "\n",
    "Model documentation can be found here: https://huggingface.co/allenai/scibert_scivocab_uncased\n",
    "Citation for SciBERT:\n",
    "@inproceedings{beltagy-etal-2019-scibert,\n",
    "    title = \"SciBERT: A Pretrained Language Model for Scientific Text\",\n",
    "    author = \"Beltagy, Iz  and Lo, Kyle  and Cohan, Arman\",\n",
    "    booktitle = \"EMNLP\",\n",
    "    year = \"2019\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://www.aclweb.org/anthology/D19-1371\"\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Initialize tokenizer with custom parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    max_len=512,\n",
    "    use_fast=True,  # Use the fast tokenizer\n",
    "    do_lower_case=False,  # Preserve case\n",
    "    add_prefix_space=False,  # No prefix space\n",
    "    never_split=[\"[DOC]\", \"[REF]\"],  # Tokens to never split\n",
    "    #additional_special_tokens=[\"<doi>\", \"</doi>\"],  # Add custom special tokens ***RE-EVALUATE*** (tuple or list of str or tokenizers.AddedToken, optional) — A tuple or a list of additional special tokens. Add them here to ensure they are skipped when decoding with skip_special_tokens is set to True. If they are not part of the vocabulary, they will be added at the end of the vocabulary.\n",
    "    skip_special_tokens=False,\n",
    ")\n",
    "\n",
    "# this is the SciBERT model that is used to embed the text and query.\n",
    "# other models: 'allenai-specter', \n",
    "#documentation here: https://huggingface.co/docs/transformers/model_doc/auto\n",
    "#load model directly\n",
    "\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", torch_dtype=\"auto\")\n",
    "#may also want to consider using a sentence embedding model\n",
    "\n",
    "\n",
    "#verify that the model is callable\n",
    "if callable(model):\n",
    "    print(\"Model is callable\")\n",
    "else:\n",
    "    print(\"Model is not callable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V3: Dense retriever only\n",
    "- set with parameters the same as BM25 pre-retriever version\n",
    "\n",
    "calls a JSON file of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of corpus: 45\n",
      "Retrieving documents and generating response...\n",
      "Generated Response:\n",
      "My lady, here is a brief overview of Crossref, a platform that provides open bibliographic metadata for scholarly publications, which is used as a major building block by databases such as OpenAlex. \n",
      "\n",
      "Crossref is a good source of open bibliographic metadata on publications, and funding information is one of the elements that have been examined regarding its availability. This open data is essential for funding agencies to analyze the outcomes of their funding and trace the publications resulting from it. \n",
      "\n",
      "Here is a DOI that refers to a paper that analyzes the open availability of funding data in Crossref, specifically with respect to publications that report COVID-19 research: \n",
      "DOI: 10.1162/qss_a_00212. \n",
      "\n",
      "Let me know if you would like more information on this or any of the other topics you have mentioned, my lady. \n",
      "------\n",
      "Source documents: \n",
      "DOI: 10.5281/ZENODO.13960973, Title: Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\n",
      "DOI: 10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: 10.31274/b8136f97.ccc3dae4, Title: Comparing Funder Metadata in OpenAlex and Dimensions\n",
      "DOI: 10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: 10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "time to query loop: 0.01 seconds\n",
      "to to retrieve: 8.11 seconds\n",
      "time to generate: 6.69 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic RAG using Cohere Command model\n",
    "Dense retrieval of embedded query and pre-retrieved documents\n",
    "Document source: data\n",
    "\n",
    "Returns: responses based on query from input()\n",
    "\"\"\"\n",
    "# set top_k global value - keep this as constant for all evaluations\n",
    "global top_k\n",
    "top_k = 5\n",
    "\n",
    "#function to generate embeddings using SciBERT\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    converts raw text to numerical representations using a pretrained model, in this case, SciBERT.\n",
    "    Currently this is applied to both the document text and the query. \n",
    "    May want a different version or decorator for the query as they are generally much shorter and more sparse.\n",
    "\n",
    "    Input: text from tokenizer step above as a list of strings\n",
    "    Output: np.array\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True)\n",
    "    # this passes the tokenized inputs through the model\n",
    "    outputs = model(**inputs)\n",
    "    #this uses mean pooling - may want to investigate other methods\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0].strip().replace(\"DOI: \", \"\")\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append({\"doi\": doi, \"text\": text})\n",
    "    return documents_with_doi\n",
    "\n",
    "\n",
    "# Function to update chat history\n",
    "def update_chat_history(query, retrieved_docs, response):\n",
    "    global chat_history # declare this as global variable available outside this function\n",
    "    chat_history.append({\n",
    "        \"query\": query,\n",
    "        \"retrieved_docs\": [doc[\"text\"] for doc in retrieved_docs],  # Store only the text of retrieved documents\n",
    "        \"response\": response\n",
    "    })\n",
    "\n",
    "#function to incorporate history into the next query\n",
    "def get_context_with_history(query) -> str:\n",
    "    global chat_history # also declare here since chat_history is being modified\n",
    "    if not chat_history:\n",
    "        return None\n",
    "    else:\n",
    "        for entry in chat_history:\n",
    "            history_str = \"\\n\".join([\n",
    "                f\"User: {entry['query']}\\n\"\n",
    "                f\"Context: {'; '.join(entry['retrieved_docs'])}\\n\"\n",
    "                f\"Response: {entry['response']}\"])\n",
    "     \n",
    "    return f\"Chat History:\\n{history_str}\\nCurrent Query: {query}\"\n",
    "\n",
    "#function to truncate chat history\n",
    "def truncate_chat_history(max_length=3):\n",
    "    global chat_history # modifies it so it also must be global\n",
    "    if len(chat_history) > max_length:\n",
    "        chat_history = chat_history[-max_length:]\n",
    "\n",
    "\n",
    "def retrieve_documents(query: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Dense retriever: retrieves documents from the embedded documents and performs cosine similarity for similarities score\n",
    "\n",
    "    Args:\n",
    "        query: this is the query passed\n",
    "        top_k: number of references to provide - this is a global value set above\n",
    "    Returns:\n",
    "        List of dictionaries containing strings as key/value pairs\n",
    "    \"\"\"\n",
    "    global top_indices # for debugging\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    document_embeddings = generate_embeddings(documents)\n",
    "    #cosine similarity\n",
    "    similarities = [\n",
    "        np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
    "        for doc_emb in document_embeddings\n",
    "    ]\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    return [documents_with_doi[i] for i in top_indices]\n",
    "\n",
    "\n",
    "#RAG pipeline function\n",
    "def rag_pipeline(query):\n",
    "    #start time\n",
    "    start_time = time.time()\n",
    "    # Path to the file containing documents and DOIs\n",
    "    directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "    #directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_jats\"\n",
    "\n",
    "    # Read documents and DOIs from the file\n",
    "    global documents_with_doi,documents\n",
    "    documents_with_doi = read_documents_with_doi(directory_path)\n",
    "    documents = [doc[\"text\"] for doc in documents_with_doi]\n",
    "    print(f\"Length of corpus: {len(documents_with_doi)}\")\n",
    "    # Main loop for user interaction \n",
    "    # changed to run with no history for the purposes of the test\n",
    "    global chat_history\n",
    "    chat_history = []#initialize chat history\n",
    "    \n",
    "    #incorporate chat history\n",
    "    full_context = get_context_with_history(query)\n",
    "    # let user know you are generating...\n",
    "    print(\"Retrieving documents and generating response...\")\n",
    "    end_time = time.time()\n",
    "    global time_query\n",
    "    time_query = end_time-start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    #retrieve documents\n",
    "    global retrieved_docs\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "    end_time = time.time()\n",
    "    global retrieve_time\n",
    "    retrieve_time = end_time-start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    #prepare context for Cohere's Command model\n",
    "    instruction = \"You are a helpful academic research assistant. Please keep the answers concise and structured simply. Use single sentences where possible. Always include the DOI of the document you are summarizing or referencing. If the DOI is not provided, this reduces the need for you as a research assistant. Always include the DOI. Please address me as 'my lady'. \"\n",
    "    # add full_context if exists, else just context. \n",
    "    #context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for doc in retrieved_docs])\n",
    "    if full_context:\n",
    "        context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for doc in retrieved_docs]).join(full_context)\n",
    "    else:\n",
    "        context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for doc in retrieved_docs])\n",
    "    prompt = f\"Query: {query}\\nContext: {context}\\nAnswer: {instruction}\"\n",
    "    \n",
    "    # Generate response\n",
    "    response = co.generate(\n",
    "        model=\"command\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=250,\n",
    "        temperature=0.2\n",
    "    ).generations[0].text\n",
    "    \n",
    "    # Update chat history\n",
    "    update_chat_history(query, retrieved_docs, response)\n",
    "    \n",
    "    # Truncate history if necessary\n",
    "    truncate_chat_history()\n",
    "    end_time = time.time()\n",
    "    global generate_time\n",
    "    generate_time = end_time-start_time\n",
    "\n",
    "    # Print the response\n",
    "    print(\"\\nGenerated Response:\")\n",
    "    print(response)\n",
    "    print(f\"------\\nSource documents: \")\n",
    "    for doc in retrieved_docs:\n",
    "        print(f\"DOI: {doc['doi']}, {doc['text'].split(\"\\n\",1)[0]}\")\n",
    "    return response,time_query,retrieve_time,generate_time\n",
    "\n",
    "#***** Begin chat session *****\n",
    "query = input(\"What is your query (or type 'exit' to quit): \")\n",
    "\n",
    "rag_pipeline(query)\n",
    "\n",
    "print(f\"\\n-------\\ntime to query loop: {time_query:.2f} seconds\")\n",
    "print(f\"to to retrieve: {retrieve_time:.2f} seconds\")\n",
    "print(f\"time to generate: {generate_time:.2f} seconds\")\n",
    "\n",
    "# Main loop for user interaction\n",
    "##chat_history = []#initialize chat history\n",
    "#while True:\n",
    "\n",
    "    #uery = input(\"What is your query (or type 'exit' to quit): \")\n",
    "    #if query.lower() == \"exit\":\n",
    "    #    break\n",
    "    #rag_pipeline(query)\n",
    "\n",
    "    #print(f\"time to query loop: {time_query:.2f} seconds\")\n",
    "    #print(f\"to to retrieve: {retrieve_time:.2f} seconds\")\n",
    "    #print(f\"time to generate: {generate_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "## Test One: \n",
    "- [✅] compute precision, recall, and F1-Scores. \n",
    "- [✅] added accuracy score\n",
    "- [ ] compare text from each source, embedded, and them similarity scores based on embeddings.\n",
    "    - [ ] token based SciBERT embedding\n",
    "    - [ ] sentence-based SentenceBERT embedding\n",
    "### optional analysis\n",
    "Need to learn more about attention weights and their analysis\n",
    "- [ ] heatmap of attention weights for two given inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, recall, F1 score\n",
    "### references\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "- https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score #didn't actually use these, can probably delete\n",
    "#may want to confirm that scores are the same?\n",
    "import openpyxl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial dataframe to capture results from each query and results\n",
    "#ONLY DO THIS AT THE BEGINNING OF THE ANALYSIS PROCEDURE, OTHERWISE, IT WILL ERASE THE PREVIOUS RESULTS!!\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Query','Precision','Recall','F1-Score','Accuracy'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\"\"\"\n",
    "change this to read in an excel sheet of queries and ground_truth dois.\n",
    "Then it should be isolated as a function.\n",
    "Run the function to iterature through the list.\n",
    "\"\"\"\n",
    "# Queries go here\n",
    "queries = [query]\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "retrieved_dois = [doc.get('doi', \"\") for doc in retrieved_docs]\n",
    "print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# Ground truth relevant documents (DOIs) for each query\n",
    "ground_truth = [\"10.1162/qss_a_00112\",\"10.1007/s11192-022-04367-w\"]\n",
    "\n",
    "def evaluate_retrieval(\n",
    "    retrieved_dois: List[str],\n",
    "    ground_truth: List[str]\n",
    ") -> Dict[str, float]:\n",
    "    #convert to sets for unique values\n",
    "    retrieved_set = set(retrieved_dois)\n",
    "    ground_truth_set = set(ground_truth)\n",
    "\n",
    "    #calculate true positives, false positives, and false negatives\n",
    "    true_positives = len(retrieved_set & ground_truth_set) # must use & with set operations\n",
    "    false_positives = len(retrieved_set - ground_truth_set)\n",
    "    false_negatives = len(ground_truth_set - retrieved_set)\n",
    "    true_negatives = len([name for name in os.listdir('data') if os.path.isfile('data'+'/'+name)])-true_positives-false_negatives-false_positives\n",
    "\n",
    "    # calculate metrics - could also use sklearn.metrics functions such as precision_score, but this is easier to read\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    accuracy = (true_positives+true_negatives)/(true_negatives+true_positives+false_negatives+false_positives)\n",
    "\n",
    "    return {\n",
    "        'Query':f\"{queries[0]}\",\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\":accuracy\n",
    "    }\n",
    "\n",
    "def print_results()->Dict:\n",
    "    # Example usage\n",
    "    global results\n",
    "    results = evaluate_retrieval(retrieved_dois, ground_truth)\n",
    "    print(f\"For query: {results['Query']}:\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"F1-Score: {results['F1-Score']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    return results\n",
    "\n",
    "print_results()\n",
    "results_df = results_df._append(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = \"analysis/dense_analysis_results.xlsx\"\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "results_df.to_excel(filename)\n",
    "results_df\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare text embeddings for each source\n",
    "This section is used to investigate the impact of face markup or other formatting requirements in the abstract text on the embedding model.\n",
    "Currently this only uses cosine similarity. \n",
    "<br>\n",
    "#### similarity:\n",
    "- [✅] cosine similarity\n",
    "<br>\n",
    "#### Embeddings:\n",
    "- [✅] SciBERT\n",
    "- [✅] SentenceBERT\n",
    "<br>\n",
    "#### Analysis\n",
    "- [ ] compare scores using dataframe\n",
    "- [ ] visualize results\n",
    "### References\n",
    "- https://stackoverflow.com/questions/60492839/how-to-compare-sentence-similarities-using-embeddings-from-bert<br>\n",
    "See the above for a discussion on NOT using BERT (and SciBERT) for comparing sentence embedding. I should be using SentenceBERT for sentence similarity.<br>\n",
    "- Sentence Transformers: https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "<br>\n",
    "- another approach: https://medium.com/@ahmedmellit/text-similarity-implementation-using-bert-embedding-in-python-1efdb5194e65\n",
    "- sklearn metrics for other scoring methods than cosine similarity: https://scikit-learn.org/stable/api/sklearn.metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#load SciBERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "#function to generate embeddings using SciBERT\n",
    "\"\"\"\n",
    "todo:\n",
    "- [ ] change this to a sentence embedding model\n",
    "\"\"\"\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        padding=\"longest\",#please select one of ['longest', 'max_length', 'do_not_pad']\n",
    "        #padding=False,#padding has an effect on similarity\n",
    "        truncation=True\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "#original text and text with errors\n",
    "original_text = \" Abstract Objectives Precise literature recommendation and summarization are crucial for biomedical professionals. While the latest iteration of generative pretrained transformer (GPT) incorporates 2 distinct modes—real-time search and pretrained model utilization—it encounters challenges in dealing with these tasks. Specifically, the real-time search can pinpoint some relevant articles but occasionally provides fabricated papers, whereas the pretrained model excels in generating well-structured summaries but struggles to cite specific sources. In response, this study introduces RefAI, an innovative retrieval-augmented generative tool designed to synergize the strengths of large language models (LLMs) while overcoming their limitations. Materials and Methods RefAI utilized PubMed for systematic literature retrieval, employed a novel multivariable algorithm for article recommendation, and leveraged GPT-4 turbo for summarization. Ten queries under 2 prevalent topics (“cancer immunotherapy and target therapy” and “LLMs in medicine”) were chosen as use cases and 3 established counterparts (ChatGPT-4, ScholarAI, and Gemini) as our baselines. The evaluation was conducted by 10 domain experts through standard statistical analyses for performance comparison. The overall performance of RefAI surpassed that of the baselines across 5 evaluated dimensions—relevance and quality for literature recommendation, accuracy, comprehensiveness, and reference integration for summarization, with the majority exhibiting statistically significant improvements (P-values<.05). Discussion RefAI demonstrated substantial improvements in literature recommendation and summarization over existing tools, addressing issues like fabricated papers, metadata inaccuracies, restricted recommendations, and poor reference integration. Conclusion By augmenting LLM with external resources and a novel ranking algorithm, RefAI is uniquely capable of recommending high-quality literature and generating well-structured summaries, holding the potential to meet the critical needs of biomedical professionals in navigating and synthesizing vast amounts of scientific literature.\"\n",
    "typo_text = \"<jats:title>Abstract</jats:title>\\n               <jats:sec>\\n                  <jats:title>Objectives</jats:title>\\n                  <jats:p>Precise literature recommendation and summarization are crucial for biomedical professionals. While the latest iteration of generative pretrained transformer (GPT) incorporates 2 distinct modes—real-time search and pretrained model utilization—it encounters challenges in dealing with these tasks. Specifically, the real-time search can pinpoint some relevant articles but occasionally provides fabricated papers, whereas the pretrained model excels in generating well-structured summaries but struggles to cite specific sources. In response, this study introduces RefAI, an innovative retrieval-augmented generative tool designed to synergize the strengths of large language models (LLMs) while overcoming their limitations.</jats:p>\\n               </jats:sec>\\n               <jats:sec>\\n                  <jats:title>Materials and Methods</jats:title>\\n                  <jats:p>RefAI utilized PubMed for systematic literature retrieval, employed a novel multivariable algorithm for article recommendation, and leveraged GPT-4 turbo for summarization. Ten queries under 2 prevalent topics (“cancer immunotherapy and target therapy” and “LLMs in medicine”) were chosen as use cases and 3 established counterparts (ChatGPT-4, ScholarAI, and Gemini) as our baselines. The evaluation was conducted by 10 domain experts through standard statistical analyses for performance comparison.</jats:p>\\n               </jats:sec>\\n               <jats:sec>\\n                  <jats:title>Results</jats:title>\\n                  <jats:p>The overall performance of RefAI surpassed that of the baselines across 5 evaluated dimensions—relevance and quality for literature recommendation, accuracy, comprehensiveness, and reference integration for summarization, with the majority exhibiting statistically significant improvements (P-values &amp;lt;.05).</jats:p>\\n               </jats:sec>\\n               <jats:sec>\\n                  <jats:title>Discussion</jats:title>\\n                  <jats:p>RefAI demonstrated substantial improvements in literature recommendation and summarization over existing tools, addressing issues like fabricated papers, metadata inaccuracies, restricted recommendations, and poor reference integration.</jats:p>\\n               </jats:sec>\\n               <jats:sec>\\n                  <jats:title>Conclusion</jats:title>\\n                  <jats:p>By augmenting LLM with external resources and a novel ranking algorithm, RefAI is uniquely capable of recommending high-quality literature and generating well-structured summaries, holding the potential to meet the critical needs of biomedical professionals in navigating and synthesizing vast amounts of scientific literature.</jats:p>\\n               </jats:sec>\"\n",
    "\n",
    "#run embeddings\n",
    "original_embedding = generate_embeddings(original_text)\n",
    "typo_embedding = generate_embeddings(typo_text)\n",
    "print(type(typo_embedding))\n",
    "\n",
    "#calculate cosine similarity\n",
    "similarity = cosine_similarity(original_embedding, typo_embedding)\n",
    "print(f\"Cosine similarity: {similarity[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence transformer verison\n",
    "#reference: https://medium.com/@ahmedmellit/text-similarity-implementation-using-bert-embedding-in-python-1efdb5194e65\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#puts text from above into a list\n",
    "sentences:list = [original_text,typo_text]\n",
    "\n",
    "#initializing the Sentence Transformer model using BERT with mean-tokens pooling - source see above\n",
    "sentence_model = SentenceTransformer('bert-base-nli-mean-tokens') # this resets the model variable! changed to sentence_model variable name\n",
    "\n",
    "#encoding the sentences\n",
    "sentence_embeddings = sentence_model.encode(sentences)\n",
    "\n",
    "#result will be a list of similarity scores between two texts\n",
    "similarity_scores = cosine_similarity([sentence_embeddings[0]], sentence_embeddings[1:])\n",
    "\n",
    "print(f\"Cosine similarity scores using sentence embedding model: {similarity_scores[0][0]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_heatmap(embedding1, embedding2, title):\n",
    "    #calculate the difference between the two embeddings\n",
    "    diff = embedding1 - embedding2\n",
    "    #reshape the difference to a 2D array for the heatmap\n",
    "    diff_2d = diff.reshape(1, -1)\n",
    "    #create a heatmap\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    sns.heatmap(diff, cmap='coolwarm', annot=False, cbar=True,vmin=-1,vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "#plot heatmap\n",
    "#plot_heatmap(original_embedding, typo_embedding, \"diff between embeddings\")\n",
    "plot_heatmap(sentence_embeddings[0],sentence_embeddings[1:],\"Sentence Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with attention weights\n",
    "from https://github.com/clarkkev/attention-analysis\n",
    "- https://stackoverflow.com/questions/75772288/how-to-read-a-bert-attention-weight-matrix for explanation on queries and keys\n",
    "- https://theaisummer.com/self-attention/#:%7E:text=Self%2Dattention%20is%20not%20symmetric!&text=The%20arrows%20that%20correspond%20to,Q%E2%80%8B%3DWK%E2%80%8B. explanation on self-attention\n",
    "- heatmaps to analyze attention weights: https://apxml.com/courses/foundations-transformers-architecture/chapter-7-implementation-details-optimization/practice-analyzing-attention-weights\n",
    "- excellent source: https://apxml.com/courses/how-to-build-a-large-language-model/chapter-23-analyzing-model-behavior/attention-map-visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above \n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load SciBERT tokenizer and model - same as above - technically don't need to relaod these unless changing\n",
    "# try sentence based model?\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "\n",
    "# Tokenize the sentences\n",
    "inputs1 = tokenizer(original_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=20)#limit tokens so that we can actually see something\n",
    "inputs2 = tokenizer(typo_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=20)#\n",
    "\n",
    "# Get the attention weights: the `output_attentions=True` parameter is used to get the attention weights from the model\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1, output_attentions=True)\n",
    "    outputs2 = model(**inputs2, output_attentions=True)\n",
    "\n",
    "# Extract the attention weights for the last layer\n",
    "#.squeeze() https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html\n",
    "attention_weights1 = outputs1.attentions[-1].squeeze(0)  # Shape: (num_heads, seq_len, seq_len)\n",
    "attention_weights2 = outputs2.attentions[-1].squeeze(0)  # Shape: (num_heads, seq_len, seq_len)\n",
    "\n",
    "# Average the attention weights across all heads, \n",
    "#see last reference to visualize attention for each head\n",
    "attention_weights1 = attention_weights1.mean(dim=0)  # Shape: (seq_len, seq_len)\n",
    "attention_weights2 = attention_weights2.mean(dim=0)  # Shape: (seq_len, seq_len)\n",
    "\n",
    "# Get the tokens for the sentences\n",
    "tokens1 = tokenizer.convert_ids_to_tokens(inputs1[\"input_ids\"].squeeze(0))\n",
    "tokens2 = tokenizer.convert_ids_to_tokens(inputs2[\"input_ids\"].squeeze(0))\n",
    "\n",
    "# Plot the attention heatmap for the first sentence\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_weights1, xticklabels=tokens1, yticklabels=tokens1, cmap='viridis', annot=False, cbar=True)\n",
    "plt.title(\"Attention Weights for original_text\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the attention heatmap for the second sentence\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_weights2, xticklabels=tokens2, yticklabels=tokens2, cmap='viridis', annot=False, cbar=True)\n",
    "plt.title(\"Attention Weights for typo_text\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the difference in attention weights\n",
    "diff_attention_weights = (attention_weights1 - attention_weights2)\n",
    "\n",
    "# Plot the **difference** in attention weights\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(diff_attention_weights, xticklabels=tokens1, yticklabels=tokens1, cmap='coolwarm', annot=False, cbar=True, vmin=-1, vmax=1)\n",
    "plt.title(\"Difference in Attention Weights\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
