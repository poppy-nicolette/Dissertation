DOI: 10.18653/v1/D19-1371
Title: SciBERT: 科学テキスト用の事前トレーニング済み言語モデル SciBERT: A Pretrained Language Model for Scientific Text
Abstract: 科学分野の NLP タスク用の大規模な注釈付きデータを取得するのは困難であり、費用もかかります。高品質で大規模なラベル付き科学データの不足に対処するために、BERT に基づく事前トレーニング済み言語モデルである SciBERT をリリースします。 SciBERT は、科学出版物の大規模なマルチドメイン コーパスに対する教師なし事前トレーニングを活用して、下流の科学 NLP タスクのパフォーマンスを向上させます。私たちは、さまざまな科学分野のデータセットを使用して、シーケンスのタグ付け、文の分類、依存関係の解析などの一連のタスクを評価します。私たちは、BERT と比較して統計的に有意な改善を実証し、これらのタスクのいくつかで新しい最先端の結果を達成しました。 Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks.