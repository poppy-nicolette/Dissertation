DOI: 10.18653/v1/2024.eacl-demo.16
Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation
Abstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.