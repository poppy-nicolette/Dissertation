{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere RAG with dense retriever and ReRank model\n",
    "- references: https://docs.cohere.com/v2/docs/rag-complete-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import cohere\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import sys\n",
    "from colorama import Fore, Style, Back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mall is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "# load secret from local .env file\n",
    "def get_key():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    _key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "    #verify if it worked\n",
    "    if _key is not None:\n",
    "        print(Fore.GREEN + \"all is good, beautiful!\")\n",
    "        return _key\n",
    "    else:\n",
    "        print(Fore.LIGHTRED_EX + \"API Key is missing\")\n",
    "\n",
    "# initilize client\n",
    "co = cohere.ClientV2(get_key())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of documents: 45\n",
      "Rank: 1\n",
      "Score: 0.7636429\n",
      "Document: DOI: 10.48550/arXiv.2401.16359\n",
      " Title: Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\n",
      "Abstract: OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\n",
      "\n",
      "\n",
      "Rank: 2\n",
      "Score: 0.6864757\n",
      "Document: DOI: 10.48550/arXiv.2409.10633\n",
      " Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "Abstract: Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\n",
      "\n",
      "\n",
      "Rank: 3\n",
      "Score: 0.66045773\n",
      "Document: DOI: 10.48550/arXiv.2404.17663\n",
      " Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "Abstract: Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\n",
      "\n",
      "\n",
      "Rank: 4\n",
      "Score: 0.56719416\n",
      "Document: DOI: 10.3145/epi.2023.mar.09\n",
      " Title: Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\n",
      "Abstract: With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\n",
      "\n",
      "\n",
      "Rank: 5\n",
      "Score: 0.46570396\n",
      "Document: DOI: 10.48550/arXiv.2502.03627\n",
      " Title: Sorting the Babble in Babel: Assessing the Performance of Language Detection Algorithms on the OpenAlex Database\n",
      "Abstract: This project aims to compare various language classification procedures, procedures combining various Python language detection algorithms and metadata-based corpora extracted from manually-annotated articles sampled from the OpenAlex database. Following an analysis of precision and recall performance for each algorithm, corpus, and language as well as of processing speeds recorded for each algorithm and corpus type, overall procedure performance at the database level was simulated using probabilistic confusion matrices for each algorithm, corpus, and language as well as a probabilistic model of relative article language frequencies for the whole OpenAlex database. Results show that procedure performance strongly depends on the importance given to each of the measures implemented: for contexts where precision is preferred, using the LangID algorithm on the greedy corpus gives the best results; however, for all cases where recall is considered at least slightly more important than precision or as soon as processing times are given any kind of consideration, the procedure combining the FastSpell algorithm and the Titles corpus outperforms all other alternatives. Given the lack of truly multilingual, large-scale bibliographic databases, it is hoped that these results help confirm and foster the unparalleled potential of the OpenAlex database for cross-linguistic, bibliometric-based research and analysis.\n",
      "\n",
      "\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2401.16359\\n Title: Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', \"DOI: 10.48550/arXiv.2409.10633\\n Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", \"DOI: 10.48550/arXiv.2404.17663\\n Title: An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.3145/epi.2023.mar.09\\n Title: Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\\nAbstract: With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\\n', 'DOI: 10.48550/arXiv.2502.03627\\n Title: Sorting the Babble in Babel: Assessing the Performance of Language Detection Algorithms on the OpenAlex Database\\nAbstract: This project aims to compare various language classification procedures, procedures combining various Python language detection algorithms and metadata-based corpora extracted from manually-annotated articles sampled from the OpenAlex database. Following an analysis of precision and recall performance for each algorithm, corpus, and language as well as of processing speeds recorded for each algorithm and corpus type, overall procedure performance at the database level was simulated using probabilistic confusion matrices for each algorithm, corpus, and language as well as a probabilistic model of relative article language frequencies for the whole OpenAlex database. Results show that procedure performance strongly depends on the importance given to each of the measures implemented: for contexts where precision is preferred, using the LangID algorithm on the greedy corpus gives the best results; however, for all cases where recall is considered at least slightly more important than precision or as soon as processing times are given any kind of consideration, the procedure combining the FastSpell algorithm and the Titles corpus outperforms all other alternatives. Given the lack of truly multilingual, large-scale bibliographic databases, it is hoped that these results help confirm and foster the unparalleled potential of the OpenAlex database for cross-linguistic, bibliometric-based research and analysis.\\n']\n",
      "length of reranked_documents: 5\n",
      "Summary: OpenAlex is a new open-source database that promises to be more inclusive than established proprietary sources, such as Web of Science and Scopus.\n",
      "\n",
      "DOI: 10.48550/arXiv.2401.16359 - OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 - Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\n",
      "\n",
      "DOI: 10.3145/epi.2023.mar.09 - With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\n",
      "\n",
      "DOI: 10.48550/arXiv.2502.03627 - This project aims to compare various language classification procedures, procedures combining various Python language detection algorithms and metadata-based corpora extracted from manually-annotated articles sampled from the OpenAlex database. Following an analysis of precision and recall performance for each algorithm, corpus, and language as well as of processing speeds recorded for each algorithm and corpus type, overall procedure performance at the database level was simulated using probabilistic confusion matrices for each algorithm, corpus, and language as well as a probabilistic model of relative article language frequencies for the whole OpenAlex database. Results show that procedure performance strongly depends on the importance given to each of the measures implemented: for contexts where precision is preferred, using the LangID algorithm on the greedy corpus gives the best results; however, for all cases where recall is considered at least slightly more important than precision or as soon as processing times are given any kind of consideration, the procedure combining the FastSpell algorithm and the Titles corpus outperforms all other alternatives. Given the lack of truly multilingual, large-scale bibliographic databases, it is hoped that these results help confirm and foster the unparalleled potential of the OpenAlex database for cross-linguistic, bibliometric-based research and analysis.\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "\u001b[96m------\n",
      "Reranked documents:\n",
      "DOI: 10.48550/arXiv.2401.16359\n",
      " Title: Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\n",
      "Abstract: OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633\n",
      " Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "Abstract: Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663\n",
      " Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "Abstract: Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\n",
      "\n",
      "DOI: 10.3145/epi.2023.mar.09\n",
      " Title: Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\n",
      "Abstract: With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\n",
      "\n",
      "DOI: 10.48550/arXiv.2502.03627\n",
      " Title: Sorting the Babble in Babel: Assessing the Performance of Language Detection Algorithms on the OpenAlex Database\n",
      "Abstract: This project aims to compare various language classification procedures, procedures combining various Python language detection algorithms and metadata-based corpora extracted from manually-annotated articles sampled from the OpenAlex database. Following an analysis of precision and recall performance for each algorithm, corpus, and language as well as of processing speeds recorded for each algorithm and corpus type, overall procedure performance at the database level was simulated using probabilistic confusion matrices for each algorithm, corpus, and language as well as a probabilistic model of relative article language frequencies for the whole OpenAlex database. Results show that procedure performance strongly depends on the importance given to each of the measures implemented: for contexts where precision is preferred, using the LangID algorithm on the greedy corpus gives the best results; however, for all cases where recall is considered at least slightly more important than precision or as soon as processing times are given any kind of consideration, the procedure combining the FastSpell algorithm and the Titles corpus outperforms all other alternatives. Given the lack of truly multilingual, large-scale bibliographic databases, it is hoped that these results help confirm and foster the unparalleled potential of the OpenAlex database for cross-linguistic, bibliometric-based research and analysis.\n",
      "\n",
      "\u001b[93m\n",
      "CITATIONS:\n",
      "source text: new,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: open-source,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: database,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: promises to be more inclusive,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: established proprietary sources,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: Web of Science,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: Scopus.,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: 10.48550/arXiv.2401.16359,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: promising open source of scholarly metadata,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: competitor to established proprietary sources,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: Web of Science,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: Scopus.,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: provides its data freely and openly,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: permits researchers to perform bibliometric studies,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: reproduced in the community without licensing barriers.,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: rapidly evolving source,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: data contained within is expanding and also quickly changing,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: question naturally arises as to the trustworthiness of its data.,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: study the reference coverage and selected metadata within each database,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: compare them with each other,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: help address this open question in bibliometrics.,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: large-scale study,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus.,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: similarity in the distribution of source reference counts,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: Web of Science and Scopus as compared to OpenAlex.,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: demonstrate that the comparison of other core metadata covered by OpenAlex,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: mixed results when broken down by journal,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: capturing more ORCID identifiers,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: fewer abstracts,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: similar number of Open Access status indicators per article,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: compared to both the Web of Science and Scopus.,\n",
      "source: DOI: 10.48550/arXiv.2401.16359\n",
      "------\n",
      "source text: 10.48550/arXiv.2409.10633,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: Clarivate's Web of Science (WoS) and Elsevier's Scopus,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: decades the main sources of bibliometric information.,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: highly curated,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: closed, proprietary databases,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: largely biased towards English-language publications,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: underestimating the use of other languages in research dissemination.,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: Launched in 2022,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: promised comprehensive, inclusive, and open-source research information.,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: already in use by scholars and research institutions,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: quality of its metadata is currently being assessed.,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: contributes to this literature,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: assessing the completeness and accuracy of OpenAlex's metadata related to language,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: comparison with WoS,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: in-depth manual validation of a sample of 6,836 articles.,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: OpenAlex exhibits a far more balanced linguistic coverage than WoS.,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: language metadata is not always accurate,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: leads OpenAlex to overestimate the place of English,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: underestimating that of other languages.,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: used critically,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: provide comprehensive and representative analyses of languages used for scholarly publishing.,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: more work is needed at infrastructural level,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: ensure the quality of metadata on language.,\n",
      "source: DOI: 10.48550/arXiv.2409.10633\n",
      "------\n",
      "source text: 10.48550/arXiv.2404.17663,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: Scopus and the Web of Science,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: foundation for research in the science of science,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: traditional databases systematically underrepresent certain disciplines and world regions.,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: new inclusive databases,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: OpenAlex,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: many studies have begun using OpenAlex as a data source,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: few critically assess its limitations.,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: conducted in collaboration with the OpenAlex team,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: addresses this gap,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: comparing OpenAlex to Scopus across a number of dimensions.,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: analysis concludes that OpenAlex is a superset of Scopus,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: reliable alternative for some analyses,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: particularly at the country level.,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: issues of metadata accuracy and completeness,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: additional research is needed to fully comprehend and address OpenAlex's limitations.,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: necessary to confidently use OpenAlex across a wider set of analyses,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: including those that are not at all possible with more constrained databases.,\n",
      "source: DOI: 10.48550/arXiv.2404.17663\n",
      "------\n",
      "source text: 10.3145/epi.2023.mar.09,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: announcement of the retirement of Microsoft Academic Graph (MAG),\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: non-profit organization OurResearch,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: provide a similar resource under the name OpenAlex.,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: compare the metadata with relevance to bibliometric analyses,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: latest MAG snapshot,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: early OpenAlex snapshot.,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: Practically all works from MAG were transferred to OpenAlex,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: preserving their bibliographic data publication year, volume, first and last page, DOI,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: number of references that are important ingredients of citation analysis.,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: More than 90% of the MAG documents have equivalent document types in OpenAlex.,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: correct and amount to more than 7%,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: document type specifications have improved significantly from MAG to OpenAlex.,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: item of bibliometric relevant metadata,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: looked at the paper-based subject classification in MAG and in OpenAlex.,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: found significantly more documents with a subject classification assignment in OpenAlex than in MAG.,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: first and second level,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: classification structure is nearly identical.,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: present data on the subject reclassifications on both levels in tabular and graphical form.,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: assessment of the consequences of the abundant subject reclassifications,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: field-normalized bibliometric evaluations,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: not in the scope of the present paper.,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: Apart from this open question,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: overall at least as suited for bibliometric analyses as MAG,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: publication years before 2021,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: maybe even better because of the broader coverage of document type assignments.,\n",
      "source: DOI: 10.3145/epi.2023.mar.09\n",
      "------\n",
      "source text: 10.48550/arXiv.2502.03627,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: aims to compare various language classification procedures,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: procedures combining various Python language detection algorithms,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: metadata-based corpora extracted from manually-annotated articles,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: OpenAlex database.,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: Following an analysis of precision and recall performance for each algorithm, corpus, and language,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: processing speeds recorded for each algorithm and corpus type,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: overall procedure performance at the database level,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: simulated using probabilistic confusion matrices for each algorithm, corpus, and language,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: probabilistic model of relative article language frequencies for the whole OpenAlex database.,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: procedure performance strongly depends on the importance given to each of the measures implemented,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: contexts where precision is preferred,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: using the LangID algorithm on the greedy corpus gives the best results,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: all cases where recall is considered at least slightly more important than precision,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: soon as processing times are given any kind of consideration,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: procedure combining the FastSpell algorithm and the Titles corpus outperforms all other alternatives.,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: lack of truly multilingual, large-scale bibliographic databases,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: hoped that these results help confirm and foster the unparalleled potential of the OpenAlex database,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n",
      "source text: cross-linguistic, bibliometric-based research and analysis.,\n",
      "source: DOI: 10.48550/arXiv.2502.03627\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "    global documents_with_doi\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0]\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append(f\"{doi} {text}\\n\")\n",
    "    return documents_with_doi\n",
    "\n",
    "# initialize empty search query\n",
    "search_queries = []\n",
    "# Embed the documents\n",
    "def document_embed(documents:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the documents from a list provided from read_documents_with_doi()\n",
    "    \"\"\"\n",
    "    doc_emb = co.embed(\n",
    "        model=\"embed-v4.0\",\n",
    "        input_type=\"search_document\",\n",
    "        texts=[doc for doc in documents],\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return doc_emb\n",
    "\n",
    "# Embed the search query\n",
    "def query_embed(search_queries:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the query from a list provided in search_queries variable\n",
    "    \"\"\"\n",
    "    query_emb = co.embed(\n",
    "        model=\"embed-v4.0\",\n",
    "        input_type=\"search_query\",\n",
    "        texts=search_queries,\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return query_emb\n",
    "\n",
    "# retrieve top_k and compute similarity using dot product\n",
    "def retrieve_top_k(top_k, query_embedded, documents_embedded, documents)->List[str]:\n",
    "    \"\"\"\n",
    "    returns the top_k documents based on dot product similarity\n",
    "    \"\"\"\n",
    "\n",
    "    scores = np.dot(query_embedded, np.transpose(documents_embedded))[0]#ordered list!\n",
    "    # takes top scores, and returns sorted list and returns indices sliced by top_k\n",
    "    max_idx = np.argsort(-scores)[:top_k]\n",
    "    # returns documents by index\n",
    "    retrieved_docs = [documents[item] for item in max_idx]\n",
    "    # returns a list of documents\n",
    "    return retrieved_docs\n",
    "\n",
    "def rerank_documents(retrieved_documents,search_queries,threshold,top_k)->List[str]:\n",
    "    \"\"\"\n",
    "    takes retrieved_documents as input along with search_queries and runs them through the \n",
    "    rerank model from cohere for semantic similarity. \n",
    "\n",
    "    top_n = top_k\n",
    "    Limits those returned by a threshold score. this is to reduce those that are irrelevant.\n",
    "    \"\"\"\n",
    "    # Rerank the documents\n",
    "    results = co.rerank(\n",
    "        model=\"rerank-v3.5\",\n",
    "        query=search_queries[0],\n",
    "        documents=[doc for doc in retrieved_documents],\n",
    "        top_n=top_k,\n",
    "        max_tokens_per_doc=4096,# defaults to 4096\n",
    "    )\n",
    "\n",
    "    # Display the reranking results\n",
    "    for idx, result in enumerate(results.results):\n",
    "        print(f\"Rank: {idx+1}\")\n",
    "        print(f\"Score: {result.relevance_score}\")\n",
    "        print(f\"Document: {retrieved_documents[result.index]}\\n\")\n",
    "\n",
    "    #returns only those over threshold\n",
    "    reranked_docs = [\n",
    "        retrieved_documents[result.index] for result in results.results if result.relevance_score >=threshold\n",
    "    ]\n",
    "    reranked_with_score = [(result.relevance_score, retrieved_documents[result.index].split(\"\\n\")[0].strip(\"DOI: \")) for result in results.results if result.relevance_score >=threshold]\n",
    "\n",
    "    print(f\"reranked_documents: {reranked_docs}\")\n",
    "    print(f\"length of reranked_documents: {len(reranked_docs)}\")\n",
    "\n",
    "    return reranked_docs, reranked_with_score\n",
    "\n",
    "def cohere_rag_pipeline(directory_path,search_queries,top_k,threshold):\n",
    "\n",
    "    # retrieve documents from directory\n",
    "    documents = read_documents_with_doi(directory_path)\n",
    "    print(f\"Length of documents: {len(documents)}\")\n",
    "    # embed the documents\n",
    "    documents_embedded = document_embed(documents)\n",
    "\n",
    "    #embed the query:\n",
    "    query_embedded = query_embed(search_queries)\n",
    "\n",
    "    # retrieve the top_k documents\n",
    "    retrieved_documents = retrieve_top_k(top_k, query_embedded, documents_embedded, documents)\n",
    "\n",
    "    # rerank the documents using the Rerank model from Cohere\n",
    "    reranked_documents, reranked_DOIs_with_score = rerank_documents(retrieved_documents,search_queries,threshold,top_k)\n",
    "    # set system instructions\n",
    "    instructions = \"\"\"\n",
    "                    You are an academic research assistant.\n",
    "                    You must include the DOI in your response.\n",
    "                    If there is no content provided, ask for a different question.\n",
    "                    Please structure your response like this:\n",
    "                    Summary: summary statement here. \n",
    "                    DOI: summary of the text associated with this DOI.\n",
    "                    Address me as, 'my lady'.\n",
    "                    \"\"\"\n",
    "    # create messages to model\n",
    "    messages = [{\"role\":\"user\",\n",
    "                \"content\": search_queries[0]},\n",
    "                {\"role\":\"system\",\n",
    "                \"content\":instructions}]\n",
    "\n",
    "    # Generate the response\n",
    "    resp = co.chat(\n",
    "        model=\"command-a-03-2025\",\n",
    "        messages=messages,\n",
    "        documents=reranked_documents,\n",
    "    )\n",
    "\n",
    "    return resp, reranked_documents, reranked_DOIs_with_score\n",
    "\n",
    "# ****** Pipeline ********\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.1\n",
    "\n",
    "response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "# Display the response\n",
    "print(response.message.content[0].text)\n",
    "print(Fore.LIGHTCYAN_EX + f\"------\\nReranked documents:\")\n",
    "for doc in reranked_documents_end:\n",
    "    print(doc)\n",
    "\n",
    "# Display the citations and source documents\n",
    "if response.message.citations:\n",
    "    print(Fore.LIGHTYELLOW_EX + \"\\nCITATIONS:\")\n",
    "    for citation in response.message.citations:\n",
    "        print(f\"source text: {citation.text},\\nsource: {citation.sources[0].document.get('content').split(\"\\n\")[0]}\\n------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.7636429, '10.48550/arXiv.2401.16359'),\n",
       " (0.6864757, '10.48550/arXiv.2409.10633'),\n",
       " (0.66045773, '10.48550/arXiv.2404.17663'),\n",
       " (0.56719416, '10.3145/epi.2023.mar.09'),\n",
       " (0.46570396, '10.48550/arXiv.2502.03627')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_DOIs_with_score_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# Analysis\n",
    "Precision, recall, accuracy, F1 scores and faithfulness\n",
    "## Precision, recall, F1 score\n",
    "### references\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "- https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "import openpyxl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Documents score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query, Precision, Recall, F1-Score, Accuracy, Balanced accuracy, Faithfulness score, Documents score, Response]\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial dataframe to capture results from each query and results\n",
    "#ONLY DO THIS AT THE BEGINNING OF THE ANALYSIS PROCEDURE, OTHERWISE, IT WILL ERASE THE PREVIOUS RESULTS!!\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Query','Precision','Recall','F1-Score','Accuracy', 'Balanced accuracy', 'Faithfulness score', 'Documents score', 'Response'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101', '10.31222/osf.io/smxe5', '10.1002/leap.1411']\n",
      "45\n",
      "For query: tell me about studies that investigated abstracts in metadata.:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.889\n",
      "Faithfulness score: 0\n",
      "\u001b[95mDocuments scores: (0.40867847, '10.1162/qss_a_00286')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2801: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Query': 'tell me about studies that investigated abstracts in metadata.',\n",
       " 'Precision': 0.0,\n",
       " 'Recall': 0.0,\n",
       " 'F1-Score': 0.8888888888888888,\n",
       " 'Accuracy': 0.8888888888888888,\n",
       " 'Balanced accuracy': 0.8888888888888888,\n",
       " 'Faithfulness score': 0,\n",
       " 'Documents score': (0.40867847, '10.1162/qss_a_00286'),\n",
       " 'Response': \"I'm sorry, my lady, I couldn't find any information about studies that investigated abstracts in metadata.\\n\\nHowever, I did find information about metadata quality and how it can be improved.\\n\\nI also found information about how the COVID-19 pandemic has changed the lexical content of abstracts.\\n\\nIf you would like to know more about any of these topics, please let me know.\"}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "\"\"\"\n",
    "change this to read in an excel sheet of queries and ground_truth dois.\n",
    "Then it should be isolated as a function.\n",
    "Run the function to iterature through the list.\n",
    "\"\"\"\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# initiates the variable\n",
    "ground_truth = []\n",
    "\n",
    "def evaluate_retrieval(retrieved_dois, ground_truth):\n",
    "    corpus_doi_list = []\n",
    "    #corpus_list is a global variable in rag_pipeline()\n",
    "    for each in range(len(documents_with_doi)):\n",
    "        a = documents_with_doi[each].split(\"\\n\")[0].strip(\"DOI: \")\n",
    "        corpus_doi_list.append(a)\n",
    "    print(len(corpus_doi_list))\n",
    "\n",
    "    def compare_lists(list1, list2, list3):\n",
    "        for val in list1:\n",
    "            if val in list2:\n",
    "                list3.append(1)\n",
    "            else:\n",
    "                list3.append(0)\n",
    "\n",
    "    #set y_true so that len(y_true)==len(corpus_doi_list)\n",
    "    y_true = []\n",
    "    compare_lists(corpus_doi_list,ground_truth,y_true)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = []\n",
    "    compare_lists(corpus_doi_list,retrieved_dois,y_pred)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # calculate metrics - could also use sklearn.metrics functions such as precision_score, but this is easier to read\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    faithfulness_score = 0\n",
    "    for each in retrieved_dois:\n",
    "        if each in response.message.content[0].text:\n",
    "            faithfulness_score+=1\n",
    "        else:\n",
    "            faithfulness_score+=0\n",
    "\n",
    "    return {\n",
    "        'Query':f\"{search_queries[0]}\",\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\":accuracy,\n",
    "        \"Balanced accuracy\":balanced_accuracy,\n",
    "        \"Faithfulness score\":faithfulness_score,\n",
    "        \"Documents score\":reranked_DOIs_with_score_end[0],\n",
    "        \"Response\":response.message.content[0].text\n",
    "    }\n",
    "\n",
    "def print_results()->Dict:\n",
    "    \"\"\"\n",
    "    Prints a nicely ordered set of results from evalaute_retrieval()\n",
    "    \"\"\"\n",
    "    global results\n",
    "    results = evaluate_retrieval(retrieved_dois, ground_truth)\n",
    "    print(f\"For query: {results['Query']}:\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"F1-Score: {results['F1-Score']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    print(f\"Balanced accuracy: {results['Balanced accuracy']:.3f}\")\n",
    "    print(f\"Faithfulness score: {results['Faithfulness score']}\")\n",
    "    print(Fore.LIGHTMAGENTA_EX + f\"Documents scores: {results['Documents score']}\")\n",
    "    return results\n",
    "\n",
    "#for debugging\n",
    "\n",
    "print_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the test from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101', '10.31222/osf.io/smxe5', '10.1002/leap.1411']\n",
      "Length of documents: 45\n",
      "Rank: 1\n",
      "Score: 0.44985238\n",
      "Document: DOI: 10.1162/qss_a_00286\n",
      " Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Abstract: Abstract The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\n",
      "\n",
      "\n",
      "Rank: 2\n",
      "Score: 0.44160134\n",
      "Document: DOI: 10.5860/crl.86.1.101\n",
      " Title: Identifying Metadata Quality Issues Across Cultures\n",
      "Abstract: Metadata are crucial for discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. In this context, we sought to understand how metadata quality, consistency, and completeness impact individuals and communities. Reviewing a sample of records, we identified and classified issues stemming from how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\n",
      "\n",
      "\n",
      "Rank: 3\n",
      "Score: 0.35952207\n",
      "Document: DOI: 10.48550/arXiv.2303.17661\n",
      " Title: MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\n",
      "Abstract: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\n",
      "\n",
      "\n",
      "Rank: 4\n",
      "Score: 0.3227399\n",
      "Document: DOI: 10.31222/osf.io/smxe5\n",
      " Title: Crossref as a source of open bibliographic metadata\n",
      "Abstract: Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\n",
      "\n",
      "\n",
      "Rank: 5\n",
      "Score: 0.112532035\n",
      "Document: DOI: 10.1002/leap.1411\n",
      " Title: Overpromotion and caution in abstracts of preprints during the <scp>COVID</scp>19 crisis\n",
      "Abstract: Abstract The abstract is known to be a promotional genre where researchers tend to exaggerate the benefit of their research and use a promotional discourse to catch the reader's attention. The COVID19 pandemic has prompted intensive research and has changed traditional publishing with the massive adoption of preprints by researchers. Our aim is to investigate whether the crisis and the ensuing scientific and economic competition have changed the lexical content of abstracts. We propose a comparative study of abstracts associated with preprints issued in response to the pandemic relative to abstracts produced during the closest prepandemic period. We show that with the increase (on average and in percentage) of positive words (especially effective ) and the slight decrease of negative words, there is a strong increase in hedge words (the most frequent of which are the modal verbs can and may ). Hedge words counterbalance the excessive use of positive words and thus invite the readers, who go probably beyond the usual audience, to be cautious with the obtained results. The abstracts of preprints urgently produced in response to the COVID19 crisis stand between uncertainty and overpromotion, illustrating the balance that authors have to achieve between promoting their results and appealing for caution.\n",
      "\n",
      "\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00286\\n Title: Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: Abstract The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: Identifying Metadata Quality Issues Across Cultures\\nAbstract: Metadata are crucial for discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. In this context, we sought to understand how metadata quality, consistency, and completeness impact individuals and communities. Reviewing a sample of records, we identified and classified issues stemming from how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: Crossref as a source of open bibliographic metadata\\nAbstract: Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', \"DOI: 10.1002/leap.1411\\n Title: Overpromotion and caution in abstracts of preprints during the <scp>COVID</scp>19 crisis\\nAbstract: Abstract The abstract is known to be a promotional genre where researchers tend to exaggerate the benefit of their research and use a promotional discourse to catch the reader's attention. The COVID19 pandemic has prompted intensive research and has changed traditional publishing with the massive adoption of preprints by researchers. Our aim is to investigate whether the crisis and the ensuing scientific and economic competition have changed the lexical content of abstracts. We propose a comparative study of abstracts associated with preprints issued in response to the pandemic relative to abstracts produced during the closest prepandemic period. We show that with the increase (on average and in percentage) of positive words (especially effective ) and the slight decrease of negative words, there is a strong increase in hedge words (the most frequent of which are the modal verbs can and may ). Hedge words counterbalance the excessive use of positive words and thus invite the readers, who go probably beyond the usual audience, to be cautious with the obtained results. The abstracts of preprints urgently produced in response to the COVID19 crisis stand between uncertainty and overpromotion, illustrating the balance that authors have to achieve between promoting their results and appealing for caution.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "I'm sorry, my lady, but I could not find any information about studies on abstracts in metadata.\n",
      "\n",
      "Is there anything else I can help you with?\n",
      "45\n",
      "For query: tell me about studies on abstracts in metadata.:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.438\n",
      "Faithfulness score: 0\n",
      "\u001b[95mDocuments scores: (0.44985238, '10.1162/qss_a_00286')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Documents score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tell me about studies that investigated abstra...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0</td>\n",
       "      <td>[(0.40867847, 10.1162/qss_a_00286), (0.3662955...</td>\n",
       "      <td>I'm sorry, my lady, I couldn't find any inform...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tell me about studies on abstracts in metadata.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.4375</td>\n",
       "      <td>0</td>\n",
       "      <td>(0.44985238, 10.1162/qss_a_00286)</td>\n",
       "      <td>I'm sorry, my lady, but I could not find any i...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  ...                                           Response\n",
       "0  tell me about studies that investigated abstra...  ...  I'm sorry, my lady, I couldn't find any inform...\n",
       "1    tell me about studies on abstracts in metadata.  ...  I'm sorry, my lady, but I could not find any i...\n",
       "\n",
       "[2 rows x 9 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the test from here\n",
    "\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# Ground truth relevant documents (DOIs) for each query\n",
    "ground_truth = [\"10.1007/s11192-022-04367-w\",\"10.1371/journal.pbio.1002542\",\"10.1007/s11192-015-1765-5\",\"10.1162/qss_a_00112\",\"10.1162/qss_a_00210\"]\n",
    "\n",
    "#***** Begin chat session *****\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.10\n",
    "\n",
    "response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "# Display the response\n",
    "print(response.message.content[0].text)\n",
    "\n",
    "new_result = print_results()\n",
    "# add the new result to the df\n",
    "results_df.loc[len(results_df)] = new_result\n",
    "\n",
    "#save the queries and responses to separate dataframe to be manually annontated\n",
    "answer_relevance_df = results_df[['Query','Response']].copy(deep=True)\n",
    "\n",
    "# save out answer_relevance_df\n",
    "filename=\"analysis/dense_answer_relevance_results.xlsx\"\n",
    "answer_relevance_df.to_excel(filename)\n",
    "\n",
    "filename = \"analysis/dense_analysis_results.xlsx\"\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "results_df.to_excel(filename)\n",
    "results_df.tail(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
