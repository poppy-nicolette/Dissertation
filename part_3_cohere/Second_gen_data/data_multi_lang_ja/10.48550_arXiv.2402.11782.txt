DOI: 10.48550/arXiv.2402.11782
Title: 言語モデルが説得力のある証拠は何ですか? What Evidence Do Language Models Find Convincing?
Abstract: 検索拡張言語モデルは、「アスパルテームは癌に関連しているか」など、主観的で議論の余地のある矛盾したクエリを扱うことがますます増えています。これらのあいまいな疑問を解決するには、広範囲の Web サイトを検索し、「この証拠のどれに説得力があると思われるか?」を検討する必要があります。この研究では、LLM がこの質問にどのように答えるかを研究します。特に、私たちは、さまざまな事実 (例: 定量的結果)、議論スタイル (例: 権威への訴え)、および回答 (はいまたはいいえ) を含む一連の現実世界の証拠文書と物議を醸すクエリを組み合わせるデータセットである ConflictingQA を構築します。このデータセットを使用して感度分析と反事実分析を実行し、LLM 予測に最も影響を与えるテキストの特徴を調査します。全体として、現在のモデルはクエリに対する Web サイトの関連性に大きく依存しており、テキストに科学的参照が含まれているか、中立的なトーンで書かれているかなど、人間が重要だと考える文体の特徴をほとんど無視していることがわかりました。総合すると、これらの結果は、RAG コーパスの品質の重要性 (誤った情報をフィルタリングする必要性など) と、おそらく人間の判断とよりよく一致するように LLM を訓練する方法の変化さえも強調しています。 Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as "is aspartame linked to cancer". To resolve these ambiguous queries, one must search through a large range of websites and consider "which, if any, of this evidence do I find convincing?". In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone. Taken together, these results highlight the importance of RAG corpus quality (e.g., the need to filter misinformation), and possibly even a shift in how LLMs are trained to better align with human judgements.