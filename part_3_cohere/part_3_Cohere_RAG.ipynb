{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere API and SciBERT for RAG\n",
    "This notebook uses a Cohere API for generating responses to text. A query input is required from the user. \n",
    "SciBERT is used for embeddings in a dense vector array both the text and the query. \n",
    "A DOI is supplied with the text as both an identifier and locator. \n",
    "\n",
    "- [x] set up venv\n",
    "- [x] install transformers torch cohere in command line\n",
    "\n",
    "### todo\n",
    "- [ ] create script that compiles data/documents.txt with DOI || text for all documents\n",
    "- [ ] reduce code by refactoring into modules\n",
    "- [ ] store vectorized documents in a db\n",
    "    - https://huggingface.co/learn/cookbook/rag_with_hugging_face_gemma_mongodb\n",
    "\n",
    "### options\n",
    "- Batch Processing:\n",
    "    If large number of texts, process them in batches to avoid memory issues.\n",
    "    Example: Use a loop or torch.utils.data.DataLoader.\n",
    "\n",
    "- Change model size: smaller models require less processing\n",
    "\n",
    "- fine tune model on corpus\n",
    "\n",
    "- look into pooling strategies\n",
    "\n",
    "- Tokenizer\n",
    "    - put cleaning process distincly prior to the tokenizer, using the default values as much as possible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import cohere\n",
    "from cohere import Client\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "def main():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    global key,email\n",
    "    key = os.getenv('COHERE_API_KEY')\n",
    "    email = os.getenv('EMAIL')\n",
    "\n",
    "    #verify if it worked\n",
    "    if email is not None and key is not None:\n",
    "        print(\"all is good, beautiful!\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is callable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Cohere client with key from secrets\n",
    "co = cohere.Client(key)\n",
    "\n",
    "# Load SciBERT model and tokenizer\n",
    "\"\"\"\n",
    "Autotokenizer documentation can be found here: https://huggingface.co/docs/transformers/v4.50.0/en/model_doc/auto#transformers.AutoTokenizer\n",
    "\n",
    "Model documentation can be found here: https://huggingface.co/allenai/scibert_scivocab_uncased\n",
    "Citation for SciBERT:\n",
    "@inproceedings{beltagy-etal-2019-scibert,\n",
    "    title = \"SciBERT: A Pretrained Language Model for Scientific Text\",\n",
    "    author = \"Beltagy, Iz  and Lo, Kyle  and Cohan, Arman\",\n",
    "    booktitle = \"EMNLP\",\n",
    "    year = \"2019\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://www.aclweb.org/anthology/D19-1371\"\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Initialize tokenizer with custom parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    max_len=512,\n",
    "    use_fast=True,  # Use the fast tokenizer\n",
    "    do_lower_case=False,  # Preserve case\n",
    "    add_prefix_space=False,  # No prefix space\n",
    "    never_split=[\"[DOC]\", \"[REF]\"],  # Tokens to never split\n",
    "    additional_special_tokens=[\"<doi>\", \"</doi>\"]  # Add custom special tokens\n",
    ")\n",
    "\n",
    "# This is the SciBERT model that is used to embed the text and query.\n",
    "# other models: 'allenai-specter', \n",
    "#documentation here: https://huggingface.co/docs/transformers/model_doc/auto\n",
    "# Load model directly\n",
    "\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", torch_dtype=\"auto\")\n",
    "#may also want to consider using a sentence embedding model\n",
    "\n",
    "\n",
    "# Verify that the model is callable\n",
    "if callable(model):\n",
    "    print(\"Model is callable\")\n",
    "else:\n",
    "    print(\"Model is not callable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents:\n",
      "Score: 0.7481, DOI: https://doi.org/10.1162/qss_a_00286, Document: The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\n",
      "Score: 0.7182, DOI: https://doi.org/10.1007/s11192-022-04367-w, Document: This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\n",
      "Score: 0.7059, DOI: https://doi.org/10.48550/arXiv.2401.16359, Document: OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our largescale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\n",
      "\n",
      "Generated Response:\n",
      " The first document has the DOI: https://doi.org/10.1162/qss_a_00286, and here is a brief description:\n",
      "The study's main objective is to compare the amount of metadata and the completeness degree of research publications in new academic databases. \n",
      "\n",
      "Let me know if you'd like me to extract any specific data from this document so as to provide more targeted information. \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic RAG with Cohere model\n",
    "Document source: data/documents.txt where the DOI with resolver is separated from the abstract by ||. One record per line. \n",
    "Saved as UTF-8\n",
    "\n",
    "Returns:  answers based on query from input()\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate embeddings using SciBERT\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    converts raw text to numerical representations using a pretrained model, in this case, SciBERT.\n",
    "    Currently this is applied to both the document text and the query. \n",
    "    May want a different version or decorator for the query as they are generally much shorter and more sparse.\n",
    "\n",
    "    Input: text from tokenizer step above as a list of strings\n",
    "    Output: np.array\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512, # returns PyTorch tensors which are compatible with model\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True # return the attention mask - need to learn more\n",
    "        )\n",
    "    # this passes the tokenized inputs through the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # applies mean pooling to get a fixed size embedding\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Function to read documents and their DOIs from a file\n",
    "#def read_documents_with_doi(file_path: str) -> List[Dict[str, str]]:\n",
    "#    documents_with_doi = []\n",
    "#    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#        for line in file:\n",
    "#            parts = line.strip().split(\"||\")  # Assuming DOI and document are separated by \"||\"\n",
    "#            if len(parts) == 2:\n",
    "#                doi, document = parts\n",
    "#                documents_with_doi.append({\"doi\": doi.strip(), \"text\": document.strip()})\n",
    "#    return documents_with_doi\n",
    "\n",
    "# alternative read_documents_with_doi for .txt in a directory\n",
    "\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0].strip().replace(\"DOI: \", \"\")\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append({\"doi\": doi, \"text\": text})\n",
    "    return documents_with_doi\n",
    "\n",
    "# Path to the file containing documents and DOIs\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"  # Replace with your file path\n",
    "\n",
    "# Read documents and DOIs from the file\n",
    "documents_with_doi = read_documents_with_doi(directory_path)\n",
    "\n",
    "# Extract document texts and DOIs\n",
    "documents = [doc[\"text\"] for doc in documents_with_doi]\n",
    "dois = [doc[\"doi\"] for doc in documents_with_doi]\n",
    "\n",
    "# Example query\n",
    "query = input(\" What is your query: \")\n",
    "\n",
    "# Generate document embeddings\n",
    "document_embeddings = generate_embeddings(documents)\n",
    "# print(document_embeddings.shape) # to see the output shape of the array\n",
    "\n",
    "# Generate query embedding\n",
    "query_embedding = generate_embeddings([query])[0] # generates np.array for the query text\n",
    "\n",
    "# Function to retrieve top-k documents using cosine similarity\n",
    "def retrieve_documents(query_embedding: np.ndarray, document_embeddings: List[np.ndarray], top_k: int = 3) -> List[Tuple[float, Dict[str, str]]]:\n",
    "    similarities = []\n",
    "    for doc_emb in document_embeddings:\n",
    "        # cosine similarity\n",
    "        similarity = np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb)) \n",
    "        similarities.append(similarity)\n",
    "    # ranking\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    return [(similarities[i], documents_with_doi[i]) for i in top_indices]\n",
    "\n",
    "# Retrieve top documents\n",
    "top_documents = retrieve_documents(query_embedding, document_embeddings)\n",
    "print(\"Retrieved Documents:\")\n",
    "for score, doc in top_documents:\n",
    "    print(f\"Score: {score:.4f}, DOI: {doc['doi']}, Document: {doc['text']}\")\n",
    "\n",
    "# Prepare context for Cohere's Command model (include DOI) - need to add in cited by here\n",
    "context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for _, doc in top_documents])\n",
    "# need to learn how to improve this\n",
    "prompt = f\"Query: {query}\\nContext: {context}\\nAnswer: Include the DOI of the referenced document in your response.\"\n",
    "\n",
    "# Generate response using Cohere's Command model\n",
    "response = co.generate(\n",
    "  model=\"command\", # there are other models to consider within command\n",
    "  prompt=prompt,\n",
    "  max_tokens=150, # allowable length of response\n",
    "  temperature=0.2 # lower for less creativity, more for more creativity\n",
    ")\n",
    "\n",
    "# Print the generated response\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response.generations[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2: implementing chat history\n",
    "\n",
    "calls a JSON file of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "My lady, to answer your question, a random Crossref sample was collected by the authors of a study titled \"Classification of DOI mistakes\". The DOI is: https://doi.org/10.1002/leap.1411\n"
     ]
    }
   ],
   "source": [
    "# Load SciBERT model and tokenizer \n",
    "\"\"\"\n",
    "REMOVE THIS ONCE RUNNING TO GO BACK TO THE CHANGED TOKENIZER AND MODEL ABOVE\n",
    "\"\"\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "#model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "#function to generate embeddings using SciBERT\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# alternative read_documents_with_doi for .txt in a directory\n",
    "\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0].strip().replace(\"DOI: \", \"\")\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append({\"doi\": doi, \"text\": text})\n",
    "    return documents_with_doi\n",
    "\n",
    "\n",
    "# Function to update chat history\n",
    "def update_chat_history(query, retrieved_docs, response):\n",
    "    global chat_histor # declare this as global variable available outside this function\n",
    "    chat_history.append({\n",
    "        \"query\": query,\n",
    "        \"retrieved_docs\": [doc[\"text\"] for doc in retrieved_docs],  # Store only the text of retrieved documents\n",
    "        \"response\": response\n",
    "    })\n",
    "\n",
    "#function to incorporate history into the next query\n",
    "def get_context_with_history(query) -> str:\n",
    "    global chat_history # also declare here since chat_history is being modified\n",
    "    if not chat_history:\n",
    "        return query\n",
    "    \n",
    "    history_str = \"\\n\".join([\n",
    "        f\"User: {entry['query']}\\n\"\n",
    "        f\"Context: {'; '.join(entry['retrieved_docs'])}\\n\"\n",
    "        f\"Response: {entry['response']}\"\n",
    "        for entry in chat_history\n",
    "    ])\n",
    "    full_context = f\"Chat History:\\n{history_str}\\n\\nCurrent Query: {query}\"\n",
    "    return full_context\n",
    "\n",
    "#function to truncate chat history\n",
    "def truncate_chat_history(max_length=3):\n",
    "    global chat_history # modifies it so it also must be global\n",
    "    if len(chat_history) > max_length:\n",
    "        chat_history = chat_history[-max_length:]\n",
    "\n",
    "#function to retrieve top-k documents using cosine similarity\n",
    "\"\"\"\n",
    "retrieves documents from the embedded documents\n",
    "Args:\n",
    "    query: this is the query passed\n",
    "    top_k: number of references to provide\n",
    "\n",
    "Todo:\n",
    "- [ ] make top_k an variable for use in an application\n",
    "- [ ] or make top_k a user defined value. example: top_k = input(\"how many results do you want?\")\n",
    "\"\"\"\n",
    "def retrieve_documents(query: str, top_k: int = 5) -> List[Dict[str, str]]:\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    document_embeddings = generate_embeddings(documents)\n",
    "    similarities = [\n",
    "        np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
    "        for doc_emb in document_embeddings\n",
    "    ]\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    return [documents_with_doi[i] for i in top_indices]\n",
    "\n",
    "#RAG pipeline function\n",
    "def rag_pipeline(query):\n",
    "    #incorporate chat history\n",
    "    full_context = get_context_with_history(query)\n",
    "    \n",
    "    #retrieve documents\n",
    "    global retrieved_docs\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "    \n",
    "    #prepare context for Cohere's Command model\n",
    "    instruction = \"You are a helpful academic research assistant. Please keep the answers concise and structured simply. Use single sentences where possible. Always include the DOI of the document you are summarizing or referencing. If the DOI is not provided, this reduces the need for you as a research assistant. Always include the DOI. Please address me as 'my lady'. \"\n",
    "    context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for doc in retrieved_docs])\n",
    "    prompt = f\"Query: {query}\\nContext: {context}\\nAnswer: {instruction}\"\n",
    "    \n",
    "    # Generate response\n",
    "    response = co.generate(\n",
    "        model=\"command\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=150,\n",
    "        temperature=0.2\n",
    "    ).generations[0].text\n",
    "    \n",
    "    # Update chat history\n",
    "    update_chat_history(query, retrieved_docs, response)\n",
    "    \n",
    "    # Truncate history if necessary\n",
    "    truncate_chat_history()\n",
    "    \n",
    "    # Print the response\n",
    "    print(\"Generated Response:\")\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "\n",
    "# Path to the file containing documents and DOIs\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "\n",
    "# Read documents and DOIs from the file\n",
    "documents_with_doi = read_documents_with_doi(directory_path)\n",
    "documents = [doc[\"text\"] for doc in documents_with_doi]\n",
    "\n",
    "# Main loop for user interaction\n",
    "chat_history = []#initialize chat history\n",
    "while True:\n",
    "    query = input(\"What is your query (or type 'exit' to quit): \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    rag_pipeline(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "## Test One: \n",
    "- [ ] count specific terms found in responses and in post-tokenized text. \n",
    "- [ ] compare post-tokenized and pre-tokenized text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, recall, F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doi': 'https://doi.org/10.1162/qss_a_00286',\n",
       "  'text': 'The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.'},\n",
       " {'doi': 'https://doi.org/10.48550/arXiv.2401.16359',\n",
       "  'text': 'OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our largescale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.'},\n",
       " {'doi': 'https://doi.org/10.48550/arXiv.2409.10633',\n",
       "  'text': \"Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\"},\n",
       " {'doi': 'https://doi.org/10.1007/s11192-022-04367-w',\n",
       "  'text': 'This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.'},\n",
       " {'doi': 'https://doi.org/10.1002/leap.1411',\n",
       "  'text': \"The abstract is known to be a promotional genre where researchers tend to exaggerate the benefit of their research and use a promotional discourse to catch the reader's attention. The COVID-19 pandemic has prompted intensive research and has changed traditional publishing with the massive adoption of preprints by researchers. Our aim is to investigate whether the crisis and the ensuing scientific and economic competition have changed the lexical content of abstracts. We propose a comparative study of abstracts associated with preprints issued in response to the pandemic relative to abstracts produced during the closest pre-pandemic period. We show that with the increase (on average and in percentage) of positive words (especially effective) and the slight decrease of negative words, there is a strong increase in hedge words (the most frequent of which are the modal verbs can and may). Hedge words counterbalance the excessive use of positive words and thus invite the readers, who go probably beyond the ‘usual’ audience, to be cautious with the obtained results. The abstracts of preprints urgently produced in response to the COVID-19 crisis stand between uncertainty and over-promotion, illustrating the balance that authors have to achieve between promoting their results and appealing for caution.\"}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved DOIs: ['https://doi.org/10.1162/qss_a_00286', 'https://doi.org/10.48550/arXiv.2401.16359', 'https://doi.org/10.48550/arXiv.2409.10633', 'https://doi.org/10.1007/s11192-022-04367-w', 'https://doi.org/10.1002/leap.1411']\n",
      "Precision: 0.4000\n",
      "Recall: 1.0000\n",
      "F1-Score: 0.5714\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Queries go here\n",
    "queries = [\n",
    "    \"Who collected a random Crossref sample?\",\n",
    "]\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "retrieved_dois = [doc.get('doi', \"\") for doc in retrieved_docs]\n",
    "print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# Ground truth relevant documents (DOIs) for each query\n",
    "ground_truth = [\n",
    "    \"https://doi.org/10.1162/qss_a_00286\",\n",
    "    \"https://doi.org/10.1007/s11192-022-04367-w\"\n",
    "]\n",
    "\n",
    "def evaluate_retrieval(\n",
    "    retrieved_dois: List[str],\n",
    "    ground_truth: List[str]\n",
    ") -> Dict[str, float]:\n",
    "    #convert to sets for unique values\n",
    "    retrieved_set = set(retrieved_dois)\n",
    "    ground_truth_set = set(ground_truth)\n",
    "\n",
    "    #calculate true positives, false positives, and false negatives\n",
    "    true_positives = len(retrieved_set & ground_truth_set)\n",
    "    false_positives = len(retrieved_set - ground_truth_set)\n",
    "    false_negatives = len(ground_truth_set - retrieved_set)\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "results = evaluate_retrieval(retrieved_dois, ground_truth)\n",
    "print(f\"Precision: {results['Precision']:.4f}\")\n",
    "print(f\"Recall: {results['Recall']:.4f}\")\n",
    "print(f\"F1-Score: {results['F1-Score']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "If using all scalar values, you must pass an index",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 3\u001b[39m\n",
      "\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#create dataframe\u001b[39;00m\n",
      "\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n",
      "\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m results_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m      5\u001b[39m results_df\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Research_proposal/Part_3/part_3_cohere/.venv/lib/python3.12/site-packages/pandas/core/frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n",
      "\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n",
      "\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n",
      "\u001b[32m    774\u001b[39m     )\n",
      "\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n",
      "\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Research_proposal/Part_3/part_3_cohere/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n",
      "\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n",
      "\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n",
      "\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Research_proposal/Part_3/part_3_cohere/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n",
      "\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n",
      "\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n",
      "\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Research_proposal/Part_3/part_3_cohere/.venv/lib/python3.12/site-packages/pandas/core/internals/construction.py:667\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n",
      "\u001b[32m    664\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPer-column arrays must each be 1-dimensional\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m    666\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indexes \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_lengths:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIf using all scalar values, you must pass an index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m    669\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_series:\n",
      "\u001b[32m    670\u001b[39m     index = union_indexes(indexes)\n",
      "\n",
      "\u001b[31mValueError\u001b[39m: If using all scalar values, you must pass an index"
     ]
    }
   ],
   "source": [
    "#create dataframe\n",
    "import pandas as pd\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count specific terms in post-tokenized text\n",
    "- [ ] identify which terms to look for. Is this from the query? Or characteristics to find?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare pre and post-tokenized text\n",
    "- [ ] cosine similarity\n",
    "<br>\n",
    "### References\n",
    "- https://stackoverflow.com/questions/60492839/how-to-compare-sentence-similarities-using-embeddings-from-bert<br>\n",
    "See the above for a discussion on NOT using BERT (and SciBERT) for comparing sentence embedding. I should be using SentenceBERT for sentence similarity.<br>\n",
    "- Sentence Transformers: https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "<br>\n",
    "- another approach: https://medium.com/@ahmedmellit/text-similarity-implementation-using-bert-embedding-in-python-1efdb5194e65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Cosine similarity: 0.8829\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#load SciBERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "#function to generate embeddings using SciBERT\n",
    "\"\"\"\n",
    "todo:\n",
    "- [ ] change this to a sentence embedding model\n",
    "\"\"\"\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        padding=\"longest\",#please select one of ['longest', 'max_length', 'do_not_pad']\n",
    "        #padding=False,#padding has an effect on similarity\n",
    "        truncation=True\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "#original text and text with errors\n",
    "original_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "typo_text = \"<jats:p>The quick brown fox jumps over the lazy dog.</jats:p>\"\n",
    "\n",
    "#run embeddings\n",
    "original_embedding = generate_embeddings(original_text)\n",
    "typo_embedding = generate_embeddings(typo_text)\n",
    "print(type(typo_embedding))\n",
    "\n",
    "#calculate cosine similarity\n",
    "similarity = cosine_similarity(original_embedding, typo_embedding)\n",
    "print(f\"Cosine similarity: {similarity[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity scores using sentence embedding model: 0.9543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "#sentence transformer verison\n",
    "#reference: https://medium.com/@ahmedmellit/text-similarity-implementation-using-bert-embedding-in-python-1efdb5194e65\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#puts text from above into a list\n",
    "sentences:list = [original_text,typo_text]\n",
    "\n",
    "#initializing the Sentence Transformer model using BERT with mean-tokens pooling - source see above\n",
    "sentence_model = SentenceTransformer('bert-base-nli-mean-tokens') # this resets the model variable!\n",
    "\n",
    "#encoding the sentences\n",
    "sentence_embeddings = sentence_model.encode(sentences)\n",
    "\n",
    "#result will be a list of similarity scores between two texts\n",
    "similarity_scores = cosine_similarity([sentence_embeddings[0]], sentence_embeddings[1:])\n",
    "\n",
    "print(f\"Cosine similarity scores using sentence embedding model: {similarity_scores[0][0]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_heatmap(embedding1, embedding2, title):\n",
    "    #calculate the difference between the two embeddings\n",
    "    diff = embedding1 - embedding2\n",
    "    #reshape the difference to a 2D array for the heatmap\n",
    "    diff_2d = diff.reshape(1, -1)\n",
    "    #create a heatmap\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    sns.heatmap(diff, cmap='coolwarm', annot=False, cbar=True,vmin=-1,vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "#plot heatmap\n",
    "#plot_heatmap(original_embedding, typo_embedding, \"diff between embeddings\")\n",
    "plot_heatmap(sentence_embeddings[0],sentence_embeddings[1:],\"Sentence Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## TF-IDF & cosine similarity\n",
    "Look for loca file /Users/poppyriddle/Documents/CODING_WORKING/Python/LIBRARIES/tfidf_cos_similarity/Tf_IDF_Cosign_similarity_heatmap_v11.ipynb\n",
    "This has a simple approach for comparing and provides a heatmap"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
