DOI: 10.18653/v1/D19-1371
Title: Scibert：科学テキストの前提条件モデル SciBERT: A Pretrained Language Model for Scientific Text
Abstract: 科学的領域でNLPタスクの大規模な注釈付きデータを取得することは挑戦的で高価です。 Bert（Devlinet。al。、2018）に基づいた前提条件の言語モデルであるScibertをリリースして、高品質で大規模なラベル付けされた科学データの欠如に対処します。 Scibertは、下流の科学的NLPタスクのパフォーマンスを改善するために、科学出版物の大規模なマルチドメインコーパスで監視されていない事前販売を活用しています。さまざまな科学ドメインのデータセットを使用して、シーケンスタグ付け、文の分類、依存関係解析など、一連のタスクで評価します。 Bertよりも統計的に有意な改善を示し、これらのいくつかのタスクで新しい最新の結果を達成します。コードモデルと前処理されたモデルは、https：//github.com/allenai/scibert/で入手できます。 Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.
