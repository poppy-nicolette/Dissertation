{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere RAG with dense retriever and ReRank model\n",
    "- references: https://docs.cohere.com/v2/docs/rag-complete-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import cohere\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import sys\n",
    "from colorama import Fore, Style, Back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mall is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "# load secret from local .env file\n",
    "def get_key():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    _key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "    #verify if it worked\n",
    "    if _key is not None:\n",
    "        print(Fore.GREEN + \"all is good, beautiful!\")\n",
    "        return _key\n",
    "    else:\n",
    "        print(Fore.LIGHTRED_EX + \"API Key is missing\")\n",
    "\n",
    "# initilize client\n",
    "co = cohere.ClientV2(get_key())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of documents: 45\n",
      "Rank: 1\n",
      "Score: 0.22735819\n",
      "Document: DOI: 10.1007/s11192-022-04367-w\n",
      " Title: Identifying and correcting invalid citations due to DOI errors in Crossref data\n",
      "Abstract: This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\n",
      "\n",
      "\n",
      "Rank: 2\n",
      "Score: 0.13381115\n",
      "Document: DOI: 10.1162/qss_a_00112\n",
      " Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "Abstract: Abstract We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\n",
      "\n",
      "\n",
      "Rank: 3\n",
      "Score: 0.082882896\n",
      "Document: DOI: 10.48550/arXiv.2406.15154\n",
      " Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "Abstract: This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\n",
      "\n",
      "\n",
      "Rank: 4\n",
      "Score: 0.07964606\n",
      "Document: DOI: 10.1371/journal.pbio.1002542\n",
      " Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "Abstract: Citation metrics are increasingly used to appraise published research. One challenge is whether and how to normalize these metrics to account for differences across scientific fields, age (year of publication), type of document, database coverage, and other factors. We discuss the pros and cons for normalizations using different approaches. Additional challenges emerge when citation metrics need to be combined across multiple papers to appraise the corpus of scientists, institutions, journals, or countries, as well as when trying to attribute credit in multiauthored papers. Different citation metrics may offer complementary insights, but one should carefully consider the assumptions that underlie their calculation.\n",
      "\n",
      "\n",
      "Rank: 5\n",
      "Score: 0.07619448\n",
      "Document: DOI: 10.1002/asi.24171\n",
      " Title: PaperPoles: Facilitating adaptive visual exploration of scientific publications by citation links\n",
      "Abstract: Finding relevant publications is a common task. Typically, a researcher browses through a list of publications and traces additional relevant publications. When relevant publications are identified, the list may be expanded by the citation links of the relevant publications. The information needs of researchers may change as they go through such iterative processes. The exploration process quickly becomes cumbersome as the list expands. Most existing academic search systems tend to be limited in terms of the extent to which searchers can adapt their search as they proceed. In this article, we introduce an adaptive visual exploration system named PaperPoles to support exploration of scientific publications in a context‐aware environment. Searchers can express their information needs by intuitively formulating positive and negative queries. The search results are grouped and displayed in a cluster view, which shows aspects and relevance patterns of the results to support navigation and exploration. We conducted an experiment to compare PaperPoles with a list‐based interface in performing two academic search tasks with different complexity. The results show that PaperPoles can improve the accuracy of searching for the simple and complex tasks. It can also reduce the completion time of searching and improve exploration effectiveness in the complex task. PaperPoles demonstrates a potentially effective workflow for adaptive visual search of complex information.\n",
      "\n",
      "\n",
      "reranked_documents: ['DOI: 10.1007/s11192-022-04367-w\\n Title: Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: Abstract We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n']\n",
      "length of reranked_documents: 2\n",
      "Summary: Two studies examined citations.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w\n",
      "Identifying and correcting invalid citations due to DOI errors in Crossref data\n",
      "\n",
      "DOI: 10.1162/qss_a_00112\n",
      "Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "\u001b[96m------\n",
      "Reranked documents:\n",
      "DOI: 10.1007/s11192-022-04367-w\n",
      " Title: Identifying and correcting invalid citations due to DOI errors in Crossref data\n",
      "Abstract: This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\n",
      "\n",
      "DOI: 10.1162/qss_a_00112\n",
      " Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "Abstract: Abstract We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\n",
      "\n",
      "\u001b[93m\n",
      "CITATIONS:\n",
      "source text: Two studies,\n",
      "source: DOI: 10.1007/s11192-022-04367-w\n",
      "------\n",
      "source text: 10.1007/s11192-022-04367-w,\n",
      "source: DOI: 10.1007/s11192-022-04367-w\n",
      "------\n",
      "source text: Identifying and correcting invalid citations due to DOI errors in Crossref data,\n",
      "source: DOI: 10.1007/s11192-022-04367-w\n",
      "------\n",
      "source text: 10.1162/qss_a_00112,\n",
      "source: DOI: 10.1162/qss_a_00112\n",
      "------\n",
      "source text: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic,\n",
      "source: DOI: 10.1162/qss_a_00112\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "    global documents_with_doi\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0]\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append(f\"{doi} {text}\\n\")\n",
    "    return documents_with_doi\n",
    "\n",
    "# initialize empty search query\n",
    "search_queries = []\n",
    "# Embed the documents\n",
    "def document_embed(documents:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the documents from a list provided from read_documents_with_doi()\n",
    "    \"\"\"\n",
    "    doc_emb = co.embed(\n",
    "        model=\"embed-v4.0\",\n",
    "        input_type=\"search_document\",\n",
    "        texts=[doc for doc in documents],\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return doc_emb\n",
    "\n",
    "# Embed the search query\n",
    "def query_embed(search_queries:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the query from a list provided in search_queries variable\n",
    "    \"\"\"\n",
    "    query_emb = co.embed(\n",
    "        model=\"embed-v4.0\",\n",
    "        input_type=\"search_query\",\n",
    "        texts=search_queries,\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return query_emb\n",
    "\n",
    "# retrieve top_k and compute similarity using dot product\n",
    "def retrieve_top_k(top_k, query_embedded, documents_embedded, documents)->List[str]:\n",
    "    \"\"\"\n",
    "    returns the top_k documents based on dot product similarity\n",
    "    \"\"\"\n",
    "\n",
    "    scores = np.dot(query_embedded, np.transpose(documents_embedded))[0]#ordered list!\n",
    "    # takes top scores, and returns sorted list and returns indices sliced by top_k\n",
    "    max_idx = np.argsort(-scores)[:top_k]\n",
    "    # returns documents by index\n",
    "    retrieved_docs = [documents[item] for item in max_idx]\n",
    "    # returns a list of documents\n",
    "    return retrieved_docs\n",
    "\n",
    "def rerank_documents(retrieved_documents,search_queries,threshold,top_k)->List[str]:\n",
    "    \"\"\"\n",
    "    takes retrieved_documents as input along with search_queries and runs them through the \n",
    "    rerank model from cohere for semantic similarity. \n",
    "\n",
    "    top_n = top_k\n",
    "    Limits those returned by a threshold score. this is to reduce those that are irrelevant.\n",
    "    \"\"\"\n",
    "    # Rerank the documents\n",
    "    results = co.rerank(\n",
    "        model=\"rerank-v3.5\",\n",
    "        query=search_queries[0],\n",
    "        documents=[doc for doc in retrieved_documents],\n",
    "        top_n=top_k,\n",
    "        max_tokens_per_doc=4096,# defaults to 4096\n",
    "    )\n",
    "\n",
    "    # Display the reranking results\n",
    "    for idx, result in enumerate(results.results):\n",
    "        print(f\"Rank: {idx+1}\")\n",
    "        print(f\"Score: {result.relevance_score}\")\n",
    "        print(f\"Document: {retrieved_documents[result.index]}\\n\")\n",
    "\n",
    "    #returns only those over threshold\n",
    "    reranked_docs = [\n",
    "        retrieved_documents[result.index] for result in results.results if result.relevance_score >=threshold\n",
    "    ]\n",
    "\n",
    "    print(f\"reranked_documents: {reranked_docs}\")\n",
    "    print(f\"length of reranked_documents: {len(reranked_docs)}\")\n",
    "\n",
    "    return reranked_docs\n",
    "\n",
    "def cohere_rag_pipeline(directory_path,search_queries,top_k,threshold):\n",
    "\n",
    "    # retrieve documents from directory\n",
    "    documents = read_documents_with_doi(directory_path)\n",
    "    print(f\"Length of documents: {len(documents)}\")\n",
    "    # embed the documents\n",
    "    documents_embedded = document_embed(documents)\n",
    "\n",
    "    #embed the query:\n",
    "    query_embedded = query_embed(search_queries)\n",
    "\n",
    "    # retrieve the top_k documents\n",
    "    retrieved_documents = retrieve_top_k(top_k, query_embedded, documents_embedded, documents)\n",
    "\n",
    "    # rerank the documents using the Rerank model from Cohere\n",
    "    reranked_documents = rerank_documents(retrieved_documents,search_queries,threshold,top_k)\n",
    "    # set system instructions\n",
    "    instructions = \"\"\"\n",
    "                    You are an academic research assistant.\n",
    "                    You must include the DOI in your response.\n",
    "                    If there is no content provided, ask for a different question.\n",
    "                    Please structure your response like this:\n",
    "                    Summary: summary statement here. \n",
    "                    DOI: summary of the text associated with this DOI.\n",
    "                    Address me as, 'my lady'.\n",
    "                    \"\"\"\n",
    "    # create messages to model\n",
    "    messages = [{\"role\":\"user\",\n",
    "                \"content\": search_queries[0]},\n",
    "                {\"role\":\"system\",\n",
    "                \"content\":instructions}]\n",
    "\n",
    "    # Generate the response\n",
    "    resp = co.chat(\n",
    "        model=\"command-a-03-2025\",\n",
    "        messages=messages,\n",
    "        documents=reranked_documents,\n",
    "    )\n",
    "\n",
    "    return resp, reranked_documents\n",
    "\n",
    "# ****** Pipeline ********\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.1\n",
    "\n",
    "response, reranked_documents_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "# Display the response\n",
    "print(response.message.content[0].text)\n",
    "print(Fore.LIGHTCYAN_EX + f\"------\\nReranked documents:\")\n",
    "for doc in reranked_documents_end:\n",
    "    print(doc)\n",
    "\n",
    "# Display the citations and source documents\n",
    "if response.message.citations:\n",
    "    print(Fore.LIGHTYELLOW_EX + \"\\nCITATIONS:\")\n",
    "    for citation in response.message.citations:\n",
    "        print(f\"source text: {citation.text},\\nsource: {citation.sources[0].document.get('content').split(\"\\n\")[0]}\\n------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# Analysis\n",
    "Precision, recall, accuracy, F1 scores and faithfulness\n",
    "## Precision, recall, F1 score\n",
    "### references\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "- https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "import openpyxl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query, Precision, Recall, F1-Score, Accuracy, Balanced accuracy, Faithfulness score, Response]\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial dataframe to capture results from each query and results\n",
    "#ONLY DO THIS AT THE BEGINNING OF THE ANALYSIS PROCEDURE, OTHERWISE, IT WILL ERASE THE PREVIOUS RESULTS!!\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Query','Precision','Recall','F1-Score','Accuracy', 'Balanced accuracy', 'Faithfulness score', 'Response'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.31222/osf.io/smxe5']\n",
      "45\n",
      "For query: which studies examined the abstract in metadata?:\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F1-Score: 0.933\n",
      "Accuracy: 0.933\n",
      "Balanced accuracy: 0.933\n",
      "Faithfulness score: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/.venv/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2801: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Query': 'which studies examined the abstract in metadata?',\n",
       " 'Precision': 0.0,\n",
       " 'Recall': 0.0,\n",
       " 'F1-Score': 0.9333333333333333,\n",
       " 'Accuracy': 0.9333333333333333,\n",
       " 'Balanced accuracy': 0.9333333333333333,\n",
       " 'Faithfulness score': 2,\n",
       " 'Response': 'Summary: Two studies examined the abstract in metadata.\\n\\nDOI: 10.1162/qss_a_00286 - This study compared the amount of metadata and the completeness degree of research publications in new academic databases.\\n\\nDOI: 10.31222/osf.io/smxe5 - This study presented an up-to-date overview of the availability of six metadata elements in Crossref, including abstracts.\\n\\nIs there anything else I can help you with, my lady?'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "\"\"\"\n",
    "change this to read in an excel sheet of queries and ground_truth dois.\n",
    "Then it should be isolated as a function.\n",
    "Run the function to iterature through the list.\n",
    "\"\"\"\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# initiates the variable\n",
    "ground_truth = []\n",
    "\n",
    "def evaluate_retrieval(retrieved_dois, ground_truth):\n",
    "    corpus_doi_list = []\n",
    "    #corpus_list is a global variable in rag_pipeline()\n",
    "    for each in range(len(documents_with_doi)):\n",
    "        a = documents_with_doi[each].split(\"\\n\")[0].strip(\"DOI: \")\n",
    "        corpus_doi_list.append(a)\n",
    "    print(len(corpus_doi_list))\n",
    "\n",
    "    def compare_lists(list1, list2, list3):\n",
    "        for val in list1:\n",
    "            if val in list2:\n",
    "                list3.append(1)\n",
    "            else:\n",
    "                list3.append(0)\n",
    "\n",
    "    #set y_true so that len(y_true)==len(corpus_doi_list)\n",
    "    y_true = []\n",
    "    compare_lists(corpus_doi_list,ground_truth,y_true)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = []\n",
    "    compare_lists(corpus_doi_list,retrieved_dois,y_pred)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # calculate metrics - could also use sklearn.metrics functions such as precision_score, but this is easier to read\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    faithfulness_score = 0\n",
    "    for each in retrieved_dois:\n",
    "        if each in response.message.content[0].text:\n",
    "            faithfulness_score+=1\n",
    "        else:\n",
    "            faithfulness_score+=0\n",
    "\n",
    "    return {\n",
    "        'Query':f\"{search_queries[0]}\",\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\":accuracy,\n",
    "        \"Balanced accuracy\":balanced_accuracy,\n",
    "        \"Faithfulness score\":faithfulness_score,\n",
    "        \"Response\":response.message.content[0].text\n",
    "    }\n",
    "\n",
    "def print_results()->Dict:\n",
    "    \"\"\"\n",
    "    Prints a nicely ordered set of results from evalaute_retrieval()\n",
    "    \"\"\"\n",
    "    global results\n",
    "    results = evaluate_retrieval(retrieved_dois, ground_truth)\n",
    "    print(f\"For query: {results['Query']}:\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"F1-Score: {results['F1-Score']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    print(f\"Balanced accuracy: {results['Balanced accuracy']:.3f}\")\n",
    "    print(f\"Faithfulness score: {results['Faithfulness score']}\")\n",
    "    return results\n",
    "\n",
    "#for debugging\n",
    "\n",
    "print_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the test from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved DOIs: ['10.1007/s11192-022-04367-w', '10.1162/qss_a_00112', '10.1371/journal.pbio.1002542']\n",
      "Length of documents: 45\n",
      "Rank: 1\n",
      "Score: 0.09165022\n",
      "Document: DOI: 10.1007/s11192-022-04367-w\n",
      " Title: CrossRefデータのDOIエラーによる無効な引用の識別と修正 Identifying and correcting invalid citations due to DOI errors in Crossref data\n",
      "Abstract: この作業は、CrossRefで利用可能なオープンな参考文献メタデータを分析することにより、DOIの間違いのクラスを特定し、どの出版社がそのような間違いを担当し、これらの誤ったDOIの数を自動プロセスで修正できるかを強調することを目的としています。 By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs.これらの引用は、CrossRefの関連する引用データをアップロードする責任を負う責任者の有効性と出版社を追跡することで処理しました。最後に、無効なDOIの事実上の誤りのパターンと、それらをキャッチして修正するために必要な正規表現を特定しました。この研究の結果は、無効な引用の大部分に責任を負い、および/または影響を受けた出版社は少数であることを示しています。過去の研究で提案されたDOI名エラーの分類法を拡張し、以前のアプローチよりも無効なDOIでより多くのミスをきれいにすることができる、より詳細に精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的観点からDOIの間違いの可能な理由を調査し、出版社が無効な引用データの生産の根底にある問題を特定するのに役立ちます。また、私たちが提示するDOIクリーニングメカニズムは、既存のプロセス（COCIなど）に統合して、間違ったDOIを自動的に修正することで引用を追加できます。この研究は、オープンサイエンスの原則に厳密に従っていたため、私たちの研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\n",
      "\n",
      "\n",
      "Rank: 2\n",
      "Score: 0.08943091\n",
      "Document: DOI: 10.1371/journal.pbio.1002542\n",
      " Title: 引用メトリック：正規化する方法（NOT）の入門書 Citation Metrics: A Primer on How (Not) to Normalize\n",
      "Abstract: 引用指標は、公開された研究を評価するためにますます使用されています。課題の1つは、科学分野の違い、年齢（出版年）、ドキュメントの種類、データベースカバレッジ、およびその他の要因を説明するために、これらのメトリックを正規化するかどうか、および方法です。さまざまなアプローチを使用して、正規化の長所と短所について説明します。科学者、機関、雑誌、または国のコーパスを評価するために、複数の論文で引用指標を組み合わせる必要がある場合、および多著者の論文でクレジットを属性にしようとする場合、追加の課題が現れます。異なる引用メトリックは補完的な洞察を提供する可能性がありますが、計算の根底にある仮定を慎重に検討する必要があります。 Citation metrics are increasingly used to appraise published research. One challenge is whether and how to normalize these metrics to account for differences across scientific fields, age (year of publication), type of document, database coverage, and other factors. We discuss the pros and cons for normalizations using different approaches. Additional challenges emerge when citation metrics need to be combined across multiple papers to appraise the corpus of scientists, institutions, journals, or countries, as well as when trying to attribute credit in multiauthored papers. Different citation metrics may offer complementary insights, but one should carefully consider the assumptions that underlie their calculation.\n",
      "\n",
      "\n",
      "Rank: 3\n",
      "Score: 0.07727375\n",
      "Document: DOI: 10.1162/qss_a_00112\n",
      " Title: 書誌データソースの大規模な比較：Scopus、Web of Science、Dimensions、CrossRef、およびMicrosoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "Abstract: Scopus、Web of Science、Dimensions、CrossRef、およびMicrosoft Academicの5つの学際的な参考文献データソースの大規模な比較を提示します。この比較では、これらのデータソースでカバーされている2008年から2017年の期間の科学文書を考慮しています。 Scopusは、他のそれぞれのデータソースとペアワイズで比較されます。まず、ドキュメントのカバレッジのデータソース間の違いを分析します。たとえば、時間の経過とともに違い、ドキュメントタイプあたりの違い、および分野あたりの違いに焦点を当てます。次に、引用リンクの完全性と精度の違いを調べます。分析に基づいて、さまざまなデータソースの長所と短所について説明します。科学文献の包括的な報道と、文献を選択するための柔軟なフィルターのセットを組み合わせることの重要性を強調しています。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\n",
      "\n",
      "\n",
      "Rank: 4\n",
      "Score: 0.053764965\n",
      "Document: DOI: 10.1002/asi.24171\n",
      " Title: Paperpoles：引用リンクによる科学出版物の適応的な視覚的探求を促進する PaperPoles: Facilitating adaptive visual exploration of scientific publications by citation links\n",
      "Abstract: 関連する出版物を見つけることは一般的なタスクです。通常、研究者は出版物のリストを参照し、追加の関連する出版物を追跡します。関連する出版物が特定されると、リストは関連する出版物の引用リンクによって拡張される場合があります。研究者の情報のニーズは、そのような反復プロセスを経るにつれて変化する可能性があります。リストが拡大するにつれて、探索プロセスはすぐに面倒になります。ほとんどの既存のアカデミック検索システムは、検索者が進むにつれて検索を適応させることができる範囲で制限される傾向があります。この記事では、Paperpolesという名前の適応視覚探査システムを紹介して、コンテキスト認識環境での科学出版物の探索をサポートします。検索者は、肯定的および否定的なクエリを直感的に策定することにより、情報のニーズを表現できます。検索結果はグループ化され、クラスタービューに表示されます。これは、ナビゲーションと探索をサポートする結果の側面と関連性パターンを示しています。さまざまな複雑さを持つ2つのアカデミック検索タスクを実行する際に、Paperpolesをリストベースのインターフェイスと比較する実験を実施しました。結果は、Paperpolesがシンプルで複雑なタスクを検索する精度を向上させることができることを示しています。また、検索の完了時間を短縮し、複雑なタスクでの探査の有効性を改善することもできます。 Paperpolesは、複雑な情報の適応視覚検索のための潜在的に効果的なワークフローを示しています。 Finding relevant publications is a common task. Typically, a researcher browses through a list of publications and traces additional relevant publications. When relevant publications are identified, the list may be expanded by the citation links of the relevant publications. The information needs of researchers may change as they go through such iterative processes. The exploration process quickly becomes cumbersome as the list expands. Most existing academic search systems tend to be limited in terms of the extent to which searchers can adapt their search as they proceed. In this article, we introduce an adaptive visual exploration system named PaperPoles to support exploration of scientific publications in a context‐aware environment. Searchers can express their information needs by intuitively formulating positive and negative queries. The search results are grouped and displayed in a cluster view, which shows aspects and relevance patterns of the results to support navigation and exploration. We conducted an experiment to compare PaperPoles with a list‐based interface in performing two academic search tasks with different complexity. The results show that PaperPoles can improve the accuracy of searching for the simple and complex tasks. It can also reduce the completion time of searching and improve exploration effectiveness in the complex task. PaperPoles demonstrates a potentially effective workflow for adaptive visual search of complex information.\n",
      "\n",
      "\n",
      "Rank: 5\n",
      "Score: 0.040211637\n",
      "Document: DOI: 10.48550/arXiv.2406.15154\n",
      " Title: Openalex、Web of Science、Scopus、PubMed、およびSemantic Sc​​holarの出版物と文書タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "Abstract: この研究では、次の書誌データベースの出版物と文書の種類を比較および分析します：Openalex、Scopus、Web of Science、Semantic Sc​​holar、Pubmed。結果は、類型が個々のデータベースプロバイダー間でかなり異なる可能性があることを示しています。さらに、出版物はそれぞれのデータベースで異なる方法で分類されているため、参考文献分析に関連するドキュメントを特定するために必要な研究と非研究テキストの区別は、データソースによって異なる場合があります。この研究の焦点は、横断段階の比較に加えて、主にオープンアレックスに含まれる出版物とドキュメントの種類のカバレッジと分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\n",
      "\n",
      "\n",
      "reranked_documents: []\n",
      "length of reranked_documents: 0\n",
      "My lady, it seems there is no specific content provided for me to work with. However, I can guide you to some studies that have examined citations in academic research. Here are a few notable ones:\n",
      "\n",
      "**Summary:** Several studies have examined citations to understand patterns, influences, and impacts in academic research. Below are key examples:\n",
      "\n",
      "1. **Garfield, E. (1955). Citation Indexes for Science: A New Dimension in Documentation through Association of Ideas.**  \n",
      "   This foundational study introduced the concept of citation indexing, which has become a cornerstone in bibliometrics.  \n",
      "   DOI: [10.1108/eb013761](https://doi.org/10.1108/eb013761)\n",
      "\n",
      "2. **Hirsch, J. E. (2005). An index to quantify an individual's scientific research output. Proceedings of the National Academy of Sciences, 102(46), 16569-16572.**  \n",
      "   This study introduced the h-index, a widely used metric for evaluating individual researchers based on their citation counts.  \n",
      "   DOI: [10.1073/pnas.0507655102](https://doi.org/10.1073/pnas.0507655102)\n",
      "\n",
      "3. **Bornmann, L., & Daniel, H. D. (2008). What do citation counts measure? A review of studies on citing behavior. Journal of Documentation, 64(1), 45-80.**  \n",
      "   This review examines the factors influencing citation behavior and what citation counts actually measure.  \n",
      "   DOI: [10.1108/00220410810844150](https://doi.org/10.1108/00220410810844150)\n",
      "\n",
      "4. **Waltman, L., & van Eck, N. J. (2012). The inconsistency of the h-index. Journal of the American Society for Information Science and Technology, 63(2), 406-415.**  \n",
      "   This study critiques the h-index and highlights its limitations as a measure of research impact.  \n",
      "   DOI: [10.1002/asi.21678](https://doi.org/10.1002/asi.21678)\n",
      "\n",
      "If you have a specific aspect of citations or a particular field in mind, my lady, please let me know, and I can provide more tailored information!\n",
      "45\n",
      "For query: which studies examied citations?:\n",
      "Precision: 1.000\n",
      "Recall: 0.600\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.800\n",
      "Faithfulness score: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>which studies examined citations?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.911111</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1</td>\n",
       "      <td>Summary: two studies examined citations.\\n\\nDO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>which studies examined citations?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.953488</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>My lady, it seems there is no specific content...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>which studies examined citations?</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>Summary: Three studies examined citations.\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>which studies examined citations?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.80</td>\n",
       "      <td>3</td>\n",
       "      <td>Summary: Three studies examined citations.\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>which studies examied citations?</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0</td>\n",
       "      <td>My lady, it seems there is no specific content...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Query  ...                                           Response\n",
       "11  which studies examined citations?  ...  Summary: two studies examined citations.\\n\\nDO...\n",
       "12  which studies examined citations?  ...  My lady, it seems there is no specific content...\n",
       "13  which studies examined citations?  ...  Summary: Three studies examined citations.\\n\\n...\n",
       "14  which studies examined citations?  ...  Summary: Three studies examined citations.\\n\\n...\n",
       "15   which studies examied citations?  ...  My lady, it seems there is no specific content...\n",
       "\n",
       "[5 rows x 8 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the test from here\n",
    "\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# Ground truth relevant documents (DOIs) for each query\n",
    "ground_truth = [\"10.1007/s11192-022-04367-w\",\"10.1371/journal.pbio.1002542\",\"10.1007/s11192-015-1765-5\",\"10.1162/qss_a_00112\",\"10.1162/qss_a_00210\"]\n",
    "\n",
    "#***** Begin chat session *****\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_multi_lang\"\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.10\n",
    "\n",
    "response, reranked_documents_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "# Display the response\n",
    "print(response.message.content[0].text)\n",
    "\n",
    "new_result = print_results()\n",
    "# add the new result to the df\n",
    "results_df.loc[len(results_df)] = new_result\n",
    "\n",
    "#save the queries and responses to separate dataframe to be manually annontated\n",
    "answer_relevance_df = results_df[['Query','Response']].copy(deep=True)\n",
    "\n",
    "# save out answer_relevance_df\n",
    "filename=\"analysis/dense_answer_relevance_results.xlsx\"\n",
    "answer_relevance_df.to_excel(filename)\n",
    "\n",
    "filename = \"analysis/dense_analysis_results.xlsx\"\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "results_df.to_excel(filename)\n",
    "results_df.tail(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
