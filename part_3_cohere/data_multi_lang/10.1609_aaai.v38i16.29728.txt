DOI: 10.1609/aaai.v38i16.29728
Title: 検索された世代の大規模な言語モデルのベンチマーク Benchmarking Large Language Models in Retrieval-Augmented Generation
Abstract: 検索された生成（RAG）は、大規模な言語モデル（LLM）の幻覚を緩和するための有望なアプローチです。ただし、既存の研究には、さまざまな大規模な言語モデルに対する検索された生成の影響に関する厳密な評価がありません。これにより、異なるLLMのRAGの機能における潜在的なボトルネックを特定することが困難になります。この論文では、検索された生成が大規模な言語モデルに与える影響を体系的に調査します。ノイズの堅牢性、否定的な拒否、情報統合、反事実的堅牢性など、RAG​​に必要な4つの基本能力におけるさまざまな大手言語モデルのパフォーマンスを分析します。この目的のために、英語と中国語の両方でRAG評価のための新しいコーパスである検索された生成ベンチマーク（RGB）を確立します。 RGBは、ベンチマーク内のインスタンスを、ケースを解決するために必要な前述の基本能力に基づいて、4つの個別のテストベッドに分割します。次に、RGBの6つの代表LLMを評価して、RAGを適用する際に現在のLLMの課題を診断します。評価により、LLMはある程度のノイズの堅牢性を示していますが、否定的な拒絶、情報統合、誤った情報への対処に関して依然として著しく苦労していることが明らかになりました。前述の評価の結果は、LLMにRAGを効果的に適用するために、まだかなりの旅がまだあることを示しています。 Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.
