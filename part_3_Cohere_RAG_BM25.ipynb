{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere API and SciBERT with BM25 as first stage retriever for RAG\n",
    "This notebook uses a Cohere API for generating responses to text. A query input is required from the user. \n",
    "SciBERT is used for embeddings in a dense vector array for the query. \n",
    "This version is different in that it uses BM25 as a sparse vectorizer for the input text. Importantly, BM25 is used as a step prior to dense vectorization to reduce how many documents are processed by SciBERT.\n",
    "A DOI is supplied with the text as both an identifier and locator. \n",
    "\n",
    "## pipeline\n",
    "1. BM25 Retrieval\n",
    "    - BM25 is used to retrieve top-k candidate documents based on keyword matching\n",
    "2. Dense embedding retrieval\n",
    "    - query is embedded using SciBERT and the retrieved documents.\n",
    "3. Re-ranking\n",
    "    - cosine similarity between query embedding and document embedding to rerank candidate docs\n",
    "4. Generation\n",
    "    - docs and query are fed to generator for answer creation. \n",
    "\n",
    "- [ ] set up venv\n",
    "- [ ] install transformers torch cohere in command line\n",
    "\n",
    "### todo\n",
    "- [ ] create script that compiles data/documents.txt with DOI || text for all documents\n",
    "- [ ] rank_bm25: https://github.com/dorianbrown/rank_bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cohere\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(\"i4WfLKa1zNNKsPU3n4ZEVuzpaTCBwztx6p6hebpO\")\n",
    "\n",
    "# Load SciBERT model and tokenizer\n",
    "\"\"\"\n",
    "documentation can be found here: https://huggingface.co/docs/transformers/v4.50.0/en/model_doc/auto#transformers.AutoTokenizer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Initialize tokenizer with custom parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    max_len=512,\n",
    "    use_fast=True,  # Use the fast tokenizer\n",
    "    do_lower_case=False,  # Preserve case\n",
    "    add_prefix_space=False,  # No prefix space\n",
    "    never_split=[\"[DOC]\", \"[REF]\"],  # Tokens to never split\n",
    "    additional_special_tokens=[\"<doi>\", \"</doi>\"]  # Add custom special tokens\n",
    ")\n",
    "\n",
    "# This is the SciBERT model that is used to embed the text and query.\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents:\n",
      "Score: 0.7395, DOI: https://doi.org/10.1162/qss_a_00286, Document: ABSTRACT  The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\n",
      "Score: 0.7095, DOI: https://doi.org/10.1007/s11192-022-04367-w, Document: Abstract  This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\n",
      "\n",
      "Generated Response:\n",
      " https://doi.org/10.1162/qss_a_00286\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic RAG with Cohere model\n",
    "Document source: data/documents.txt where the DOI with resolver is separated from the abstract by ||. One record per line. \n",
    "Saved as UTF-8\n",
    "\n",
    "Returns:  answers based on query from input()\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate embeddings using SciBERT\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    converts raw text to numerical representations using a pretrained model, in this case, SciBERT\n",
    "    Input: text from tokenizer step above as a list of strings\n",
    "    Output: np.array\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512, # returns PyTorch tensors which are compatible with model\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True # return the attention mask - need to learn more\n",
    "        )\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Function to read documents and their DOIs from a file\n",
    "def read_documents_with_doi(file_path: str) -> List[Dict[str, str]]:\n",
    "    documents_with_doi = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\"||\")  # Assuming DOI and document are separated by \"||\"\n",
    "            if len(parts) == 2:\n",
    "                doi, document = parts\n",
    "                documents_with_doi.append({\"doi\": doi.strip(), \"text\": document.strip()})\n",
    "    return documents_with_doi\n",
    "\n",
    "# Path to the file containing documents and DOIs\n",
    "file_path = \"data/documents.txt\"  # Replace with your file path\n",
    "\n",
    "# Read documents and DOIs from the file\n",
    "documents_with_doi = read_documents_with_doi(file_path)\n",
    "\n",
    "# Extract document texts and DOIs\n",
    "documents = [doc[\"text\"] for doc in documents_with_doi]\n",
    "dois = [doc[\"doi\"] for doc in documents_with_doi]\n",
    "\n",
    "# Example query\n",
    "query = input(\" What is your query: \")\n",
    "\n",
    "# Generate document embeddings\n",
    "document_embeddings = generate_embeddings(documents)\n",
    "\n",
    "# Generate query embedding\n",
    "query_embedding = generate_embeddings([query])[0] # generates np.array for the query text\n",
    "\n",
    "# Function to retrieve top-k documents using cosine similarity\n",
    "def retrieve_documents(query_embedding: np.ndarray, document_embeddings: List[np.ndarray], top_k: int = 2) -> List[Tuple[float, Dict[str, str]]]:\n",
    "    similarities = []\n",
    "    for doc_emb in document_embeddings:\n",
    "        # cosine similarity\n",
    "        similarity = np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb)) \n",
    "        similarities.append(similarity)\n",
    "    # ranking\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    return [(similarities[i], documents_with_doi[i]) for i in top_indices]\n",
    "\n",
    "# Retrieve top documents\n",
    "top_documents = retrieve_documents(query_embedding, document_embeddings)\n",
    "print(\"Retrieved Documents:\")\n",
    "for score, doc in top_documents:\n",
    "    print(f\"Score: {score:.4f}, DOI: {doc['doi']}, Document: {doc['text']}\")\n",
    "\n",
    "# Prepare context for Cohere's Command model (include DOI)\n",
    "context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for _, doc in top_documents])\n",
    "prompt = f\"Query: {query}\\nContext: {context}\\nAnswer: Include the DOI of the referenced document in your response.\"\n",
    "\n",
    "# Generate response using Cohere's Command model\n",
    "response = co.generate(\n",
    "  model=\"command\",\n",
    "  prompt=prompt,\n",
    "  max_tokens=150,\n",
    "  temperature=0.7\n",
    ")\n",
    "\n",
    "# Print the generated response\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response.generations[0].text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
