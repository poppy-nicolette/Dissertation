DOI: 10.3115/v1/d14-1162
Title: グローブ: 単語表現のためのグローバル ベクトル Glove: Global Vectors for Word Representation
Abstract: 単語のベクトル空間表現を学習するための最近の方法は、ベクトル演算を使用して詳細な意味論的規則性および構文規則性を捕捉することに成功しましたが、これらの規則性の起源は依然として不透明なままです。このような規則性が単語ベクトルに現れるために必要なモデルの特性を分析し、明示します。その結果、文献にある 2 つの主要なモデル ファミリ、つまりグローバル行列因数分解とローカル コンテキスト ウィンドウ法の利点を組み合わせた、新しいグローバル対数線形回帰モデルが誕生しました。私たちのモデルは、スパース行列全体や大規模なコーパス内の個々のコンテキスト ウィンドウではなく、単語-単語共起行列の非ゼロ要素のみをトレーニングすることにより、統計情報を効率的に活用します。このモデルは、最近の単語類推タスクで 75% のパフォーマンスを示したように、意味のある部分構造を持つベクトル空間を生成します。また、類似性タスクや固有表現認識においても関連モデルよりも優れたパフォーマンスを発揮します。 Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition.