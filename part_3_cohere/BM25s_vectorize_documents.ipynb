{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25 tokenization and vectorization\n",
    "This notebook tokenizes, vectorizes, and stores the vectors in a local JSON file. \n",
    "<br>\n",
    "adopted from the bm25s.ipynb notebook based off:<br>\n",
    "BM25 Sparse<br>\n",
    "https://bm25s.github.io/\n",
    "\n",
    "## documentation\n",
    "https://github.com/xhluca/bm25s\n",
    "\n",
    "## citation\n",
    "```\n",
    "@misc{bm25s,\n",
    "      title={BM25S: Orders of magnitude faster lexical search via eager sparse scoring}, \n",
    "      author={Xing Han Lù},\n",
    "      year={2024},\n",
    "      eprint={2407.03618},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.IR},\n",
    "      url={https://arxiv.org/abs/2407.03618}, \n",
    "}\n",
    "```\n",
    "\n",
    "## requires:\n",
    "pip3 install bm25s[full]<br>\n",
    "pip3 install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bm25s\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the corpus and identifier for each document\n",
    "This should be a separate function/module. <br>\n",
    "It takes the txt files in data folder, and makes a separate corpus and identifier file. <br>\n",
    "these get used as inputs for the BM25s and as outputs. The identifier only has the DOI with resolver and a title. This may be easier for the UI. <br>\n",
    "The corpus should be sent to the generator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "# Create your corpus here\n",
    "\n",
    "input_dir = '/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data'\n",
    "\n",
    "#initialize lists\n",
    "corpus = []\n",
    "identifier = []\n",
    "\n",
    "#read each file in input_dir\n",
    "for file_name in os.listdir(input_dir):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(input_dir,file_name)\n",
    "\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.readlines()\n",
    "\n",
    "            doi = content[0].lstrip(\"DOI: \")\n",
    "            title = content[1].strip(\"\\n\")\n",
    "            abstract = content[2].lstrip()\n",
    "            \"\"\"\n",
    "            This provides just the title and abstract to be sent to the generator.\n",
    "            This creates a list of strings where each document is a string.\n",
    "            \"\"\"\n",
    "            if doi and title and abstract:\n",
    "                document = f\"{doi} {title} {abstract}\"\n",
    "                corpus.append(document)\n",
    "\n",
    "            \"\"\" this will create a separate 'corpus' to be returned as the identifier\n",
    "            and returned values from the retriever. This version might be a little less\n",
    "            verbose and user friendly. \n",
    "            However, you will want to send the corpus above to the generator.\n",
    "            \"\"\"\n",
    "            if title and doi:\n",
    "                resolver_doi = f\"https://doi.org/{doi}\"\n",
    "                link = f\"URL: {resolver_doi} for title: {title}\"\n",
    "                identifier.append(link)\n",
    "                    \n",
    "\n",
    "\n",
    "#export corpus and url lists for import later\n",
    "with open('corpus.pkl', 'wb') as file:\n",
    "    pickle.dump(corpus, file)\n",
    "\n",
    "with open('identifier.pkl','wb') as file:\n",
    "    pickle.dump(identifier, file)\n",
    "\n",
    "print('complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenization and saving the vectors\n",
    "## options:\n",
    "- stemming: uncomment to use english Porter stemmer. See documentation here: https://www.nltk.org/howto/stem.html\n",
    "- stopwords removal: can pass a list or chose language specific one, such as \"en\". Documentation here: \n",
    "    - optional stopwords list can be passed:\n",
    "        ```python\n",
    "        #provide your own stopwords list if you don't like the default one\n",
    "        stopwords = [\"a\", \"the\"]\n",
    "        ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding newlines for mmindex: 100%|██████████| 61.0k/61.0k [00:00<00:00, 119MB/s]\n"
     ]
    }
   ],
   "source": [
    "#optional stemmer\n",
    "#stemmer = Stemmer.Stemmer(\"english\")\n",
    "\n",
    "#Tokenize the corpus and index it - removes stopwords \n",
    "#you can also add a stemmer here as an arg: stemmer=stemmer\n",
    "corpus_tokens = bm25s.tokenize(corpus, stopwords=\"en\", show_progress=True)\n",
    "\n",
    "# Create the BM25 model and index the corpus\n",
    "retriever = bm25s.BM25(corpus=corpus, method='lucene') # 'robertson', 'lucene', 'atire'\n",
    "retriever.index(corpus_tokens)\n",
    "\n",
    "# Save the index\n",
    "# You can save the corpus along with the model - technically, this should go up in the previous cell after the retriever was created.\n",
    "retriever.save(\"bm25/bm25\", corpus=corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bm25s.BM25"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stop here. \n",
    "The above code was just to set up the vectorized tokens. Below is just testing to make sure it works with the document set. <br>\n",
    "The code below will be integrated into the RAG\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading the index\n",
    "These are the vectorized tokens. Only the corpus needs to be saved. <br>\n",
    "The identifier corpus is used instead of an id# assigned to each document to be seen by the user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of corpus list: 43\n",
      "--------\n",
      "length of identifier list: 43\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# reimport corpus and url lists\n",
    "with open('corpus.pkl', 'rb') as file:\n",
    "    corpus_list = pickle.load(file)\n",
    "print(f\"length of corpus list: {len(corpus_list)}\")\n",
    "with open('identifier.pkl', 'rb') as file:\n",
    "    identifier_list = pickle.load(file)\n",
    "print(f\"--------\\nlength of identifier list: {len(identifier_list)}\")\n",
    "\n",
    "#retriever = bm25s.BM25(corpus=corpus_list) \n",
    "# ...and load the retriever model and corpus when you need them\n",
    "retriever = bm25s.BM25.load(\"bm25/bm25\", load_corpus=True, mmap=True)\n",
    "# set load_corpus=False if you don't need the corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "Rank 1 (score: 2.30): URL: https://doi.org/10.3145/epi.2023.mar.09\n",
      " for title: Title: Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\n",
      "------\n",
      "Rank 2 (score: 2.03): URL: https://doi.org/10.31274/b8136f97.ccc3dae4\n",
      " for title: Title: Comparing Funder Metadata in OpenAlex and Dimensions\n",
      "------\n",
      "Rank 3 (score: 1.89): URL: https://doi.org/10.1590/SciELOPreprints.11205\n",
      " for title: Title: On the Open Road to Universal Indexing: OpenAlex and OpenJournal Systems\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "#You can now search the corpus with a query\n",
    "query = input(\"what is your query\")\n",
    "#you can also add a stemmer here as an arg: stemmer=stemmer\n",
    "query_tokens = bm25s.tokenize(query,\n",
    "                            stopwords=True,\n",
    "                            lower=True)\n",
    "\n",
    "#note: if you pass a new corpus here, it must have the same length as your indexed corpus\n",
    "#in this case, I am passing the new list 'identifier_list' - it contains just the DOI and title\n",
    "# you can also pass 'corpus', or 'corpus_list'\n",
    "if len(corpus_list)==len(identifier_list):\n",
    "    results, scores = retriever.retrieve(query_tokens, corpus=identifier_list, k=3, return_as=\"tuple\")\n",
    "else:\n",
    "    print(\"The len of the corpus_list does not equal the identifier_list\")\n",
    "    print(f\"length of corpus list: {len(corpus_list)}\")\n",
    "    print(f\"length of identifier_list: {len(identifier_list)}\")\n",
    "#loop through results\n",
    "if all(score ==0.00 for score in scores[0]):\n",
    "    print(\"Nothing found, please try another query.\")\n",
    "else:\n",
    "    for i in range(results.shape[1]):\n",
    "        doc, score = results[0, i], scores[0, i]\n",
    "        print(f\"------\\nRank {i+1} (score: {score:.2f}): {doc}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
