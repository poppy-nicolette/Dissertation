{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere API and SciBERT with BM25 as first stage retriever for RAG\n",
    "This notebook uses a Cohere API for generating responses to text. A query input is required from the user. \n",
    "SciBERT is used for embeddings in a dense vector array for the query. \n",
    "This version is different in that it uses BM25 as a sparse vectorizer for the input text. Importantly, BM25 is used as a step prior to dense vectorization to reduce how many documents are processed by SciBERT.\n",
    "A DOI is supplied with the text as both an identifier and locator. \n",
    "\n",
    "## pipeline\n",
    "1. BM25 Retrieval\n",
    "    - BM25 is used to retrieve top-k candidate documents based on keyword matching\n",
    "2. Dense embedding retrieval\n",
    "    - query is embedded using SciBERT and the retrieved documents.\n",
    "3. Re-ranking\n",
    "    - cosine similarity between query embedding and document embedding to rerank candidate docs\n",
    "4. Generation\n",
    "    - docs and query are fed to generator for answer creation. \n",
    "\n",
    "- [ ] set up venv\n",
    "- [ ] install transformers torch cohere in command line\n",
    "\n",
    "### todo\n",
    "- [ ] create script that compiles data/documents.txt with DOI || text for all documents\n",
    "- [ ] rank_bm25: https://github.com/dorianbrown/rank_bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cohere\n",
    "from cohere import Client\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import logging # finding where functions are taking too long\n",
    "#for BM25s\n",
    "import bm25s\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pickle\n",
    "\n",
    "# load secret from local .env file\n",
    "def get_key():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    _key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "    #verify if it worked\n",
    "    if _key is not None:\n",
    "        print(\"all is good, beautiful!\")\n",
    "        return _key\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all is good, beautiful!\n",
      "Model is callable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(get_key()) \n",
    "\n",
    "# Load SciBERT model and tokenizer\n",
    "\"\"\"\n",
    "documentation can be found here: https://huggingface.co/docs/transformers/v4.50.0/en/model_doc/auto#transformers.AutoTokenizer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Initialize tokenizer with custom parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    max_len=512,\n",
    "    use_fast=True,  # Use the fast tokenizer\n",
    "    do_lower_case=False,  # Preserve case\n",
    "    add_prefix_space=False,  # No prefix space\n",
    "    never_split=[\"[DOC]\", \"[REF]\"],  # Tokens to never split\n",
    "    #additional_special_tokens=[\"<doi>\", \"</doi>\"],  # Add custom special tokens ***RE-EVALUATE*** (tuple or list of str or tokenizers.AddedToken, optional) — A tuple or a list of additional special tokens. Add them here to ensure they are skipped when decoding with skip_special_tokens is set to True. If they are not part of the vocabulary, they will be added at the end of the vocabulary.\n",
    "    skip_special_tokens=False,\n",
    ")\n",
    "\n",
    "# this is the SciBERT model that is used to embed the text and query.\n",
    "# other models: 'allenai-specter', \n",
    "#documentation here: https://huggingface.co/docs/transformers/model_doc/auto\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "#verify that the model is callable\n",
    "if callable(model):\n",
    "    print(\"Model is callable\")\n",
    "else:\n",
    "    print(\"Model is not callable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V3 BM25 Pre-retriever\n",
    "Includes the following:\n",
    "- BM25 pre-retriever\n",
    "- SciBERT embedding of query and pre-retrieved documents\n",
    "- cosine similarity between embeddings of query and documents\n",
    "- response instruction\n",
    "- context includes DOI, Title, and Abstract as augmentation to query and instruction.\n",
    "- response\n",
    "- follow up with retrieved documents for verfication. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of corpus list: 43\n",
      "--------\n",
      "length of identifier list: 43\n",
      "Retrieving documents and generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "My lady, here is a brief summary of the research you have requested regarding cross-database comparison and OpenAlex as an alternative for bibliometric analysis: \n",
      "\n",
      "DOI: 10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar, emphasizes the importance of OpenAlex as an alternative to Scopus and the Web of Science, and compares the publication and document types among these databases. \n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses, suggests that OpenAlex is an unreliable alternative to Scopus and the Web of Science due to its limitations in metadata accuracy and completeness. \n",
      "\n",
      "DOI: 10.48550/arXiv.2402.01788, Title: LitLLM: A Toolkit for Scientific Literature Review, promotes the use of OpenAlex as an alternative to established proprietary providers for bibliometric analyses at libraries and universities. \n",
      "\n",
      "DOI: 10.48550/arXiv.2410.04231, Title: Metadata-based Data Exploration with Ret\n",
      "------\n",
      "Source documents: \n",
      "DOI: 10.48550/arXiv.2406.15154, Title:  Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar Abstract: This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\n",
      "DOI: 10.48550/arXiv.2404.17663, Title:  An analysis of the suitability of OpenAlex for bibliometric analyses Abstract: Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\n",
      "DOI: 10.48550/arXiv.2402.01788, Title:  LitLLM: A Toolkit for Scientific Literature Review Abstract: Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on the user-provided abstract. Finally, the related work section is generated based on the re-ranked results and the abstract. There is a substantial reduction in time and effort for literature review compared to traditional methods, establishing our toolkit as an efficient alternative. Our open-source toolkit is accessible at https://github.com/shubhamagarwal92/LitLLM and Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM) with the video demo at https://youtu.be/E2ggOZBAFw0.\n",
      "DOI: 10.48550/arXiv.2410.04231, Title:  Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models Abstract: Developing the capacity to effectively search for requisite datasets is an urgent requirement to assist data users in identifying relevant datasets considering the very limited available metadata. For this challenge, the utilization of third-party data is emerging as a valuable source for improvement. Our research introduces a new architecture for data exploration which employs a form of Retrieval-Augmented Generation (RAG) to enhance metadata-based data discovery. The system integrates large language models (LLMs) with external vector databases to identify semantic relationships among diverse types of datasets. The proposed framework offers a new method for evaluating semantic similarity among heterogeneous data sources and for improving data exploration. Our study includes experimental results on four critical tasks: 1) recommending similar datasets, 2) suggesting combinable datasets, 3) estimating tags, and 4) predicting variables. Our results demonstrate that RAG can enhance the selection of relevant datasets, particularly from different categories, when compared to conventional metadata approaches. However, performance varied across tasks and models, which confirms the significance of selecting appropriate techniques based on specific use cases. The findings suggest that this approach holds promise for addressing challenges in data exploration and discovery, although further refinement is necessary for estimation tasks.\n",
      "DOI: 10.1590/SciELOPreprints.11205, Title:  On the Open Road to Universal Indexing: OpenAlex and OpenJournal Systems Abstract: This study examines OpenAlex’s indexing of journals using Open Journal Systems (JUOJS), reflecting two open source software initiatives supporting inclusive scholarly participation. By analyzing a dataset of 47,625 active JUOJS, we reveal that 71% of these journals have at least one article indexed in OpenAlex. Our findings underscore the central role of Crossref DOIs in achieving indexing, with 97% of the journals using Crossref DOIs included in OpenAlex. However, this technical dependency reflects broader structural inequities, as resource-limited journals, particularly those from low-income countries (47% of JUOJS) and non-English language journals (55%-64% of JUOJS), remain underrepresented. Our work highlights the theoretical implications of scholarly infrastructure dependencies and their role in perpetuating systemic disparities in global knowledge visibility. We argue that even inclusive bibliographic databases like OpenAlex must actively address financial, infrastructural, and linguistic barriers to foster equitable indexing on a global scale. By conceptualizing the relationship between indexing mechanisms, persistent identifiers, and structural inequities, this study provides a critical lens for rethinking the dynamics of universal indexing and its realization in a global, multilingual scholarly ecosystem.\n",
      "time to query loop: 0.01 seconds\n",
      "to to retrieve: 0.76 seconds\n",
      "time to generate: 9.56 seconds\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic RAG with BM25s pre-retriever using Cohere Command model\n",
    "BM25s pre-retriever based on query.\n",
    "Dense retrieval of embedded query and pre-retrieved documents\n",
    "Document source: data\n",
    "\n",
    "Returns: responses based on query from input()\n",
    "\"\"\"\n",
    "# set top_k global value - keep this as constant for all evaluations\n",
    "global top_k\n",
    "top_k = 5\n",
    "\n",
    "# BM25s pre-retriever function\n",
    "def bm25_retriever(query:str)->Tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        query: str\n",
    "    Outputs:\n",
    "        Tuple of two np.arrays, one for results and one for scores\n",
    "    \"\"\"\n",
    "    global results, scores,query_tokens\n",
    "    #you can also add a stemmer here as an arg: stemmer=stemmer\n",
    "    query_tokens = bm25s.tokenize(query,stopwords=True,lower=True)\n",
    "\n",
    "    #note: if you pass a new corpus here, it must have the same length as your indexed corpus\n",
    "    #in this case, I am passing the new list 'identifier_list' - it contains just the DOI and title\n",
    "    # you can also pass 'corpus', or 'corpus_list'\n",
    "    if len(corpus_list)!=len(identifier_list):\n",
    "        raise ValueError(\"The len of the corpus_list does not equal the len of the identifier_list\")\n",
    "\n",
    "    # retrieve indices\n",
    "    results, scores = retriever.retrieve(query_tokens, corpus=identifier_list, k=top_k, return_as=\"tuple\")\n",
    "\n",
    "    # check if no results found\n",
    "    if all(score == 0.00 for score in scores[0]):\n",
    "        print(\"Nothing found, please try another query.\")\n",
    "        return [],[] # returning empty lists if 0 for scores, one for results, one for scores\n",
    "\n",
    "    return results[0],scores[0]\n",
    "\n",
    "#function to generate embeddings using SciBERT\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    converts raw text to numerical representations using a pretrained model, in this case, SciBERT.\n",
    "    Currently this is applied to both the document text and the query. \n",
    "    May want a different version or decorator for the query as they are generally much shorter and more sparse.\n",
    "\n",
    "    Input: text from tokenizer step above as a list of strings\n",
    "    Output: np.array\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True)# return the attention mask - need to learn more)\n",
    "    \n",
    "    # this passes the tokenized inputs through the model\n",
    "    outputs = model(**inputs)\n",
    "    #this uses mean pooling - may want to investigate other methods\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Function to update chat history\n",
    "def update_chat_history(query, retrieved_docs, response):\n",
    "    global chat_history # declare this as global variable available outside this function\n",
    "    chat_history.append({\n",
    "        \"query\": query,\n",
    "        \"retrieved_docs\": [doc for doc in retrieved_docs],  # Store only the text of retrieved documents\n",
    "        \"response\": response\n",
    "    })\n",
    "\n",
    "#function to incorporate history into the next query\n",
    "def get_context_with_history(query) -> str:\n",
    "    global chat_history, full_context # also declare here since chat_history is being modified\n",
    "    if not chat_history:\n",
    "        return query\n",
    "    \n",
    "    history_str = \"\\n\".join([\n",
    "        f\"User: {entry['query']}\\n\"\n",
    "        f\"Context: {'; '.join(entry['retrieved_docs'])}\\n\"\n",
    "        f\"Response: {entry['response']}\"\n",
    "        for entry in chat_history\n",
    "    ])\n",
    "    full_context = f\"Chat History:\\n{history_str}\\n\\nCurrent Query: {query}\"\n",
    "    return full_context\n",
    "\n",
    "#function to truncate chat history\n",
    "def truncate_chat_history(max_length=3):\n",
    "    global chat_history # modifies it so it also must be global\n",
    "    if len(chat_history) > max_length:\n",
    "        chat_history = chat_history[-max_length:]\n",
    "\n",
    "\n",
    "def retrieve_documents(query: str) -> List[Dict[str, str]]:\n",
    "\n",
    "    # set global for debugging - can be limited to parsed_docs for production\n",
    "    global document_embeddings,similarities,documents_list,bm25_results,top_indices, parsed_docs,sorted_documents_list\n",
    "    # Use BM25 retriever to get initial documents\n",
    "    \n",
    "    bm25_results, bm25_scores = bm25_retriever(query)\n",
    "    \n",
    "    # check if empty\n",
    "    if len(bm25_results) == 0:\n",
    "        return []  # Return empty list if BM25 found no results\n",
    "    \n",
    "    # map indices to documents in corpus_list\n",
    "    documents_list = [corpus_list[i] for i in range(len(bm25_results))]\n",
    "    \n",
    "    # Generate embeddings for BM25 results\n",
    "    document_embeddings = generate_embeddings([doc for doc in documents_list])  #documents_list is a list of top_k results\n",
    "\n",
    "    # generate embeddings for query\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    \n",
    "    #cosine similarity\n",
    "    similarities = [\n",
    "        np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
    "        for doc_emb in document_embeddings\n",
    "    ]\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    sorted_documents_list = [documents_list[i] for i in top_indices]\n",
    "\n",
    "    parsed_docs = []\n",
    "    for doc in sorted_documents_list:\n",
    "        lines = doc.split('\\n')\n",
    "        parsed_doc = {\n",
    "            \"doi\": lines[0],\n",
    "            \"title\":lines[1].replace(\"Title: \",\"\"),\n",
    "            \"abstract\":lines[2].replace(\"Abstract: \",\"\")\n",
    "        }\n",
    "        parsed_docs.append(parsed_doc)\n",
    "    \n",
    "    return parsed_docs\n",
    "\n",
    "#RAG pipeline function\n",
    "def rag_pipeline(query):\n",
    "    global documents_list,corpus_list,identifier_list,retriever\n",
    "    #start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # uses BM25 as a pre-retriever based on the query\n",
    "    # load indexed corpus for BM25 pre-retriever\n",
    "    # reimport corpus and url lists\n",
    "    with open('corpus.pkl', 'rb') as file:\n",
    "        corpus_list = pickle.load(file)\n",
    "    print(f\"length of corpus list: {len(corpus_list)}\")\n",
    "    with open('identifier.pkl', 'rb') as file:\n",
    "        identifier_list = pickle.load(file)\n",
    "    print(f\"--------\\nlength of identifier list: {len(identifier_list)}\")\n",
    "\n",
    "    # this is where its getting the documents - this comes from BM25s_vectorize_documents.ipynb\n",
    "    retriever = bm25s.BM25.load(\"bm25/bm25\", load_corpus=True, mmap=True)\n",
    "\n",
    "    #incorporate chat history\n",
    "    full_context = get_context_with_history(query)\n",
    "    # let user know you are generating...\n",
    "    print(\"Retrieving documents and generating response...\")\n",
    "    end_time = time.time()\n",
    "    global time_query\n",
    "    time_query = end_time-start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    #retrieve documents\n",
    "    global retrieved_docs\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "    end_time = time.time()\n",
    "    global retrieve_time\n",
    "    retrieve_time = end_time-start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    #prepare context for Cohere's Command model\n",
    "    instruction = \"You are a helpful academic research assistant. Please keep the answers concise and structured simply. Use single sentences where possible. Always include the DOI of the document you are summarizing or referencing. If you do not provide the DOI, this reduces the need for you as a research assistant. Always include the DOI. Please address me as 'my lady'. \"\n",
    "    #context = \"\\n\".join([f\"DOI: {doc[0]}, Text: {doc[1]}\" for doc in retrieved_docs])\n",
    "    context = \"\\n\".join([f\"DOI: {doc['doi']}, Title: {doc['title']}, Abstract: {doc['abstract']}\" for doc in retrieved_docs])\n",
    "    prompt = f\"Query: {query}\\nContext: {context}\\nAnswer: {instruction}\"\n",
    "    \n",
    "    # Generate response - see documentation: https://docs.cohere.com/reference/generate-v1\n",
    "    response = co.generate(\n",
    "        model=\"command\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=250,\n",
    "        temperature=0.2\n",
    "    ).generations[0].text\n",
    "    \n",
    "    # Update chat history\n",
    "    update_chat_history(query, retrieved_docs, response)\n",
    "    \n",
    "    # Truncate history if necessary\n",
    "    truncate_chat_history()\n",
    "    end_time = time.time()\n",
    "    global generate_time\n",
    "    generate_time = end_time-start_time\n",
    "\n",
    "    # Print the response\n",
    "    print(\"Generated Response:\")\n",
    "    print(response)\n",
    "    print(f\"------\\nSource documents: \")\n",
    "    for doc in retrieved_docs:\n",
    "        print(f\"DOI: {doc['doi']}, Title: {doc['title']}\")\n",
    "    return response,time_query,retrieve_time,generate_time\n",
    "\n",
    "\n",
    "\n",
    "# Main loop for user interaction \n",
    "# changed to run with no history for the purposes of the test\n",
    "chat_history = []#initialize chat history\n",
    "\n",
    "query = input(\"What is your query? \")\n",
    "\n",
    "rag_pipeline(query)\n",
    "\n",
    "print(f\"time to query loop: {time_query:.2f} seconds\")\n",
    "print(f\"to to retrieve: {retrieve_time:.2f} seconds\")\n",
    "print(f\"time to generate: {generate_time:.2f} seconds\")\n",
    "\n",
    "# Main loop for user interaction\n",
    "##chat_history = []#initialize chat history\n",
    "#while True:\n",
    "\n",
    "    #uery = input(\"What is your query (or type 'exit' to quit): \")\n",
    "    #if query.lower() == \"exit\":\n",
    "    #    break\n",
    "    #rag_pipeline(query)\n",
    "\n",
    "    #print(f\"time to query loop: {time_query:.2f} seconds\")\n",
    "    #print(f\"to to retrieve: {retrieve_time:.2f} seconds\")\n",
    "    #print(f\"time to generate: {generate_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Analysis\n",
    "Analysis is only looking at retrieval effectiveness with impacts on relevance. It is not system quality or user utility.\n",
    "## Test One: \n",
    "- [ ] compute precision, recall, and F1-Scores. ✅\n",
    "- [ ] added accuracy score ** need to confirm my formula is correct **\n",
    "- [ ] compare text from each source, embedded, and them similarity scores based on embeddings.\n",
    "    - [ ] token based SciBERT embedding\n",
    "    - [ ] sentence-based SentenceBERT embedding\n",
    "### optional analysis\n",
    "Need to learn more about attention weights and their analysis\n",
    "- [ ] heatmap of attention weights for two given inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, recall, F1 score\n",
    "### references\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "- https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score #didn't actually use these, can probably delete\n",
    "#may want to confirm that scores are the same?\n",
    "import openpyxl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm retrieved_docs has content\n",
    "print(f\"Query: {query}\")\n",
    "retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial dataframe to capture results from each query and results\n",
    "#ONLY DO THIS AT THE BEGINNING OF THE ANALYSIS PROCEDURE, OTHERWISE, IT WILL ERASE THE PREVIOUS RESULTS!!\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Query','Precision','Recall','F1-Score','Accuracy'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\"\"\"\n",
    "change this to read in an excel sheet of queries and ground_truth dois.\n",
    "Then it should be isolated as a function.\n",
    "Run the function to iterature through the list.\n",
    "\"\"\"\n",
    "# Queries go here\n",
    "queries = [query]\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "retrieved_dois = [doc.get('doi', \"\") for doc in retrieved_docs]\n",
    "print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# Ground truth relevant documents (DOIs) for each query\n",
    "ground_truth = [\"10.1162/qss_a_00286\",\"10.1002/leap.1411\",\"10.1007/s11192-020-03632-0\",\"10.48550/arXiv.2402.01788\",\"10.48550/arXiv.2401.16359\",\"10.31222/osf.io/smxe5\"]\n",
    "\n",
    "def evaluate_retrieval(\n",
    "    retrieved_dois: List[str],\n",
    "    ground_truth: List[str]\n",
    ") -> Dict[str, float]:\n",
    "    #convert to sets for unique values\n",
    "    retrieved_set = set(retrieved_dois)\n",
    "    ground_truth_set = set(ground_truth)\n",
    "\n",
    "    #calculate true positives, false positives, and false negatives\n",
    "    true_positives = len(retrieved_set & ground_truth_set) # must use & with set operations\n",
    "    false_positives = len(retrieved_set - ground_truth_set)\n",
    "    false_negatives = len(ground_truth_set - retrieved_set)\n",
    "    true_negatives = len([name for name in os.listdir('data') if os.path.isfile('data'+'/'+name)])-true_positives-false_negatives-false_positives\n",
    "\n",
    "    # calculate metrics - could also use sklearn.metrics functions such as precision_score, but this is easier to read\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "    accuracy = (true_positives+true_negatives)/(true_negatives+true_positives+false_negatives+false_positives)\n",
    "\n",
    "    return {\n",
    "        'Query':f\"{queries[0]}\",\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\":accuracy\n",
    "    }\n",
    "\n",
    "def print_results()->Dict:\n",
    "    # Example usage\n",
    "    global results\n",
    "    results = evaluate_retrieval(retrieved_dois, ground_truth)\n",
    "    print(f\"For query: {results['Query']}:\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"F1-Score: {results['F1-Score']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    return results\n",
    "\n",
    "print_results()\n",
    "results_df = results_df._append(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save out to analysis\n",
    "filename = \"analysis/BM25_analysis_results.xlsx\"\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "results_df.to_excel(filename)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare text embeddings for each source\n",
    "This section is used to investigate the impact of face markup or other formatting requirements in the abstract text on the embedding model.\n",
    "Currently this only uses cosine similarity. \n",
    "<br>\n",
    "#### similarity:\n",
    "- [✅] cosine similarity\n",
    "<br>\n",
    "#### Embeddings:\n",
    "- [✅] SciBERT\n",
    "- [✅] SentenceBERT\n",
    "<br>\n",
    "#### Analysis\n",
    "- [ ] compare scores using dataframe\n",
    "- [ ] visualize results\n",
    "### References\n",
    "- https://stackoverflow.com/questions/60492839/how-to-compare-sentence-similarities-using-embeddings-from-bert<br>\n",
    "See the above for a discussion on NOT using BERT (and SciBERT) for comparing sentence embedding. I should be using SentenceBERT for sentence similarity.<br>\n",
    "- Sentence Transformers: https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "<br>\n",
    "- another approach: https://medium.com/@ahmedmellit/text-similarity-implementation-using-bert-embedding-in-python-1efdb5194e65\n",
    "- sklearn metrics for other scoring methods than cosine similarity: https://scikit-learn.org/stable/api/sklearn.metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#load SciBERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "#function to generate embeddings using SciBERT\n",
    "\"\"\"\n",
    "todo:\n",
    "- [ ] change this to a sentence embedding model\n",
    "\"\"\"\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        padding=\"longest\",#please select one of ['longest', 'max_length', 'do_not_pad']\n",
    "        #padding=False,#padding has an effect on similarity\n",
    "        truncation=True\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "#original text and text with errors\n",
    "original_text = \" Abstract Objectives Precise literature recommendation and summarization are crucial for biomedical professionals. While the latest iteration of generative pretrained transformer (GPT) incorporates 2 distinct modes—real-time search and pretrained model utilization—it encounters challenges in dealing with these tasks. Specifically, the real-time search can pinpoint some relevant articles but occasionally provides fabricated papers, whereas the pretrained model excels in generating well-structured summaries but struggles to cite specific sources. In response, this study introduces RefAI, an innovative retrieval-augmented generative tool designed to synergize the strengths of large language models (LLMs) while overcoming their limitations. Materials and Methods RefAI utilized PubMed for systematic literature retrieval, employed a novel multivariable algorithm for article recommendation, and leveraged GPT-4 turbo for summarization. Ten queries under 2 prevalent topics (“cancer immunotherapy and target therapy” and “LLMs in medicine”) were chosen as use cases and 3 established counterparts (ChatGPT-4, ScholarAI, and Gemini) as our baselines. The evaluation was conducted by 10 domain experts through standard statistical analyses for performance comparison. The overall performance of RefAI surpassed that of the baselines across 5 evaluated dimensions—relevance and quality for literature recommendation, accuracy, comprehensiveness, and reference integration for summarization, with the majority exhibiting statistically significant improvements (P-values<.05). Discussion RefAI demonstrated substantial improvements in literature recommendation and summarization over existing tools, addressing issues like fabricated papers, metadata inaccuracies, restricted recommendations, and poor reference integration. Conclusion By augmenting LLM with external resources and a novel ranking algorithm, RefAI is uniquely capable of recommending high-quality literature and generating well-structured summaries, holding the potential to meet the critical needs of biomedical professionals in navigating and synthesizing vast amounts of scientific literature.\"\n",
    "typo_text = \"<jats:title>Abstract</jats:title>\\n               <jats:sec>\\n                  <jats:title>Objectives</jats:title>\\n                  <jats:p>Precise literature recommendation and summarization are crucial for biomedical professionals. While the latest iteration of generative pretrained transformer (GPT) incorporates 2 distinct modes—real-time search and pretrained model utilization—it encounters challenges in dealing with these tasks. Specifically, the real-time search can pinpoint some relevant articles but occasionally provides fabricated papers, whereas the pretrained model excels in generating well-structured summaries but struggles to cite specific sources. In response, this study introduces RefAI, an innovative retrieval-augmented generative tool designed to synergize the strengths of large language models (LLMs) while overcoming their limitations.</jats:p>\\n               </jats:sec>\\n               <jats:sec>\\n                  <jats:title>Materials and Methods</jats:title>\\n                  <jats:p>RefAI utilized PubMed for systematic literature retrieval, employed a novel multivariable algorithm for article recommendation, and leveraged GPT-4 turbo for summarization. Ten queries under 2 prevalent topics (“cancer immunotherapy and target therapy” and “LLMs in medicine”) were chosen as use cases and 3 established counterparts (ChatGPT-4, ScholarAI, and Gemini) as our baselines. The evaluation was conducted by 10 domain experts through standard statistical analyses for performance comparison.</jats:p>\\n               </jats:sec>\\n               <jats:sec>\\n                  <jats:title>Results</jats:title>\\n                  <jats:p>The overall performance of RefAI surpassed that of the baselines across 5 evaluated dimensions—relevance and quality for literature recommendation, accuracy, comprehensiveness, and reference integration for summarization, with the majority exhibiting statistically significant improvements (P-values &amp;lt;.05).</jats:p>\\n               </jats:sec>\\n               <jats:sec>\\n                  <jats:title>Discussion</jats:title>\\n                  <jats:p>RefAI demonstrated substantial improvements in literature recommendation and summarization over existing tools, addressing issues like fabricated papers, metadata inaccuracies, restricted recommendations, and poor reference integration.</jats:p>\\n               </jats:sec>\\n               <jats:sec>\\n                  <jats:title>Conclusion</jats:title>\\n                  <jats:p>By augmenting LLM with external resources and a novel ranking algorithm, RefAI is uniquely capable of recommending high-quality literature and generating well-structured summaries, holding the potential to meet the critical needs of biomedical professionals in navigating and synthesizing vast amounts of scientific literature.</jats:p>\\n               </jats:sec>\"\n",
    "\n",
    "#run embeddings\n",
    "original_embedding = generate_embeddings(original_text)\n",
    "typo_embedding = generate_embeddings(typo_text)\n",
    "print(type(typo_embedding))\n",
    "\n",
    "#calculate cosine similarity\n",
    "similarity = cosine_similarity(original_embedding, typo_embedding)\n",
    "print(f\"Cosine similarity using SciBERT: {similarity[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence transformer verison\n",
    "#reference: https://medium.com/@ahmedmellit/text-similarity-implementation-using-bert-embedding-in-python-1efdb5194e65\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#puts text from above into a list\n",
    "sentences:list = [original_text,typo_text]\n",
    "\n",
    "#initializing the Sentence Transformer model using BERT with mean-tokens pooling - source see above\n",
    "sentence_model = SentenceTransformer('bert-base-nli-mean-tokens') # this resets the model variable! changed to sentence_model variable name\n",
    "\n",
    "#encoding the sentences\n",
    "sentence_embeddings = sentence_model.encode(sentences)\n",
    "\n",
    "#result will be a list of similarity scores between two texts\n",
    "similarity_scores = cosine_similarity([sentence_embeddings[0]], sentence_embeddings[1:])\n",
    "\n",
    "print(f\"Cosine similarity scores using sentence embedding model: {similarity_scores[0][0]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_heatmap(embedding1, embedding2, title):\n",
    "    #calculate the difference between the two embeddings\n",
    "    diff = embedding1 - embedding2\n",
    "    #reshape the difference to a 2D array for the heatmap\n",
    "    diff_2d = diff.reshape(1, -1)\n",
    "    #create a heatmap\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    sns.heatmap(diff, cmap='coolwarm', annot=False, cbar=True,vmin=-1,vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "#plot heatmap\n",
    "#plot_heatmap(original_embedding, typo_embedding, \"diff between embeddings\")\n",
    "plot_heatmap(sentence_embeddings[0],sentence_embeddings[1:],\"Sentence Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with attention weights\n",
    "from https://github.com/clarkkev/attention-analysis\n",
    "- https://stackoverflow.com/questions/75772288/how-to-read-a-bert-attention-weight-matrix for explanation on queries and keys\n",
    "- https://theaisummer.com/self-attention/#:%7E:text=Self%2Dattention%20is%20not%20symmetric!&text=The%20arrows%20that%20correspond%20to,Q%E2%80%8B%3DWK%E2%80%8B. explanation on self-attention\n",
    "- heatmaps to analyze attention weights: https://apxml.com/courses/foundations-transformers-architecture/chapter-7-implementation-details-optimization/practice-analyzing-attention-weights\n",
    "- excellent source: https://apxml.com/courses/how-to-build-a-large-language-model/chapter-23-analyzing-model-behavior/attention-map-visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above \n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load SciBERT tokenizer and model - same as above - technically don't need to relaod these unless changing\n",
    "# try sentence based model?\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "\n",
    "# Tokenize the sentences\n",
    "inputs1 = tokenizer(original_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=20)#limit tokens so that we can actually see something\n",
    "inputs2 = tokenizer(typo_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=20)#\n",
    "\n",
    "# Get the attention weights: the `output_attentions=True` parameter is used to get the attention weights from the model\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1, output_attentions=True)\n",
    "    outputs2 = model(**inputs2, output_attentions=True)\n",
    "\n",
    "# Extract the attention weights for the last layer\n",
    "#.squeeze() https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html\n",
    "attention_weights1 = outputs1.attentions[-1].squeeze(0)  # Shape: (num_heads, seq_len, seq_len)\n",
    "attention_weights2 = outputs2.attentions[-1].squeeze(0)  # Shape: (num_heads, seq_len, seq_len)\n",
    "\n",
    "# Average the attention weights across all heads, \n",
    "#see last reference to visualize attention for each head\n",
    "attention_weights1 = attention_weights1.mean(dim=0)  # Shape: (seq_len, seq_len)\n",
    "attention_weights2 = attention_weights2.mean(dim=0)  # Shape: (seq_len, seq_len)\n",
    "\n",
    "# Get the tokens for the sentences\n",
    "tokens1 = tokenizer.convert_ids_to_tokens(inputs1[\"input_ids\"].squeeze(0))\n",
    "tokens2 = tokenizer.convert_ids_to_tokens(inputs2[\"input_ids\"].squeeze(0))\n",
    "\n",
    "# Plot the attention heatmap for the first sentence\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_weights1, xticklabels=tokens1, yticklabels=tokens1, cmap='viridis', annot=False, cbar=True)\n",
    "plt.title(\"Attention Weights for original_text\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the attention heatmap for the second sentence\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_weights2, xticklabels=tokens2, yticklabels=tokens2, cmap='viridis', annot=False, cbar=True)\n",
    "plt.title(\"Attention Weights for typo_text\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the difference in attention weights\n",
    "diff_attention_weights = (attention_weights1 - attention_weights2)\n",
    "\n",
    "# Plot the **difference** in attention weights\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(diff_attention_weights, xticklabels=tokens1, yticklabels=tokens1, cmap='coolwarm', annot=False, cbar=True, vmin=-1, vmax=1)\n",
    "plt.title(\"Difference in Attention Weights\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
