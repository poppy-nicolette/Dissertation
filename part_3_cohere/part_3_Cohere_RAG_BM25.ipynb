{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere API and SciBERT with BM25 as first stage retriever for RAG\n",
    "This notebook uses a Cohere API for generating responses to text. A query input is required from the user. \n",
    "SciBERT is used for embeddings in a dense vector array for the query. \n",
    "This version is different in that it uses BM25 as a sparse vectorizer for the input text. Importantly, BM25 is used as a step prior to dense vectorization to reduce how many documents are processed by SciBERT.\n",
    "A DOI is supplied with the text as both an identifier and locator. \n",
    "\n",
    "## pipeline\n",
    "1. BM25 Retrieval\n",
    "    - BM25 is used to retrieve top-k candidate documents based on keyword matching\n",
    "2. Dense embedding retrieval\n",
    "    - query is embedded using SciBERT and the retrieved documents.\n",
    "3. Re-ranking\n",
    "    - cosine similarity between query embedding and document embedding to rerank candidate docs\n",
    "4. Generation\n",
    "    - docs and query are fed to generator for answer creation. \n",
    "\n",
    "- [ ] set up venv\n",
    "- [ ] install transformers torch cohere in command line\n",
    "\n",
    "### todo\n",
    "- [ ] create script that compiles data/documents.txt with DOI || text for all documents\n",
    "- [ ] rank_bm25: https://github.com/dorianbrown/rank_bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import cohere\n",
    "from cohere import Client\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import logging # finding where functions are taking too long\n",
    "#for BM25s\n",
    "import bm25s\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "def main():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    global key,email\n",
    "    key = os.getenv('COHERE_API_KEY')\n",
    "    email = os.getenv('EMAIL')\n",
    "\n",
    "    #verify if it worked\n",
    "    if email is not None and key is not None:\n",
    "        print(\"all is good, beautiful!\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(key) \n",
    "\n",
    "# Load SciBERT model and tokenizer\n",
    "\"\"\"\n",
    "documentation can be found here: https://huggingface.co/docs/transformers/v4.50.0/en/model_doc/auto#transformers.AutoTokenizer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Initialize tokenizer with custom parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    max_len=512,\n",
    "    use_fast=True,  # Use the fast tokenizer\n",
    "    do_lower_case=False,  # Preserve case\n",
    "    add_prefix_space=False,  # No prefix space\n",
    "    never_split=[\"[DOC]\", \"[REF]\"],  # Tokens to never split\n",
    "    additional_special_tokens=[\"<doi>\", \"</doi>\"]  # Add custom special tokens ***RE-EVALUATE***\n",
    ")\n",
    "\n",
    "# This is the SciBERT model that is used to embed the text and query.\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "#verify that the model is callable\n",
    "if callable(model):\n",
    "    print(\"Model is callable\")\n",
    "else:\n",
    "    print(\"Model is not callable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Addition of BM25 for retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reimport corpus and url lists\n",
    "with open('corpus.pkl', 'rb') as file:\n",
    "    corpus_list = pickle.load(file)\n",
    "print(f\"length of corpus list: {len(corpus_list)}\")\n",
    "with open('identifier.pkl', 'rb') as file:\n",
    "    identifier_list = pickle.load(file)\n",
    "print(f\"--------\\nlength of identifier list: {len(identifier_list)}\")\n",
    "\n",
    "#retriever = bm25s.BM25(corpus=corpus_list) \n",
    "# ...and load the retriever model and corpus when you need them\n",
    "retriever = bm25s.BM25.load(\"bm25/bm25\", load_corpus=True, mmap=True)\n",
    "# set load_corpus=False if you don't need the corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2: implemented chat history\n",
    "\n",
    "calls a JSON file of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of corpus list: 43\n",
      "--------\n",
      "length of identifier list: 43\n",
      "Retrieving documents and generating response...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "My lady, here is an explanation of typos in metadata and their significance:\n",
      "\n",
      "Misspellings and other typing errors in metadata fields, such as author names, titles, and affiliations, can have implications for the discoverability and accuracy of academic literature. These typos may occur during data entry or transcription processes. \n",
      "\n",
      "Such errors can lead to challenges in correctly identifying and matching records, searching for accurate information, and ensuring proper attribution of authors and institutions. \n",
      "\n",
      "These typos contribute to the challenge of metadata accuracy and completeness, which is a significant limitation of OpenAlex and other bibliographic databases. This is especially the case when considering author names. \n",
      "\n",
      "Critical evaluation and addressing these typos in metadata are essential steps to enhance the reliability and usability of bibliographic databases in research and analysis, ensuring accurate representation and recognition of scholarly contributions. \n",
      "\n",
      "Is there anything further I may reveal regarding the analysis of typos in metadata and their impact? \n",
      "------\n",
      "Source documents: \n",
      "DOI: 10.48550/arXiv.2404.17663, Title:  An analysis of the suitability of OpenAlex for bibliometric analyses Abstract: Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\n",
      "DOI: 10.48550/arXiv.2406.15154, Title:  Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar Abstract: This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\n",
      "DOI: 10.48550/arXiv.2402.01788, Title:  LitLLM: A Toolkit for Scientific Literature Review Abstract: Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on the user-provided abstract. Finally, the related work section is generated based on the re-ranked results and the abstract. There is a substantial reduction in time and effort for literature review compared to traditional methods, establishing our toolkit as an efficient alternative. Our open-source toolkit is accessible at https://github.com/shubhamagarwal92/LitLLM and Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM) with the video demo at https://youtu.be/E2ggOZBAFw0.\n",
      "DOI: 10.48550/arXiv.2410.04231, Title:  Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models Abstract: Developing the capacity to effectively search for requisite datasets is an urgent requirement to assist data users in identifying relevant datasets considering the very limited available metadata. For this challenge, the utilization of third-party data is emerging as a valuable source for improvement. Our research introduces a new architecture for data exploration which employs a form of Retrieval-Augmented Generation (RAG) to enhance metadata-based data discovery. The system integrates large language models (LLMs) with external vector databases to identify semantic relationships among diverse types of datasets. The proposed framework offers a new method for evaluating semantic similarity among heterogeneous data sources and for improving data exploration. Our study includes experimental results on four critical tasks: 1) recommending similar datasets, 2) suggesting combinable datasets, 3) estimating tags, and 4) predicting variables. Our results demonstrate that RAG can enhance the selection of relevant datasets, particularly from different categories, when compared to conventional metadata approaches. However, performance varied across tasks and models, which confirms the significance of selecting appropriate techniques based on specific use cases. The findings suggest that this approach holds promise for addressing challenges in data exploration and discovery, although further refinement is necessary for estimation tasks.\n",
      "DOI: 10.1590/SciELOPreprints.11205, Title:  On the Open Road to Universal Indexing: OpenAlex and OpenJournal Systems Abstract: This study examines OpenAlexâ€™s indexing of journals using Open Journal Systems (JUOJS), reflecting two open source software initiatives supporting inclusive scholarly participation. By analyzing a dataset of 47,625 active JUOJS, we reveal that 71% of these journals have at least one article indexed in OpenAlex. Our findings underscore the central role of Crossref DOIs in achieving indexing, with 97% of the journals using Crossref DOIs included in OpenAlex. However, this technical dependency reflects broader structural inequities, as resource-limited journals, particularly those from low-income countries (47% of JUOJS) and non-English language journals (55%-64% of JUOJS), remain underrepresented. Our work highlights the theoretical implications of scholarly infrastructure dependencies and their role in perpetuating systemic disparities in global knowledge visibility. We argue that even inclusive bibliographic databases like OpenAlex must actively address financial, infrastructural, and linguistic barriers to foster equitable indexing on a global scale. By conceptualizing the relationship between indexing mechanisms, persistent identifiers, and structural inequities, this study provides a critical lens for rethinking the dynamics of universal indexing and its realization in a global, multilingual scholarly ecosystem.\n",
      "time to query loop: 0.01 seconds\n",
      "to to retrieve: 0.80 seconds\n",
      "time to generate: 7.22 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# set top_k global value\n",
    "global top_k\n",
    "top_k = 5\n",
    "\n",
    "# BM25s pre-retriever function\n",
    "def bm25_retriever(query:str)->Tuple[np.array, np.array]:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        query: str\n",
    "    Outputs:\n",
    "        Tuple of two np.arrays, one for results and one for scores\n",
    "    \"\"\"\n",
    "    global results, scores,query_tokens\n",
    "    #you can also add a stemmer here as an arg: stemmer=stemmer\n",
    "    query_tokens = bm25s.tokenize(query,stopwords=True,lower=True)\n",
    "\n",
    "    #note: if you pass a new corpus here, it must have the same length as your indexed corpus\n",
    "    #in this case, I am passing the new list 'identifier_list' - it contains just the DOI and title\n",
    "    # you can also pass 'corpus', or 'corpus_list'\n",
    "    if len(corpus_list)!=len(identifier_list):\n",
    "        raise ValueError(\"The len of the corpus_list does not equal the len of the identifier_list\")\n",
    "\n",
    "    # retrieve indices\n",
    "    results, scores = retriever.retrieve(query_tokens, corpus=identifier_list, k=top_k, return_as=\"tuple\")\n",
    "\n",
    "    # check if no results found\n",
    "    if all(score == 0.00 for score in scores[0]):\n",
    "        print(\"Nothing found, please try another query.\")\n",
    "        return [],[] # returning empty lists if 0 for scores, one for results, one for scores\n",
    "\n",
    "    return results[0],scores[0]\n",
    "\n",
    "#function to generate embeddings using SciBERT\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# Function to update chat history\n",
    "def update_chat_history(query, retrieved_docs, response):\n",
    "    global chat_history # declare this as global variable available outside this function\n",
    "    chat_history.append({\n",
    "        \"query\": query,\n",
    "        \"retrieved_docs\": [doc for doc in retrieved_docs],  # Store only the text of retrieved documents\n",
    "        \"response\": response\n",
    "    })\n",
    "\n",
    "#function to incorporate history into the next query\n",
    "def get_context_with_history(query) -> str:\n",
    "    global chat_history # also declare here since chat_history is being modified\n",
    "    if not chat_history:\n",
    "        return query\n",
    "    \n",
    "    history_str = \"\\n\".join([\n",
    "        f\"User: {entry['query']}\\n\"\n",
    "        f\"Context: {'; '.join(entry['retrieved_docs'])}\\n\"\n",
    "        f\"Response: {entry['response']}\"\n",
    "        for entry in chat_history\n",
    "    ])\n",
    "    full_context = f\"Chat History:\\n{history_str}\\n\\nCurrent Query: {query}\"\n",
    "    return full_context\n",
    "\n",
    "#function to truncate chat history\n",
    "def truncate_chat_history(max_length=3):\n",
    "    global chat_history # modifies it so it also must be global\n",
    "    if len(chat_history) > max_length:\n",
    "        chat_history = chat_history[-max_length:]\n",
    "\n",
    "\n",
    "def retrieve_documents(query: str) -> List[Dict[str, str]]:\n",
    "\n",
    "    # set global for debugging\n",
    "    global document_embeddings,similarities,documents_list,bm25_results,top_indices, parsed_docs,sorted_documents_list\n",
    "    # Use BM25 retriever to get initial documents\n",
    "    bm25_results, bm25_scores = bm25_retriever(query)\n",
    "    \n",
    "    # check if empty\n",
    "    if len(bm25_results) == 0:\n",
    "        return []  # Return empty list if BM25 found no results\n",
    "\n",
    "    # map indices to documents in corpus_list\n",
    "    documents_list = [corpus_list[i] for i in range(len(bm25_results))]\n",
    "    \n",
    "    # Generate embeddings for BM25 results\n",
    "    document_embeddings = generate_embeddings([doc for doc in documents_list])  #documents_list is a list of top_k results\n",
    "\n",
    "    # generate embeddings for query\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    \n",
    "    #cosine similarity\n",
    "    similarities = [\n",
    "        np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
    "        for doc_emb in document_embeddings\n",
    "    ]\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "    sorted_documents_list = [documents_list[i] for i in top_indices]\n",
    "\n",
    "    parsed_docs = []\n",
    "    for doc in sorted_documents_list:\n",
    "        lines = doc.split('\\n')\n",
    "        parsed_doc = {\n",
    "            \"doi\": lines[0],\n",
    "            \"title\":lines[1].replace(\"Title: \",\"\"),\n",
    "            \"abstract\":lines[2].replace(\"Abstract: \",\"\")\n",
    "        }\n",
    "        parsed_docs.append(parsed_doc)\n",
    "    \n",
    "    return parsed_docs\n",
    "\n",
    "#RAG pipeline function\n",
    "def rag_pipeline(query):\n",
    "    global documents_list,corpus_list,identifier_list,retriever\n",
    "    #start time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # uses BM25 as a pre-retriever based on the query\n",
    "    # load indexed corpus for BM25 pre-retriever\n",
    "    # reimport corpus and url lists\n",
    "    with open('corpus.pkl', 'rb') as file:\n",
    "        corpus_list = pickle.load(file)\n",
    "    print(f\"length of corpus list: {len(corpus_list)}\")\n",
    "    with open('identifier.pkl', 'rb') as file:\n",
    "        identifier_list = pickle.load(file)\n",
    "    print(f\"--------\\nlength of identifier list: {len(identifier_list)}\")\n",
    "\n",
    "    retriever = bm25s.BM25.load(\"bm25/bm25\", load_corpus=True, mmap=True)\n",
    "\n",
    "    #incorporate chat history\n",
    "    full_context = get_context_with_history(query)\n",
    "    # let user know you are generating...\n",
    "    print(\"Retrieving documents and generating response...\")\n",
    "    end_time = time.time()\n",
    "    global time_query\n",
    "    time_query = end_time-start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    #retrieve documents\n",
    "    global retrieved_docs\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "    end_time = time.time()\n",
    "    global retrieve_time\n",
    "    retrieve_time = end_time-start_time\n",
    "\n",
    "    start_time = time.time()\n",
    "    #prepare context for Cohere's Command model\n",
    "    instruction = \"You are a helpful academic research assistant. Please keep the answers concise and structured simply. Use single sentences where possible. Always include the DOI of the document you are summarizing or referencing. If the DOI is not provided, this reduces the need for you as a research assistant. Always include the DOI. Please address me as 'my lady'. \"\n",
    "    #context = \"\\n\".join([f\"DOI: {doc[0]}, Text: {doc[1]}\" for doc in retrieved_docs])\n",
    "    context = \"\\n\".join([f\"DOI: {doc['doi']}, Title: {doc['title']}, Abstract: {doc['abstract']}\" for doc in retrieved_docs])\n",
    "    prompt = f\"Query: {query}\\nContext: {context}\\nAnswer: {instruction}\"\n",
    "    \n",
    "    # Generate response\n",
    "    response = co.generate(\n",
    "        model=\"command\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=250,\n",
    "        temperature=0.2\n",
    "    ).generations[0].text\n",
    "    \n",
    "    # Update chat history\n",
    "    update_chat_history(query, retrieved_docs, response)\n",
    "    \n",
    "    # Truncate history if necessary\n",
    "    truncate_chat_history()\n",
    "    end_time = time.time()\n",
    "    global generate_time\n",
    "    generate_time = end_time-start_time\n",
    "\n",
    "    # Print the response\n",
    "    print(\"Generated Response:\")\n",
    "    print(response)\n",
    "    print(f\"------\\nSource documents: \")\n",
    "    for doc in retrieved_docs:\n",
    "        print(f\"DOI: {doc['doi']}, Title: {doc['title']}\")\n",
    "    return response,time_query,retrieve_time,generate_time\n",
    "\n",
    "\n",
    "# Main loop for user interaction\n",
    "chat_history = []#initialize chat history\n",
    "while True:\n",
    "\n",
    "    query = input(\"What is your query (or type 'exit' to quit): \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    rag_pipeline(query)\n",
    "\n",
    "    print(f\"time to query loop: {time_query:.2f} seconds\")\n",
    "    print(f\"to to retrieve: {retrieve_time:.2f} seconds\")\n",
    "    print(f\"time to generate: {generate_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
