{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create questions\n",
    "This notebook creates questions using LLM-assist from Cohere models. <br>\n",
    "This follows the method outlined in Teixera de Lima et al., (2025), Wang et al., (2024), and Bruni et al., (2025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mall is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import sys\n",
    "from colorama import Fore, Style, Back\n",
    "import pandas as pd\n",
    "\n",
    "# load secret from local .env file\n",
    "def get_key():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    _key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "    #verify if it worked\n",
    "    if _key is not None:\n",
    "        print(Fore.GREEN + \"all is good, beautiful!\")\n",
    "        return _key\n",
    "    else:\n",
    "        print(Fore.LIGHTRED_EX + \"API Key is missing\")\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.ClientV2(get_key(),client_name=\"little_steve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up functions\n",
    "\n",
    "def embed_documents(documents: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
    "    response = co.embed(texts=documents, model=\"embed-v4.0\",input_type=\"search_query\").embeddings\n",
    "    return response\n",
    "\n",
    "def generate_questions(documents: List[str]) -> List[str]:\n",
    "    \"\"\"Generate questions using the chat endpoint in the V2 API.\"\"\"\n",
    "    instruction = \"\"\"\n",
    "    Develop questions based on the given documents. \n",
    "    The questions should be easy to understandby a novice and only require looking up facts. \n",
    "    An example question: What is augmentation in RAG? \n",
    "    Generate one question that can be answered with at least 5 of the following documents.\n",
    "    \"\"\"\n",
    "    prompt = f\"{instruction} \\n\".join(documents)\n",
    "    response = co.chat(\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],#V2 requires an object\n",
    "        model=\"command-a-03-2025\",\n",
    "        temperature=0.2, # default is 3\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    return [response.message.content[0].text]  # Adjust if you want multiple questions\n",
    "\n",
    "def find_true_positives(question: str, documents: List[str]) -> List[str]:\n",
    "    \"\"\"Identify the top 5 documents relevant to the question using the rerank endpoint.\"\"\"\n",
    "    response = co.rerank(\n",
    "        query=question,\n",
    "        documents=documents,\n",
    "        model=\"rerank-english-v3.0\",\n",
    "        top_n=5,\n",
    "    )\n",
    "    return [doc for doc in response.results]\n",
    "\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0]\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append(f\"{doi} {text}\\n\")\n",
    "    return documents_with_doi\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Step 2: Generate questions\n",
    "    questions = generate_questions(documents)\n",
    "    \n",
    "    # Step 3: Find true positives for each question\n",
    "    for question in questions:\n",
    "        true_positives = find_true_positives(question, documents)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"True Positives by DOI: {[documents[doc.index].split(\"\\n\")[0].strip(\"DOI: \") for doc in true_positives]}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# go ahead and embed so that we don't have to do this each time\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "documents = read_documents_with_doi(directory_path)\n",
    "#embeddings = embed_documents(documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " - need to run loop for x number questions\n",
    " - collect questions, true positives, and answer in pandas df\n",
    " - save to excel file called golden_set.xlsx\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "# randomize order of documents\n",
    "random.shuffle(documents)\n",
    "embeddings = embed_documents(documents)\n",
    "# now, generate a question\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up functions\n",
    "\n",
    "def embed_documents(documents: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
    "    response = co.embed(texts=documents, model=\"embed-v4.0\",input_type=\"search_query\").embeddings\n",
    "    return response\n",
    "\n",
    "def generate_questions(documents: List[str]) -> List[str]:\n",
    "    \"\"\"Generate questions using the chat endpoint in the V2 API.\"\"\"\n",
    "    instruction = \"\"\"\n",
    "    Evaluate the following question and answer pair for correctness. \n",
    "    Use the provided documents as evidence. \n",
    "    Please answer yes or no, with a reason. \n",
    "    QUESTION: What are the key challenges and potential solutions for improving metadata quality?\n",
    "    ANSWER: \n",
    "    ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2409.10633', '10.48550/arXiv.2502.03627', '10.48550/arXiv.2508.18620', '10.48550/arXiv.2401.16359']\n",
    "    \"\"\"\n",
    "    prompt = f\"{instruction} \\n\".join(documents)\n",
    "    response = co.chat(\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],#V2 requires an object\n",
    "        model=\"command-a-03-2025\",\n",
    "        temperature=0.2, # default is 3\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    return [response.message.content[0].text]  # Adjust if you want multiple questions\n",
    "\n",
    "def find_true_positives(question: str, documents: List[str]) -> List[str]:\n",
    "    \"\"\"Identify the top 5 documents relevant to the question using the rerank endpoint.\"\"\"\n",
    "    response = co.rerank(\n",
    "        query=question,\n",
    "        documents=documents,\n",
    "        model=\"rerank-english-v3.0\",\n",
    "        top_n=5,\n",
    "    )\n",
    "    return [doc for doc in response.results]\n",
    "\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0]\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append(f\"{doi} {text}\\n\")\n",
    "    return documents_with_doi\n",
    "\n",
    "\n",
    "def EvaluateQuestion():\n",
    "    \n",
    "    # Step 2: Generate questions\n",
    "    questions = generate_questions(documents)\n",
    "    \n",
    "    # Step 3: Find true positives for each question\n",
    "    for question in questions:\n",
    "        true_positives = find_true_positives(question, documents)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"True Positives by DOI: {[documents[doc.index].split(\"\\n\")[0].strip(\"DOI: \") for doc in true_positives]}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# go ahead and embed so that we don't have to do this each time\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "documents = read_documents_with_doi(directory_path)\n",
    "#embeddings = embed_documents(documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    " - need to run loop for x number questions\n",
    " - collect questions, true positives, and answer in pandas df\n",
    " - save to excel file called golden_set.xlsx\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "# randomize order of documents\n",
    "random.shuffle(documents)\n",
    "embeddings = embed_documents(documents)\n",
    "# now, generate a question\n",
    "EvaluateQuestion()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check questions from a list of DOIs\n",
    "Questions and true positives were adjusted untill complete agreement between llm and human evaluator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up functions\n",
    "\n",
    "def embed_documents(documents: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
    "    response = co.embed(texts=documents, model=\"embed-v4.0\",input_type=\"search_query\").embeddings\n",
    "    return response\n",
    "\n",
    "def generate_questions(documents: List[str]) -> List[str]:\n",
    "    \"\"\"Generate questions using the chat endpoint in the V2 API.\"\"\"\n",
    "    instruction = \"\"\"\n",
    "    Evaluate the following question and list of DOIs for correctness. \n",
    "    Use the provided documents as evidence. IF there are less than 5, recommend a document to include.\n",
    "    Please answer yes or no, with a brief summary of each DOI. \n",
    "    QUESTION: which studies examined the abstract in metadata?\n",
    "    ANSWER: \n",
    "    [\"10.1002/leap.1411\",\"10.1007/s11192-020-03632-0\",\"10.1162/qss_a_00286\",\"10.1162/qss_a_00022\",\"10.31222/osf.io/smxe5\"]\n",
    "    \"\"\"\n",
    "    prompt = f\"{instruction} \\n\".join(documents)\n",
    "    response = co.chat(\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],#V2 requires an object\n",
    "        model=\"command-a-03-2025\",\n",
    "        temperature=0.2, # default is 3\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    return [response.message.content[0].text]  # Adjust if you want multiple questions\n",
    "\n",
    "def find_true_positives(question: str, documents: List[str]) -> List[str]:\n",
    "    \"\"\"Identify the top 5 documents relevant to the question using the rerank endpoint.\"\"\"\n",
    "    response = co.rerank(\n",
    "        query=question,\n",
    "        documents=documents,\n",
    "        model=\"rerank-english-v3.0\",\n",
    "        top_n=5,\n",
    "    )\n",
    "    return [doc for doc in response.results]\n",
    "\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0]\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append(f\"{doi} {text}\\n\")\n",
    "    return documents_with_doi\n",
    "\n",
    "\n",
    "def EvaluateQuestion():\n",
    "    \n",
    "    # Step 2: Generate questions\n",
    "    questions = generate_questions(documents)\n",
    "    \n",
    "    # Step 3: Find true positives for each question\n",
    "    for question in questions:\n",
    "        true_positives = find_true_positives(question, documents)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"True Positives by DOI: {[documents[doc.index].split(\"\\n\")[0].strip(\"DOI: \") for doc in true_positives]}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# go ahead and embed so that we don't have to do this each time\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "documents = read_documents_with_doi(directory_path)\n",
    "#embeddings = embed_documents(documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: **Evaluation of the provided DOI list for the question \"which studies examined the abstract in metadata?\"**\n",
      "\n",
      "**Correct DOIs (Yes):**\n",
      "1. **10.1002/leap.1411**  \n",
      "   - *Summary*: Examines the lexical content of abstracts in preprints during the COVID-19 crisis, comparing them to pre-pandemic abstracts. Focuses on promotional language, positive/negative words, and hedge words.  \n",
      "   - *Relevance*: Directly analyzes abstracts and their metadata.  \n",
      "\n",
      "2. **10.1007/s11192-020-03632-0**  \n",
      "   - *Summary*: Assesses the type of articles published during the first 3 months of the COVID-19 pandemic, comparing them to H1N1 articles. Extracts and evaluates abstracts from PubMed.  \n",
      "   - *Relevance*: Explicitly examines abstracts in metadata.  \n",
      "\n",
      "3. **10.1162/qss_a_00286**  \n",
      "   - *Summary*: Compares the completeness of metadata, including abstracts, across eight free-access scholarly databases using a Crossref sample of 115,000 records.  \n",
      "   - *Relevance*: Directly evaluates abstract metadata.  \n",
      "\n",
      "4. **10.1162/qss_a_00022**  \n",
      "   - *Summary*: Describes Crossref's scholarly metadata, including abstracts, and its role in the research ecosystem. Highlights the availability and importance of abstracts.  \n",
      "   - *Relevance*: Focuses on abstract metadata in Crossref.  \n",
      "\n",
      "5. **10.31222/osf.io/smxe5**  \n",
      "   - *Summary*: Analyzes the availability of metadata elements, including abstracts, in Crossref over time, emphasizing improvements and publisher efforts.  \n",
      "   - *Relevance*: Directly examines abstract metadata.  \n",
      "\n",
      "**Incorrect DOIs (No):**  \n",
      "All other DOIs provided do not explicitly examine abstracts in metadata. They focus on other aspects such as retrieval-augmented generation (RAG), metadata quality in general, bibliometric analyses, or specific applications of metadata without a focus on abstracts.  \n",
      "\n",
      "**Recommendation for Additional Document:**  \n",
      "Since the list already includes 5 relevant DOIs, no additional document is needed. However, if expanding, consider **10.52\n",
      "True Positives by DOI: ['10.1162/qss_a_00286', '10.1002/leap.1411', '10.1007/s11192-020-03632-0', '10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " - need to run loop for x number questions\n",
    " - collect questions, true positives, and answer in pandas df\n",
    " - save to excel file called golden_set.xlsx\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "# randomize order of documents\n",
    "random.shuffle(documents)\n",
    "embeddings = embed_documents(documents)\n",
    "# now, generate a question\n",
    "EvaluateQuestion()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
