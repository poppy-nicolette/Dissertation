<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Poppy Riddle’s Dissertation Research – Home</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2f5df379a58b258e96c21c0638c20c03.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-de4de07f82120c2cab44c9d2883d6f16.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Home</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link active" href="./index.html" aria-current="page"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./data.html"> 
<span class="menu-text">Data</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./results.html"> 
<span class="menu-text">Results</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="./recommendations.html"> 
<span class="menu-text">Recommendations</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#an-analysis-of-crossref-and-openalex-as-external-knowledge-sources-for-retrieval-augmented-generation" id="toc-an-analysis-of-crossref-and-openalex-as-external-knowledge-sources-for-retrieval-augmented-generation" class="nav-link active" data-scroll-target="#an-analysis-of-crossref-and-openalex-as-external-knowledge-sources-for-retrieval-augmented-generation">An Analysis of Crossref and OpenAlex as External Knowledge Sources for Retrieval-Augmented Generation</a>
  <ul class="collapse">
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background">Background</a></li>
  <li><a href="#problem-statement" id="toc-problem-statement" class="nav-link" data-scroll-target="#problem-statement">Problem statement</a></li>
  <li><a href="#purpose-research-questions" id="toc-purpose-research-questions" class="nav-link" data-scroll-target="#purpose-research-questions">Purpose &amp; Research Questions</a></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology">Methodology</a></li>
  <li><a href="#main-results" id="toc-main-results" class="nav-link" data-scroll-target="#main-results">Main results</a>
  <ul class="collapse">
  <li><a href="#rq1" id="toc-rq1" class="nav-link" data-scroll-target="#rq1">RQ1</a></li>
  <li><a href="#rq2" id="toc-rq2" class="nav-link" data-scroll-target="#rq2">RQ2</a></li>
  <li><a href="#rq3" id="toc-rq3" class="nav-link" data-scroll-target="#rq3">RQ3</a></li>
  </ul></li>
  <li><a href="#recommendations" id="toc-recommendations" class="nav-link" data-scroll-target="#recommendations">Recommendations</a>
  <ul class="collapse">
  <li><a href="#multiple-abstracts-need-to-be-available" id="toc-multiple-abstracts-need-to-be-available" class="nav-link" data-scroll-target="#multiple-abstracts-need-to-be-available">Multiple Abstracts Need to be Available</a></li>
  <li><a href="#clean-out-jats-tags-on-rest-api" id="toc-clean-out-jats-tags-on-rest-api" class="nav-link" data-scroll-target="#clean-out-jats-tags-on-rest-api">Clean Out JATS Tags on REST API</a></li>
  <li><a href="#multiple-language-values" id="toc-multiple-language-values" class="nav-link" data-scroll-target="#multiple-language-values">Multiple Language Values</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Poppy Riddle’s Dissertation Research</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="an-analysis-of-crossref-and-openalex-as-external-knowledge-sources-for-retrieval-augmented-generation" class="level1">
<h1>An Analysis of Crossref and OpenAlex as External Knowledge Sources for Retrieval-Augmented Generation</h1>
<p>A preprint article version of this can be found here: <a href="">DOI</a><br></p>
<section id="summary" class="level3">
<h3 class="anchored" data-anchor-id="summary">Summary</h3>
<p>This project page outlines my dissertation research for a PhD in Information Science at Dalhousie University. <br> In this research, I investigated the presence of errors or noise in bibliographic metadata, specifically the titles and abstracts from two bibliographic databases: Crossref and OpenAlex. Focused on errors and noise that may impact retrieval-augmented generation (RAG) pipelines, 86% of a random sample of journal articles had a error, (such as missing information, or containing multiple languages), or noise, (such as hyperlinks or face markup). An exploration of the effects of two types of errors and noise, specifically multilingual text and face markup, found significant effects on RAG performance. While face markup was not found to affect RAG retrieval or generation metrics, multilingual text was found to beneficially affect the retriever and negatively affect the generator.</p>
</section>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>In the last few years, AI has been rapidly deployed in the academic context <a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> as an aid for discovery and summarization, integrated into tools for systematic review <a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>, identifying citation contexts<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>, or to promote publisher-owned works (Scopus AI<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>, Web of Science Research Assistant<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a>, ProQuest One<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>, and PrimoVE Research Assistant<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a> ). These conversational search tools are augmenting or replacing keyword-based search systems. Instead, users can enter questions, request summaries, or ask for similar works. Large language models (LLMs) are the key technology enabling these tools to interpret text semantically and generating well composed responses to user’s information needs. However, LLMs have shortcomings due to the limitations of their training data and their potential to create syntactically correct but factually wrong responses <a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>. RAG architectures solve this problem by using external knowledge sources to augment the knowledge from the LLM. Bibliographic metadata has potential to serve as a source of external knowledge for these systems.</p>
</section>
<section id="problem-statement" class="level2">
<h2 class="anchored" data-anchor-id="problem-statement">Problem statement</h2>
<p>Prior research has found that typos<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>, irrelevant information<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a><a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>, formatting characters from scanned PDFs<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>, or invisible formatting characters<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>, as forms of noise negatively impact language models whether they are used for embedding or generation. However, prior studies from the IR and NLP communities have not examined errors and noise found in metadata. Abstract and title elements have been investigated for accuracy<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>, availability<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>, and their agreement with language values<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>, yet prior studies from the LIS community have not looked at noise such as characters, encodings, or irrelevant text that may occur in metadata. Understanding the breadth and occurrence of both errors and noise in titles and abstracts has implications for text cleaning processes or choice of pipeline components. <br> There exists a gap in the literature linking IR research on RAG with LIS research on metadata quality as it pertained to RAG use of metadata as external knowledge. LIS research has yet to integrate IR definitions of noise within a metadata quality assessment of bibliographic metadata or compare its occurrence between two database sources. IR research has yet to examine the effects of errors and noise observed in bibliographic metadata on a naïve RAG architecture.</p>
</section>
<section id="purpose-research-questions" class="level2">
<h2 class="anchored" data-anchor-id="purpose-research-questions">Purpose &amp; Research Questions</h2>
<p>Assuming the metadata elements of the title and abstract can be used as the external knowledge source for RAG, the purpose of this research was to identify errors and noise in title and abstract content that may affect RAG performance, compare metadata sources for error and noise, and to observe the effect on RAG. A study in three parts was used to investigate the following using modified IS&amp;R dimensions for assigning independent, dependent and control variables. <br> To answer the problems and challenges introduced above, three research questions align with each part of the study.<br></p>
<ol type="1">
<li>What errors or noise may be observed in the title and abstract from a random sample of Crossref metadata?</li>
<li>How do Crossref and OpenAlex compare for errors and noise in the same random sample?</li>
<li>What is the effect of observed errors and noise in metadata on retriever performance and generated responses? <br></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="files/base_RAG.svg" class="img-fluid figure-img" style="width:100.0%" alt="A diagram of a basic RAG architecture used in this study."></p>
<figcaption>Diagram of a basic RAG architecture used for this study.</figcaption>
</figure>
</div>
</section>
<section id="methodology" class="level2">
<h2 class="anchored" data-anchor-id="methodology">Methodology</h2>
<p>Methodologically, each RQ was answered in three parts with the first two parts focused on quantitatively measuring metadata quality and the third part quantitatively measuring effect. Part 1 utilized a random sample from Crossref to observe how the title and abstract contents may contain errors or noise from the perspective of using them in a RAG application as a document source. Part 2 compared the same set of metadata from OpenAlex to observe any differences. Part 3 used errors and noise observed in Parts 1 and 2 to create perturbed datasets which were run through a RAG instrument to measure the effect on the retriever and the generator. All three parts contributed to an information science approach that expanded metadata quality to include the perspective from NLP with types of noise, documented differences for non-matching works between two databases, and explored how errors and noise affected a RAG instrument.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="files/overall_framework.svg" class="img-fluid figure-img" style="width:80.0%" alt="The overall framework for each part of the study."></p>
<figcaption>Overall framework for each part of the study.</figcaption>
</figure>
</div>
</section>
<section id="main-results" class="level2">
<h2 class="anchored" data-anchor-id="main-results">Main results</h2>
<section id="rq1" class="level3">
<h3 class="anchored" data-anchor-id="rq1">RQ1</h3>
<p>In RQ1, it was unknown what types of problems may exist within the text of titles and abstracts sampled from Crossref. For titles, journal article titles were correct 66.5% in the subset compared with 78% correct with proceedings articles and 90% correct with book chapters. For abstracts, proceedings articles were most correct (78%) followed by book chapters (72%) and journal articles (16%). Missing information and information loss are the two biggest challenges as shown in past research<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a> and in these results. Processes for addressing missing information and information loss, particularly for language values, where 80.5% of journal articles, 70% of book chapters, and 72% of proceedings articles did not have them. This was followed by the inclusion of multiple languages in titles and abstracts (as inconsistent value representation) and the inclusion of face markup such as MathML and tex-math (as datatype noise) as the most important problems that may directly affect RAG. Other characteristics were also observed such as HTML encodings, hyperlinks to repositories, figure and table captions, and copyright statements. When examined from a perspective of metadata quality that includes noise, Crossref metadata, particularly journal articles, has many errors and noise that may require preprocessing prior to use in RAG.<br> In the table below are errors and noise categories applied to problems found in the title subset analysis across journal articles, book chapters, and conference proceedings. <br></p>
<table class="table-light table-hover caption-top table">
<caption>Title errors and noise from subset analysis</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Category</th>
<th style="text-align: center;">Count</th>
<th style="text-align: right;">Percentage of subset</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">No error or noise</td>
<td style="text-align: center;">174</td>
<td style="text-align: right;">58.0%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Incorrect element</td>
<td style="text-align: center;">47</td>
<td style="text-align: right;">15.7%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Information loss</td>
<td style="text-align: center;">27</td>
<td style="text-align: right;">9.0%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Datatype noise</td>
<td style="text-align: center;">17</td>
<td style="text-align: right;">5.7%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Inconsistent value representation</td>
<td style="text-align: center;">10</td>
<td style="text-align: right;">3.3%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Missing information</td>
<td style="text-align: center;">9</td>
<td style="text-align: right;">3.0%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Semantic noise</td>
<td style="text-align: center;">8</td>
<td style="text-align: right;">2.7%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Incorrect value</td>
<td style="text-align: center;">8</td>
<td style="text-align: right;">2.7%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: center;">300</td>
<td style="text-align: right;">100.0%</td>
</tr>
</tbody>
</table>
<p><br> In the table below are errors and noise categories applied to problems found in the abstract subset analysis across journal articles, book chapters, and conference proceedings. <br></p>
<table class="table-light table-hover caption-top table">
<caption>Abstract errors and noise from subset analysis</caption>
<thead>
<tr class="header">
<th style="text-align: left;">Categories</th>
<th style="text-align: center;">Counts</th>
<th style="text-align: right;">Percentage of subset</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">No error or noise</td>
<td style="text-align: center;">112</td>
<td style="text-align: right;">37.3%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Incorrect value</td>
<td style="text-align: center;">86</td>
<td style="text-align: right;">28.7%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Datatype noise</td>
<td style="text-align: center;">29</td>
<td style="text-align: right;">9.7%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Semantic noise</td>
<td style="text-align: center;">21</td>
<td style="text-align: right;">7.0%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Inconsistent value representation</td>
<td style="text-align: center;">20</td>
<td style="text-align: right;">6.7%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Information loss</td>
<td style="text-align: center;">20</td>
<td style="text-align: right;">6.7%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Missing information</td>
<td style="text-align: center;">9</td>
<td style="text-align: right;">3.0%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Incorrect element</td>
<td style="text-align: center;">2</td>
<td style="text-align: right;">0.7%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Information gain</td>
<td style="text-align: center;">1</td>
<td style="text-align: right;">0.3%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Total</td>
<td style="text-align: center;">300</td>
<td style="text-align: right;">100.0%</td>
</tr>
</tbody>
</table>
</section>
<section id="rq2" class="level3">
<h3 class="anchored" data-anchor-id="rq2">RQ2</h3>
<p>In RQ2, 99% (9,998) of DOIs in Part 1 were found in the OpenAlex database. Non-matching titles (223, 2.2%) and abstracts (688, 6.9%) were examined as subsets. Information gains were observed for language (51% of the shared corpus) and missing titles (0.2%) and abstracts (0.6%), which addressed in part one of the major problems identified in Part 1. For the non-matching items, information value representation affected 26% of those non-matching titles and 2.6% of non-matching abstracts. Mismatches between abstracts (5.7%) and titles (3.1%) was observed was due to introduction of datatype noise, adding to the noise already found in the Crossref titles (13.5%) and abstracts (6.5%). Other than information loss and gain as the two largest categories explaining mismatches between the data from the two databases, datatype noise and inconsistent value representation were selected as the errors and noise to explore in Part 3. Overall, the OpenAlex metadata improves upon the Crossref data and is clean of certain face markup, such as JATS tags, however there are areas of information loss and the introduction of noise that should be addressed if using this metadata as a source. <br> In the table below, non-matching titles were manually inspected and labels for errors and noise were applied as explanations for the difference between the two databases.</p>
<table class="table-light table-hover caption-top table">
<caption>Title mismatches labelled by error and noise classification scheme.</caption>
<colgroup>
<col style="width: 35%">
<col style="width: 7%">
<col style="width: 30%">
<col style="width: 26%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Error and noise classification</th>
<th style="text-align: center;">Count</th>
<th style="text-align: center;">Percentage of non-matching</th>
<th style="text-align: right;">Percentage of whole set</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Information loss</td>
<td style="text-align: center;">126</td>
<td style="text-align: center;">56.5%</td>
<td style="text-align: right;">1.3%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Inconsistent value representation</td>
<td style="text-align: center;">58</td>
<td style="text-align: center;">26.0%</td>
<td style="text-align: right;">0.6%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Information gain</td>
<td style="text-align: center;">23</td>
<td style="text-align: center;">10.3%</td>
<td style="text-align: right;">0.2%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Semantic noise</td>
<td style="text-align: center;">6</td>
<td style="text-align: center;">2.7%</td>
<td style="text-align: right;">0.1%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Datatype noise</td>
<td style="text-align: center;">7</td>
<td style="text-align: center;">3.1%</td>
<td style="text-align: right;">0.1%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Missing information</td>
<td style="text-align: center;">2</td>
<td style="text-align: center;">0.9%</td>
<td style="text-align: right;">0.0%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Incorrect value</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.4%</td>
<td style="text-align: right;">0.0%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Total</td>
<td style="text-align: center;">223</td>
<td style="text-align: center;">100.0%</td>
<td style="text-align: right;">2.2%</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>In the table below, non-matching abstracts were manually inspected and explanatory labels for the difference between the two datasets were applied.</p>
<table class="table-light table-hover caption-top table">
<caption>Differences in abstracts between Crossref and OpenAlex.</caption>
<colgroup>
<col style="width: 32%">
<col style="width: 7%">
<col style="width: 28%">
<col style="width: 32%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Error and noise classification</th>
<th style="text-align: center;">Count</th>
<th style="text-align: center;">Percentage of non-matching</th>
<th style="text-align: right;">Percentage of the total dataset</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Information loss</td>
<td style="text-align: center;">563</td>
<td style="text-align: center;">81.8%</td>
<td style="text-align: right;">5.6%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Information gain</td>
<td style="text-align: center;">63</td>
<td style="text-align: center;">9.2%</td>
<td style="text-align: right;">0.6%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Datatype noise</td>
<td style="text-align: center;">39</td>
<td style="text-align: center;">5.7%</td>
<td style="text-align: right;">0.4%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Inconsistent value representation</td>
<td style="text-align: center;">18</td>
<td style="text-align: center;">2.6%</td>
<td style="text-align: right;">0.2%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Semantic noise</td>
<td style="text-align: center;">4</td>
<td style="text-align: center;">0.6%</td>
<td style="text-align: right;">0.0%</td>
</tr>
<tr class="even">
<td style="text-align: left;">Incorrect value</td>
<td style="text-align: center;">1</td>
<td style="text-align: center;">0.1%</td>
<td style="text-align: right;">0.0%</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Total</td>
<td style="text-align: center;">688</td>
<td style="text-align: center;">100%</td>
<td style="text-align: right;">6.9%</td>
</tr>
</tbody>
</table>
</section>
<section id="rq3" class="level3">
<h3 class="anchored" data-anchor-id="rq3">RQ3</h3>
<p>In RQ3, the purpose was to explore how observed errors and noise affect the outcomes of RAG retrieval and generation. The use of JATs tags (datatype noise) was justified by results from Part 1 (100% for JATS and 13% for other types in the automated check and 5.7% in the subset analysis) and Part 2 (100% for JATS and 6.5% of other types). Multilingual titles and abstracts (inconsistent value representation) were found in the data in Part 1 (6.7%) and in Part 2 (2.7%). <br> Two significant results were observed. Multilingual text was found to have a beneficial effect with higher document similarity scores (p=0.00006, F-value=9.81, 0.95 CI) when used with multilingual capable models though this did not result in significantly different answer relevance scores. However, the same multilingual text was at a disadvantage as evidenced by significantly lower faithfulness scores (p=0.01, 0.95 CI), where at times, the generator struggled to incorporate the multilingual context. <br></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="files/Round1_document_scores_combined.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption>Round 1 results for the document similarity scores showing the statistically significant benefit of multilingual text in the title and abstract.</figcaption>
</figure>
</div>
<p><br> In the figure below, the clean dataset is on the left, the JATS-tagged text scores are in the middle, and the multilingual text is on the right.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="files/Round2_faithfulness.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption>Round 2 faithfulness scores showing the negative effect of multilingual text on an English-only generator model.</figcaption>
</figure>
</div>
<p>No significant effects were observed due to the JATS noise in the other dataset.</p>
</section>
</section>
<section id="recommendations" class="level2">
<h2 class="anchored" data-anchor-id="recommendations">Recommendations</h2>
<section id="multiple-abstracts-need-to-be-available" class="level3">
<h3 class="anchored" data-anchor-id="multiple-abstracts-need-to-be-available">Multiple Abstracts Need to be Available</h3>
<p>In Part 1, it was found that 13% of journal article metadata do not match between the REST and XML API responses. This was due to multiple abstracts or multiple titles in the XML response which are not reported in the REST API. I suggest more infrastructure changes are needed by database managers to make multiple abstracts more visible and available through REST APIs, the primary means of harvesting data from both Crossref and OpenAlex. I think much can be done to improve this from Crossref with even the addition of a flag if other versions of the abstract are available in the XML API for harvesting single records. For OpenAlex, adding in an ingest path from the XML API would enrich their metadata and make it more aligned with publisher’s or author’s intent, though this is not needed if Crossref expands the REST API. Additionally, this would benefit bibliometrics and metadata quality research with a more accurate representation of abstracts.</p>
</section>
<section id="clean-out-jats-tags-on-rest-api" class="level3">
<h3 class="anchored" data-anchor-id="clean-out-jats-tags-on-rest-api">Clean Out JATS Tags on REST API</h3>
<p>To level the playing field between Crossref and other databases, The Crossref REST API response should clean JATS tags on export or provide clear documentation on best practices for cleaning them. While the effect was not significant in this research, a minimal amount of JATS tags was introduced as a perturbation and more is used in practice by some journals/publishers according to observations from Part 1. However, such a change by Crossref would require extensive outreach and testing for its effects on interoperability with users of abstract metadata.</p>
</section>
<section id="multiple-language-values" class="level3">
<h3 class="anchored" data-anchor-id="multiple-language-values">Multiple Language Values</h3>
<p>As there are multiple places in the Crossref XML API where language attributes can be, OpenAlex should expand the ingest of all language attribute values from the XML API or Crossref should provide a data dump based on the XML API schema. If multiple versions of an abstract exist, there should be agreement between reported languages at the journal level and the languages in the specific title or abstract attributes. As an example, the language attribute, if maintained at the journal level, should include a tuple of all languages that have been used, such as (‘en’, ‘de’). Alternatively, Crossref could collect this information and make it available in the REST API.</p>


</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Taylor, J., Dagan, K., Youngberg, M., Kaufman, T., &amp; Radding, J. (2025). A Survey of AI tools in Library Tech: Accelerating into and Unlocking Streamlined Enhanced Convenient Empowering Game-Changers. Journal of Electronic Resources Librarianship, 37(2), 217–229. https://doi.org/10.1080/1941126X.2025.2497738<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Han, B., Susnjak, T., &amp; Mathrani, A. (2024). Automating Systematic Literature Reviews with Retrieval-Augmented Generation: A Comprehensive Overview. Applied Sciences, 14(19), 9103. https://doi.org/10.3390/app14199103<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>https://scite.ai/assistant<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>https://www.elsevier.com/products/scopus/scopus-ai<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5"><p>https://clarivate.com/academia-government/scientific-and-academic-research/research-discovery-and-referencing/web-of-science/web-of-science-research-assistant/<a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6"><p>https://about.proquest.com/en/products-services/ProQuest-One-Academic/<a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7"><p>https://ir.clarivate.com/news-events/press-releases/news-details/2024/Clarivate-Launches-Generative-AI-Powered-Primo-Research-Assistant/default.aspx<a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8"><p>Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., Küttler, H., Lewis, M., Yih, W., Rocktäschel, T., Riedel, S., &amp; Kiela, D. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. Advances in Neural Information Processing Systems, 33, 9459–9474. https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html<a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9"><p>Cho, S., Jeong, S., Seo, J., Hwang, T., &amp; Park, J. C. (2024). Typos that Broke the RAG’s Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations (arXiv:2404.13948). arXiv. https://doi.org/10.48550/arXiv.2404.13948<a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10"><p>Chen, J., Lin, H., Han, X., &amp; Sun, L. (2024). Benchmarking Large Language Models in Retrieval-Augmented Generation. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16), Article 16. https://doi.org/10.1609/aaai.v38i16.29728<a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11"><p>Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E. H., Schärli, N., &amp; Zhou, D. (2023). Large Language Models Can Be Easily Distracted by Irrelevant Context. Proceedings of the 40th International Conference on Machine Learning, 31210–31227. https://proceedings.mlr.press/v202/shi23a.html<a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12"><p>Zhang, J., Zhang, Q., Wang, B., Ouyang, L., Wen, Z., Li, Y., Chow, K.-H., He, C., &amp; Zhang, W. (2025). OCR Hinders RAG: Evaluating the Cascading Impact of OCR on Retrieval-Augmented Generation. 17443–17453. https://openaccess.thecvf.com/content/ICCV2025/html/Zhang_OCR_Hinders_RAG_Evaluating_the_Cascading_Impact_of_OCR_on_ICCV_2025_paper.html<a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13"><p>Stambolic, V., Dhar, A., &amp; Cavigelli, L. (2025). RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation (arXiv:2510.11195). arXiv. https://doi.org/10.48550/arXiv.2510.11195<a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14"><p>Delgado-Quirós, L., &amp; Ortega, J. L. (2024). Completeness degree of publication metadata in eight free-access scholarly databases. Quantitative Science Studies, 5(1), 31–49. https://doi.org/10.1162/qss_a_00286<a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15"><p>Eck, N. J. van, &amp; Waltman, L. (2022). Crossref as a source of open bibliographic metadata. OSF. https://doi.org/10.31222/osf.io/smxe5<a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16"><p>Céspedes, L., Kozlowski, D., Pradier, C., Sainte-Marie, M. H., Shokida, N. S., Benz, P., Poitras, C., Ninkov, A. B., Ebrahimy, S., Ayeni, P., Filali, S., Li, B., &amp; Larivière, V. (2024). Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness (arXiv:2409.10633). arXiv. https://doi.org/10.48550/arXiv.2409.10633<a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17"><p>Delgado-Quirós, L., &amp; Ortega, J. L. (2024). Completeness degree of publication metadata in eight free-access scholarly databases. Quantitative Science Studies, 5(1), 31–49. https://doi.org/10.1162/qss_a_00286<a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>