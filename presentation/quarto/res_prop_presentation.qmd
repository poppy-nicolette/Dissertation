---
title: "Is bibliometric metadata ready for RAG?"
subtitle: "Research Proposal, PhD Program in Information Science"
author: "Poppy Riddle"
format: 
  revealjs:
    theme: [default, www/custom.scss]
    progress: true
    slide-number: true
    preview-links: auto
    embed-resources: false
    scrollable: true
lightbox: true
editor: visual
background-transition: fade
project:
  execute-dir: project
---
# hello!
<!--# https://htmlcolorcodes.com/color-chart/web-safe-color-chart/ -->

<!--# more helpful stuff: https://emilhvitfeldt.com/blog -->

<!--# plugins should you want them: https://github.com/hakimel/reveal.js/wiki/Plugins,-Tools-and-Hardware -->

## Presentation overview

-   Introduction
-   Part 1: Random sample
-   Part 2: Comparing sources
-   Part 3: RAG in action
-   Concluding thoughts and questions

# Introduction

## Background {.scrollable}
::: notes
metadata is data on scholarly resources, such as journal articles,

Efforts in community to improve such as COMET, California Digital Library - enrichment process

Generative AI in academic libraries: ScopusAI, WoS Research Assistant, ProQuest One, PrimoVE Research Assistant

External knowledge is important as LLMs rely on training data

Metadata may be available on paywalled works
:::

Metadata has been used for discovery and analysis

Efforts to improve metadata: COMET

Generative AI in academic libraries

Metadata is a good source of external knowledge

Metadata can be more available than full text

## What is RAG?

LLM-based chat application
![](images/LLM_chat.svg){fig-align="center" width="120%" }

Retrieval-augmented generation
![](images/rag_basic.svg){fig-align="center" width="160%" }

## Theorhetical position
::: notes
notes on describing Ingwersen & Järvelin's framework
:::
[Ingwersen & Järvelin's Cognitive Model of IS&R]{.italic}
![](images/cog_model_p271.png){fig-align="center"}

[Detailed view of the model]{.italic}
![](images/cog_model_detail_p284.png)

## Proof of concept {.scrollable}
Summer of 2024 experimented using abstracts as a source of RAG applications

![](images/rag_based_nav.png){fig-align="center"}
[published in NLP4Sustainability Workshop, Proceedings of the 21st Conference on Natural Language Processing (KONVENS), Volume 2: Workshops,
pages 282–290, 2025]{.blue}

![](images/ocean_science_viz.png){fig-align="center"}
[published in IV 2025 : 29th International Conference Information Visualisation, Darmstadt, DE]{.blue}

## Proposition
[If we use metadata as a knowledge source in RAG, is metadata quality a problem?]{.yellow .absolute left="20%" top="30%" right="20%"}

## Past research 
::: notes
past studies have examined coverage, completeness, accuracy, but not content.
:::

### [OpenAlex quality]{.yellow}
- Culbert et al. (2024): shared corpus of OpenAlex and other databases<br>
- Delgado-Quiros & Ortega (2024): metadata in Crossref and OpenAlex<br>
- Alonso-Alverez & van Eck (2024): analysis of OpenAlex<br>

### [RAG and metadata]{.yellow}
- Braun et al. (2024): Linzbach et al. (2023): query types<br>
- Kang & Kim (2023): metadata in RAG<br>
- Hayashi et al. (2024): metadata as document source<br>
- Agarwal et al. (2024): bibliometric abstracts from Semantic Scholar for kw extraction<br>

### [Perturbations in RAG]{.yellow}
- F. Shi et al.(2023): irrelevant information in contexts<br>
- Cho et al. (2024): typos affecting retrieval performance<br>


## 3 areas should be explored:

1.  metadata characterization
2.  metadata source
3.  evaluate characteristic impact through case study

## Gaps addressed

-   Lack of characterization of title and abstract metadata

-   Metadata source: compares two sources for effects of aggregation and transformation

-   evidence for bibliometric metadata directly as a document source in RAG

## Framework

![](images/framework.svg){fig-align="center" }



# Part 1: Characterize metadata {.scrollable}

## Characterize metadata from Crossref {.scrollable}

What are the characteristics of textual metadata from a registration agency, such as Crossref?

![](images/part_1.svg){fig-align="center" }

## Example: {.scrollable}

Abstract metadata from <https://api.crossref.org/works/10.1103/physrevd.110.036015>

![](images/abstract.png){fig-align="center"}

Or, another example from <https://api.crossref.org/works/10.33024/hjk.v15i1.3791>:

![](images/abstract2.png){fig-align="center"}

Or one that contains geometric shapes, from <https://api.crossref.org/works/10.1108/eemcs-04-2022-0108>

![](images/abstract3.png){fig-align="center"}

## 

[Past studies have examined metadata for presence and accuracy, but have not examined content.]{.yellow .absolute left="20%" top="30%" right="20%"}

# Part 2: Compare data sources {.scrollable}

## Compare the same metadata from Crossref and OpenAlex {.scrollable}

![](images/part_2_framework.svg){width="587"}


##  {.scrollable}

::::: columns
::: {.column width="50%"}
-   Same DOIs from Part 1

-   Compare for changes

    -   Missing

    -   Incorrect

    -   Wrong element

    -   Different

    -   Representation
:::

::: {.column width="50%"}
-   Subset analysis

-   Code into patterns

-   Analyze full dataset with patterns
:::
:::::

##  

[Past studies have examined both databases for their appropriateness for bibliometric analysis, but not for NLP tasks.]{.yellow .absolute left="20%" top="30%" right="20%"}

# Part 3: Evaluate some characteristics in action

## What happens when we put metadata in a RAG application? {.scrollable}
::: notes
Three datasets of 45 works containing the DOI, the title, and the abstract:

A clean dataset

With JATS tags prepending and appending the title and abstract.

A translated version in Japanese prepending the English

-   Analyze results from the retriever:

    -   precision

    -   recall

    -   F1

    -   accuracy

    -   balanced accuracy

    -   document scores

-   Analyze results from the generator model:

    -   faithfulness

    -   answer relevance

:::

![](images/part_3_framework.svg){fig-align="center"}

## Example of measures in action

![](images/part_3_example.png){fig-align="center"}

[An example of response relevance scoring and faithfulness]{.italic}

![](images/part_3_scores.png){fig-align="center"}

[Scores recorded for each query]{.italic}

# Summary

## Purpose:

[[Bibliometric metadata]{.yellow} can be used as a source of [external knowledge]{.yellow} in RAG, however, it is unknown what [characteristics]{.yellow} or errors may impact RAG [performance]{.yellow}.]{.absolute left="15%" top="30%" right="15%"}

## Benefits

[This study should help us understand how [characteristic]{.yellow} from Part 1 and how a [data source]{.yellow} from Part 2, may [impact retrieval and generation]{.yellow} in a RAG application.]{.absolute left="20%" top="30%" right="20%"}

##  Benefits

[Additionally, it also provides a handy set of [patterns]{.yellow} that can be used to [identify]{.yellow} such [problems]{.yellow} before using the metadata, or for the purpose of cleaning.]{.absolute left="20%" top="30%" right="20%"}

##  Benefits

[It [characterizes metadata]{.yellow} from Crossref and OpenAlex in a way that has not been done before.]{.absolute left="20%" top="30%" right="20%"}

##  Benefits

[Introduces the [Crossref XML API]{.yellow} as part of analyzing metadata completeness and accuracy.]{.absolute left="20%" top="30%" right="20%"}

##  Benefits

[It contributes to research on the [effect of perturbations]{.yellow} in embedding and generation models.]{.absolute left="20%" top="30%" right="20%"}

##  Benefits

[Raised my own skills in [Python]{.yellow}, knowledge of how [search and retrieval]{.yellow} is changing, how [LLMs]{.yellow} work and their limitations.]{.absolute left="20%" top="30%" right="20%"}

# Thank you!

And now, its time for questions.

# Sneak peek
Who wants to see some results?

## Part 1 summary results {.scrollable}
::: notes
put results into nice infographics, but use what you have first
:::
![](images/part_2.svg){width="587"}

## Part 2 summary results {.scrollable}
::: notes
put results into nice infographics, but use what you have first
:::
![](images/part_2.svg){width="587"}

## Part 3 summary results {.scrollable}
::: notes
no notes
:::
[Metrics for each RAG pipeline using the same queries.]{.italic}

![](images/part_3_results_all.svg){ fig-align="center"}

[Mean document scores for top DOIs]{.italic}

![](images/part_3_doc_score.svg){fig-align="center"}


# Thank you!
Poppy Riddle
[Department of Information Science PhD Program]{.blue}

## References
insert bibliography here