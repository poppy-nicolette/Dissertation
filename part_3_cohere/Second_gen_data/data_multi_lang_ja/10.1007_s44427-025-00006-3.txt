DOI: 10.1007/s44427-025-00006-3
Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas
Abstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.
