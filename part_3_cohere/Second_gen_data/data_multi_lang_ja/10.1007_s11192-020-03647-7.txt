DOI: 10.1007/s11192-020-03647-7
Title: 書誌学的分析におけるサンプルサイズ Sample size in bibliometric analysis
Abstract: 書誌学的分析は通常、完全な出版物セットに依存できますが、これは普遍的に当てはまるわけではありません。たとえば、オーストラリア (ERA 内) と英国 (RAE/REF 内) は、研究者の成果のごく一部または一部に依存する可能性のある機関研究評価を使用しています。 2014年から2018年にWeb of Scienceにインデックス付けされた同様の成果物（21,000～28,000件の論文とレビュー）を持つ10の大学の出版物に対してカテゴリ正規化引用影響度（CNCI）を使用して、機関データの「サンプル」が母集団CNCIの平均および/または正しい相対的状態を正確に表すことができる程度を調査します。完全な施設データから始めて、200 件未満の論文の 10,000 施設のサンプルにわたって平均 CNCI に大きなばらつきがあることがわかり、これは分析上の最小値である可能性があることを示唆していますが、定性的レビューにはより小さなサンプルが許容される可能性があります。 DAIS-ID クラスターで代表される研究者セットの「トップ」CNCI 論文を検討すると、1,000 件の論文のサンプルが相対的な (絶対的ではない) 機関引用パフォーマンスの良いガイドとなることがわかります。これは、多数の優秀な成績を収めた個人によって左右されます。ただし、そのようなサンプルは、小規模または研究集約度の低い単位での「引用度の高い」論文が少ないことによって混乱する可能性があります。私たちは、評価プロセスにおけるこれの重要性と、大学のランキングが本質的に不安定で一般的に信頼できないというさらなる証拠に注目します。 While bibliometric analysis is normally able to rely on complete publication sets this is not universally the case. For example, Australia (in ERA) and the UK (in the RAE/REF) use institutional research assessment that may rely on small or fractional parts of researcher output. Using the Category Normalised Citation Impact (CNCI) for the publications of ten universities with similar output (21,000–28,000 articles and reviews) indexed in the Web of Science for 2014–2018, we explore the extent to which a ‘sample’ of institutional data can accurately represent the averages and/or the correct relative status of the population CNCIs. Starting with full institutional data, we find a high variance in average CNCI across 10,000 institutional samples of fewer than 200 papers, which we suggest may be an analytical minimum although smaller samples may be acceptable for qualitative review. When considering the ‘top’ CNCI paper in researcher sets represented by DAIS-ID clusters, we find that samples of 1000 papers provide a good guide to relative (but not absolute) institutional citation performance, which is driven by the abundance of high performing individuals. However, such samples may be perturbed by scarce ‘highly cited’ papers in smaller or less research-intensive units. We draw attention to the significance of this for assessment processes and the further evidence that university rankings are innately unstable and generally unreliable.
