{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere API and SciBERT with BM25 as first stage retriever for RAG\n",
    "This notebook uses a Cohere API for generating responses to text. A query input is required from the user. \n",
    "SciBERT is used for embeddings in a dense vector array for the query. \n",
    "This version is different in that it uses BM25 as a sparse vectorizer for the input text. Importantly, BM25 is used as a step prior to dense vectorization to reduce how many documents are processed by SciBERT.\n",
    "A DOI is supplied with the text as both an identifier and locator. \n",
    "\n",
    "## pipeline\n",
    "1. BM25 Retrieval\n",
    "    - BM25 is used to retrieve top-k candidate documents based on keyword matching\n",
    "2. Dense embedding retrieval\n",
    "    - query is embedded using SciBERT and the retrieved documents.\n",
    "3. Re-ranking\n",
    "    - cosine similarity between query embedding and document embedding to rerank candidate docs\n",
    "4. Generation\n",
    "    - docs and query are fed to generator for answer creation. \n",
    "\n",
    "- [ ] set up venv\n",
    "- [ ] install transformers torch cohere in command line\n",
    "\n",
    "### todo\n",
    "- [ ] create script that compiles data/documents.txt with DOI || text for all documents\n",
    "- [ ] rank_bm25: https://github.com/dorianbrown/rank_bm25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import cohere\n",
    "from cohere import Client\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import logging # finding where functions are taking too long\n",
    "\n",
    "\n",
    "def main():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    global key,email\n",
    "    key = os.getenv('COHERE_API_KEY')\n",
    "    email = os.getenv('EMAIL')\n",
    "\n",
    "    #verify if it worked\n",
    "    if email is not None and key is not None:\n",
    "        print(\"all is good, beautiful!\")\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is callable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(key) \n",
    "\n",
    "# Load SciBERT model and tokenizer\n",
    "\"\"\"\n",
    "documentation can be found here: https://huggingface.co/docs/transformers/v4.50.0/en/model_doc/auto#transformers.AutoTokenizer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Initialize tokenizer with custom parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    max_len=512,\n",
    "    use_fast=True,  # Use the fast tokenizer\n",
    "    do_lower_case=False,  # Preserve case\n",
    "    add_prefix_space=False,  # No prefix space\n",
    "    never_split=[\"[DOC]\", \"[REF]\"],  # Tokens to never split\n",
    "    additional_special_tokens=[\"<doi>\", \"</doi>\"]  # Add custom special tokens\n",
    ")\n",
    "\n",
    "# This is the SciBERT model that is used to embed the text and query.\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "#verify that the model is callable\n",
    "if callable(model):\n",
    "    print(\"Model is callable\")\n",
    "else:\n",
    "    print(\"Model is not callable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<string>, line 45)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mFile \u001b[39m\u001b[32m<string>:45\u001b[39m\n",
      "\u001b[31m    \u001b[39m\u001b[31mdocuments_with_doi = [] #initialize list\u001b[39m\n",
      "                                            ^\n",
      "\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic RAG with BN25 retriever with Cohere model\n",
    "Document source: data folder \n",
    "Saved as UTF-8\n",
    "\n",
    "Returns:  answers based on query from input()\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate embeddings using SciBERT\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    converts raw text to numerical representations using a pretrained model, in this case, SciBERT.\n",
    "    Currently this is applied to both the document text and the query. \n",
    "    May want a different version or decorator for the query as they are generally much shorter and more sparse.\n",
    "\n",
    "    Input: text from tokenizer step above as a list of strings\n",
    "    Output: np.array\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512, # returns PyTorch tensors which are compatible with model\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True # return the attention mask - need to learn more\n",
    "        )\n",
    "\n",
    "    # this passes the tokenized inputs through the model    \n",
    "    outputs = model(**inputs)\n",
    "    #applies mean pooling to get a fixed size embedding - there are other methods to explore\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Function to read documents and their DOIs from a file\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "    documents_with_doi = [] #initialize list\n",
    "    \n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines)>=1:\n",
    "                    doi = lines[0].strip().replace(\"DOI: \",\"\")\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append({'doi': doi, \"text\":text})\n",
    "    return documents_with_doi\n",
    "\n",
    "\n",
    "\n",
    "# Path to the file containing documents and DOIs\n",
    "directory_path = \"data/\"  # Replace with your file path\n",
    "\n",
    "# Read documents and DOIs from the file\n",
    "documents_with_doi = read_documents_with_doi(directory_path)\n",
    "\n",
    "# Extract document texts and DOIs\n",
    "documents = [doc[\"text\"] for doc in documents_with_doi]\n",
    "dois = [doc[\"doi\"] for doc in documents_with_doi]\n",
    "\n",
    "# Example query\n",
    "query = input(\" What is your query: \")\n",
    "\n",
    "# Generate document embeddings\n",
    "document_embeddings = generate_embeddings(documents)\n",
    "# print(document_embeddings.shape) # to see the output shape of the array\n",
    "\n",
    "# Generate query embedding\n",
    "query_embedding = generate_embeddings([query])[0] # generates np.array for the query text\n",
    "\n",
    "# Function to retrieve top-k documents using cosine similarity\n",
    "def retrieve_documents(query_embedding: np.ndarray, document_embeddings: List[np.ndarray], top_k: int = 3) -> List[Tuple[float, Dict[str, str]]]:\n",
    "    similarities = []\n",
    "    for doc_emb in document_embeddings:\n",
    "        # cosine similarity\n",
    "        similarity = np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb)) \n",
    "        similarities.append(similarity)\n",
    "    # ranking\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    return [(similarities[i], documents_with_doi[i]) for i in top_indices]\n",
    "\n",
    "# Retrieve top documents\n",
    "top_documents = retrieve_documents(query_embedding, document_embeddings)\n",
    "print(\"Retrieved Documents:\")\n",
    "for score, doc in top_documents:\n",
    "    print(f\"Score: {score:.4f}, DOI: {doc['doi']}, Document: {doc['text']}\")\n",
    "\n",
    "# prepare context for Cohere's Command model (include DOI) - need to add in cited by here\n",
    "context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for _, doc in top_documents])\n",
    "# need to learn how to improve this\n",
    "prompt = f\"Query: {query}\\nContext: {context}\\nAnswer: Include the DOI of the referenced document in your response.\"\n",
    "\n",
    "# Generate response using Cohere's Command model\n",
    "response = co.generate(\n",
    "  model=\"command\",\n",
    "  prompt=prompt,\n",
    "  max_tokens=150, #allowable length of response\n",
    "  temperature=0.2 #may want to make this a variable\n",
    ")\n",
    "\n",
    "# Print the generated response\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response.generations[0].text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
