{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere API and SciBERT for RAG - V5\n",
    "This notebook uses a Cohere API for generating responses to text. A query input is required from the user. \n",
    "SciBERT is used for embeddings in a dense vector array both the text and the query. \n",
    "A DOI is supplied with the text as both an identifier and locator. \n",
    "\n",
    "- [x] set up venv\n",
    "- [x] install transformers torch cohere in command line\n",
    "\n",
    "### todo\n",
    "- [ ] create script that compiles data/documents.txt with DOI || text for all documents\n",
    "- [ ] reduce code by refactoring into modules\n",
    "- [ ] store vectorized documents in a db\n",
    "    - https://huggingface.co/learn/cookbook/rag_with_hugging_face_gemma_mongodb\n",
    "\n",
    "### options\n",
    "- Batch Processing:\n",
    "    If large number of texts, process them in batches to avoid memory issues.\n",
    "    Example: Use a loop or torch.utils.data.DataLoader.\n",
    "\n",
    "- Change model size: smaller models require less processing\n",
    "\n",
    "- fine tune model on corpus - i don't think this is an option\n",
    "\n",
    "- look into pooling strategies\n",
    "- concurrency? \n",
    "\n",
    "- Tokenizer\n",
    "    - put cleaning process distincly prior to the tokenizer, using the default values as much as possible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cohere\n",
    "from cohere import Client\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import sys\n",
    "\n",
    "# load secret from local .env file\n",
    "def get_key():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    _key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "    #verify if it worked\n",
    "    if _key is not None:\n",
    "        print(\"all is good, beautiful!\")\n",
    "        return _key\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all is good, beautiful!\n",
      "Model is callable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initialize Cohere client with key from secrets\n",
    "co = cohere.ClientV2(get_key())\n",
    "\n",
    "# load SciBERT model and tokenizer\n",
    "\"\"\"\n",
    "Autotokenizer documentation can be found here: https://huggingface.co/docs/transformers/v4.50.0/en/model_doc/auto#transformers.AutoTokenizer\n",
    "\n",
    "Model documentation can be found here: https://huggingface.co/allenai/scibert_scivocab_uncased\n",
    "Citation for SciBERT:\n",
    "@inproceedings{beltagy-etal-2019-scibert,\n",
    "    title = \"SciBERT: A Pretrained Language Model for Scientific Text\",\n",
    "    author = \"Beltagy, Iz  and Lo, Kyle  and Cohan, Arman\",\n",
    "    booktitle = \"EMNLP\",\n",
    "    year = \"2019\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://www.aclweb.org/anthology/D19-1371\"\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Initialize tokenizer with custom parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"allenai/specter\",\n",
    "    max_len=512,\n",
    "    #use_fast=True,  # Use the fast tokenizer\n",
    "    do_lower_case=False,  # Preserve case\n",
    "    add_prefix_space=False,  # No prefix space\n",
    "    never_split=[\"[DOC]\", \"[REF]\"],  # Tokens to never split\n",
    "    #additional_special_tokens=[\"<doi>\", \"</doi>\"],  # Add custom special tokens ***RE-EVALUATE*** (tuple or list of str or tokenizers.AddedToken, optional) — A tuple or a list of additional special tokens. Add them here to ensure they are skipped when decoding with skip_special_tokens is set to True. If they are not part of the vocabulary, they will be added at the end of the vocabulary.\n",
    "    skip_special_tokens=False,\n",
    ")\n",
    "\n",
    "# this is the SciBERT model that is used to embed the text and query.\n",
    "# other models: 'allenai-specter', \n",
    "#documentation here: https://huggingface.co/docs/transformers/model_doc/auto and https://github.com/allenai/specter\n",
    "# load model directly\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"allenai/specter\")\n",
    "model = AutoModel.from_pretrained(\"allenai/specter\", torch_dtype=\"auto\")\n",
    "\n",
    "#model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", torch_dtype=\"auto\")\n",
    "#may also want to consider using a sentence embedding model\n",
    "\n",
    "###***** Specter2 use *********\n",
    "\"\"\"\n",
    "Should probably make a new notebook as it requires the use of adapters\n",
    "Reference:\n",
    "    https://huggingface.co/allenai/specter2\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#verify that the model is callable\n",
    "if callable(model):\n",
    "    print(\"Model is callable\")\n",
    "else:\n",
    "    print(\"Model is not callable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V3: Dense retriever only\n",
    "- set with parameters the same as BM25 pre-retriever version\n",
    "\n",
    "calls a JSON file of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Basic RAG using Cohere Command model\n",
    "Dense retrieval of embedded query and pre-retrieved documents\n",
    "Document source: data\n",
    "\n",
    "Returns: responses based on query from input()\n",
    "\"\"\"\n",
    "# set top_k global value - keep this as constant for all evaluations\n",
    "global top_k\n",
    "top_k = 5\n",
    "\n",
    "#function to generate embeddings using SciBERT\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    converts raw text to numerical representations using a pretrained model, in this case, SciBERT.\n",
    "    Currently this is applied to both the document text and the query. \n",
    "    May want a different version or decorator for the query as they are generally much shorter and more sparse.\n",
    "\n",
    "    Input: text from tokenizer step above as a list of strings\n",
    "    Output: np.array\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True)\n",
    "    # this passes the tokenized inputs through the model\n",
    "    outputs = model(**inputs)\n",
    "    #this uses mean pooling - may want to investigate other methods\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0].strip().replace(\"DOI: \", \"\")\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append({\"doi\": doi, \"text\": text})\n",
    "    return documents_with_doi\n",
    "\n",
    "\n",
    "# Function to update chat history\n",
    "def update_chat_history(query, retrieved_docs, response)->None:\n",
    "    global chat_history\n",
    "    try:\n",
    "        chat_history.append({\n",
    "            \"query\": query,\n",
    "            \"retrieved_docs\": [doc[\"text\"] for doc in retrieved_docs],  # Store only the text of retrieved documents\n",
    "            \"response\": response\n",
    "        })\n",
    "    except:\n",
    "        chat_history.append({\n",
    "            \"query\": query,\n",
    "            \"retrieved_docs\": None,\n",
    "            \"response\": response\n",
    "        })\n",
    "\n",
    "\n",
    "#function to incorporate history into the next query\n",
    "def get_context_with_history(query) -> str:\n",
    "    global chat_history # also declare here since chat_history is being modified\n",
    "    if not chat_history:\n",
    "        return None\n",
    "    else:\n",
    "        for entry in chat_history:\n",
    "            history_str = \"\\n\".join([\n",
    "                f\"User: {entry['query']}\\n\"\n",
    "                f\"Context: {'; '.join(entry['retrieved_docs'])}\\n\"\n",
    "                f\"Response: {entry['response']}\"])\n",
    "     \n",
    "    return f\"Chat History:\\n{history_str}\\nCurrent Query: {query}\"\n",
    "\n",
    "#function to truncate chat history\n",
    "def truncate_chat_history(max_length=3):\n",
    "    global chat_history # modifies it so it also must be global\n",
    "    if len(chat_history) > max_length:\n",
    "        chat_history = chat_history[-max_length:]\n",
    "\n",
    "\n",
    "def retrieve_documents(query: str, threshold:float, documents:List) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Dense retriever: retrieves documents from the embedded documents and performs cosine similarity for similarities score\n",
    "\n",
    "    Args:\n",
    "        query: this is the query passed\n",
    "        threshold: value for similarity cutoff\n",
    "    Returns:\n",
    "        List of dictionaries containing strings as key/value pairs\n",
    "    \"\"\"\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    document_embeddings = generate_embeddings(documents)\n",
    "    #cosine similarity\n",
    "    similarities = [\n",
    "        np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
    "        for doc_emb in document_embeddings\n",
    "    ]\n",
    "    # Filter documents based on the threshold\n",
    "    filtered_indices = [i for i, sim in enumerate(similarities) if sim >= threshold]\n",
    "    #sorts on similarity score then reverses order, slices only to top_k\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    try:\n",
    "        return [documents_with_doi[i] for i in top_indices]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def retrieve_documents_2(query:str, threshold:float, documents:List,documents_with_doi:List) -> List[Dict[str,str]]:\n",
    "    \"\"\"\n",
    "    Dense retriever: retrieves documents from the embedded documents and performs cosine similarity for similarities score\n",
    "\n",
    "    Args:\n",
    "        query: this is the query passed\n",
    "        threshold: value for similarity cutoff\n",
    "    Returns:\n",
    "        List of dictionaries containing strings as key/value pairs\n",
    "    \"\"\"\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    document_embeddings = generate_embeddings(documents)\n",
    "    #cosine similarity\n",
    "    similarities = [\n",
    "        np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
    "        for doc_emb in document_embeddings\n",
    "    ]\n",
    "    #test_tuple = (i,float(sim) for i,sim in enumerate(similarities) if sim >= threshold)\n",
    "    test_tuple = []\n",
    "    for i,sim in enumerate(similarities):\n",
    "        if sim >= threshold:\n",
    "            a = (i,sim)\n",
    "            test_tuple.append(a)\n",
    "    toppy_top = sorted(test_tuple,key=lambda score: score[1], reverse=True)\n",
    "    filtered_list = toppy_top[:5]\n",
    "    global top_indices\n",
    "    top_indices = [idx for idx,score in filtered_list]\n",
    "    try:\n",
    "        return [documents_with_doi[i] for i in top_indices]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "#RAG pipeline function\n",
    "def rag_pipeline(query):\n",
    "\n",
    "\n",
    "    # Main loop for user interaction \n",
    "    # changed to run with no history for the purposes of the test\n",
    "    global chat_history\n",
    "    chat_history = []#initialize chat history\n",
    "    \n",
    "    #incorporate chat history\n",
    "    full_context = get_context_with_history(query)\n",
    "    # let user know you are generating...\n",
    "    print(\"Retrieving documents and generating response...\")\n",
    "\n",
    "    #retrieve documents\n",
    "    # ***** SET THRESHOLD LEVEL HERE!********\n",
    "    #retrieved_docs = retrieve_documents(query,0.80)\n",
    "    retrieved_docs = retrieve_documents_2(query,0.1,documents,documents_with_doi)\n",
    "\n",
    "\n",
    "    #prepare context for Cohere's Command model\n",
    "    instruction = \"\"\"You are an academic research assistant.\n",
    "                    If there are 0 documents in context, then state that you can not provide an answer.\n",
    "                    If there is context, your response should be structured in a paragraph of less than 250 words in the following order: \n",
    "                    Summary answer of once sentence.\n",
    "                    DOI: {insert DOI here} - Summary of supporting document 1\n",
    "                    DOI: {insert DOI here} - Summary of supporting document 2\n",
    "                    Concluding statement. \n",
    "                    \"\"\"  \n",
    "    # add full_context if exists, else just context. \n",
    "    if retrieved_docs:    \n",
    "        if full_context:\n",
    "            context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for doc in retrieved_docs]).join(full_context)\n",
    "        else:\n",
    "            context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for doc in retrieved_docs])\n",
    "    else:\n",
    "        context = f\"There are 0 documents related to your query.\"\n",
    "    prompt = f\"Instruction: {instruction}\\nQuery: {query}\\nContext: {context}\\n\"\n",
    "    \n",
    "    \n",
    "    # Generate response - updated to V2 - see docs: https://docs.cohere.com/reference/chat\n",
    "\n",
    "    #V2\n",
    "    response = co.chat(\n",
    "        model=\"command-a-03-2025\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        ).message.content[0].text\n",
    "    # Update chat history\n",
    "    update_chat_history(query, retrieved_docs, response)\n",
    "    \n",
    "    # Truncate history if necessary\n",
    "    truncate_chat_history()\n",
    "\n",
    "    # Print the response\n",
    "    print(\"\\nGenerated Response:\")\n",
    "    print(response)\n",
    "    print(f\"------\\nSource documents: \")#used for debugging\n",
    "    try:\n",
    "        for doc in retrieved_docs:\n",
    "            doi_with_retriever = f\"https://doi.org/{doc['doi']}\"\n",
    "            print(f\"DOI: {doi_with_retriever}, {doc['text'].split(\"\\n\",1)[0]}\")\n",
    "        return response, retrieved_docs\n",
    "    except:\n",
    "        print(f\"No documents found\")\n",
    "        return response, retrieved_docs\n",
    "\n",
    "#***** Begin chat session *****\n",
    "\n",
    "# Path to the file containing documents and DOIs\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_jats\"\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_multi_lang\"\n",
    "\n",
    "# Read documents and DOIs from the file\n",
    "#documents_with_doi = read_documents_with_doi(directory_path)\n",
    "#documents = [doc[\"text\"] for doc in documents_with_doi]\n",
    "#print(f\"Length of documents: {len(documents)}\")\n",
    "#print(f\"Length of corpus: {len(documents_with_doi)}\")\n",
    "\n",
    "#query = input(\"What is your query (or type 'exit' to quit): \")\n",
    "\n",
    "#rag_pipeline(query)\n",
    "\n",
    "\n",
    "\n",
    "# Main loop for user interaction\n",
    "##chat_history = []#initialize chat history\n",
    "#while True:\n",
    "\n",
    "    #uery = input(\"What is your query (or type 'exit' to quit): \")\n",
    "    #if query.lower() == \"exit\":\n",
    "    #    break\n",
    "    #rag_pipeline(query)\n",
    "\n",
    "    #print(f\"time to query loop: {time_query:.2f} seconds\")\n",
    "    #print(f\"to to retrieve: {retrieve_time:.2f} seconds\")\n",
    "    #print(f\"time to generate: {generate_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "## Test One: \n",
    "- [✅] compute precision, recall, and F1-Scores. \n",
    "- [✅] added accuracy score\n",
    "- [ ] compare text from each source, embedded, and them similarity scores based on embeddings.\n",
    "    - [ ] token based SciBERT embedding\n",
    "    - [ ] sentence-based SentenceBERT embedding\n",
    "### optional analysis\n",
    "Need to learn more about attention weights and their analysis\n",
    "- [✅] heatmap of attention weights for two given inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, recall, F1 score\n",
    "### references\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "- https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from colorama import Fore, Back, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query, Precision, Recall, F1-Score, Accuracy, Balanced accuracy, Faithfulness score, Response]\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial dataframe to capture results from each query and results\n",
    "#ONLY DO THIS AT THE BEGINNING OF THE ANALYSIS PROCEDURE, OTHERWISE, IT WILL ERASE THE PREVIOUS RESULTS!!\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Query','Precision','Recall','F1-Score','Accuracy', 'Balanced accuracy', 'Faithfulness score', 'Response'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "evaluate_retrieval() function takes 4 inputs and generates a dictionary of results to be \n",
    "printed out by the function print_results()\n",
    "\n",
    "print_results() function takes the 4 inputs and passes them to evaluate_retrieval(). \n",
    "print_results is the only one call in the test_loop() function and calls evaluate_retrieval() for \n",
    "each run of the pipeline function. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def evaluate_retrieval(retrieved_dois, ground_truth, response, query:str)->Dict:\n",
    "    corpus_doi_list = []\n",
    "    #corpus_list is a global variable in rag_pipeline()\n",
    "    for each in range(len(documents_with_doi)):\n",
    "        a = documents_with_doi[each].get('doi',\"\")\n",
    "        corpus_doi_list.append(a)\n",
    "    print(len(corpus_doi_list))\n",
    "\n",
    "    def compare_lists(list1, list2, list3):\n",
    "        for val in list1:\n",
    "            if val in list2:\n",
    "                list3.append(1)\n",
    "            else:\n",
    "                list3.append(0)\n",
    "\n",
    "    #set y_true so that len(y_true)==len(corpus_doi_list)\n",
    "    y_true = []\n",
    "    compare_lists(corpus_doi_list,ground_truth,y_true)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = []\n",
    "    compare_lists(corpus_doi_list,retrieved_dois,y_pred)\n",
    "\n",
    "\n",
    "    # calculate metrics - could also use sklearn.metrics functions such as precision_score, but this is easier to read\n",
    "    precision = precision_score(y_true, y_pred, average=\"micro\")\n",
    "    recall = recall_score(y_true, y_pred,average=\"micro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    faithfulness_score = 0\n",
    "    for each in retrieved_dois:\n",
    "        if each in response:\n",
    "            faithfulness_score+=1\n",
    "        else:\n",
    "            faithfulness_score+=0\n",
    "\n",
    "    return {\n",
    "        'Query':f\"{query}\",\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\":accuracy,\n",
    "        \"Balanced accuracy\":balanced_accuracy,\n",
    "        \"Faithfulness score\":faithfulness_score,\n",
    "        \"Response\":response\n",
    "    }\n",
    "\n",
    "def print_results(retrieved_dois, ground_truth, response, query:str)->Dict:\n",
    "    \"\"\"\n",
    "    Prints a nicely ordered set of results from evalaute_retrieval()\n",
    "    \"\"\"\n",
    "\n",
    "    results = evaluate_retrieval(retrieved_dois, ground_truth, response, query)\n",
    "    print(f\"For query: {results['Query']}:\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"F1-Score: {results['F1-Score']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    print(f\"Balanced accuracy: {results['Balanced accuracy']:.3f}\")\n",
    "    print(f\"Faithfulness score: {results['Faithfulness score']}\")\n",
    "    return results\n",
    "\n",
    "#print_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [✅] load golden_set.xlsx\n",
    "- [✅] iterate through the list 5 times\n",
    "- [✅] add sleep that prevents API overusage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>n_ground_truth</th>\n",
       "      <th>expected_response</th>\n",
       "      <th>Refernces:</th>\n",
       "      <th>Response\\nBM25</th>\n",
       "      <th>Response\\nDense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>which studies examined the abstract in metadata?</td>\n",
       "      <td>[\"10.1002/leap.1411\",\"10.1007/s11192-020-03632...</td>\n",
       "      <td>5</td>\n",
       "      <td>Lexical content changes in abstracts during th...</td>\n",
       "      <td>DOI: 10.1002/leap.1411\\nInvestigates lexical c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>which studies examined citations?</td>\n",
       "      <td>[\"10.1007/s11192-022-04367-w\",\"10.1371/journal...</td>\n",
       "      <td>5</td>\n",
       "      <td>Identifying and correcting invalid citations d...</td>\n",
       "      <td>DOI: 10.1007/s11192-022-04367-w\\nFocuses on id...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tell me about OpenAlex.</td>\n",
       "      <td>[\"10.3145/epi.2023.mar.09\",\"10.1590/SciELOPrep...</td>\n",
       "      <td>7</td>\n",
       "      <td>OpenAlex is presented as a promising open-sour...</td>\n",
       "      <td>DOI: 10.3145/epi.2023.mar.09\\nCompares OpenAle...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tell me about Crossref.</td>\n",
       "      <td>[\"10.1162/qss_a_00212\",\"10.31274/b8136f97.ccc3...</td>\n",
       "      <td>9</td>\n",
       "      <td>Crossref is portrayed as a major source of sch...</td>\n",
       "      <td>DOI: 10.1162/qss_a_00212 - Examines Crossref’s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which papers evaluate the linguistic coverage ...</td>\n",
       "      <td>[\"10.1007/s11192-015-1765-5\",\"10.48550/arXiv.2...</td>\n",
       "      <td>5</td>\n",
       "      <td>Biases in Traditional Databases: WoS and Scopu...</td>\n",
       "      <td>DOI: 10.1007/s11192-015-1765-5 - Compares the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Which papers address funding metadata, its ava...</td>\n",
       "      <td>[\"10.1162/qss_a_00210\",\"10.1162/qss_a_00212\",\"...</td>\n",
       "      <td>5</td>\n",
       "      <td>Assessing Availability: Highlighting gaps in f...</td>\n",
       "      <td>DOI: 10.1162/qss_a_00210 - Examines the availa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Which papers discuss the use of Retrieval-Augm...</td>\n",
       "      <td>[\"10.1007/978-3-031-88708-6_3\",\"10.1609/aaai.v...</td>\n",
       "      <td>5</td>\n",
       "      <td>Evaluation and Benchmarking: Diagnosing RAG’s ...</td>\n",
       "      <td>DOI: 10.1007/978-3-031-88708-6_3 \\nInvestigate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is Crossref’s role in the scholarly resea...</td>\n",
       "      <td>[\"10.1162/qss_a_00022\",\"10.1162/qss_a_00210\",\"...</td>\n",
       "      <td>5</td>\n",
       "      <td>Crossref plays a central role in the scholarly...</td>\n",
       "      <td>DOI: 10.1162/qss_a_00022\\n Describes Crossref ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the key features and limitations of O...</td>\n",
       "      <td>[\"10.3145/epi.2023.mar.09\",\"10.1590/SciELOPrep...</td>\n",
       "      <td>5</td>\n",
       "      <td>OpenAlex is highlighted as a promising open-so...</td>\n",
       "      <td>DOI: 10.3145/epi.2023.mar.09\\nKey Features: Pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the strengths and weaknesses of Web o...</td>\n",
       "      <td>[\"10.1007/s11192-015-1765-5\",\"10.1162/qss_a_00...</td>\n",
       "      <td>5</td>\n",
       "      <td>Web of Science (WoS) is recognized for its str...</td>\n",
       "      <td>DOI: 10.1007/s11192-015-1765-5\\nStrengths: Com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How is RAG used to improve question answering ...</td>\n",
       "      <td>[\"10.1007/978-3-031-88708-6_3\",\"10.1109/ACCESS...</td>\n",
       "      <td>5</td>\n",
       "      <td>RAG is used to improve question answering and ...</td>\n",
       "      <td>DOI: 10.1007/978-3-031-88708-6_3\\nApplication:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What are the main challenges in normalizing ci...</td>\n",
       "      <td>[\"10.1007/s11192-015-1765-5\",\"10.1162/qss_a_00...</td>\n",
       "      <td>5</td>\n",
       "      <td>The main challenges in normalizing citation me...</td>\n",
       "      <td>DOI: 10.1007/s11192-015-1765-5\\nChallenge: Bia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What methods are used to detect and correct er...</td>\n",
       "      <td>[\"10.5281/ZENODO.13960973\",\"10.1007/s11192-022...</td>\n",
       "      <td>5</td>\n",
       "      <td>Methods used to detect and correct errors in b...</td>\n",
       "      <td>DOI: 10.5281/ZENODO.13960973\\nMethod: Uses mis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tell me about how RAG works.</td>\n",
       "      <td>[\"10.1007/978-3-031-88708-6_3\",\"10.1609/aaai.v...</td>\n",
       "      <td>5</td>\n",
       "      <td>These papers explain RAG as a framework that:\\...</td>\n",
       "      <td>DOI: 10.1007/978-3-031-88708-6_3\\nInvestigates...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  ... Response\\nDense\n",
       "0    which studies examined the abstract in metadata?  ...             NaN\n",
       "1                   which studies examined citations?  ...             NaN\n",
       "2                             Tell me about OpenAlex.  ...             NaN\n",
       "3                             Tell me about Crossref.  ...             NaN\n",
       "4   Which papers evaluate the linguistic coverage ...  ...             NaN\n",
       "5   Which papers address funding metadata, its ava...  ...             NaN\n",
       "6   Which papers discuss the use of Retrieval-Augm...  ...             NaN\n",
       "7   What is Crossref’s role in the scholarly resea...  ...             NaN\n",
       "8   What are the key features and limitations of O...  ...             NaN\n",
       "9   What are the strengths and weaknesses of Web o...  ...             NaN\n",
       "10  How is RAG used to improve question answering ...  ...             NaN\n",
       "11  What are the main challenges in normalizing ci...  ...             NaN\n",
       "12  What methods are used to detect and correct er...  ...             NaN\n",
       "13                       tell me about how RAG works.  ...             NaN\n",
       "\n",
       "[14 rows x 7 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_set_df = pd.read_excel(\"golden_set.xlsx\")\n",
    "#golden_set_df_test = golden_set_df.head(3)\n",
    "golden_set_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of documents: 45\n",
      "Length of corpus: 45\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Several studies have examined the availability, completeness, and quality of abstracts in metadata across various bibliographic databases.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This study evaluates the availability of six metadata elements, including abstracts, in Crossref, noting improvements over time but highlighting the need for further efforts by publishers to ensure full openness of bibliographic metadata.  \n",
      "DOI: 10.1162/qss_a_00286 - The research compares the completeness degree of publication metadata, including abstracts, in eight free-access scholarly databases, finding that third-party databases generally have higher metadata quality and completeness rates compared to academic search engines.  \n",
      "\n",
      "Concluding statement: These studies collectively emphasize the importance of abstract metadata in bibliographic databases, with a focus on its availability, completeness, and reliability across different platforms.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00286', '10.48550/arXiv.2404.17663']\n",
      "45\n",
      "For query: which studies examined the abstract in metadata?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 2\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 0\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary answer:** Several studies have examined citations, focusing on the comparison of bibliographic databases, citation metrics, and the completeness of metadata in scholarly databases.  \n",
      "\n",
      "DOI: 10.1162/qss_a_00112 - This study conducts a large-scale comparison of five bibliographic data sources (Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic), analyzing differences in document coverage and the completeness and accuracy of citation links across disciplines and time periods.  \n",
      "DOI: 10.1371/journal.pbio.1002542 - The study discusses the challenges and approaches to normalizing citation metrics, addressing issues such as field differences, document types, and database coverage, while emphasizing the need for careful consideration of underlying assumptions in citation analysis.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - This research critically assesses OpenAlex as a bibliometric data source by comparing it to Scopus, concluding that while OpenAlex is a reliable alternative for certain analyses, metadata accuracy and completeness remain areas for improvement.  \n",
      "\n",
      "**Concluding statement:** These studies collectively highlight the importance of understanding the strengths and limitations of different bibliographic databases and citation metrics in scholarly research, particularly in the context of emerging inclusive databases like OpenAlex.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00286']\n",
      "45\n",
      "For query: which studies examined citations?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 1\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: OpenAlex is an emerging inclusive bibliometric database that, while showing promise as a reliable alternative to traditional databases like Scopus, still requires further research to address limitations in metadata accuracy and completeness.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - This study compares OpenAlex to Scopus, concluding that OpenAlex is a superset of Scopus and a viable alternative for certain analyses, particularly at the country level, but highlights issues with metadata accuracy and completeness that need further investigation.  \n",
      "DOI: 10.31222/osf.io/smxe5 - While not directly about OpenAlex, this document discusses the improving availability of open bibliographic metadata in Crossref, which is relevant to the broader context of open data infrastructures that OpenAlex leverages.  \n",
      "DOI: 10.1162/qss_a_00212 - This study focuses on funding data in Crossref and proprietary databases, indirectly underscoring the importance of open data infrastructures like OpenAlex in addressing gaps in traditional databases.  \n",
      "\n",
      "Concluding statement: OpenAlex represents a significant step toward more inclusive and open bibliometric data, but ongoing efforts to enhance its metadata quality and completeness are essential for its broader adoption and reliability in scientometric analyses.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.1162/qss_a_00022, Title: Crossref: The sustainable source of community-owned scholarly metadata\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00212', '10.1371/journal.pbio.1002542', '10.1162/qss_a_00022']\n",
      "45\n",
      "For query: Tell me about OpenAlex.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.519\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 2\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Crossref is a key provider of open bibliographic metadata, offering improved availability of metadata elements over time, though challenges remain in achieving full openness and metadata quality.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref has seen improvements in the availability of metadata elements like reference lists, abstracts, and ORCIDs, particularly for journal articles, but many publishers still need to enhance efforts for full openness.  \n",
      "DOI: 10.1162/qss_a_00112 - A large-scale comparison of bibliographic data sources, including Crossref, highlights its strengths in comprehensive coverage but also underscores the need for flexible filters to address differences in document coverage and citation accuracy.  \n",
      "DOI: 10.5860/crl.86.1.101 - Metadata quality, consistency, and completeness in systems like Crossref are influenced by cultural and strategic choices, revealing tensions between standardization and sociocultural representation.  \n",
      "\n",
      "Concluding statement: Crossref plays a vital role in advancing open bibliographic metadata, yet ongoing efforts are necessary to address gaps in openness, metadata quality, and representation across diverse contexts.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.5860/crl.86.1.101, Title: Identifying Metadata Quality Issues Across Cultures\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112', '10.5860/crl.86.1.101']\n",
      "45\n",
      "For query: Tell me about Crossref.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.569\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 3\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Several papers evaluate the linguistic coverage and language-related metadata in scholarly databases, focusing on completeness, accuracy, and comparisons across different platforms.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 - This study assesses the linguistic coverage of OpenAlex by comparing its language metadata with Web of Science (WoS) and manually validating a sample of 6,836 articles. It finds that OpenAlex has more balanced linguistic coverage than WoS but notes inaccuracies in language metadata, leading to overestimation of English and underestimation of other languages.  \n",
      "DOI: 10.1162/qss_a_00286 - This paper evaluates the completeness of publication metadata, including language, in eight free-access scholarly databases. It highlights that academic search engines like Google Scholar and Microsoft Academic have lower completeness rates compared to third-party databases like Dimensions and OpenAlex, which exhibit higher metadata quality.  \n",
      "DOI: 10.31222/osf.io/smxe5 - This study examines the availability of metadata elements, including language, in Crossref, noting improvements over time but emphasizing the need for publishers to enhance openness and completeness of bibliographic metadata.  \n",
      "\n",
      "Concluding statement: These studies collectively underscore the importance of evaluating linguistic coverage and metadata accuracy in scholarly databases, revealing strengths and weaknesses across platforms and highlighting areas for improvement in metadata quality and inclusivity.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2409.10633, Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1162/qss_a_00286', '10.31222/osf.io/smxe5', '10.48550/arXiv.2409.10633']\n",
      "45\n",
      "For query: Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 4\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Several papers address funding metadata, its availability, and analysis in scholarly databases, particularly focusing on Crossref, Scopus, and Web of Science.  \n",
      "\n",
      "DOI: 10.1162/qss_a_00212 - This study examines the open availability of funding data in Crossref, especially for COVID-19 research, and compares it with proprietary databases like Scopus and Web of Science, highlighting limited coverage and quality issues in funding metadata.  \n",
      "DOI: 10.31222/osf.io/smxe5 - The paper assesses the availability of funding information in Crossref as part of broader bibliographic metadata, noting improvements over time but emphasizing the need for publishers to enhance openness.  \n",
      "DOI: 10.1162/qss_a_00112 - This large-scale comparison of bibliographic data sources, including Crossref, Scopus, and Web of Science, discusses differences in coverage, completeness, and accuracy, indirectly touching on funding metadata as part of broader metadata analysis.  \n",
      "\n",
      "Concluding statement: These papers collectively highlight the challenges and opportunities in accessing and analyzing funding metadata across scholarly databases, underscoring the need for improved data quality and openness.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "Retrieved DOIs: ['10.1162/qss_a_00212', '10.31222/osf.io/smxe5', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112']\n",
      "45\n",
      "For query: Which papers address funding metadata, its availability, or its analysis in scholarly databases?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 5\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary Answer:** Several papers discuss the application of Retrieval-Augmented Generation (RAG) in large language models, focusing on its advancements, benchmarking, multi-hop query improvements, metadata-based data exploration, and the relationship between relevance and utility in RAG systems.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - This survey paper examines the progression of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG, and evaluates their components (retrieval, generation, augmentation) while highlighting challenges and future research directions.  \n",
      "DOI: 10.1609/aaai.v38i16.29728 - The paper introduces the Retrieval-Augmented Generation Benchmark (RGB) to evaluate LLMs' performance in RAG across four fundamental abilities, revealing strengths in noise robustness but weaknesses in negative rejection and information integration.  \n",
      "DOI: 10.48550/arXiv.2406.13213 - This work proposes Multi-Meta-RAG, a method using LLM-extracted metadata for database filtering to enhance RAG's performance on multi-hop queries, significantly improving results on the MultiHop-RAG benchmark.  \n",
      "DOI: 10.48550/arXiv.2410.04231 - The study introduces a RAG-based architecture for metadata-driven data exploration, integrating LLMs with vector databases to improve dataset discovery across heterogeneous sources, though performance varies by task.  \n",
      "DOI: 10.1007/978-3-031-88708-6_3 - This research investigates the correlation between document relevance and utility in RAG systems, finding a small positive relationship that diminishes with larger context sizes and emphasizing the role of effective retrieval models.  \n",
      "\n",
      "**Concluding Statement:** These papers collectively advance the understanding of RAG's role in LLMs, addressing challenges, proposing innovative solutions, and establishing benchmarks for future research and development.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2312.10997, Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "DOI: https://doi.org/10.1609/aaai.v38i16.29728, Title: Benchmarking Large Language Models in Retrieval-Augmented Generation\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.13213, Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2410.04231, Title: Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\n",
      "DOI: https://doi.org/10.1007/978-3-031-88708-6_3, Title: Is Relevance Propagated from Retriever to Generator in RAG?\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1609/aaai.v38i16.29728', '10.48550/arXiv.2406.13213', '10.48550/arXiv.2410.04231', '10.1007/978-3-031-88708-6_3']\n",
      "45\n",
      "For query: Which papers discuss the use of Retrieval-Augmented Generation (RAG) in large language models or related applications?:\n",
      "Precision: 0.956\n",
      "Recall: 0.956\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.887\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 6\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Crossref plays a crucial role in the scholarly research ecosystem by providing open bibliographic metadata and supporting the traceability of research outputs, though its coverage and data quality require ongoing improvements.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref serves as a key source of open bibliographic metadata, with improvements in the availability of elements like reference lists, abstracts, and funding information over time, though further efforts from publishers are needed to achieve full openness.  \n",
      "DOI: 10.1162/qss_a_00212 - Crossref facilitates the tracking of publications resulting from research funding, particularly for COVID-19 studies, but its funding data coverage and quality are limited compared to proprietary databases like Scopus and Web of Science.  \n",
      "DOI: 10.1162/qss_a_00112 - Crossref is one of several multidisciplinary bibliographic data sources, offering comprehensive coverage of scientific literature, though its strengths and weaknesses vary compared to alternatives like Scopus, Web of Science, and Dimensions.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While Crossref is not directly analyzed in this study, the emergence of inclusive databases like OpenAlex highlights the need for accurate and complete metadata, which is relevant to Crossref’s role in the ecosystem.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - Crossref’s role in providing bibliographic data is contextualized by the variability in publication and document types across databases, emphasizing the importance of standardized classification for bibliometric analyses.  \n",
      "\n",
      "Concluding statement: Crossref is a vital component of the scholarly research ecosystem, offering open access to bibliographic metadata and supporting research traceability, but its effectiveness depends on continued enhancements in data coverage, quality, and standardization.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.1162/qss_a_00212', '10.1162/qss_a_00112', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2406.15154']\n",
      "45\n",
      "For query: What is Crossref’s role in the scholarly research ecosystem?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 7\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: OpenAlex is a promising, inclusive bibliometric database that offers comprehensive coverage and serves as a reliable alternative to proprietary databases like Scopus, but it faces limitations in metadata accuracy and completeness.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - This study compares OpenAlex to Scopus, concluding that OpenAlex is a superset of Scopus, particularly useful for country-level analyses, but highlights issues with metadata accuracy and completeness that require further research to address.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - The analysis emphasizes OpenAlex's growing importance as a free alternative for bibliometric analyses, noting that publication and document type classifications differ significantly across databases, which impacts the identification of relevant documents.  \n",
      "\n",
      "Concluding statement: While OpenAlex shows great potential as an inclusive and comprehensive bibliometric database, addressing its limitations in metadata quality and standardization is crucial for its broader adoption in diverse bibliometric analyses.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1162/qss_a_00212']\n",
      "45\n",
      "For query: What are the key features and limitations of OpenAlex as a bibliometric database?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 2\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 8\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Web of Science (WoS) is a well-established bibliometric database with strengths in comprehensive coverage of scientific literature and accurate citation links, but it has weaknesses in underrepresenting certain disciplines and regions, and its proprietary nature limits accessibility.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2406.15154 - This study highlights that WoS, like other proprietary databases, classifies publications differently, which can affect the distinction between research and non-research texts, impacting bibliometric analysis.  \n",
      "DOI: 10.1162/qss_a_00112 - The large-scale comparison emphasizes WoS's strength in comprehensive coverage and accurate citation links but notes the importance of flexible filters for literature selection, suggesting limitations in granularity.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While not directly comparing WoS, this study underscores the underrepresentation of certain disciplines and regions in traditional databases like WoS, highlighting a key weakness compared to emerging inclusive databases like OpenAlex.  \n",
      "\n",
      "Concluding statement: WoS remains a cornerstone in bibliometric research due to its reliability and extensive coverage, but its limitations in inclusivity and accessibility are increasingly addressed by alternative databases, necessitating careful consideration of its use depending on research needs.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.48550/arXiv.2404.17663', '10.31222/osf.io/smxe5', '10.1162/qss_a_00286']\n",
      "45\n",
      "For query: What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 9\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary Answer:** Retrieval-Augmented Generation (RAG) improves question answering and information retrieval systems by integrating external knowledge sources, enhancing accuracy, reducing hallucinations, and enabling multi-hop reasoning, while also addressing challenges like outdated knowledge and non-transparent reasoning.\n",
      "\n",
      "DOI: 10.48550/arXiv.2109.05052 - This study investigates knowledge conflicts in question answering, highlighting models' over-reliance on parametric knowledge, which leads to hallucinations. It proposes a method to mitigate this, improving out-of-distribution generalization and encouraging models to use contextual information effectively.  \n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - The M-RAG method enhances open-domain question answering by incorporating metadata retrieval and generative models like GPT-4, achieving up to 46% performance improvement over baseline systems and 6% over traditional RAG by improving answer accuracy and source attribution.  \n",
      "DOI: 10.48550/arXiv.2406.13213 - Multi-Meta-RAG addresses the limitations of traditional RAG in multi-hop queries by using LLM-extracted metadata for database filtering, significantly improving performance on multi-hop benchmarks and demonstrating its effectiveness in domain-specific applications.  \n",
      "DOI: 10.1145/3626772.3657848 - This perspective paper discusses the opportunities and challenges of applying LLMs to information retrieval, emphasizing that LLMs cannot replace search engines but can enhance them by learning to interact with search systems, paving the way for future IR advancements.  \n",
      "DOI: 10.48550/arXiv.2312.10997 - This survey provides a comprehensive overview of RAG paradigms, detailing their tripartite foundation (retrieval, generation, augmentation) and highlighting advancements, challenges, and future research directions in integrating LLMs with external knowledge sources.  \n",
      "\n",
      "**Concluding Statement:** RAG systems, through various innovations like metadata integration, multi-hop reasoning, and knowledge conflict resolution, significantly enhance the capabilities of question answering and information retrieval systems, addressing critical challenges such as hallucination and outdated knowledge while paving the way for future advancements in the field.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2109.05052, Title: Entity-Based Knowledge Conflicts in Question Answering\n",
      "DOI: https://doi.org/10.6109/jkiice.2023.27.12.1489, Title: M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.13213, Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "DOI: https://doi.org/10.1145/3626772.3657848, Title: Large Language Models and Future of Information Retrieval: Opportunities and Challenges\n",
      "DOI: https://doi.org/10.48550/arXiv.2312.10997, Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "Retrieved DOIs: ['10.48550/arXiv.2109.05052', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2406.13213', '10.1145/3626772.3657848', '10.48550/arXiv.2312.10997']\n",
      "45\n",
      "For query: How is RAG used to improve question answering or information retrieval systems?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 10\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary answer:** Normalizing citation metrics across scientific fields is challenging due to differences in citation practices, database coverage, document types, and metadata accuracy, compounded by the need for comprehensive and standardized bibliographic data sources.  \n",
      "\n",
      "DOI: 10.1371/journal.pbio.1002542 - This document highlights the challenges in normalizing citation metrics, including differences across fields, publication age, document types, and database coverage, emphasizing the need for careful consideration of underlying assumptions in metric calculations.  \n",
      "DOI: 10.1162/qss_a_00112 - A comparative analysis of major bibliographic databases (Scopus, Web of Science, Dimensions, Crossref, Microsoft Academic) reveals significant variations in coverage, citation accuracy, and discipline-specific representation, underscoring the difficulty of standardization.  \n",
      "DOI: 10.31222/osf.io/smxe5 - This study focuses on Crossref's metadata availability, noting improvements over time but persistent gaps in openness, which affects the reliability of citation metrics across fields.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - The analysis of publication types in databases like OpenAlex, Scopus, and Web of Science highlights discrepancies in classification and coverage, complicating cross-field normalization efforts.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While OpenAlex is praised as a more inclusive alternative to traditional databases, its limitations in metadata accuracy and completeness pose challenges for robust bibliometric analyses across fields.  \n",
      "\n",
      "**Concluding statement:** Addressing these challenges requires standardized metadata practices, improved database interoperability, and a deeper understanding of field-specific citation cultures to ensure fair and accurate normalization of citation metrics.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.1162/qss_a_00112', '10.31222/osf.io/smxe5', '10.48550/arXiv.2406.15154', '10.48550/arXiv.2404.17663']\n",
      "45\n",
      "For query: What are the main challenges in normalizing citation metrics across scientific fields?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 11\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary answer:** Methods to detect and correct errors in bibliographic datasets include analyzing missing data patterns, comparing coverage and typologies across databases, and assessing metadata completeness and accuracy.  \n",
      "\n",
      "DOI: 10.5281/ZENODO.13960973 - This study introduces a method to detect errors in bibliographic datasets by analyzing missing data patterns, exemplified by identifying incorrectly affiliated papers in ETH Zurich’s publication metadata, with potential applications to other data types.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - The research highlights discrepancies in publication and document types across databases like OpenAlex, Scopus, and Web of Science, emphasizing the need for standardized classification to improve bibliometric analysis.  \n",
      "DOI: 10.1162/qss_a_00112 - This large-scale comparison of bibliographic data sources (Scopus, Web of Science, Dimensions, Crossref, Microsoft Academic) evaluates coverage, completeness, and accuracy of citation links, identifying strengths and weaknesses of each source.  \n",
      "DOI: 10.31222/osf.io/smxe5 - The study assesses the availability of open bibliographic metadata in Crossref, noting improvements over time but highlighting the need for publishers to enhance metadata openness.  \n",
      "\n",
      "**Concluding statement:** Error detection and correction in bibliographic datasets rely on methodologies such as pattern analysis, cross-database comparisons, and metadata completeness assessments, underscoring the importance of standardized practices and open data initiatives for improving data quality.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.5281/ZENODO.13960973, Title: Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "Retrieved DOIs: ['10.5281/ZENODO.13960973', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.31222/osf.io/smxe5']\n",
      "45\n",
      "For query: What methods are used to detect and correct errors in bibliographic datasets?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 4\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 12\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: The provided context does not directly address how RAG (Retrieval-Augmented Generation) works, as the documents focus on bibliometric analyses, metadata quality, and citation metrics rather than RAG.  \n",
      "\n",
      "DOI: 10.1371/journal.pbio.1002542 - This document discusses the challenges and considerations in normalizing citation metrics across different scientific fields, publication ages, and document types, but does not mention RAG.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - This study compares OpenAlex to Scopus for bibliometric analyses, highlighting OpenAlex's strengths and limitations, yet remains unrelated to RAG.  \n",
      "DOI: 10.31222/osf.io/smxe5 - The paper examines the availability of open bibliographic metadata in Crossref, focusing on improvements and gaps, without addressing RAG.  \n",
      "DOI: 10.5860/crl.86.1.101 - This research explores metadata quality issues across cultures, emphasizing tensions between standardization and cultural representation, but does not discuss RAG.  \n",
      "DOI: 10.1162/qss_a_00112 - The study compares five bibliographic data sources (Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic) in terms of coverage, completeness, and accuracy, yet remains unrelated to RAG.  \n",
      "\n",
      "Concluding statement: Since none of the provided documents discuss RAG, I cannot provide an answer to the query based on the given context.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.5860/crl.86.1.101, Title: Identifying Metadata Quality Issues Across Cultures\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.31222/osf.io/smxe5', '10.5860/crl.86.1.101', '10.1162/qss_a_00112']\n",
      "45\n",
      "For query: tell me about how RAG works.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.438\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 13\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Several studies have examined the availability, completeness, and accuracy of abstracts in metadata across various bibliographic databases.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This study provides an overview of the availability of six metadata elements, including abstracts, in Crossref, noting improvements over time but highlighting the need for further efforts by publishers to ensure full openness.  \n",
      "DOI: 10.1162/qss_a_00286 - The research compares the completeness degree of publication metadata, including abstracts, in eight free-access scholarly databases, finding that academic search engines have lower completeness rates compared to third-party databases.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - This analysis focuses on publication and document types in multiple databases, including OpenAlex, and discusses the variability in metadata classification, which indirectly relates to the handling of abstracts.  \n",
      "\n",
      "Concluding statement: These studies collectively emphasize the importance of abstract metadata in bibliographic databases, highlighting variations in availability, completeness, and accuracy across different platforms, with third-party databases generally outperforming academic search engines in metadata quality.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00286', '10.48550/arXiv.2404.17663']\n",
      "45\n",
      "For query: which studies examined the abstract in metadata?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 0\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary answer:** Several studies have examined citations, focusing on the completeness, accuracy, and normalization of citation metrics across various bibliographic databases.  \n",
      "\n",
      "DOI: 10.1162/qss_a_00112 - This study compares five bibliographic data sources (Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic) and analyzes differences in the completeness and accuracy of citation links, highlighting the strengths and weaknesses of each database.  \n",
      "DOI: 10.1371/journal.pbio.1002542 - The paper discusses the challenges and approaches to normalizing citation metrics, emphasizing the need to account for field-specific differences, document types, and database coverage when appraising research.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While primarily assessing OpenAlex's suitability for bibliometric analyses, this study also compares it to Scopus, touching on metadata accuracy and completeness, which are critical for reliable citation-based analyses.  \n",
      "\n",
      "**Concluding statement:** These studies collectively underscore the importance of understanding the limitations and strengths of different bibliographic databases when examining citations, as well as the need for careful normalization and critical evaluation of citation metrics.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00286']\n",
      "45\n",
      "For query: which studies examined citations?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 1\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: OpenAlex is an emerging, inclusive bibliometric database that offers a reliable alternative to traditional databases like Scopus and Web of Science, particularly for country-level analyses, though it still faces limitations in metadata accuracy and completeness.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - An analysis comparing OpenAlex to Scopus reveals that OpenAlex is a superset of Scopus, making it suitable for certain bibliometric analyses, especially at the country level, but highlights ongoing issues with metadata accuracy and completeness that require further research.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This study examines the availability of open bibliographic metadata in Crossref, noting improvements over time but emphasizing the need for publishers to enhance openness, which indirectly supports the value of initiatives like OpenAlex in promoting comprehensive and accessible metadata.\n",
      "\n",
      "Concluding statement: While OpenAlex shows promise as an inclusive and comprehensive bibliometric resource, addressing its current limitations in metadata quality will be crucial for its broader adoption and reliability in diverse research contexts.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.1162/qss_a_00022, Title: Crossref: The sustainable source of community-owned scholarly metadata\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00212', '10.1371/journal.pbio.1002542', '10.1162/qss_a_00022']\n",
      "45\n",
      "For query: Tell me about OpenAlex.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.519\n",
      "Faithfulness score: 2\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 2\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Crossref is a key provider of open bibliographic metadata, contributing to the availability and accessibility of scholarly publication data, though challenges remain in ensuring full openness and metadata quality.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref has seen improvements in the availability of metadata elements like reference lists, abstracts, and ORCIDs, particularly for journal articles, but many publishers still need to enhance efforts for full openness.  \n",
      "DOI: 10.1162/qss_a_00112 - A large-scale comparison of bibliographic data sources, including Crossref, highlights its strengths in comprehensive coverage but also underscores the need for flexible filters to address variations in metadata completeness and accuracy.  \n",
      "DOI: 10.5860/crl.86.1.101 - Metadata quality issues, including those in Crossref, reflect tensions between standardization and cultural representation, emphasizing the need for interventions to improve consistency and completeness.  \n",
      "\n",
      "Concluding statement: Crossref plays a vital role in open bibliographic metadata, yet ongoing efforts are required to address gaps in openness, metadata quality, and cultural representation to maximize its utility in scholarly communication.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.5860/crl.86.1.101, Title: Identifying Metadata Quality Issues Across Cultures\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112', '10.5860/crl.86.1.101']\n",
      "45\n",
      "For query: Tell me about Crossref.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.569\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 3\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Several papers evaluate the linguistic coverage or language-related metadata in scholarly databases, highlighting differences in completeness, accuracy, and inclusivity across various platforms.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 - This study assesses the linguistic coverage of OpenAlex, comparing its metadata accuracy and completeness with Web of Science (WoS) and finding that OpenAlex offers a more balanced representation of languages but still has inaccuracies in language metadata.  \n",
      "DOI: 10.1162/qss_a_00286 - This research evaluates the completeness of publication metadata, including language, in eight free-access scholarly databases, concluding that third-party databases generally have higher metadata quality than academic search engines.  \n",
      "DOI: 10.31222/osf.io/smxe5 - This paper examines the availability of open bibliographic metadata in Crossref, including language information, and notes improvements over time but identifies gaps in full openness.  \n",
      "\n",
      "In conclusion, these studies collectively emphasize the importance of evaluating linguistic coverage and metadata accuracy in scholarly databases, revealing both strengths and limitations across different platforms.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2409.10633, Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1162/qss_a_00286', '10.31222/osf.io/smxe5', '10.48550/arXiv.2409.10633']\n",
      "45\n",
      "For query: Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 4\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Several papers address funding metadata, its availability, and analysis in scholarly databases, particularly focusing on Crossref, Scopus, and Web of Science.  \n",
      "\n",
      "DOI: 10.1162/qss_a_00212 - This paper analyzes the open availability of funding data in Crossref, especially for COVID-19 research, and compares it with proprietary databases like Scopus and Web of Science, highlighting limited coverage and quality issues in funding metadata.  \n",
      "DOI: 10.31222/osf.io/smxe5 - The study evaluates the availability of funding information as part of bibliographic metadata in Crossref, noting improvements over time but emphasizing the need for further efforts by publishers to ensure full openness.  \n",
      "DOI: 10.1162/qss_a_00112 - This large-scale comparison of bibliographic data sources, including Crossref, Scopus, and Web of Science, examines differences in coverage, completeness, and accuracy, indirectly addressing the availability and reliability of funding metadata.  \n",
      "\n",
      "Concluding statement: These papers collectively highlight the challenges and opportunities in accessing and analyzing funding metadata across scholarly databases, underscoring the need for improved data quality and openness.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "Retrieved DOIs: ['10.1162/qss_a_00212', '10.31222/osf.io/smxe5', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112']\n",
      "45\n",
      "For query: Which papers address funding metadata, its availability, or its analysis in scholarly databases?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 5\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary Answer:** Several papers discuss the application of Retrieval-Augmented Generation (RAG) in large language models, focusing on its advancements, benchmarking, multi-hop query improvements, metadata-based data exploration, and the propagation of relevance from retriever to generator.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - This survey paper examines the progression of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG, and explores their tripartite foundation of retrieval, generation, and augmentation techniques, while also discussing challenges and future research directions.  \n",
      "DOI: 10.1609/aaai.v38i16.29728 - The paper introduces the Retrieval-Augmented Generation Benchmark (RGB) to evaluate the performance of LLMs in RAG across four fundamental abilities: noise robustness, negative rejection, information integration, and counterfactual robustness, highlighting areas for improvement.  \n",
      "DOI: 10.48550/arXiv.2406.13213 - This work proposes Multi-Meta-RAG, a method that uses LLM-extracted metadata and database filtering to enhance RAG's ability to handle multi-hop queries, significantly improving performance on the MultiHop-RAG benchmark.  \n",
      "DOI: 10.48550/arXiv.2410.04231 - The study introduces a RAG-based architecture for metadata-based data exploration, integrating LLMs with external vector databases to improve semantic dataset discovery, with experimental results demonstrating its effectiveness across various tasks.  \n",
      "DOI: 10.1007/978-3-031-88708-6_3 - This research investigates the correlation between relevance and utility in RAG systems, finding a small positive correlation that diminishes with larger context sizes and emphasizing the importance of effective retrieval models for downstream performance.  \n",
      "\n",
      "**Concluding Statement:** Collectively, these papers highlight RAG's potential to enhance LLMs while identifying challenges such as multi-hop reasoning, relevance propagation, and task-specific performance variations, underscoring the need for continued research and refinement.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2312.10997, Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "DOI: https://doi.org/10.1609/aaai.v38i16.29728, Title: Benchmarking Large Language Models in Retrieval-Augmented Generation\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.13213, Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2410.04231, Title: Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\n",
      "DOI: https://doi.org/10.1007/978-3-031-88708-6_3, Title: Is Relevance Propagated from Retriever to Generator in RAG?\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1609/aaai.v38i16.29728', '10.48550/arXiv.2406.13213', '10.48550/arXiv.2410.04231', '10.1007/978-3-031-88708-6_3']\n",
      "45\n",
      "For query: Which papers discuss the use of Retrieval-Augmented Generation (RAG) in large language models or related applications?:\n",
      "Precision: 0.956\n",
      "Recall: 0.956\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.887\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 6\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Crossref plays a crucial role in the scholarly research ecosystem by providing open bibliographic metadata, supporting funding traceability, and serving as a comprehensive data source for bibliometric analyses, though its data completeness and quality require ongoing improvement.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref is highlighted as a key provider of open bibliographic metadata, with improvements noted in the availability of elements like reference lists, abstracts, and funding information, though further efforts from publishers are needed to achieve full openness.  \n",
      "DOI: 10.1162/qss_a_00212 - The study emphasizes Crossref’s role in enabling funding agencies to trace publications resulting from their grants, particularly for COVID-19 research, but identifies limitations in coverage and data quality compared to proprietary databases like Scopus and Web of Science.  \n",
      "DOI: 10.1162/qss_a_00112 - Crossref is compared with other bibliographic data sources, revealing its strengths in comprehensive coverage and flexibility, though it faces challenges in citation link completeness and accuracy.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While not directly about Crossref, this study underscores the importance of inclusive databases like OpenAlex, which complement Crossref by addressing underrepresentation in traditional databases, though metadata accuracy remains a concern.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - This analysis highlights the variability in publication and document typologies across databases, including Crossref, and emphasizes the growing importance of open alternatives like OpenAlex for bibliometric analyses.  \n",
      "\n",
      "Concluding statement: Crossref remains a vital component of the scholarly ecosystem, facilitating open access to metadata and supporting research transparency, but continued enhancements in data completeness and quality are essential to maximize its utility.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.1162/qss_a_00212', '10.1162/qss_a_00112', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2406.15154']\n",
      "45\n",
      "For query: What is Crossref’s role in the scholarly research ecosystem?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 7\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: OpenAlex is a promising, inclusive bibliometric database that offers a superset of data compared to Scopus, particularly at the country level, but it faces limitations in metadata accuracy and completeness, and its publication and document type classifications differ from other databases.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - This study compares OpenAlex to Scopus, concluding that OpenAlex is a reliable alternative for certain analyses, especially at the country level, but highlights issues with metadata accuracy and completeness that require further research to address.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - The analysis reveals significant differences in publication and document type classifications between OpenAlex and other databases like Scopus, Web of Science, and PubMed, emphasizing the need for careful consideration when using OpenAlex for bibliometric studies.  \n",
      "\n",
      "Concluding statement: While OpenAlex shows potential as a free and inclusive alternative to proprietary databases, its limitations in metadata quality and classification consistency necessitate further investigation to ensure its reliable use across diverse bibliometric analyses.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1162/qss_a_00212']\n",
      "45\n",
      "For query: What are the key features and limitations of OpenAlex as a bibliometric database?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 2\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 8\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Web of Science (WoS) is a well-established bibliometric database with strengths in its comprehensive coverage of scientific literature and citation accuracy, but it faces weaknesses such as limited inclusivity of certain disciplines and regions, and its proprietary nature, which restricts access.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.15154 - This study highlights that WoS, like other proprietary databases, classifies publications differently, which can affect the distinction between research and non-research texts, impacting bibliometric analyses. It also emphasizes the growing importance of free alternatives like OpenAlex.  \n",
      "DOI: 10.1162/qss_a_00112 - The large-scale comparison underscores WoS's strengths in coverage and citation accuracy but notes that it systematically underrepresents certain disciplines and regions, a limitation shared with Scopus. The study stresses the need for flexible filters to enhance literature selection.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While WoS has been foundational in the science of science, it is criticized for underrepresenting specific disciplines and regions. OpenAlex emerges as a more inclusive alternative, though it still faces challenges in metadata accuracy and completeness.  \n",
      "\n",
      "Concluding statement: WoS remains a cornerstone in bibliometric research due to its extensive coverage and reliable citation data, but its proprietary status and biases in representation necessitate the exploration of more inclusive and accessible alternatives like OpenAlex.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.48550/arXiv.2404.17663', '10.31222/osf.io/smxe5', '10.1162/qss_a_00286']\n",
      "45\n",
      "For query: What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 9\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary Answer:** Retrieval-Augmented Generation (RAG) improves question answering and information retrieval systems by integrating external knowledge sources, enhancing accuracy, reducing hallucinations, and enabling multi-hop reasoning, as demonstrated through various methods like metadata retrieval, database filtering, and knowledge conflict mitigation.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2109.05052 - The study formalizes knowledge conflicts in question answering, where contextual information contradicts learned knowledge, and proposes a method to mitigate over-reliance on parametric knowledge, reducing hallucinations and improving generalization.  \n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - M-RAG enhances open-domain question answering by incorporating metadata retrieval and generative models like GPT-4, achieving up to 46% performance improvement over baseline systems and 6% over existing RAG methods.  \n",
      "DOI: 10.48550/arXiv.2406.13213 - Multi-Meta-RAG addresses multi-hop queries by using LLM-extracted metadata for database filtering, significantly improving document retrieval and reasoning in complex question answering scenarios.  \n",
      "DOI: 10.1145/3626772.3657848 - This perspective paper discusses the opportunities and challenges of applying LLMs to information retrieval, emphasizing the need for LLMs to interact with search engines rather than replace them.  \n",
      "DOI: 10.48550/arXiv.2312.10997 - A comprehensive survey of RAG highlights its role in addressing LLM limitations like hallucination and outdated knowledge by integrating external databases, and outlines advancements in retrieval, generation, and augmentation techniques.  \n",
      "\n",
      "**Concluding Statement:** RAG has emerged as a critical technique for enhancing question answering and information retrieval systems by leveraging external knowledge, with innovations like metadata integration, multi-hop reasoning, and conflict resolution driving significant improvements in accuracy and reliability.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2109.05052, Title: Entity-Based Knowledge Conflicts in Question Answering\n",
      "DOI: https://doi.org/10.6109/jkiice.2023.27.12.1489, Title: M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.13213, Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "DOI: https://doi.org/10.1145/3626772.3657848, Title: Large Language Models and Future of Information Retrieval: Opportunities and Challenges\n",
      "DOI: https://doi.org/10.48550/arXiv.2312.10997, Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "Retrieved DOIs: ['10.48550/arXiv.2109.05052', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2406.13213', '10.1145/3626772.3657848', '10.48550/arXiv.2312.10997']\n",
      "45\n",
      "For query: How is RAG used to improve question answering or information retrieval systems?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 10\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary Answer:** Normalizing citation metrics across scientific fields is challenging due to variations in citation practices, database coverage, document types, and metadata completeness, which require careful consideration of normalization methods and data source limitations.  \n",
      "\n",
      "DOI: 10.1371/journal.pbio.1002542 - The article highlights the challenges in normalizing citation metrics, including differences across fields, publication age, document types, and database coverage, emphasizing the need to carefully evaluate normalization approaches and underlying assumptions.  \n",
      "DOI: 10.1162/qss_a_00112 - This study compares major bibliographic databases (Scopus, Web of Science, Dimensions, Crossref, Microsoft Academic) and identifies significant differences in coverage, citation accuracy, and discipline-specific representation, underscoring the importance of selecting appropriate data sources for normalization.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - The analysis reveals discrepancies in publication and document type classifications across databases, including OpenAlex, Scopus, and Web of Science, which complicates normalization efforts due to inconsistent categorization of research and non-research texts.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While OpenAlex is positioned as a more inclusive alternative to traditional databases like Scopus, its limitations in metadata accuracy and completeness highlight the need for further research to ensure reliable normalization across fields.  \n",
      "\n",
      "**Concluding Statement:** Effective normalization of citation metrics requires addressing disparities in citation practices, database coverage, and metadata quality, alongside critical evaluation of data sources and normalization methodologies to ensure fairness and accuracy across scientific disciplines.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.1162/qss_a_00112', '10.31222/osf.io/smxe5', '10.48550/arXiv.2406.15154', '10.48550/arXiv.2404.17663']\n",
      "45\n",
      "For query: What are the main challenges in normalizing citation metrics across scientific fields?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 4\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 11\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary answer:** Methods to detect and correct errors in bibliographic datasets include analyzing missing data patterns, comparing data across multiple sources, and assessing metadata completeness and accuracy.  \n",
      "\n",
      "DOI: 10.5281/ZENODO.13960973 - This study introduces a method to detect errors in bibliographic datasets by analyzing missing data patterns, exemplified by identifying incorrectly affiliated papers in ETH Zurich’s publication metadata, with potential applications to other data types.  \n",
      "DOI: 10.1162/qss_a_00112 - A large-scale comparison of bibliographic data sources (Scopus, Web of Science, Dimensions, Crossref, Microsoft Academic) highlights discrepancies in coverage, completeness, and accuracy of citation links, emphasizing the need for flexible filters and comprehensive coverage.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - This research compares publication and document types across OpenAlex, Web of Science, Scopus, PubMed, and Semantic Scholar, revealing significant variations in typologies and classification, which impact bibliometric analysis.  \n",
      "DOI: 10.31222/osf.io/smxe5 - The study evaluates the openness of bibliographic metadata in Crossref, noting improvements in availability of elements like reference lists and ORCIDs but identifying gaps that require publisher efforts for full openness.  \n",
      "\n",
      "**Concluding statement:** Error detection and correction in bibliographic datasets rely on methodologies such as pattern analysis, cross-source comparison, and metadata evaluation, with ongoing efforts needed to enhance data quality and openness.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.5281/ZENODO.13960973, Title: Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "Retrieved DOIs: ['10.5281/ZENODO.13960973', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.31222/osf.io/smxe5']\n",
      "45\n",
      "For query: What methods are used to detect and correct errors in bibliographic datasets?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 4\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 12\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: The provided context does not contain information about how RAG (Retrieval-Augmented Generation) works, so I cannot provide an answer.\n",
      "\n",
      "Since there is no relevant information in the context about RAG, I am unable to provide a structured response with DOIs and summaries.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.5860/crl.86.1.101, Title: Identifying Metadata Quality Issues Across Cultures\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.31222/osf.io/smxe5', '10.5860/crl.86.1.101', '10.1162/qss_a_00112']\n",
      "45\n",
      "For query: tell me about how RAG works.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.438\n",
      "Faithfulness score: 0\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 13\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Several studies have examined the presence and completeness of abstracts in metadata across various bibliographic databases.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This study provides an overview of the availability of six metadata elements, including abstracts, in Crossref, noting improvements over time but highlighting the need for further efforts by publishers to ensure full openness.  \n",
      "DOI: 10.1162/qss_a_00286 - The research compares the completeness degree of publication metadata, including abstracts, in eight free-access scholarly databases, finding that third-party databases generally have higher completeness rates compared to academic search engines.  \n",
      "\n",
      "Concluding statement: These studies collectively emphasize the importance of abstract availability in metadata for scholarly research and highlight variations in completeness across different databases, underscoring the need for continued efforts to enhance metadata quality and openness.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00286', '10.48550/arXiv.2404.17663']\n",
      "45\n",
      "For query: which studies examined the abstract in metadata?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 2\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 0\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Several studies have examined citations, focusing on the comparison of bibliographic databases, citation metrics, and the completeness of metadata in scholarly databases.  \n",
      "\n",
      "DOI: 10.1162/qss_a_00112 - This study conducts a large-scale comparison of five bibliographic data sources (Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic), analyzing differences in document coverage and the completeness and accuracy of citation links, highlighting the strengths and weaknesses of each source.  \n",
      "DOI: 10.1371/journal.pbio.1002542 - The study discusses the challenges and approaches to normalizing citation metrics, emphasizing the need to account for field-specific differences, document types, and other factors when using citation metrics for research appraisal.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While primarily assessing OpenAlex's suitability for bibliometric analyses, this study also compares it to Scopus, touching on metadata accuracy and completeness, which indirectly relates to citation data reliability.  \n",
      "\n",
      "Concluding statement: These studies collectively contribute to understanding the reliability and limitations of citation data across various bibliographic databases, offering insights into their use for bibliometric analyses and research evaluation.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00286']\n",
      "45\n",
      "For query: which studies examined citations?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 1\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: OpenAlex is an emerging, inclusive bibliometric database that offers a reliable alternative to traditional databases like Scopus and Web of Science, particularly for certain types of analyses, but requires further research to address its limitations in metadata accuracy and completeness.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - An analysis comparing OpenAlex to Scopus reveals that OpenAlex is a superset of Scopus, making it a viable alternative for some analyses, especially at the country level, but highlights issues with metadata accuracy and completeness that need further investigation.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This study examines the availability of open bibliographic metadata in Crossref, noting improvements over time but emphasizing the need for publishers to enhance openness, which is relevant to OpenAlex as it relies on such metadata for its inclusivity.\n",
      "\n",
      "Concluding statement: While OpenAlex shows promise as an inclusive and comprehensive bibliometric database, addressing its limitations in metadata quality and completeness is crucial for its broader application in scientometric analyses.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.1162/qss_a_00022, Title: Crossref: The sustainable source of community-owned scholarly metadata\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00212', '10.1371/journal.pbio.1002542', '10.1162/qss_a_00022']\n",
      "45\n",
      "For query: Tell me about OpenAlex.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.519\n",
      "Faithfulness score: 2\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 2\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Crossref is a key provider of open bibliographic metadata, offering improved availability of metadata elements over time, though challenges remain in ensuring full openness and metadata quality across publishers and cultures.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref has seen improvements in the availability of metadata elements like reference lists, abstracts, and ORCIDs, particularly for journal articles, but many publishers still need to enhance efforts for full openness.  \n",
      "DOI: 10.1162/qss_a_00112 - A large-scale comparison of bibliographic data sources, including Crossref, highlights its strengths in comprehensive coverage but also underscores the need for flexible filters to address variations in metadata completeness and accuracy.  \n",
      "DOI: 10.5860/crl.86.1.101 - Metadata quality in Crossref and other systems is influenced by cultural interpretations, resource constraints, and standardization efforts, revealing tensions between representation and visibility.  \n",
      "\n",
      "Concluding statement: Crossref plays a vital role in advancing open bibliographic metadata, yet ongoing efforts are necessary to address publisher compliance, metadata accuracy, and cultural considerations for broader usability and inclusivity.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.5860/crl.86.1.101, Title: Identifying Metadata Quality Issues Across Cultures\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112', '10.5860/crl.86.1.101']\n",
      "45\n",
      "For query: Tell me about Crossref.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.569\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 3\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Several papers evaluate the linguistic coverage or language-related metadata in scholarly databases, focusing on completeness, accuracy, and biases.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 - This study assesses the linguistic coverage of OpenAlex, comparing its metadata accuracy and completeness with Web of Science (WoS) and conducting manual validation. It finds OpenAlex has more balanced linguistic coverage but notes inaccuracies in language metadata, overestimating English and underestimating other languages.  \n",
      "DOI: 10.1162/qss_a_00286 - This research compares metadata completeness in eight free-access scholarly databases, including language-related fields. It highlights that academic search engines have lower completeness rates compared to third-party databases, which exhibit higher metadata quality but face challenges in integrating diverse sources.  \n",
      "DOI: 10.31222/osf.io/smxe5 - This paper examines the availability of metadata elements in Crossref, including language information, and finds improvements over time but notes the need for further efforts by publishers to ensure full openness.  \n",
      "\n",
      "These studies collectively emphasize the importance of evaluating linguistic coverage and metadata quality in scholarly databases, identifying both strengths and limitations across different platforms.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2409.10633, Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1162/qss_a_00286', '10.31222/osf.io/smxe5', '10.48550/arXiv.2409.10633']\n",
      "45\n",
      "For query: Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 4\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Several papers address funding metadata, its availability, and analysis in scholarly databases, particularly focusing on Crossref, Scopus, and Web of Science.  \n",
      "\n",
      "DOI: 10.1162/qss_a_00212 - This paper analyzes the open availability of funding data in Crossref, especially for COVID-19 research, and compares it with Scopus and Web of Science, highlighting limited coverage and quality issues in funding metadata.  \n",
      "DOI: 10.31222/osf.io/smxe5 - The study examines the availability of funding information in Crossref as part of broader bibliographic metadata, noting improvements over time but emphasizing the need for further efforts by publishers to ensure full openness.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While primarily assessing OpenAlex's suitability for bibliometric analyses, this paper touches on metadata accuracy and completeness, which indirectly relates to the reliability of funding metadata in scholarly databases.  \n",
      "\n",
      "Concluding statement: These papers collectively highlight the challenges and opportunities in improving the availability and quality of funding metadata in scholarly databases, with a particular focus on Crossref, Scopus, and Web of Science.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "Retrieved DOIs: ['10.1162/qss_a_00212', '10.31222/osf.io/smxe5', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112']\n",
      "45\n",
      "For query: Which papers address funding metadata, its availability, or its analysis in scholarly databases?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 5\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary Answer:** Several papers discuss the application of Retrieval-Augmented Generation (RAG) in large language models, focusing on its advancements, benchmarking, multi-hop query improvements, metadata-based data exploration, and the relationship between relevance and utility in RAG systems.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - This survey paper provides a comprehensive overview of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG, while examining retrieval, generation, and augmentation techniques, and outlining challenges and future research directions.  \n",
      "DOI: 10.1609/aaai.v38i16.29728 - The paper introduces the Retrieval-Augmented Generation Benchmark (RGB) to evaluate LLMs' performance in RAG across four fundamental abilities, revealing strengths in noise robustness but weaknesses in negative rejection and information integration.  \n",
      "DOI: 10.48550/arXiv.2406.13213 - This work proposes Multi-Meta-RAG, a method using LLM-extracted metadata for database filtering to enhance RAG's handling of multi-hop queries, significantly improving performance on the MultiHop-RAG benchmark.  \n",
      "DOI: 10.48550/arXiv.2410.04231 - The study introduces a RAG-based architecture for metadata-driven data exploration, demonstrating improved dataset selection across heterogeneous sources, though performance varies by task and model.  \n",
      "DOI: 10.1007/978-3-031-88708-6_3 - This research investigates the correlation between document relevance and utility in RAG systems, finding a small positive relationship that diminishes with larger context sizes and emphasizing the importance of effective retrieval models.  \n",
      "\n",
      "**Concluding Statement:** Collectively, these papers highlight RAG's potential to enhance LLMs while identifying areas for improvement, such as multi-hop reasoning, benchmarking, and relevance-utility dynamics, underscoring the need for continued research and refinement.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2312.10997, Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "DOI: https://doi.org/10.1609/aaai.v38i16.29728, Title: Benchmarking Large Language Models in Retrieval-Augmented Generation\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.13213, Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2410.04231, Title: Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\n",
      "DOI: https://doi.org/10.1007/978-3-031-88708-6_3, Title: Is Relevance Propagated from Retriever to Generator in RAG?\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1609/aaai.v38i16.29728', '10.48550/arXiv.2406.13213', '10.48550/arXiv.2410.04231', '10.1007/978-3-031-88708-6_3']\n",
      "45\n",
      "For query: Which papers discuss the use of Retrieval-Augmented Generation (RAG) in large language models or related applications?:\n",
      "Precision: 0.956\n",
      "Recall: 0.956\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.887\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 6\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Crossref plays a crucial role in the scholarly research ecosystem by providing open bibliographic metadata, supporting funding traceability, and serving as a comprehensive data source for bibliometric analyses.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref enhances the openness of bibliographic metadata, including reference lists, abstracts, and funding information, though further efforts from publishers are needed to achieve full transparency.  \n",
      "DOI: 10.1162/qss_a_00212 - Crossref facilitates funding agencies in tracing publications resulting from their grants, despite limitations in coverage and data quality compared to proprietary databases like Scopus and Web of Science.  \n",
      "DOI: 10.1162/qss_a_00112 - Crossref is a key multidisciplinary bibliographic data source, offering comprehensive coverage and accurate citation links, though it differs from other databases in scope and functionality.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While OpenAlex emerges as a more inclusive alternative to traditional databases, Crossref remains a vital resource for bibliometric analyses, particularly in complementing newer databases.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - Crossref contributes to the diversity of bibliographic databases, supporting the distinction between research and non-research texts, which is essential for accurate bibliometric studies.  \n",
      "\n",
      "In conclusion, Crossref is integral to the scholarly ecosystem by promoting open metadata, aiding funding traceability, and providing a robust data source for bibliometric research, despite ongoing challenges in data completeness and coverage.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.1162/qss_a_00212', '10.1162/qss_a_00112', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2406.15154']\n",
      "45\n",
      "For query: What is Crossref’s role in the scholarly research ecosystem?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 7\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: OpenAlex is a promising, inclusive bibliometric database with broader coverage than Scopus, particularly at the country level, but it faces limitations in metadata accuracy and completeness, requiring further research to fully address these issues.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - This study compares OpenAlex to Scopus, concluding that OpenAlex is a superset of Scopus and a reliable alternative for certain analyses, especially at the country level. However, it highlights issues with metadata accuracy and completeness, necessitating additional research to fully understand and mitigate these limitations.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - This analysis compares OpenAlex with other databases, emphasizing its growing importance as a free alternative for bibliometric analyses. It notes that publication and document typologies differ significantly across databases, impacting the distinction between research and non-research texts, which is critical for bibliometric studies.  \n",
      "\n",
      "Concluding statement: While OpenAlex offers a more inclusive and accessible option for bibliometric research, its limitations in metadata quality and classification consistency must be addressed to ensure its reliability across diverse analytical contexts.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1162/qss_a_00212']\n",
      "45\n",
      "For query: What are the key features and limitations of OpenAlex as a bibliometric database?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 2\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 8\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Web of Science (WoS) is a well-established bibliometric database with strengths in comprehensive coverage of scientific literature and a structured approach to document classification, but it faces limitations in inclusivity and accessibility compared to emerging alternatives.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2406.15154 - This study highlights that WoS, like other proprietary databases, exhibits variability in document typologies and classification compared to OpenAlex, which is increasingly favored for its open-access nature and broader coverage.  \n",
      "DOI: 10.1162/qss_a_00112 - A large-scale comparison reveals that WoS provides robust coverage and citation accuracy but underscores the need for flexible filtering options, which are critical for tailored bibliometric analyses.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While WoS has been foundational in science of science research, it underrepresents certain disciplines and regions, a gap partially addressed by newer databases like OpenAlex, which offers greater inclusivity despite metadata accuracy concerns.  \n",
      "\n",
      "In conclusion, WoS remains a reliable tool for bibliometric analysis due to its structured and comprehensive nature, but its proprietary status and limitations in inclusivity position emerging open-access databases as viable alternatives for certain research needs.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.48550/arXiv.2404.17663', '10.31222/osf.io/smxe5', '10.1162/qss_a_00286']\n",
      "45\n",
      "For query: What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 9\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary Answer:** Retrieval-Augmented Generation (RAG) improves question answering and information retrieval systems by integrating external knowledge sources, enhancing accuracy, reducing hallucinations, and enabling multi-hop reasoning, as demonstrated through various methods like metadata retrieval, database filtering, and knowledge conflict mitigation.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2109.05052 - The study formalizes knowledge conflicts in question answering, where contextual information contradicts learned parametric knowledge, and proposes a method to mitigate over-reliance on memorized information, reducing hallucinations and improving generalization.  \n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - M-RAG enhances open-domain question answering by incorporating metadata retrieval and generative models like GPT-4, achieving up to 46% performance improvement by enabling source attribution and accurate external knowledge integration.  \n",
      "DOI: 10.48550/arXiv.2406.13213 - Multi-Meta-RAG addresses multi-hop queries by using LLM-extracted metadata for database filtering, significantly improving document retrieval and reasoning over multiple evidence sources.  \n",
      "DOI: 10.1145/3626772.3657848 - This perspective paper discusses the role of LLMs in information retrieval, emphasizing their inability to replace search engines but highlighting their potential to interact with IR systems to enhance user queries.  \n",
      "DOI: 10.48550/arXiv.2312.10997 - A comprehensive survey on RAG highlights its ability to merge LLMs' intrinsic knowledge with external databases, addressing challenges like hallucination and outdated knowledge, while outlining advancements and future research directions.  \n",
      "\n",
      "**Concluding Statement:** RAG’s integration of external knowledge and advanced techniques like metadata retrieval and database filtering significantly enhances question answering and information retrieval systems, addressing critical challenges such as hallucinations and multi-hop reasoning.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2109.05052, Title: Entity-Based Knowledge Conflicts in Question Answering\n",
      "DOI: https://doi.org/10.6109/jkiice.2023.27.12.1489, Title: M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.13213, Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "DOI: https://doi.org/10.1145/3626772.3657848, Title: Large Language Models and Future of Information Retrieval: Opportunities and Challenges\n",
      "DOI: https://doi.org/10.48550/arXiv.2312.10997, Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "Retrieved DOIs: ['10.48550/arXiv.2109.05052', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2406.13213', '10.1145/3626772.3657848', '10.48550/arXiv.2312.10997']\n",
      "45\n",
      "For query: How is RAG used to improve question answering or information retrieval systems?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 10\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary Answer:** Normalizing citation metrics across scientific fields is challenging due to differences in citation practices, database coverage, document types, and metadata completeness, which vary significantly across disciplines and data sources.  \n",
      "\n",
      "DOI: 10.1371/journal.pbio.1002542 - Citation metrics face normalization challenges due to variations in scientific fields, publication age, document types, and database coverage, with additional complexities arising when combining metrics across multiple papers or attributing credit in multi-authored works.  \n",
      "DOI: 10.1162/qss_a_00112 - A comparison of bibliographic data sources (Scopus, Web of Science, Dimensions, Crossref, Microsoft Academic) reveals significant differences in coverage, citation accuracy, and discipline-specific representation, underscoring the need for flexible filtering and comprehensive coverage in normalization efforts.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - Publication and document types vary considerably across databases (OpenAlex, Scopus, Web of Science, PubMed, Semantic Scholar), complicating the distinction between research and non-research texts and affecting bibliometric analyses.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - OpenAlex, while a promising inclusive alternative to traditional databases like Scopus, faces limitations in metadata accuracy and completeness, requiring further research to ensure its reliability for diverse bibliometric analyses.  \n",
      "\n",
      "**Concluding Statement:** Addressing these challenges requires standardized metadata practices, interdisciplinary collaboration, and critical evaluation of data sources to ensure equitable and accurate citation metric normalization across fields.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.1162/qss_a_00112', '10.31222/osf.io/smxe5', '10.48550/arXiv.2406.15154', '10.48550/arXiv.2404.17663']\n",
      "45\n",
      "For query: What are the main challenges in normalizing citation metrics across scientific fields?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 4\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 11\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary answer:** Methods to detect and correct errors in bibliographic datasets include analyzing missing data patterns, comparing data across multiple sources, and assessing metadata completeness and accuracy.  \n",
      "\n",
      "DOI: 10.5281/ZENODO.13960973 - This study introduces a method to detect errors in bibliographic datasets by analyzing missing data patterns, exemplified by identifying incorrectly affiliated papers in ETH Zurich’s publication metadata, with potential applications to other data types.  \n",
      "DOI: 10.1162/qss_a_00112 - A large-scale comparison of bibliographic data sources (Scopus, Web of Science, Dimensions, Crossref, Microsoft Academic) highlights differences in coverage, completeness, and accuracy of citation links, emphasizing the need for flexible filters and comprehensive coverage.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - This research compares publication and document types across OpenAlex, Web of Science, Scopus, PubMed, and Semantic Scholar, revealing inconsistencies in classification and coverage, particularly for OpenAlex as a free alternative.  \n",
      "DOI: 10.31222/osf.io/smxe5 - The study evaluates the availability of open bibliographic metadata in Crossref, noting improvements over time but identifying gaps in publisher efforts to ensure full openness.  \n",
      "\n",
      "**Concluding statement:** Error detection and correction in bibliographic datasets rely on methods such as pattern analysis, cross-source comparisons, and metadata completeness assessments, with ongoing efforts to enhance data quality and openness.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.5281/ZENODO.13960973, Title: Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "Retrieved DOIs: ['10.5281/ZENODO.13960973', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.31222/osf.io/smxe5']\n",
      "45\n",
      "For query: What methods are used to detect and correct errors in bibliographic datasets?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 4\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 12\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: The provided context does not contain information about how RAG (Retrieval-Augmented Generation) works, so I cannot provide an answer based on the given documents.  \n",
      "\n",
      "Concluding statement: Since none of the documents in the context discuss RAG or its mechanisms, I am unable to provide a summary or supporting details on this topic.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.5860/crl.86.1.101, Title: Identifying Metadata Quality Issues Across Cultures\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.31222/osf.io/smxe5', '10.5860/crl.86.1.101', '10.1162/qss_a_00112']\n",
      "45\n",
      "For query: tell me about how RAG works.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.438\n",
      "Faithfulness score: 0\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 13\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Several studies have examined the availability, completeness, and accuracy of abstracts in metadata across various bibliographic databases.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This study provides an overview of the availability of six metadata elements, including abstracts, in Crossref, noting improvements over time but highlighting the need for further efforts by publishers to ensure full openness.  \n",
      "DOI: 10.1162/qss_a_00286 - The research compares the completeness degree of publication metadata, including abstracts, in eight free-access scholarly databases, finding that academic search engines have lower completeness rates compared to third-party databases.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - This analysis assesses the suitability of OpenAlex for bibliometric analyses, including the completeness and accuracy of metadata such as abstracts, and compares it to Scopus.  \n",
      "\n",
      "Concluding statement: These studies collectively emphasize the importance of abstract metadata in bibliographic databases, highlighting variations in availability, completeness, and accuracy across different platforms and the need for continued efforts to improve metadata quality.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00286', '10.48550/arXiv.2404.17663']\n",
      "45\n",
      "For query: which studies examined the abstract in metadata?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 0\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Several studies have examined citations, focusing on bibliographic data sources, citation metrics, and metadata completeness.  \n",
      "\n",
      "DOI: 10.1162/qss_a_00112 - This study compares five bibliographic data sources (Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic) and analyzes differences in citation link completeness and accuracy, emphasizing the importance of combining comprehensive coverage with flexible filters.  \n",
      "DOI: 10.1371/journal.pbio.1002542 - The paper discusses challenges in normalizing citation metrics across fields, document types, and databases, highlighting the need for careful consideration of underlying assumptions when using these metrics.  \n",
      "DOI: 10.1162/qss_a_00286 - This research evaluates the completeness of publication metadata in eight free-access scholarly databases, finding that third-party databases generally have higher metadata quality and completeness compared to academic search engines.  \n",
      "\n",
      "In conclusion, these studies collectively contribute to understanding the nuances of citation analysis, the reliability of different data sources, and the challenges in standardizing citation metrics and metadata completeness.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00286']\n",
      "45\n",
      "For query: which studies examined citations?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 1\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: OpenAlex is an emerging, inclusive bibliometric database that offers a reliable alternative to traditional databases like Scopus and Web of Science, particularly for certain types of analyses, but requires further research to address its limitations in metadata accuracy and completeness.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - An analysis comparing OpenAlex to Scopus reveals that OpenAlex is a superset of Scopus, making it a viable alternative for some analyses, especially at the country level, but highlights issues with metadata accuracy and completeness that necessitate further research.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This study examines the availability of open bibliographic metadata in Crossref, noting improvements over time but emphasizing the need for publishers to enhance efforts for full openness, which is relevant to OpenAlex as it relies on such metadata for its inclusivity.\n",
      "\n",
      "Concluding statement: While OpenAlex shows promise as an inclusive and comprehensive bibliometric database, addressing its current limitations in metadata quality and completeness is crucial for its broader adoption and reliability in diverse research analyses.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.1162/qss_a_00022, Title: Crossref: The sustainable source of community-owned scholarly metadata\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00212', '10.1371/journal.pbio.1002542', '10.1162/qss_a_00022']\n",
      "45\n",
      "For query: Tell me about OpenAlex.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.519\n",
      "Faithfulness score: 2\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 2\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Crossref is a key provider of open bibliographic metadata, contributing to the availability and accessibility of scholarly publication data, though challenges remain in ensuring full openness and metadata quality.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref has seen improvements in the availability of metadata elements such as reference lists, abstracts, and ORCIDs, particularly for journal articles, but many publishers still need to enhance efforts for full openness.  \n",
      "DOI: 10.1162/qss_a_00112 - A large-scale comparison of bibliographic data sources, including Crossref, highlights its strengths in coverage and citation accuracy, emphasizing the need for comprehensive and flexible data sources in scholarly research.  \n",
      "DOI: 10.5860/crl.86.1.101 - Metadata quality, consistency, and completeness in systems like Crossref are influenced by cultural and strategic factors, underscoring the need for interventions to address tensions between standardization and cultural representation.  \n",
      "\n",
      "Concluding statement: Crossref plays a vital role in advancing open bibliographic metadata, yet ongoing efforts are required to address gaps in openness, metadata quality, and cultural representation to fully realize its potential in scholarly communication.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.5860/crl.86.1.101, Title: Identifying Metadata Quality Issues Across Cultures\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112', '10.5860/crl.86.1.101']\n",
      "45\n",
      "For query: Tell me about Crossref.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.569\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 3\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Several papers evaluate linguistic coverage and language-related metadata in scholarly databases, highlighting differences in completeness, accuracy, and bias across various platforms.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 - This study assesses the linguistic coverage of OpenAlex, comparing its metadata accuracy and completeness with Web of Science (WoS). It finds that OpenAlex offers more balanced linguistic coverage but notes inaccuracies in language metadata, overestimating English and underestimating other languages.  \n",
      "DOI: 10.1162/qss_a_00286 - This paper compares the completeness of publication metadata, including language, in eight free-access scholarly databases. It reveals that academic search engines have lower completeness rates, while third-party databases exhibit higher metadata quality, though integration issues lead to information loss.  \n",
      "DOI: 10.31222/osf.io/smxe5 - This research evaluates the availability of open bibliographic metadata in Crossref, including language information, and finds improvements over time but highlights the need for publishers to enhance metadata openness.  \n",
      "DOI: 10.1162/qss_a_00112 - This large-scale comparison of bibliographic data sources analyzes coverage differences, including document types and disciplines, but does not explicitly focus on linguistic metadata.  \n",
      "\n",
      "Concluding statement: These studies collectively underscore the importance of evaluating linguistic coverage and metadata quality in scholarly databases, revealing both strengths and limitations across platforms and emphasizing the need for continued improvements in metadata accuracy and inclusivity.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2409.10633, Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1162/qss_a_00286', '10.31222/osf.io/smxe5', '10.48550/arXiv.2409.10633']\n",
      "45\n",
      "For query: Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 4\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 4\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Several papers address funding metadata, its availability, and analysis in scholarly databases, particularly focusing on Crossref, Scopus, and Web of Science.  \n",
      "\n",
      "DOI: 10.1162/qss_a_00212 - This paper analyzes the open availability of funding data in Crossref, especially for COVID-19 research, and compares it with Scopus and Web of Science, highlighting limited coverage and quality issues in these databases.  \n",
      "DOI: 10.31222/osf.io/smxe5 - The study evaluates the availability of funding information as part of bibliographic metadata in Crossref, noting improvements over time but emphasizing the need for further efforts by publishers to ensure full openness.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While primarily assessing OpenAlex's suitability for bibliometric analyses, this paper touches on metadata accuracy and completeness, which indirectly relates to funding metadata availability in scholarly databases.  \n",
      "\n",
      "Concluding statement: These papers collectively highlight the challenges and opportunities in accessing and analyzing funding metadata across scholarly databases, with a particular focus on improving openness and data quality in platforms like Crossref.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "Retrieved DOIs: ['10.1162/qss_a_00212', '10.31222/osf.io/smxe5', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112']\n",
      "45\n",
      "For query: Which papers address funding metadata, its availability, or its analysis in scholarly databases?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 5\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary Answer:** Several papers discuss the use of Retrieval-Augmented Generation (RAG) in large language models, focusing on its applications, benchmarking, improvements for multi-hop queries, metadata-based data exploration, and the relationship between relevance and utility in RAG systems.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - This survey paper provides a comprehensive overview of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG, and examines their tripartite foundation (retrieval, generation, augmentation). It highlights state-of-the-art technologies, evaluation frameworks, and future research directions.  \n",
      "DOI: 10.1609/aaai.v38i16.29728 - This paper introduces the Retrieval-Augmented Generation Benchmark (RGB) to evaluate LLMs' performance in RAG across four fundamental abilities: noise robustness, negative rejection, information integration, and counterfactual robustness, revealing challenges in current RAG applications.  \n",
      "DOI: 10.48550/arXiv.2406.13213 - The study proposes Multi-Meta-RAG, a method using LLM-extracted metadata and database filtering to improve RAG for multi-hop queries, demonstrating significant enhancements on the MultiHop-RAG benchmark.  \n",
      "DOI: 10.48550/arXiv.2410.04231 - This research introduces a RAG-based architecture for metadata-driven data exploration, integrating LLMs with vector databases to enhance dataset discovery, with experimental results showing promise across tasks like dataset recommendation and tag estimation.  \n",
      "DOI: 10.1007/978-3-031-88708-6_3 - The paper investigates the relationship between relevance and utility in RAG systems, finding a small positive correlation that diminishes with larger context sizes and emphasizing the importance of effective retrieval models for downstream performance.  \n",
      "\n",
      "**Concluding Statement:** These papers collectively advance the understanding and application of RAG in LLMs, addressing challenges, proposing innovative solutions, and establishing benchmarks for future research.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2312.10997, Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "DOI: https://doi.org/10.1609/aaai.v38i16.29728, Title: Benchmarking Large Language Models in Retrieval-Augmented Generation\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.13213, Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2410.04231, Title: Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\n",
      "DOI: https://doi.org/10.1007/978-3-031-88708-6_3, Title: Is Relevance Propagated from Retriever to Generator in RAG?\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1609/aaai.v38i16.29728', '10.48550/arXiv.2406.13213', '10.48550/arXiv.2410.04231', '10.1007/978-3-031-88708-6_3']\n",
      "45\n",
      "For query: Which papers discuss the use of Retrieval-Augmented Generation (RAG) in large language models or related applications?:\n",
      "Precision: 0.956\n",
      "Recall: 0.956\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.887\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 6\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Crossref plays a crucial role in the scholarly research ecosystem by providing open bibliographic metadata, supporting funding traceability, and serving as a comprehensive data source for bibliometric analyses.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref is highlighted as a key provider of open bibliographic metadata, with improvements noted in the availability of metadata elements like reference lists, abstracts, and funding information, though further efforts from publishers are needed for full openness.  \n",
      "DOI: 10.1162/qss_a_00212 - The study emphasizes Crossref’s role in enabling funding agencies to trace publications resulting from their funding, particularly for COVID-19 research, while noting limitations in coverage and quality of funding data.  \n",
      "DOI: 10.1162/qss_a_00112 - Crossref is compared with other bibliographic data sources, showcasing its strengths in comprehensive coverage and flexibility, though differences in citation accuracy and completeness are noted.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While not directly about Crossref, this study underscores the importance of inclusive databases like OpenAlex, which complement Crossref in addressing underrepresentation in traditional databases.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - This analysis highlights the variability in publication and document types across databases, positioning Crossref as a valuable resource in the context of free alternatives like OpenAlex for bibliometric analyses.  \n",
      "\n",
      "In conclusion, Crossref is a vital component of the scholarly research ecosystem, facilitating open access to metadata, supporting funding transparency, and contributing to comprehensive bibliometric analyses, though ongoing improvements are necessary to address existing limitations.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.1162/qss_a_00212', '10.1162/qss_a_00112', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2406.15154']\n",
      "45\n",
      "For query: What is Crossref’s role in the scholarly research ecosystem?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 7\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: OpenAlex is a promising, inclusive bibliometric database with extensive coverage, particularly at the country level, but it faces limitations in metadata accuracy and completeness compared to traditional databases like Scopus and Web of Science.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - This study compares OpenAlex to Scopus, concluding that OpenAlex is a superset of Scopus and a reliable alternative for certain analyses, especially at the country level. However, it highlights issues with metadata accuracy and completeness, necessitating further research to address these limitations for broader use.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - This analysis compares OpenAlex with other databases, emphasizing its growing importance as a free alternative. It notes significant differences in publication and document type classifications across databases, which impacts bibliometric analysis, and underscores the need for standardized typologies.  \n",
      "\n",
      "Concluding statement: While OpenAlex offers a more inclusive and accessible option for bibliometric research, its current limitations in metadata quality and standardization require careful consideration and further development to ensure its reliability across diverse analytical contexts.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1162/qss_a_00212']\n",
      "45\n",
      "For query: What are the key features and limitations of OpenAlex as a bibliometric database?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 2\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 8\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Web of Science (WoS) is a well-established bibliometric database with strengths in comprehensive coverage of scientific literature and accurate citation links, but it has limitations in representing certain disciplines and world regions, and its proprietary nature restricts accessibility.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2406.15154 - This study highlights that WoS, like other proprietary databases, classifies publications differently, which can impact bibliometric analyses, and emphasizes the growing importance of free alternatives like OpenAlex.  \n",
      "DOI: 10.1162/qss_a_00112 - The large-scale comparison underscores WoS's strength in comprehensive coverage and accurate citation links but notes the need for flexible filters to refine literature selections, a feature that may be limited in WoS.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - This analysis points out that WoS, alongside Scopus, systematically underrepresents certain disciplines and regions, a weakness addressed by newer, more inclusive databases like OpenAlex.  \n",
      "\n",
      "In conclusion, while WoS remains a cornerstone in bibliometric research due to its extensive coverage and reliable citation data, its proprietary nature and biases in representation are significant drawbacks, prompting the exploration of alternative databases.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.48550/arXiv.2404.17663', '10.31222/osf.io/smxe5', '10.1162/qss_a_00286']\n",
      "45\n",
      "For query: What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 9\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary Answer:** Retrieval-Augmented Generation (RAG) improves question answering and information retrieval systems by integrating external knowledge sources, enhancing accuracy, reducing hallucinations, and enabling multi-hop reasoning, while also addressing challenges like outdated knowledge and non-transparent reasoning.\n",
      "\n",
      "DOI: 10.48550/arXiv.2109.05052 - This study investigates knowledge conflicts in question answering, highlighting models' over-reliance on parametric knowledge, which leads to hallucinations. It proposes a method to mitigate this, improving generalization and reducing hallucinations, emphasizing the need for evaluating model tendencies to hallucinate rather than read.  \n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - The paper introduces M-RAG, a method that enhances open-domain question answering by incorporating metadata retrieval and generative models like GPT-3.5 and GPT-4. This approach improves answer accuracy by up to 46% compared to baseline systems and 6% over existing RAG methods, enabling retrieval of external information not present in LLMs.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - Multi-Meta-RAG is proposed to address RAG's limitations in multi-hop queries by using database filtering with LLM-extracted metadata. This method significantly improves performance on multi-hop benchmarks, demonstrating its effectiveness in selecting relevant documents for complex queries.  \n",
      "\n",
      "DOI: 10.1145/3626772.3657848 - This perspective paper discusses the opportunities and challenges of applying LLMs to information retrieval, arguing that LLMs will not replace search engines but will need to learn to interact with them. It outlines future research directions for integrating LLMs into IR systems.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - This survey provides a comprehensive overview of RAG, detailing its progression from Naive to Modular RAG. It highlights RAG's ability to enhance LLM accuracy, credibility, and knowledge integration, while also addressing challenges like hallucination and outdated knowledge.  \n",
      "\n",
      "**Concluding Statement:** RAG has emerged as a transformative approach in question answering and information retrieval, addressing key limitations of LLMs by leveraging external knowledge. Its advancements, as evidenced by methods like M-RAG and Multi-Meta-RAG, demonstrate significant improvements in accuracy, reasoning, and generalization, while ongoing research continues to refine its capabilities and applications.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2109.05052, Title: Entity-Based Knowledge Conflicts in Question Answering\n",
      "DOI: https://doi.org/10.6109/jkiice.2023.27.12.1489, Title: M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.13213, Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "DOI: https://doi.org/10.1145/3626772.3657848, Title: Large Language Models and Future of Information Retrieval: Opportunities and Challenges\n",
      "DOI: https://doi.org/10.48550/arXiv.2312.10997, Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "Retrieved DOIs: ['10.48550/arXiv.2109.05052', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2406.13213', '10.1145/3626772.3657848', '10.48550/arXiv.2312.10997']\n",
      "45\n",
      "For query: How is RAG used to improve question answering or information retrieval systems?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 10\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary answer:** Normalizing citation metrics across scientific fields is challenging due to differences in citation practices, document types, database coverage, and metadata completeness, which vary significantly across disciplines and data sources.  \n",
      "\n",
      "DOI: 10.1371/journal.pbio.1002542 - This study highlights the challenges in normalizing citation metrics, including differences across fields, publication age, document types, and database coverage, emphasizing the need for careful consideration of underlying assumptions when using such metrics.  \n",
      "DOI: 10.1162/qss_a_00112 - A large-scale comparison of bibliographic data sources (Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic) reveals significant variations in coverage, completeness, and accuracy of citation links, underscoring the importance of flexible filtering and comprehensive coverage for normalization efforts.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - This analysis demonstrates that publication and document types differ considerably across databases, complicating the distinction between research and non-research texts, which is critical for accurate bibliometric analysis and normalization.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While OpenAlex emerges as a more inclusive alternative to traditional databases like Scopus, its limitations in metadata accuracy and completeness highlight ongoing challenges in creating a universally reliable data source for cross-field normalization.  \n",
      "\n",
      "**Concluding statement:** The variability in citation practices, document classification, and database coverage across fields and data sources necessitates a nuanced approach to normalizing citation metrics, with continued efforts to improve metadata completeness and accuracy in emerging inclusive databases.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.1162/qss_a_00112', '10.31222/osf.io/smxe5', '10.48550/arXiv.2406.15154', '10.48550/arXiv.2404.17663']\n",
      "45\n",
      "For query: What are the main challenges in normalizing citation metrics across scientific fields?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 4\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 11\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary Answer:** Methods to detect and correct errors in bibliographic datasets include analyzing missing data patterns, comparing data across multiple sources, and assessing metadata completeness and accuracy.  \n",
      "\n",
      "DOI: 10.5281/ZENODO.13960973 - This paper introduces a method to detect errors in bibliographic datasets by analyzing missing data patterns, exemplified by identifying incorrectly affiliated papers in ETH Zurich’s publication metadata. The method is versatile and can be applied to various data types, offering potential corrections for both data providers and users.  \n",
      "DOI: 10.1162/qss_a_00112 - This study compares five major bibliographic data sources (Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic) to assess differences in coverage, completeness, and accuracy of citation links, highlighting the need for comprehensive coverage and flexible filtering in bibliographic datasets.  \n",
      "DOI: 10.31222/osf.io/smxe5 - The paper evaluates the availability of open bibliographic metadata in Crossref, noting improvements over time but emphasizing the need for publishers to enhance metadata openness, particularly for elements like reference lists and funding information.  \n",
      "\n",
      "**Concluding Statement:** Error detection and correction in bibliographic datasets rely on methodologies such as pattern analysis, cross-source comparison, and metadata completeness assessments, which collectively improve data quality and reliability for scholarly research.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.5281/ZENODO.13960973, Title: Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "Retrieved DOIs: ['10.5281/ZENODO.13960973', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.31222/osf.io/smxe5']\n",
      "45\n",
      "For query: What methods are used to detect and correct errors in bibliographic datasets?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 12\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: The provided context does not contain information about how RAG (Retrieval-Augmented Generation) works, so I cannot provide an answer.\n",
      "\n",
      "Since there is no relevant information in the provided context about RAG, I am unable to fulfill the request. The context primarily focuses on citation metrics, bibliographic databases, and metadata quality, which are unrelated to the topic of RAG.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.5860/crl.86.1.101, Title: Identifying Metadata Quality Issues Across Cultures\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.31222/osf.io/smxe5', '10.5860/crl.86.1.101', '10.1162/qss_a_00112']\n",
      "45\n",
      "For query: tell me about how RAG works.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.438\n",
      "Faithfulness score: 0\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 13\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Several studies have examined the availability, completeness, and quality of abstracts in metadata across various bibliographic databases.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This study provides an overview of the availability of six metadata elements, including abstracts, in Crossref, noting improvements over time but highlighting the need for further efforts by publishers to ensure full openness.  \n",
      "DOI: 10.1162/qss_a_00286 - The research compares the completeness degree of publication metadata, including abstracts, in eight free-access scholarly databases, finding that academic search engines have lower completeness rates compared to third-party databases.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - This analysis assesses the suitability of OpenAlex for bibliometric analyses, including the completeness and accuracy of metadata such as abstracts, and compares it to Scopus.  \n",
      "\n",
      "Concluding statement: These studies collectively emphasize the importance of abstract metadata in bibliographic databases, highlighting variations in availability, completeness, and quality across different platforms, with implications for open access and bibliometric research.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00286', '10.48550/arXiv.2404.17663']\n",
      "45\n",
      "For query: which studies examined the abstract in metadata?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 0\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary answer:** Several studies have examined citations, focusing on the comparison of bibliographic databases, citation metrics, and the completeness of metadata in scholarly databases.  \n",
      "\n",
      "DOI: 10.1162/qss_a_00112 - This study compares five bibliographic data sources (Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic) and specifically analyzes differences in the completeness and accuracy of citation links, highlighting the strengths and weaknesses of each database.  \n",
      "DOI: 10.1371/journal.pbio.1002542 - The study discusses the challenges and normalization approaches for citation metrics, emphasizing their use in appraising research across fields, documents, and institutions, while cautioning about underlying assumptions.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While primarily assessing OpenAlex's suitability for bibliometric analyses, this study also compares it to Scopus, touching on metadata accuracy and completeness, which indirectly relates to citation data reliability.  \n",
      "DOI: 10.1162/qss_a_00286 - This research evaluates the completeness of publication metadata in eight free-access scholarly databases, including aspects relevant to citation data, such as bibliographic information and identifiers, though citations are not the primary focus.  \n",
      "\n",
      "**Concluding statement:** While not all studies directly focus on citations, they collectively contribute to understanding the reliability and limitations of citation data across various bibliographic databases and metrics.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00286']\n",
      "45\n",
      "For query: which studies examined citations?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 4\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 1\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: OpenAlex is an emerging, inclusive bibliometric database that offers a reliable alternative to traditional databases like Scopus and Web of Science, particularly for certain types of analyses, but requires further research to address its limitations in metadata accuracy and completeness.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - An analysis comparing OpenAlex to Scopus reveals that OpenAlex is a superset of Scopus, making it a viable alternative for some bibliometric analyses, especially at the country level, although issues with metadata accuracy and completeness necessitate further research to fully leverage its potential.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - While not directly about OpenAlex, this study highlights the improving availability of open bibliographic metadata in Crossref, which is relevant to the broader context of open data infrastructures that support databases like OpenAlex.\n",
      "\n",
      "Concluding statement: OpenAlex shows promise as an inclusive and comprehensive bibliometric database, but ongoing efforts to enhance its metadata quality and completeness are essential to fully realize its potential as a reliable tool for diverse scientometric analyses.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.1162/qss_a_00022, Title: Crossref: The sustainable source of community-owned scholarly metadata\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00212', '10.1371/journal.pbio.1002542', '10.1162/qss_a_00022']\n",
      "45\n",
      "For query: Tell me about OpenAlex.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.519\n",
      "Faithfulness score: 2\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 2\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Crossref is a key provider of open bibliographic metadata, contributing to the availability and accessibility of scholarly publication data, though challenges remain in ensuring full openness and metadata quality.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref has seen improvements in the availability of metadata elements such as reference lists, abstracts, and ORCIDs, particularly for journal articles, but many publishers still need to enhance efforts for full openness.  \n",
      "DOI: 10.1162/qss_a_00112 - A large-scale comparison of bibliographic data sources, including Crossref, highlights its strengths in comprehensive coverage but also underscores the need for flexible filters to address variations in metadata completeness and accuracy.  \n",
      "DOI: 10.5860/crl.86.1.101 - Metadata quality, consistency, and completeness in systems like Crossref are influenced by cultural and strategic factors, revealing tensions between standardization and sociocultural representation.  \n",
      "\n",
      "Concluding statement: Crossref plays a vital role in advancing open bibliographic metadata, yet ongoing efforts are required to address gaps in openness, metadata quality, and cultural representation to fully realize its potential.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.5860/crl.86.1.101, Title: Identifying Metadata Quality Issues Across Cultures\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112', '10.5860/crl.86.1.101']\n",
      "45\n",
      "For query: Tell me about Crossref.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.569\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 3\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Several papers evaluate the linguistic coverage or language-related metadata in scholarly databases, highlighting differences in completeness, accuracy, and inclusivity across various platforms.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 - This study assesses the linguistic coverage of OpenAlex, comparing it with Web of Science (WoS) and manually validating a sample of articles, finding that OpenAlex offers more balanced linguistic coverage but with inaccuracies in language metadata.  \n",
      "DOI: 10.1162/qss_a_00286 - This research compares metadata completeness in eight free-access scholarly databases, including language-related fields, revealing that third-party databases like OpenAlex and Dimensions have higher completeness rates compared to academic search engines.  \n",
      "DOI: 10.31222/osf.io/smxe5 - This paper examines the availability of metadata elements, including language, in Crossref, noting improvements over time but emphasizing the need for publishers to enhance openness and completeness.  \n",
      "\n",
      "These studies collectively underscore the importance of evaluating linguistic coverage and metadata quality in scholarly databases, with OpenAlex emerging as a promising but imperfect alternative to traditional proprietary databases. Further efforts are needed to improve metadata accuracy and inclusivity across platforms.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2409.10633, Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1162/qss_a_00286', '10.31222/osf.io/smxe5', '10.48550/arXiv.2409.10633']\n",
      "45\n",
      "For query: Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 4\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: Several papers address funding metadata, its availability, and analysis in scholarly databases, particularly focusing on Crossref, Scopus, and Web of Science.  \n",
      "\n",
      "DOI: 10.1162/qss_a_00212 - This paper analyzes the open availability of funding data in Crossref, especially for COVID-19 research, and compares it with proprietary databases like Scopus and Web of Science, highlighting limited coverage and quality issues in funding metadata.  \n",
      "DOI: 10.31222/osf.io/smxe5 - The study examines the availability of funding information as part of bibliographic metadata in Crossref, noting improvements over time but emphasizing the need for publishers to enhance openness and completeness.  \n",
      "DOI: 10.1162/qss_a_00112 - While primarily a large-scale comparison of bibliographic data sources, this paper touches on the completeness and accuracy of metadata, including funding information, across platforms like Scopus, Web of Science, and Crossref.  \n",
      "\n",
      "Concluding statement: These papers collectively highlight the challenges and opportunities in improving the availability and quality of funding metadata in scholarly databases, with a particular focus on open data infrastructures like Crossref.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "Retrieved DOIs: ['10.1162/qss_a_00212', '10.31222/osf.io/smxe5', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112']\n",
      "45\n",
      "For query: Which papers address funding metadata, its availability, or its analysis in scholarly databases?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 5\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary Answer:** Several papers discuss the application of Retrieval-Augmented Generation (RAG) in large language models, focusing on its advancements, benchmarking, multi-hop query improvements, metadata-based data exploration, and the relationship between relevance and utility in RAG systems.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - This survey paper provides a comprehensive overview of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG, highlighting their tripartite foundation (retrieval, generation, augmentation) and state-of-the-art technologies, while also addressing challenges and future research directions.  \n",
      "DOI: 10.1609/aaai.v38i16.29728 - This study introduces the Retrieval-Augmented Generation Benchmark (RGB) to evaluate the performance of LLMs in RAG across four fundamental abilities (noise robustness, negative rejection, information integration, counterfactual robustness), revealing areas for improvement in applying RAG to LLMs.  \n",
      "DOI: 10.48550/arXiv.2406.13213 - The paper proposes Multi-Meta-RAG, a method that uses LLM-extracted metadata for database filtering to enhance RAG's performance on multi-hop queries, significantly improving results on the MultiHop-RAG benchmark.  \n",
      "DOI: 10.48550/arXiv.2410.04231 - This research introduces a RAG-based architecture for metadata-driven data exploration, integrating LLMs with external vector databases to improve dataset discovery, with experimental results demonstrating its effectiveness across various tasks despite varying performance.  \n",
      "DOI: 10.1007/978-3-031-88708-6_3 - The study investigates the relationship between relevance and utility in RAG systems, finding a small positive correlation that diminishes with larger context sizes and concluding that more effective retrieval models generally enhance downstream RAG performance.  \n",
      "\n",
      "**Concluding Statement:** These papers collectively advance the understanding and application of RAG in LLMs, addressing challenges, proposing innovative solutions, and establishing benchmarks for future research and development.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2312.10997, Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "DOI: https://doi.org/10.1609/aaai.v38i16.29728, Title: Benchmarking Large Language Models in Retrieval-Augmented Generation\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.13213, Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2410.04231, Title: Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\n",
      "DOI: https://doi.org/10.1007/978-3-031-88708-6_3, Title: Is Relevance Propagated from Retriever to Generator in RAG?\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1609/aaai.v38i16.29728', '10.48550/arXiv.2406.13213', '10.48550/arXiv.2410.04231', '10.1007/978-3-031-88708-6_3']\n",
      "45\n",
      "For query: Which papers discuss the use of Retrieval-Augmented Generation (RAG) in large language models or related applications?:\n",
      "Precision: 0.956\n",
      "Recall: 0.956\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.887\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 6\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Crossref plays a crucial role in the scholarly research ecosystem by providing open bibliographic metadata and supporting the traceability of research outputs, though its coverage and data quality require ongoing improvement.  \n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref serves as a key source of open bibliographic metadata, with improvements noted in the availability of elements like reference lists and funding information, though further efforts from publishers are needed for full openness.  \n",
      "DOI: 10.1162/qss_a_00212 - Crossref facilitates the tracking of research publications funded by agencies, particularly for COVID-19 research, but its funding data coverage and quality are limited compared to proprietary databases like Scopus and Web of Science.  \n",
      "DOI: 10.1162/qss_a_00112 - Crossref is one of several multidisciplinary bibliographic data sources, offering comprehensive coverage of scientific literature, though it differs from others like Scopus and Web of Science in terms of document coverage, citation accuracy, and completeness.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While not directly about Crossref, this study highlights the emergence of inclusive databases like OpenAlex, which complement traditional sources and underscore the need for accurate and complete metadata across all platforms.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - This analysis emphasizes the variability in publication and document typologies across databases, including Crossref, and the growing importance of open alternatives like OpenAlex for bibliometric analyses.  \n",
      "\n",
      "Concluding statement: Crossref’s role in promoting open access to bibliographic metadata and supporting research traceability is vital, but addressing gaps in coverage and data quality remains essential for its continued effectiveness in the scholarly ecosystem.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.1162/qss_a_00212', '10.1162/qss_a_00112', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2406.15154']\n",
      "45\n",
      "For query: What is Crossref’s role in the scholarly research ecosystem?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 7\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: OpenAlex is a promising, inclusive bibliometric database that offers comprehensive coverage and serves as a reliable alternative to traditional databases like Scopus, but it faces limitations in metadata accuracy and completeness.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - This study compares OpenAlex to Scopus, concluding that OpenAlex is a superset of Scopus, particularly useful for country-level analyses, but highlights issues with metadata accuracy and completeness that require further research to address.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - The analysis emphasizes OpenAlex's growing importance as a free alternative for bibliometric analyses, noting significant differences in publication and document type classifications across databases, which impacts its usability for certain studies.  \n",
      "\n",
      "Concluding statement: While OpenAlex demonstrates potential as an inclusive and comprehensive bibliometric database, addressing its metadata limitations is crucial for its broader application in diverse bibliometric analyses.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.1162/qss_a_00212, Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.1162/qss_a_00212']\n",
      "45\n",
      "For query: What are the key features and limitations of OpenAlex as a bibliometric database?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 2\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 8\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Web of Science (WoS) is a well-established bibliometric database with strengths in comprehensive coverage of scientific literature and accurate citation links, but it faces weaknesses in underrepresentation of certain disciplines and regions, as well as being a proprietary and costly resource.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2406.15154 - This study highlights that WoS, like other proprietary databases, classifies publications differently, which can affect the distinction between research and non-research texts, impacting bibliometric analysis.  \n",
      "DOI: 10.1162/qss_a_00112 - The large-scale comparison emphasizes WoS's strength in comprehensive coverage and accurate citation links but notes the importance of flexible filters for literature selection, an area where WoS may be less adaptable compared to newer databases.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - This analysis underscores WoS's systematic underrepresentation of certain disciplines and world regions, a limitation that has spurred the development of more inclusive alternatives like OpenAlex.  \n",
      "\n",
      "In conclusion, while WoS remains a cornerstone in bibliometric research due to its reliability and extensive coverage, its proprietary nature, cost, and biases in representation are significant drawbacks that have led to the rise of alternative databases.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.1162/qss_a_00286, Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.48550/arXiv.2404.17663', '10.31222/osf.io/smxe5', '10.1162/qss_a_00286']\n",
      "45\n",
      "For query: What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.550\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 9\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary Answer:** Retrieval-Augmented Generation (RAG) improves question answering and information retrieval systems by integrating external knowledge sources, enhancing accuracy, reducing hallucinations, and enabling multi-hop reasoning, as demonstrated through various methods like metadata retrieval, database filtering, and mitigation of parametric over-reliance.  \n",
      "\n",
      "DOI: 10.48550/arXiv.2109.05052 - This study investigates knowledge conflicts in question answering, highlighting models' over-reliance on parametric knowledge, which causes hallucinations. It proposes a method to mitigate this, improving out-of-distribution generalization by 4%-7% and encouraging models to use contextual information effectively.  \n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - The M-RAG method enhances open-domain question answering by incorporating metadata retrieval and generative models like GPT-4, achieving up to 46% performance improvement over baseline systems and 6% over traditional RAG, while ensuring accurate source attribution.  \n",
      "DOI: 10.48550/arXiv.2406.13213 - Multi-Meta-RAG addresses multi-hop queries by using LLM-extracted metadata for database filtering, significantly improving document selection and performance on multi-hop benchmarks, demonstrating RAG's adaptability to complex queries.  \n",
      "DOI: 10.1145/3626772.3657848 - This perspective paper discusses the role of LLMs in information retrieval, emphasizing that LLMs cannot replace search engines but can interact with them to enhance user experience, while addressing challenges like hallucination.  \n",
      "DOI: 10.48550/arXiv.2312.10997 - A comprehensive survey on RAG highlights its ability to merge LLMs' intrinsic knowledge with external databases, improving accuracy and credibility, and outlines advancements in retrieval, generation, and augmentation techniques.  \n",
      "\n",
      "**Concluding Statement:** RAG’s integration of external knowledge and advanced techniques like metadata retrieval and database filtering significantly enhances question answering and information retrieval systems, addressing challenges such as hallucinations and multi-hop reasoning, while paving the way for future research and applications.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.48550/arXiv.2109.05052, Title: Entity-Based Knowledge Conflicts in Question Answering\n",
      "DOI: https://doi.org/10.6109/jkiice.2023.27.12.1489, Title: M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.13213, Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "DOI: https://doi.org/10.1145/3626772.3657848, Title: Large Language Models and Future of Information Retrieval: Opportunities and Challenges\n",
      "DOI: https://doi.org/10.48550/arXiv.2312.10997, Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "Retrieved DOIs: ['10.48550/arXiv.2109.05052', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2406.13213', '10.1145/3626772.3657848', '10.48550/arXiv.2312.10997']\n",
      "45\n",
      "For query: How is RAG used to improve question answering or information retrieval systems?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 5\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 10\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary answer:** Normalizing citation metrics across scientific fields is challenging due to differences in citation practices, database coverage, document types, and metadata completeness, which vary significantly across disciplines and data sources.  \n",
      "\n",
      "DOI: 10.1371/journal.pbio.1002542 - This study highlights the challenges in normalizing citation metrics, emphasizing the need to account for field-specific differences, publication age, document types, and database coverage, while also addressing issues in combining metrics across multiple papers and attributing credit in multiauthored works.  \n",
      "DOI: 10.1162/qss_a_00112 - A large-scale comparison of bibliographic data sources (Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic) reveals significant differences in coverage, completeness, and accuracy of citation links across disciplines, underscoring the complexity of normalizing metrics when relying on diverse data sources.  \n",
      "DOI: 10.48550/arXiv.2406.15154 - This analysis demonstrates that publication and document types vary considerably between databases, complicating the distinction between research and non-research texts, which is critical for accurate bibliometric analysis across fields.  \n",
      "DOI: 10.48550/arXiv.2404.17663 - While OpenAlex emerges as a more inclusive alternative to traditional databases like Scopus, its limitations in metadata accuracy and completeness highlight ongoing challenges in ensuring reliable cross-field normalization.  \n",
      "\n",
      "**Concluding statement:** The variability in citation practices, database coverage, and metadata quality across scientific fields and data sources necessitates careful consideration and standardized approaches to achieve meaningful normalization of citation metrics.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.1162/qss_a_00112', '10.31222/osf.io/smxe5', '10.48550/arXiv.2406.15154', '10.48550/arXiv.2404.17663']\n",
      "45\n",
      "For query: What are the main challenges in normalizing citation metrics across scientific fields?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 4\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 11\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "**Summary Answer:** Methods to detect and correct errors in bibliographic datasets include analyzing missing data patterns, comparing data across multiple sources, and assessing metadata completeness and accuracy.  \n",
      "\n",
      "DOI: 10.5281/ZENODO.13960973 - This study introduces a method to detect errors in bibliographic datasets by analyzing missing data patterns, exemplified by identifying incorrectly affiliated papers in ETH Zurich’s publication metadata. The method is versatile and can be applied to various data types, offering potential corrections for both data providers and users.  \n",
      "DOI: 10.1162/qss_a_00112 - This research compares five major bibliographic data sources (Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic) to assess differences in document coverage, citation accuracy, and completeness. It highlights the need for comprehensive coverage and flexible filtering to improve dataset quality.  \n",
      "DOI: 10.31222/osf.io/smxe5 - This paper evaluates the availability of open bibliographic metadata in Crossref, identifying improvements in metadata elements like reference lists and author affiliations but noting gaps that require publisher efforts for full openness.  \n",
      "\n",
      "**Concluding Statement:** Error detection and correction in bibliographic datasets rely on methodologies such as pattern analysis, cross-source comparison, and metadata completeness assessments, as demonstrated by the studies above, which collectively contribute to enhancing data quality and reliability.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.5281/ZENODO.13960973, Title: Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2406.15154, Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "Retrieved DOIs: ['10.5281/ZENODO.13960973', '10.1371/journal.pbio.1002542', '10.48550/arXiv.2406.15154', '10.1162/qss_a_00112', '10.31222/osf.io/smxe5']\n",
      "45\n",
      "For query: What methods are used to detect and correct errors in bibliographic datasets?:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 3\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 12\n",
      "Retrieving documents and generating response...\n",
      "\n",
      "Generated Response:\n",
      "Summary answer: The provided context does not contain information about how RAG (Retrieval-Augmented Generation) works, so I cannot provide an answer based on the given documents.\n",
      "\n",
      "Since the context does not include any information related to RAG, I am unable to provide a summary or supporting documents.\n",
      "------\n",
      "Source documents: \n",
      "DOI: https://doi.org/10.1371/journal.pbio.1002542, Title: Citation Metrics: A Primer on How (Not) to Normalize\n",
      "DOI: https://doi.org/10.48550/arXiv.2404.17663, Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n",
      "DOI: https://doi.org/10.31222/osf.io/smxe5, Title: Crossref as a source of open bibliographic metadata\n",
      "DOI: https://doi.org/10.5860/crl.86.1.101, Title: Identifying Metadata Quality Issues Across Cultures\n",
      "DOI: https://doi.org/10.1162/qss_a_00112, Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542', '10.48550/arXiv.2404.17663', '10.31222/osf.io/smxe5', '10.5860/crl.86.1.101', '10.1162/qss_a_00112']\n",
      "45\n",
      "For query: tell me about how RAG works.:\n",
      "Precision: 0.778\n",
      "Recall: 0.778\n",
      "F1-Score: 0.778\n",
      "Accuracy: 0.778\n",
      "Balanced accuracy: 0.438\n",
      "Faithfulness score: 0\n",
      "\u001b[95msleeping for 30 seconds...\n",
      "\u001b[94mNext one....\n",
      "\u001b[96mWorking on row: 13\n",
      "\u001b[95m!!!!! All Done!!!!!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Path to the file containing documents and DOIs\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_jats\"\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_multi_lang\"\n",
    "\n",
    "# Read documents and DOIs from the file\n",
    "documents_with_doi = read_documents_with_doi(directory_path)\n",
    "documents = [doc[\"text\"] for doc in documents_with_doi]\n",
    "print(f\"Length of documents: {len(documents)}\")\n",
    "print(f\"Length of corpus: {len(documents_with_doi)}\")\n",
    "\n",
    "# define test loop\n",
    "def test_loop(query:str,ground_truth:List[str]):\n",
    "    #run the test from here\n",
    "    # set top_k global value - keep this as constant for all evaluations\n",
    "    global top_k\n",
    "    top_k = 5\n",
    "    #***** Begin chat session *****\n",
    "    response,retrieved_docs = rag_pipeline(query)\n",
    "\n",
    "    # Extract DOIs from retrieved documents\n",
    "    retrieved_dois = [doc.get('doi', \"\") for doc in retrieved_docs]\n",
    "    print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "    new_result = print_results(retrieved_dois, ground_truth, response, query)\n",
    "    results_df.loc[len(results_df)] = new_result\n",
    "\n",
    "    #save the queries and responses to separate dataframe to be manually annontated\n",
    "    answer_relevance_df = results_df[['Query','Response']].copy(deep=True)\n",
    "\n",
    "    # save out answer_relevance_df\n",
    "    filename=\"analysis/dense_answer_relevance_results.xlsx\"\n",
    "    answer_relevance_df.to_excel(filename)\n",
    "\n",
    "    filename = \"analysis/dense_analysis_results.xlsx\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    results_df.to_excel(filename)\n",
    "\n",
    "    #time sleep to avoid exceeding API limit\n",
    "    sleepy_time = 30\n",
    "    print(Fore.LIGHTMAGENTA_EX + f\"sleeping for {sleepy_time} seconds...\")\n",
    "    time.sleep(sleepy_time)\n",
    "    print(Fore.LIGHTBLUE_EX + f\"Next one....\")\n",
    "    return results_df\n",
    "\n",
    "#golden_set_df_test['Response\\nDense'] = golden_set_df_test.apply(lambda x: test_loop(x.query,x.ground_truth), axis=1)\n",
    "golden_set_df_query = golden_set_df['query'].to_list()\n",
    "golden_set_df_ground_truth = golden_set_df['ground_truth'].to_list()\n",
    "\n",
    "loop_length = 5\n",
    "while loop_length:\n",
    "    for i in range(len(golden_set_df_query)):\n",
    "        test_loop(golden_set_df_query[i],golden_set_df_ground_truth[i])\n",
    "        print(Fore.LIGHTCYAN_EX + f\"Working on row: {i}\")\n",
    "    loop_length = loop_length-1\n",
    "\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"!!!!! All Done!!!!!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now check your results in the analysis folder\n",
    "## Be sure to run the test 5 times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## compare text embeddings for each source\n",
    "This section is used to investigate the impact of face markup or other formatting requirements in the abstract text on the embedding model.\n",
    "Currently this only uses cosine similarity. \n",
    "<br>\n",
    "#### similarity:\n",
    "- [✅] cosine similarity\n",
    "<br>\n",
    "#### Embeddings:\n",
    "- [✅] SciBERT\n",
    "- [✅] SentenceBERT\n",
    "<br>\n",
    "#### Analysis\n",
    "- [ ] compare scores using dataframe\n",
    "- [ ] visualize results\n",
    "### References\n",
    "- https://stackoverflow.com/questions/60492839/how-to-compare-sentence-similarities-using-embeddings-from-bert<br>\n",
    "See the above for a discussion on NOT using BERT (and SciBERT) for comparing sentence embedding. I should be using SentenceBERT for sentence similarity.<br>\n",
    "- Sentence Transformers: https://www.sbert.net/docs/sentence_transformer/pretrained_models.html\n",
    "<br>\n",
    "- another approach: https://medium.com/@ahmedmellit/text-similarity-implementation-using-bert-embedding-in-python-1efdb5194e65\n",
    "- sklearn metrics for other scoring methods than cosine similarity: https://scikit-learn.org/stable/api/sklearn.metrics.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#load SciBERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "def get_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state.mean(dim=1).numpy()\n",
    "\n",
    "#function to generate embeddings using SciBERT\n",
    "\"\"\"\n",
    "todo:\n",
    "- [ ] change this to a sentence embedding model\n",
    "\"\"\"\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        padding=\"longest\",#please select one of ['longest', 'max_length', 'do_not_pad']\n",
    "        #padding=False,#padding has an effect on similarity\n",
    "        truncation=True\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "#original text and text with errors\n",
    "original_text = \" Abstract Objectives Precise literature recommendation and summarization are crucial for biomedical professionals. While the latest iteration of generative pretrained transformer (GPT) incorporates 2 distinct modes—real-time search and pretrained model utilization—it encounters challenges in dealing with these tasks. Specifically, the real-time search can pinpoint some relevant articles but occasionally provides fabricated papers, whereas the pretrained model excels in generating well-structured summaries but struggles to cite specific sources. In response, this study introduces RefAI, an innovative retrieval-augmented generative tool designed to synergize the strengths of large language models (LLMs) while overcoming their limitations. Materials and Methods RefAI utilized PubMed for systematic literature retrieval, employed a novel multivariable algorithm for article recommendation, and leveraged GPT-4 turbo for summarization. Ten queries under 2 prevalent topics (“cancer immunotherapy and target therapy” and “LLMs in medicine”) were chosen as use cases and 3 established counterparts (ChatGPT-4, ScholarAI, and Gemini) as our baselines. The evaluation was conducted by 10 domain experts through standard statistical analyses for performance comparison. The overall performance of RefAI surpassed that of the baselines across 5 evaluated dimensions—relevance and quality for literature recommendation, accuracy, comprehensiveness, and reference integration for summarization, with the majority exhibiting statistically significant improvements (P-values<.05). Discussion RefAI demonstrated substantial improvements in literature recommendation and summarization over existing tools, addressing issues like fabricated papers, metadata inaccuracies, restricted recommendations, and poor reference integration. Conclusion By augmenting LLM with external resources and a novel ranking algorithm, RefAI is uniquely capable of recommending high-quality literature and generating well-structured summaries, holding the potential to meet the critical needs of biomedical professionals in navigating and synthesizing vast amounts of scientific literature.\"\n",
    "typo_text = \"<jats:title>Abstract</jats:title>\\n               <jats:sec>\\n                  <jats:title>Objectives</jats:title>\\n                  <jats:p>Precise literature recommendation and summarization are crucial for biomedical professionals. While the latest iteration of generative pretrained transformer (GPT) incorporates 2 distinct modes—real-time search and pretrained model utilization—it encounters challenges in dealing with these tasks. Specifically, the real-time search can pinpoint some relevant articles but occasionally provides fabricated papers, whereas the pretrained model excels in generating well-structured summaries but struggles to cite specific sources. In response, this study introduces RefAI, an innovative retrieval-augmented generative tool designed to synergize the strengths of large language models (LLMs) while overcoming their limitations.</jats:p>\\n               </jats:sec>\\n               <jats:sec>\\n                  <jats:title>Materials and Methods</jats:title>\\n                  <jats:p>RefAI utilized PubMed for systematic literature retrieval, employed a novel multivariable algorithm for article recommendation, and leveraged GPT-4 turbo for summarization. Ten queries under 2 prevalent topics (“cancer immunotherapy and target therapy” and “LLMs in medicine”) were chosen as use cases and 3 established counterparts (ChatGPT-4, ScholarAI, and Gemini) as our baselines. The evaluation was conducted by 10 domain experts through standard statistical analyses for performance comparison.</jats:p>\\n               </jats:sec>\\n               <jats:sec>\\n                  <jats:title>Results</jats:title>\\n                  <jats:p>The overall performance of RefAI surpassed that of the baselines across 5 evaluated dimensions—relevance and quality for literature recommendation, accuracy, comprehensiveness, and reference integration for summarization, with the majority exhibiting statistically significant improvements (P-values &amp;lt;.05).</jats:p>\\n               </jats:sec>\\n               <jats:sec>\\n                  <jats:title>Discussion</jats:title>\\n                  <jats:p>RefAI demonstrated substantial improvements in literature recommendation and summarization over existing tools, addressing issues like fabricated papers, metadata inaccuracies, restricted recommendations, and poor reference integration.</jats:p>\\n               </jats:sec>\\n               <jats:sec>\\n                  <jats:title>Conclusion</jats:title>\\n                  <jats:p>By augmenting LLM with external resources and a novel ranking algorithm, RefAI is uniquely capable of recommending high-quality literature and generating well-structured summaries, holding the potential to meet the critical needs of biomedical professionals in navigating and synthesizing vast amounts of scientific literature.</jats:p>\\n               </jats:sec>\"\n",
    "\n",
    "#run embeddings\n",
    "original_embedding = generate_embeddings(original_text)\n",
    "typo_embedding = generate_embeddings(typo_text)\n",
    "print(type(typo_embedding))\n",
    "\n",
    "#calculate cosine similarity\n",
    "similarity = cosine_similarity(original_embedding, typo_embedding)\n",
    "print(f\"Cosine similarity: {similarity[0][0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence transformer verison\n",
    "#reference: https://medium.com/@ahmedmellit/text-similarity-implementation-using-bert-embedding-in-python-1efdb5194e65\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "#puts text from above into a list\n",
    "sentences:list = [original_text,typo_text]\n",
    "\n",
    "#initializing the Sentence Transformer model using BERT with mean-tokens pooling - source see above\n",
    "sentence_model = SentenceTransformer('bert-base-nli-mean-tokens') # this resets the model variable! changed to sentence_model variable name\n",
    "\n",
    "#encoding the sentences\n",
    "sentence_embeddings = sentence_model.encode(sentences)\n",
    "\n",
    "#result will be a list of similarity scores between two texts\n",
    "similarity_scores = cosine_similarity([sentence_embeddings[0]], sentence_embeddings[1:])\n",
    "\n",
    "print(f\"Cosine similarity scores using sentence embedding model: {similarity_scores[0][0]:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_heatmap(embedding1, embedding2, title):\n",
    "    #calculate the difference between the two embeddings\n",
    "    diff = embedding1 - embedding2\n",
    "    #reshape the difference to a 2D array for the heatmap\n",
    "    diff_2d = diff.reshape(1, -1)\n",
    "    #create a heatmap\n",
    "    plt.figure(figsize=(12, 2))\n",
    "    sns.heatmap(diff, cmap='coolwarm', annot=False, cbar=True,vmin=-1,vmax=1)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "#plot heatmap\n",
    "#plot_heatmap(original_embedding, typo_embedding, \"diff between embeddings\")\n",
    "plot_heatmap(sentence_embeddings[0],sentence_embeddings[1:],\"Sentence Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with attention weights\n",
    "from https://github.com/clarkkev/attention-analysis\n",
    "- https://stackoverflow.com/questions/75772288/how-to-read-a-bert-attention-weight-matrix for explanation on queries and keys\n",
    "- https://theaisummer.com/self-attention/#:%7E:text=Self%2Dattention%20is%20not%20symmetric!&text=The%20arrows%20that%20correspond%20to,Q%E2%80%8B%3DWK%E2%80%8B. explanation on self-attention\n",
    "- heatmaps to analyze attention weights: https://apxml.com/courses/foundations-transformers-architecture/chapter-7-implementation-details-optimization/practice-analyzing-attention-weights\n",
    "- excellent source: https://apxml.com/courses/how-to-build-a-large-language-model/chapter-23-analyzing-model-behavior/attention-map-visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above \n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load SciBERT tokenizer and model - same as above - technically don't need to relaod these unless changing\n",
    "# try sentence based model?\n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "\n",
    "# Tokenize the sentences\n",
    "inputs1 = tokenizer(original_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=20)#limit tokens so that we can actually see something\n",
    "inputs2 = tokenizer(typo_text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=20)#\n",
    "\n",
    "# Get the attention weights: the `output_attentions=True` parameter is used to get the attention weights from the model\n",
    "with torch.no_grad():\n",
    "    outputs1 = model(**inputs1, output_attentions=True)\n",
    "    outputs2 = model(**inputs2, output_attentions=True)\n",
    "\n",
    "# Extract the attention weights for the last layer\n",
    "#.squeeze() https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html\n",
    "attention_weights1 = outputs1.attentions[-1].squeeze(0)  # Shape: (num_heads, seq_len, seq_len)\n",
    "attention_weights2 = outputs2.attentions[-1].squeeze(0)  # Shape: (num_heads, seq_len, seq_len)\n",
    "\n",
    "# Average the attention weights across all heads, \n",
    "#see last reference to visualize attention for each head\n",
    "attention_weights1 = attention_weights1.mean(dim=0)  # Shape: (seq_len, seq_len)\n",
    "attention_weights2 = attention_weights2.mean(dim=0)  # Shape: (seq_len, seq_len)\n",
    "\n",
    "# Get the tokens for the sentences\n",
    "tokens1 = tokenizer.convert_ids_to_tokens(inputs1[\"input_ids\"].squeeze(0))\n",
    "tokens2 = tokenizer.convert_ids_to_tokens(inputs2[\"input_ids\"].squeeze(0))\n",
    "\n",
    "# Plot the attention heatmap for the first sentence\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_weights1, xticklabels=tokens1, yticklabels=tokens1, cmap='viridis', annot=False, cbar=True)\n",
    "plt.title(\"Attention Weights for original_text\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the attention heatmap for the second sentence\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attention_weights2, xticklabels=tokens2, yticklabels=tokens2, cmap='viridis', annot=False, cbar=True)\n",
    "plt.title(\"Attention Weights for typo_text\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate the difference in attention weights\n",
    "diff_attention_weights = (attention_weights1 - attention_weights2)\n",
    "\n",
    "# Plot the **difference** in attention weights\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(diff_attention_weights, xticklabels=tokens1, yticklabels=tokens1, cmap='coolwarm', annot=False, cbar=True, vmin=-1, vmax=1)\n",
    "plt.title(\"Difference in Attention Weights\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
