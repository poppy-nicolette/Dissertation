{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere RAG with dense retriever and ReRank model\n",
    "- references: https://docs.cohere.com/v2/docs/rag-complete-example\n",
    "- [ ] check performance with SciFact\n",
    "<br>\n",
    "“This work was supported by compute credits from a Cohere Labs Research Grant, these grants are designed to support academic partners conducting research with the goal of releasing scientific artifacts and data for good projects.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import cohere\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import sys\n",
    "from colorama import Fore, Style, Back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mall is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "# load secret from local .env file\n",
    "def get_key():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    _key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "    #verify if it worked\n",
    "    if _key is not None:\n",
    "        print(Fore.GREEN + \"all is good, beautiful!\")\n",
    "        return _key\n",
    "    else:\n",
    "        print(Fore.LIGHTRED_EX + \"API Key is missing\")\n",
    "\n",
    "# initilize client\n",
    "co = cohere.ClientV2(get_key())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0]\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append(f\"{doi} {text}\\n\")\n",
    "    return documents_with_doi\n",
    "\n",
    "# initialize empty search query\n",
    "search_queries = []\n",
    "# Embed the documents\n",
    "def document_embed(documents:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the documents from a list provided from read_documents_with_doi()\n",
    "    \"\"\"\n",
    "    doc_emb = co.embed(\n",
    "        #model=\"embed-v4.0\",\n",
    "        model=\"embed-english-v3.0\",\n",
    "        input_type=\"search_document\",\n",
    "        texts=[doc for doc in documents],\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return doc_emb\n",
    "\n",
    "# Embed the search query\n",
    "def query_embed(search_queries:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the query from a list provided in search_queries variable\n",
    "    \"\"\"\n",
    "    query_emb = co.embed(\n",
    "        #model=\"embed-v4.0\",\n",
    "        model=\"embed-english-v3.0\",\n",
    "        input_type=\"search_query\",\n",
    "        texts=search_queries,\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return query_emb\n",
    "\n",
    "# retrieve top_k and compute similarity using dot product\n",
    "def retrieve_top_k(top_k, query_embedded, documents_embedded, documents)->List[str]:\n",
    "    \"\"\"\n",
    "    returns the top_k documents based on dot product similarity\n",
    "    \"\"\"\n",
    "\n",
    "    scores = np.dot(query_embedded, np.transpose(documents_embedded))[0]#ordered list!\n",
    "    # takes top scores, and returns sorted list and returns indices sliced by top_k\n",
    "    max_idx = np.argsort(-scores)[:top_k]\n",
    "    # returns documents by index\n",
    "    retrieved_docs = [documents[item] for item in max_idx]\n",
    "    # returns a list of documents\n",
    "    return retrieved_docs\n",
    "\n",
    "def rerank_documents(retrieved_documents,search_queries,threshold,top_k)->List[str]:\n",
    "    \"\"\"\n",
    "    takes retrieved_documents as input along with search_queries and runs them through the \n",
    "    rerank model from cohere for semantic similarity. \n",
    "\n",
    "    top_n = top_k\n",
    "    Limits those returned by a threshold score. this is to reduce those that are irrelevant.\n",
    "    \"\"\"\n",
    "    # Rerank the documents\n",
    "    results = co.rerank(\n",
    "        #model=\"rerank-v3.5\",\n",
    "        model=\"rerank-english-v3.0\",\n",
    "        query=search_queries[0],\n",
    "        documents=[doc for doc in retrieved_documents],\n",
    "        top_n=top_k,\n",
    "        max_tokens_per_doc=4096,# defaults to 4096\n",
    "    )\n",
    "\n",
    "    # Display the reranking results\n",
    "    #for idx, result in enumerate(results.results):\n",
    "    #    print(f\"Rank: {idx+1}\")\n",
    "    #    print(f\"Score: {result.relevance_score}\")\n",
    "    #    print(f\"Document: {retrieved_documents[result.index]}\\n\")\n",
    "\n",
    "    #returns only those over threshold\n",
    "    reranked_docs = [\n",
    "        retrieved_documents[result.index] for result in results.results if result.relevance_score >=threshold\n",
    "    ]\n",
    "    reranked_with_score = [(result.relevance_score, retrieved_documents[result.index].split(\"\\n\")[0].strip(\"DOI: \")) for result in results.results if result.relevance_score >=threshold]\n",
    "\n",
    "    print(f\"reranked_documents: {reranked_docs}\")\n",
    "    print(f\"length of reranked_documents: {len(reranked_docs)}\")\n",
    "\n",
    "    return reranked_docs, reranked_with_score\n",
    "\n",
    "def cohere_rag_pipeline(directory_path,search_queries,top_k,threshold):\n",
    "\n",
    "    # retrieve documents from directory\n",
    "    documents = read_documents_with_doi(directory_path)\n",
    "    print(f\"Length of documents: {len(documents)}\")\n",
    "    # embed the documents\n",
    "    documents_embedded = document_embed(documents)\n",
    "\n",
    "    #embed the query:\n",
    "    query_embedded = query_embed(search_queries)\n",
    "\n",
    "    # retrieve the top_k documents\n",
    "    retrieved_documents = retrieve_top_k(top_k, query_embedded, documents_embedded, documents)\n",
    "\n",
    "    # rerank the documents using the Rerank model from Cohere\n",
    "    reranked_documents, reranked_DOIs_with_score = rerank_documents(retrieved_documents,search_queries,threshold,top_k)\n",
    "    # set system instructions\n",
    "    instructions = \"\"\"\n",
    "                    You are an academic research assistant.\n",
    "                    You must include the DOI in your response.\n",
    "                    If there is no content provided, ask for a different question.\n",
    "                    Please structure your response like this:\n",
    "                    Summary: summary statement here. \n",
    "                    DOI: summary of the text associated with this DOI.\n",
    "                    Address me as, 'my lady'.\n",
    "                    \"\"\"\n",
    "    # create messages to model\n",
    "    messages = [{\"role\":\"user\",\n",
    "                \"content\": search_queries[0]},\n",
    "                {\"role\":\"system\",\n",
    "                \"content\":instructions}]\n",
    "\n",
    "    # Generate the response\n",
    "    resp = co.chat(\n",
    "        #model=\"command-a-03-2025\",\n",
    "        model=\"command-r-08-2024\",\n",
    "        #model=\"command-r7b-12-2024\", #https://docs.cohere.com/docs/command-r7b\n",
    "        messages=messages,\n",
    "        documents=reranked_documents,\n",
    "    )\n",
    "\n",
    "    return resp, reranked_documents, reranked_DOIs_with_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#  run here to test functions avove\n",
    "# ****** Pipeline ********\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.1\n",
    "\n",
    "response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "# Display the response\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"{response.message.content[0].text}\")\n",
    "print(Fore.LIGHTCYAN_EX + f\"------\\nReranked documents:\")\n",
    "for doc in reranked_documents_end:\n",
    "    print(doc)\n",
    "\n",
    "# Display the citations and source documents\n",
    "if response.message.citations:\n",
    "    print(Fore.LIGHTYELLOW_EX + \"\\nCITATIONS:\")\n",
    "    for citation in response.message.citations:\n",
    "        print(f\"source text: {citation.text},\\nsource: {citation.sources[0].document.get('content').split(\"\\n\")[0]}\\n------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# Analysis\n",
    "Precision, recall, accuracy, F1 scores and faithfulness\n",
    "## Precision, recall, F1 score\n",
    "### references\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "- https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from colorama import Fore, Back, Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "import time\n",
    "from colorama import Fore, Back, Style\n",
    "\"\"\"\n",
    "change this to read in an excel sheet of queries and ground_truth dois.\n",
    "Then it should be isolated as a function.\n",
    "Run the function to iterature through the list.\n",
    "\"\"\"\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "#retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "#print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# initiates the variable\n",
    "ground_truth = []\n",
    "\n",
    "\n",
    "def evaluate_retrieval(retrieved_dois, ground_truth):\n",
    "    corpus_doi_list = []\n",
    "    #corpus_list is a global variable in rag_pipeline()\n",
    "    for each in range(len(documents_with_doi)):\n",
    "        a = documents_with_doi[each].split(\"\\n\")[0].strip(\"DOI: \")\n",
    "        corpus_doi_list.append(a)\n",
    "    print(len(corpus_doi_list))\n",
    "\n",
    "    def compare_lists(list1, list2, list3):\n",
    "        for val in list1:\n",
    "            if val in list2:\n",
    "                list3.append(1)\n",
    "            else:\n",
    "                list3.append(0)\n",
    "\n",
    "    #set y_true so that len(y_true)==len(corpus_doi_list)\n",
    "    y_true = []\n",
    "    compare_lists(corpus_doi_list,ground_truth,y_true)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = []\n",
    "    compare_lists(corpus_doi_list,retrieved_dois,y_pred)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # calculate metrics - could also use sklearn.metrics functions such as precision_score, but this is easier to read\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    faithfulness_score = 0\n",
    "    for each in retrieved_dois:\n",
    "        if each in response.message.content[0].text:\n",
    "            faithfulness_score+=1\n",
    "        else:\n",
    "            faithfulness_score+=0\n",
    "\n",
    "    return {\n",
    "        'Query':f\"{search_queries[0]}\",\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\":accuracy,\n",
    "        \"Balanced accuracy\":balanced_accuracy,\n",
    "        \"Faithfulness score\":faithfulness_score,\n",
    "        \"Documents score\":str(reranked_DOIs_with_score_end),#converted to string because pandas was only taking the first tuple not the entire list - use ast.literal_eval() to unpack later.\n",
    "        \"Response\":response.message.content[0].text\n",
    "    }\n",
    "\n",
    "def print_results()->Dict:\n",
    "    \"\"\"\n",
    "    Prints a nicely ordered set of results from evalaute_retrieval()\n",
    "    \"\"\"\n",
    "    results = evaluate_retrieval(retrieved_dois, ground_truth)\n",
    "    print(f\"For query: {results['Query']}:\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"F1-Score: {results['F1-Score']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    print(f\"Balanced accuracy: {results['Balanced accuracy']:.3f}\")\n",
    "    print(f\"Faithfulness score: {results['Faithfulness score']}\")\n",
    "    print(f\"Documents scores: {results['Documents score']}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "#for debugging\n",
    "\n",
    "#print_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the test from here *OLD*\n",
    "this is the one-at-a-time version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#run the test from here\n",
    "\n",
    "# Ground truth relevant documents (DOIs) for each query\n",
    "ground_truth = [\"10.1002/leap.1411\",\"10.1007/s11192-020-03632-0\",\"10.1162/qss_a_00286\",\"10.1162/qss_a_00022\",\"10.31222/osf.io/smxe5\"]\n",
    "\n",
    "#***** Begin chat session *****\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_jats\"\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_multi_lang\"\n",
    "\n",
    "documents_with_doi = read_documents_with_doi(directory_path)\n",
    "documents = [doc[0].split('\\n')[1:] for doc in documents_with_doi]\n",
    "print(f\"Length of documents: {len(documents)}\")\n",
    "print(f\"Length of corpus: {len(documents_with_doi)}\")\n",
    "\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.10\n",
    "\n",
    "response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# Display the response\n",
    "print(Fore.LIGHTYELLOW_EX + f\"{response.message.content[0].text}\")\n",
    "\n",
    "new_result = print_results()\n",
    "# add the new result to the df\n",
    "results_df.loc[len(results_df)] = new_result\n",
    "\n",
    "#save the queries and responses to separate dataframe to be manually annontated\n",
    "answer_relevance_df = results_df[['Query','Response']].copy(deep=True)\n",
    "\n",
    "# save out answer_relevance_df\n",
    "filename=\"analysis/dense_answer_relevance_results.xlsx\"\n",
    "answer_relevance_df.to_excel(filename)\n",
    "\n",
    "filename = \"analysis/dense_analysis_results.xlsx\"\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "results_df.to_excel(filename)\n",
    "results_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## automated version \n",
    "Currently Works!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Documents score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query, Precision, Recall, F1-Score, Accuracy, Balanced accuracy, Faithfulness score, Documents score, Response]\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial dataframe to capture results from each query and results\n",
    "#ONLY DO THIS AT THE BEGINNING OF THE ANALYSIS PROCEDURE, OTHERWISE, IT WILL ERASE THE PREVIOUS RESULTS!!\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Query','Precision','Recall','F1-Score','Accuracy', 'Balanced accuracy', 'Faithfulness score', 'Documents score', 'Response'])\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m Golder set loaded!\n"
     ]
    }
   ],
   "source": [
    "golden_set_df = pd.read_excel(\"golden_set.xlsx\")\n",
    "#golden_set_df_test = golden_set_df.head(3)\n",
    "#golden_set_df\n",
    "print(Fore.LIGHTGREEN_EX + f\" Golder set loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of documents: 45\n",
      "Length of corpus: 45\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00286\\n Title: 8つのフリーアクセス学術データベースの出版メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースにおけるメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000を超えるレコードのランダムなCrossRefサンプルを選択し、7つのデータベース（Dimensions、Google Scholar、Microsoft Academic、Openalex、Scilit、Semantic Sc\\u200b\\u200bholar、およびThe Lens）で検索されました。この情報、これらのフィールドの完全性レート、およびデータベース間の合意を説明するフィールドを観察するために、7つの特性（要約、アクセス、書誌情報、書誌情報、文書の種類、公開日、言語、識別子）を観察しました。結果は、アカデミック検索エンジン（Google Scholar、Microsoft Academic、およびSemantic Sc\\u200b\\u200bholar）が少ない情報を収集し、完全性が低いことを示しています。逆に、サードパーティのデータベース（寸法、OpenAlex、Scilit、およびレンズ）は、メタデータの品質が高く、完全性が高くなります。アカデミック検索エンジンには、Webをcrawって信頼できる記述データを取得する能力がないと結論付けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することから得られる情報の喪失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: メタエンハンス：大学図書館の電子論文と学位論文のメタデータの品質改善 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタルオブジェクトがデジタルライブラリインターフェイスを通じて発見されるために重要です。ただし、さまざまな理由により、デジタルオブジェクトのメタデータは、しばしば不完全で、一貫性がなく、誤った値を示します。ケーススタディとして、電子論文と論文（ETD）の7つの重要な分野を使用して、学術メタデータを自動的に検出、修正、および正規化する方法を調査します。メタエンハンスを提案します。メタエンハンスは、これらの分野の品質を改善するために最先端の人工知能方法を利用するフレームワークです。 Metaenhanceを評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 ETDを含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークでMetaenhanceをテストし、提案された方法は、7つのフィールドのうち5つで0.85から1.00の範囲のエラーとF1スコアの検出にほぼ完全なF1スコアを達成したことを発見しました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化全体のメタデータの品質の問題を特定する Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、標準的な形式でコンテキスト、技術、および管理情報を提供することにより、発見とアクセスに不可欠です。しかし、メタデータは、社会文化的表現、リソースの制約、および標準化されたシステムの間の緊張の場所でもあります。正式および非公式の介入は、品質の問題、アイデンティティを主張する政治的行為、または視界を最大化するための戦略的選択と解釈される場合があります。これに関連して、メタデータの品質、一貫性、および完全性が個人やコミュニティにどのように影響するかを理解しようとしました。記録のサンプルをレビューして、メタデータとコミュニティが互いに反対して、意図的に文化的な意味を反映する（またはそうでない）方法に起因する問題を特定して分類しました。 Metadata are crucial for discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. In this context, we sought to understand how metadata quality, consistency, and completeness impact individuals and communities. Reviewing a sample of records, we identified and classified issues stemming from how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101']\n",
      "\u001b[93mMy lady, here are the studies that examined the abstract in metadata:\n",
      "\n",
      "## Summary: Three studies examined the abstract in metadata, focusing on metadata quality, completeness, and improvement.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00286\n",
      "This study compared the amount of metadata and the completeness degree of research publications in new academic databases. It found that academic search engines gather less information and have a lower degree of completeness compared to third-party databases, which have higher metadata quality and completeness rates.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2303.17661\n",
      "The study investigated methods to automatically detect, correct, and canonicalize scholarly metadata using electronic theses and dissertations as a case study. It proposed MetaEnhance, a framework that utilises advanced artificial intelligence methods to improve metadata quality, achieving nearly perfect F1-scores in error detection and correction.\n",
      "\n",
      "## DOI: 10.5860/crl.86.1.101\n",
      "This study aimed to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a sample of records, it identified and classified issues stemming from the tension between metadata and communities in reflecting cultural meanings.\n",
      "45\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.575\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.4213137, '10.1162/qss_a_00286'), (0.19544049, '10.48550/arXiv.2303.17661'), (0.12951279, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0\n",
      "Length of documents: 45\n",
      "reranked_documents: []\n",
      "length of reranked_documents: 0\n",
      "Retrieved DOIs: []\n",
      "\u001b[93mMy lady, could you please provide me with some context or a specific topic you are interested in regarding citation studies? I can then offer a more tailored response with relevant DOIs.\n",
      "45\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.500\n",
      "Faithfulness score: 0\n",
      "Documents score: []\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.1007/s11192-023-04923-y\\n Title: Openalexの不足している機関：考えられる理由、意味、および解決策 Missing institutions in OpenAlex: possible reasons, implications, and solutions\\nAbstract: オープンサイエンスの出現では、データ品質が高いオープンデータプラットフォームが必要です。 2022年1月に開始されたグローバルな研究システムの完全に開かれたカタログとして、OpenAlexは、定量的科学研究で広く使用されている簡単なデータアクセシビリティと幅広いデータカバレッジの2つの主要な利点を特徴としています。驚くべきことに、Openalexはライデン大学のランキングの重要なデータソースとして採用されています。ただし、Openalexのジャーナル記事メタデータには、機関が欠落しているという深刻なデータ品質の問題があります。この研究では、3種類の制度情報（FII）、部分的に欠落している機関情報（PMII）、および完全に欠落している機関情報（CMII）を定義することにより、問題とその結果と解決策の考えられる理由を調査します。私たちの結果は、不足している機関の問題がOpenalexのジャーナル記事の60％以上で発生することを示しています。この問題は、特に初期のメタデータや社会科学や人文科学で広まっています。データのサブサンプルを使用して、問題の考えられる理由、歪んだ結果のリスク、および不足している機関の問題に対する可能な解決策をさらに調査します。目的は、オープンリソースのデータ品質改善の重要性を高め、定量的科学研究およびより広い文脈でのオープンリソースの責任ある使用をサポートすることです。 The advent of open science calls for open data platforms with high data quality. As a fully open catalog of the global research system launched in January 2022, OpenAlex features two main advantages of easy data accessibility and broad data coverage, which has been widely used in quantitative science studies. Remarkably, OpenAlex is adopted as an important data source for Leiden university ranking. However, there is a severe data quality problem of missing institutions in journal article metadata in OpenAlex. This study investigates the possible reasons for the problem and its consequences and solutions by defining three types of institutional information—full institutional information (FII), partially missing institutional information (PMII) and completely missing institutional information (CMII). Our results show that the problem of missing institutions occurs in more than 60% of the journal articles in OpenAlex. The problem is particularly widespread in metadata from the early years and in the social sciences and humanities. Using sub-samples of the data, we further explore the possible reasons for the problem, the risk it might represent for distorted results, and possible solutions to the problem of missing institutions. The aim is to raise the importance of data quality improvements in open resources, and thus to support the responsible use of open resources in quantitative science studies and also in broader contexts.\\n', 'DOI: 10.1590/SciELOPreprints.11205\\n Title: ユニバーサルインデックスへのオープンロードで：OpenAlexおよびOpenJournal Systems On the Open Road to Universal Indexing: OpenAlex and OpenJournal Systems\\nAbstract: この調査では、OpenAlexのオープンジャーナルシステム（JUOJS）を使用したジャーナルのインデックス作成を検証し、包括的な学術参加をサポートする2つのオープンソースソフトウェアイニシアチブを反映しています。 47,625のアクティブなJuojsのデータセットを分析することにより、これらのジャーナルの71％がOpenAlexで少なくとも1つの記事をインデックス付けされていることを明らかにします。私たちの調査結果は、OpenAlexに含まれるCrossRef doiを使用してジャーナルの97％を使用して、インデックス作成の達成におけるCrossRef DOIの中心的な役割を強調しています。ただし、この技術的依存は、特に低所得国（Juojsの47％）および非英語言語ジャーナル（Juojsの55％-64％）からのリソース制限されたジャーナル（Juojsの55％-64％）の雑誌として、より広範な構造的不平等を反映しています。私たちの研究は、学術インフラストラクチャの依存関係の理論的意味と、世界的な知識の可視性における体系的な格差を永続させる上でのその役割を強調しています。 OpenAlexのような包括的な書誌データベースでさえ、世界規模で公平な索引付けを促進するために、財務、インフラ、および言語の障壁に積極的に対処する必要があると主張します。インデックス作成メカニズム、永続的な識別子、および構造的不平等との関係を概念化することにより、この研究は、グローバルな多言語学術生態系における普遍的なインデックス作成のダイナミクスとその実現を再考するための重要なレンズを提供します。 This study examines OpenAlex’s indexing of journals using Open Journal Systems (JUOJS), reflecting two open source software initiatives supporting inclusive scholarly participation. By analyzing a dataset of 47,625 active JUOJS, we reveal that 71% of these journals have at least one article indexed in OpenAlex. Our findings underscore the central role of Crossref DOIs in achieving indexing, with 97% of the journals using Crossref DOIs included in OpenAlex. However, this technical dependency reflects broader structural inequities, as resource-limited journals, particularly those from low-income countries (47% of JUOJS) and non-English language journals (55%-64% of JUOJS), remain underrepresented. Our work highlights the theoretical implications of scholarly infrastructure dependencies and their role in perpetuating systemic disparities in global knowledge visibility. We argue that even inclusive bibliographic databases like OpenAlex must actively address financial, infrastructural, and linguistic barriers to foster equitable indexing on a global scale. By conceptualizing the relationship between indexing mechanisms, persistent identifiers, and structural inequities, this study provides a critical lens for rethinking the dynamics of universal indexing and its realization in a global, multilingual scholarly ecosystem.\\n', 'DOI: 10.48550/arXiv.2404.01985\\n Title: 彼はOpenalex、Scopus、Web of Scienceのオープンアクセスカバレッジ he open access coverage of OpenAlex, Scopus and Web of Science\\nAbstract: Diamond Open Access（OA）ジャーナルは、著者と読者の両方に無料の公開モデルを提供しますが、主要な書誌データベースでのインデックスの欠如は、これらのジャーナルの取り込みを評価する際の課題を提示します。さらに、出版言語や出版国などのOAの特性は、OAジャーナルがより多様であり、地域社会にサービスを提供することを目指しているという議論を支持するためにしばしば使用されてきましたが、OAジャーナルの地理的および言語的特性に関連する経験的証拠の現在の欠如があります。 OpenAlexとオープンアクセスジャーナルのディレクトリをベンチマークとして使用して、このペーパーでは、フィールド、国、言語による科学とスコープスのWebでの著者とジャーナルの報道を通じて、ダイヤモンドと金の報道を調査します。結果は、WOSとSCOPUSでのより低いカバレッジ、およびダイヤモンドOAの局所範囲を示しています。英語のみのジャーナルのシェアは、ゴールドジャーナルの間でかなり高くなっています。高所得国は、社会科学と人文科学のダイヤモンドジャーナルを除き、すべてのドメインとジャーナルの種類で著者のシェアが最も高い。ダイヤモンドOAインデックスの現在の景観を理解することは、より包括的なOAモデルに向けて政策と実践を進めることで、学術通信ネットワークを支援することができます。 Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2401.16359', '10.48550/arXiv.2404.17663', '10.1007/s11192-023-04923-y', '10.1590/SciELOPreprints.11205', '10.48550/arXiv.2404.01985']\n",
      "\u001b[93m## Summary: OpenAlex is an open-source, scholarly metadata platform that provides free and open access to data, enabling researchers to conduct bibliometric studies without licensing barriers.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2401.16359\n",
      "This report compares OpenAlex's reference coverage and selected metadata with established databases like Web of Science and Scopus. It finds that OpenAlex has comparable average source reference numbers and internal coverage rates, demonstrating its potential as a competitive source for bibliometric analyses.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.17663\n",
      "This study, conducted in collaboration with the OpenAlex team, compares OpenAlex to Scopus across various dimensions. It concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for certain analyses, particularly at the country level. However, issues with metadata accuracy and completeness highlight the need for further research to fully understand and address OpenAlex's limitations.\n",
      "\n",
      "## DOI: 10.1007/s11192-023-04923-y\n",
      "This research investigates the problem of missing institutions in OpenAlex's journal article metadata. It defines three types of institutional information and finds that the issue occurs in over 60% of journal articles, particularly in early years and in the social sciences and humanities. The aim is to improve data quality in open resources and promote responsible use in quantitative science studies and broader contexts.\n",
      "\n",
      "## DOI: 10.1590/SciELOPreprints.11205\n",
      "This study examines OpenAlex's indexing of journals using Open Journal Systems (JUOJS), reflecting open-source software initiatives for inclusive scholarly participation. It reveals that 71% of journals have at least one article indexed in OpenAlex and highlights the central role of Crossref DOIs in achieving indexing. However, the technical dependency reflects broader structural inequities, particularly for resource-limited journals from low-income countries and non-English language journals.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.01985\n",
      "This paper investigates the coverage of diamond and gold open access (OA) journals through authorship and journal coverage in the Web of Science and Scopus. It uses OpenAlex and the Directory of Open Access Journals as benchmarks. The results show lower coverage in WoS and Scopus and the local scope of diamond OA. The study argues that understanding the current landscape of diamond OA indexing can aid the scholarly communications network in advancing towards more inclusive OA models.\n",
      "\n",
      "My lady, OpenAlex appears to be a promising platform for researchers, offering open access to data and supporting inclusive scholarly participation. However, it also faces challenges related to data quality and limitations, which require further investigation and improvement.\n",
      "45\n",
      "For query: ['Tell me about OpenAlex.']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.773\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9965301, '10.48550/arXiv.2401.16359'), (0.9940428, '10.48550/arXiv.2404.17663'), (0.99040353, '10.1007/s11192-023-04923-y'), (0.9678993, '10.1590/SciELOPreprints.11205'), (0.96405166, '10.48550/arXiv.2404.01985')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: CrossRef：コミュニティ所有の学術メタデータの持続可能な供給源 Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このホワイトペーパーでは、CrossRefによって収集および利用可能になった学術的メタデータと、学術研究の生態系におけるその重要性について説明します。 1億600万件以上の記録を含み、年間平均11％のレートで拡大するCrossrefのメタデータは、出版社、著者、図書館員、資金提供者、および研究者向けの学術データの主要な情報源の1つになりました。メタデータセットは、ジャーナルやカンファレンスペーパーなどの従来のタイプだけでなく、データセット、レポート、プリプリント、ピアレビュー、助成金など、13のコンテンツタイプで構成されています。メタデータは、基本的な出版物メタデータに限定されませんが、全文、資金調達とライセンス情報、引用リンク、修正、更新、撤回などに関する情報への要約とリンクを含めることもできます。メタデータは、REST APIやOAI-PMHを含む多くのAPIを通じて利用できます。この論文では、CrossRefが提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、CrossRefの研究エコシステムにおける役割と、引用データの提供の進化など、長年にわたるメタデータのキュレーションの傾向にも注目しています。 Crossrefのメタデータで使用される研究を要約し、将来のメタデータの品質と検索を改善する計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022']\n",
      "\u001b[93mMy lady, Crossref is a sustainable source of community-owned scholarly metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "Crossref's metadata, which includes over 106 million records and expands at an average rate of 11% a year, is a major source of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including traditional types like journals and conference papers, as well as data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication information but can also include abstracts and links to full text, funding and license information, citation links, and information about corrections, updates, and retractions. This makes Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications.\n",
      "45\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.556\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.8348113, '10.1162/qss_a_00022')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: オープンアレックスの言語カバレッジの評価：メタデータの精度と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: ClarivateのWeb of Science（WOS）とElsevierのScopusは、数十年にわたって書誌情報の主要なソースでした。高度にキュレーションされていますが、これらの閉鎖された独自のデータベースは、英語の出版物に大きく偏っており、研究普及における他の言語の使用を過小評価しています。 2022年に発売されたOpenalexは、包括的、包括的、オープンソースの研究情報を約束しました。すでに学者や研究機関が使用している間、そのメタデータの質は現在評価されています。この論文は、言語に関連するOpenalexのメタデータの完全性と正確性、WOSとの比較、および6,836の記事のサンプルの詳細な手動検証を通じて、この文献に貢献しています。結果は、オープンアレックスがWOSよりもはるかにバランスのとれた言語カバレッジを示すことを示しています。ただし、言語メタデータは必ずしも正確ではないため、Openalexは他の言語のそれを過小評価しながら英語の場所を過大評価します。批判的に使用すると、OpenAlexは学術出版に使用される言語の包括的かつ代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャレベルでより多くの作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8つのフリーアクセス学術データベースの出版メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースにおけるメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000を超えるレコードのランダムなCrossRefサンプルを選択し、7つのデータベース（Dimensions、Google Scholar、Microsoft Academic、Openalex、Scilit、Semantic Sc\\u200b\\u200bholar、およびThe Lens）で検索されました。この情報、これらのフィールドの完全性レート、およびデータベース間の合意を説明するフィールドを観察するために、7つの特性（要約、アクセス、書誌情報、書誌情報、文書の種類、公開日、言語、識別子）を観察しました。結果は、アカデミック検索エンジン（Google Scholar、Microsoft Academic、およびSemantic Sc\\u200b\\u200bholar）が少ない情報を収集し、完全性が低いことを示しています。逆に、サードパーティのデータベース（寸法、OpenAlex、Scilit、およびレンズ）は、メタデータの品質が高く、完全性が高くなります。アカデミック検索エンジンには、Webをcrawって信頼できる記述データを取得する能力がないと結論付けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することから得られる情報の喪失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.1162/qss_a_00286']\n",
      "\u001b[93mMy lady, here are two papers that evaluate the linguistic coverage or language-related metadata in scholarly databases:\n",
      "\n",
      "## Summary: Papers assessing the completeness and accuracy of metadata related to language in scholarly databases.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2409.10633\n",
      "Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "Abstract: This paper assesses the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with Clarivate's Web of Science (WoS), as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00286\n",
      "Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Abstract: This study compares the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, a random Crossref sample of more than 115,000 records was selected and searched in seven databases. Seven characteristics were analyzed to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines gather less information and have a low degree of completeness, while third-party databases have more metadata quality and a higher completeness rate.\n",
      "45\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.588\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.9998287, '10.48550/arXiv.2409.10633'), (0.4870177, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープンファンダーメタデータの可用性と完全性：オランダの研究評議会によって資金提供された出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金を提供する研究の結果に関する情報を収集するためにかなりの努力を費やしています。資金提供者が資金調達に関連する出版物の出力を追跡するのを支援するために、CrossRefは2013年にFundRefを開始し、出版社が永続的な識別子を使用して資金情報を登録できるようにしました。ただし、資金調達の研究の結果であるため、資金提供者のメタデータを含める必要があるため、Funder Metadataのカバレッジを評価することは困難です。この論文では、研究者が特定の資金提供機関による資金提供の結果であるオランダ研究評議会NWOが報告した5,004の出版物を調べました。これらの記事の67％のみがCrossRefの資金情報を含んでおり、NWOをNWOにリンクしたFunder Nameおよび/またはFunder IDとしてNWOを認めているサブセット（それぞれ53％と45％）が含まれています。 Web of Science（WOS）、Scopus、およびDimensionsはすべて、記事の全文の資金調達声明から追加の資金情報を推測することができます。レンズの資金情報は、主にCrossRefのそれに対応しており、PubMedから取得した可能性のある追加の資金情報があります。私たちは、独自のデータベースと比較して、CrossRefのメタデータの資金調達のカバレッジと完全性における出版社間の興味深い違いを観察し、資金調達のオープンメタデータの質を高める可能性を強調しています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: 資金調達COVID-19研究：オープンデータインフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 彼らが提供する資金調達の結果を分析するには、資金提供機関が資金から生じる出版物を追跡できるようにすることが不可欠です。 Covid-19に関連する研究を報告する出版物の資金調達データに焦点を当て、CrossRefでの資金提供データの可用性を研究しています。また、ScopusとWeb of Scienceの2つの独自の書誌データベースで利用可能な資金データとの比較も提示します。私たちの分析により、CrossRefの資金調達データの限られたカバレッジが明らかになりました。また、特にScopusでは、資金調達データの質に関連する問題を示しています。 CrossRefでの資金提供データの可用性を改善するための推奨事項を提供しています。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: 開いた書誌メタデータの源としてのCrossref Crossref as a source of open bibliographic metadata\\nAbstract: CrossRefの学術出版物の書誌メタデータのオープン可用性を促進するために、いくつかのイニシアチブが採用されています。 CrossRefでの6つのメタデータ要素の可用性の最新の概要を示します。参照リスト、要約、ORCID、著者の提携、資金情報、およびライセンス情報です。私たちの分析は、これらのメタデータ要素の可用性が、少なくともCrossRefで最も一般的な出版タイプであるジャーナル記事で、時間とともに改善されたことを示しています。ただし、分析では、多くの出版社が書誌メタデータの完全な開放性を実現するために追加の努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.1162/qss_a_00212', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mMy lady, here are some papers that address funding metadata, its availability, or its analysis in scholarly databases:\n",
      "\n",
      "## Summary: Papers discussing the availability and completeness of funding metadata in scholarly databases.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00210\n",
      "This paper discusses the efforts of research funders to collect information on the outcomes of the research they fund. It highlights the challenges in assessing the coverage of funder metadata and presents a case study on publications funded by the Dutch Research Council. The study finds that only 67% of articles contain funding information in Crossref, with interesting differences observed between publishers.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00212\n",
      "This paper focuses on the open availability of funding data in Crossref, specifically for publications related to COVID-19 research. It presents a comparison with funding data from Scopus and Web of Science, revealing limited coverage and quality issues in Crossref. Recommendations are provided to improve the open availability of funding data.\n",
      "\n",
      "## DOI: 10.31222/osf.io/smxe5\n",
      "This paper explores Crossref as a source of open bibliographic metadata, including funding information. It provides an overview of the availability of six metadata elements in Crossref, showing improvements over time for journal articles. However, the analysis indicates that many publishers need to make additional efforts to achieve full openness of bibliographic metadata.\n",
      "45\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.688\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.9913892, '10.1162/qss_a_00210'), (0.8077641, '10.1162/qss_a_00212'), (0.7324005, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模な言語モデルの検索された生成：調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模な言語モデル（LLMS）は印象的な能力を紹介しますが、幻覚、時代遅れの知識、非透明な、追跡不可能な推論プロセスなどの課題に遭遇します。検索された生成（RAG）は、外部データベースから知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の正確性と信頼性が向上し、ドメイン固有の情報の継続的な知識の更新と統合が可能になります。 RAGは、外部データベースの広大で動的なリポジトリとLLMSの本質的な知識を相乗的に統合します。この包括的なレビューペーパーでは、素朴なぼろきれ、高度なぼろ、モジュラーラグを含むRAGパラダイムの進行に関する詳細な調査を提供します。検索、生成、増強技術を含む、RAGフレームワークの三者基盤を細心の注意を払って精査します。この論文は、これらの各重要なコンポーネントに組み込まれた最先端のテクノロジーを強調し、RAGシステムの進歩を深く理解することを提供します。さらに、このペーパーでは、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を描き、研究開発の将来の道を指摘しています。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", 'DOI: 10.1609/aaai.v38i16.29728\\n Title: 検索された世代の大規模な言語モデルのベンチマーク Benchmarking Large Language Models in Retrieval-Augmented Generation\\nAbstract: 検索された生成（RAG）は、大規模な言語モデル（LLM）の幻覚を緩和するための有望なアプローチです。ただし、既存の研究には、さまざまな大規模な言語モデルに対する検索された生成の影響に関する厳密な評価がありません。これにより、異なるLLMのRAGの機能における潜在的なボトルネックを特定することが困難になります。この論文では、検索された生成が大規模な言語モデルに与える影響を体系的に調査します。ノイズの堅牢性、否定的な拒否、情報統合、反事実的堅牢性など、RAG\\u200b\\u200bに必要な4つの基本能力におけるさまざまな大手言語モデルのパフォーマンスを分析します。この目的のために、英語と中国語の両方でRAG評価のための新しいコーパスである検索された生成ベンチマーク（RGB）を確立します。 RGBは、ベンチマーク内のインスタンスを、ケースを解決するために必要な前述の基本能力に基づいて、4つの個別のテストベッドに分割します。次に、RGBの6つの代表LLMを評価して、RAGを適用する際に現在のLLMの課題を診断します。評価により、LLMはある程度のノイズの堅牢性を示していますが、否定的な拒絶、情報統合、誤った情報への対処に関して依然として著しく苦労していることが明らかになりました。前述の評価の結果は、LLMにRAGを効果的に適用するために、まだかなりの旅がまだあることを示しています。 Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.48550/arXiv.2410.04231\\n Title: 大規模な言語モデルの検索された生成によるメタデータベースのデータ探査 Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\\nAbstract: 必要なデータセットを効果的に検索する能力を開発することは、非常に限られた利用可能なメタデータを考慮して、データユーザーが関連するデータセットを特定するのを支援するための緊急の要件です。この課題のために、サードパーティのデータの利用は、改善の貴重なソースとして浮上しています。私たちの研究では、メタデータベースのデータ発見を強化するために検索された世代（RAG）の形式を採用するデータ探索のための新しいアーキテクチャを紹介します。このシステムは、大規模な言語モデル（LLM）を外部ベクトルデータベースと統合して、多様なタイプのデータセット間のセマンティック関係を識別します。提案されたフレームワークは、不均一なデータソース間のセマンティックな類似性を評価し、データ探索を改善するための新しい方法を提供します。私たちの研究には、4つの重要なタスクに関する実験結果が含まれています。1）同様のデータセットの推奨、2）組み合わせ可能なデータセットの提案、3）タグの推定、4）変数の予測。我々の結果は、RAGが従来のメタデータアプローチと比較した場合、特に異なるカテゴリから関連するデータセットの選択を強化できることを示しています。ただし、パフォーマンスはタスクとモデルによって異なり、特定のユースケースに基づいて適切な手法を選択することの重要性を確認します。調査結果は、このアプローチがデータ調査と発見の課題に対処するための約束を保持していることを示唆していますが、推定タスクにはさらなる改良が必要です。 Developing the capacity to effectively search for requisite datasets is an urgent requirement to assist data users in identifying relevant datasets considering the very limited available metadata. For this challenge, the utilization of third-party data is emerging as a valuable source for improvement. Our research introduces a new architecture for data exploration which employs a form of Retrieval-Augmented Generation (RAG) to enhance metadata-based data discovery. The system integrates large language models (LLMs) with external vector databases to identify semantic relationships among diverse types of datasets. The proposed framework offers a new method for evaluating semantic similarity among heterogeneous data sources and for improving data exploration. Our study includes experimental results on four critical tasks: 1) recommending similar datasets, 2) suggesting combinable datasets, 3) estimating tags, and 4) predicting variables. Our results demonstrate that RAG can enhance the selection of relevant datasets, particularly from different categories, when compared to conventional metadata approaches. However, performance varied across tasks and models, which confirms the significance of selecting appropriate techniques based on specific use cases. The findings suggest that this approach holds promise for addressing challenges in data exploration and discovery, although further refinement is necessary for estimation tasks.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1609/aaai.v38i16.29728', '10.48550/arXiv.2406.13213', '10.48550/arXiv.2410.04231', '10.6109/jkiice.2023.27.12.1489']\n",
      "\u001b[93mMy lady, here are the papers that discuss the use of Retrieval-Augmented Generation (RAG) in large language models or related applications:\n",
      "\n",
      "Summary: The papers explore RAG's potential to enhance large language models, mitigate hallucinations, and improve data exploration and discovery.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997\n",
      "Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "This comprehensive review paper examines the progression of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG. It scrutinises the tripartite foundation of RAG frameworks, encompassing retrieval, generation, and augmentation techniques. The paper highlights state-of-the-art technologies in these components, offering insights into RAG system advancements.\n",
      "\n",
      "DOI: 10.1609/aaai.v38i16.29728\n",
      "Title: Benchmarking Large Language Models in Retrieval-Augmented Generation\n",
      "The paper investigates RAG's impact on large language models, analysing performance in noise robustness, negative rejection, information integration, and counterfactual robustness. It establishes the Retrieval-Augmented Generation Benchmark (RGB) to evaluate LLMs' challenges in applying RAG. The assessment reveals LLMs' struggles with negative rejection, information integration, and false information, indicating a need for further development.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213\n",
      "Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "This paper introduces Multi-Meta-RAG, a method that uses database filtering with LLM-extracted metadata to improve RAG selection for multi-hop queries. Traditional RAG applications have shown poor performance in answering such queries, but Multi-Meta-RAG significantly improves results on the MultiHop-RAG benchmark.\n",
      "\n",
      "DOI: 10.48550/arXiv.2410.04231\n",
      "Title: Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\n",
      "The research introduces a new architecture for data exploration, employing RAG to enhance metadata-based data discovery. The system integrates LLMs with external vector databases to identify semantic relationships among diverse datasets. The proposed framework evaluates semantic similarity and improves data exploration. Experimental results show RAG's effectiveness in selecting relevant datasets from different categories.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "Title: M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\n",
      "This paper proposes M-RAG, a method for effective search in open-domain question answering systems. It utilises embeddings with metadata and generative models like GPT-3.5-Turbo-16K and GPT-4 for automated answer generation. Experimental results demonstrate significant performance improvements over existing RAG methods and external inference systems.\n",
      "45\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) in large language models or related applications?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9999918, '10.48550/arXiv.2312.10997'), (0.99997985, '10.1609/aaai.v38i16.29728'), (0.99964714, '10.48550/arXiv.2406.13213'), (0.9996345, '10.48550/arXiv.2410.04231'), (0.9891816, '10.6109/jkiice.2023.27.12.1489')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: CrossRef：コミュニティ所有の学術メタデータの持続可能な供給源 Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このホワイトペーパーでは、CrossRefによって収集および利用可能になった学術的メタデータと、学術研究の生態系におけるその重要性について説明します。 1億600万件以上の記録を含み、年間平均11％のレートで拡大するCrossrefのメタデータは、出版社、著者、図書館員、資金提供者、および研究者向けの学術データの主要な情報源の1つになりました。メタデータセットは、ジャーナルやカンファレンスペーパーなどの従来のタイプだけでなく、データセット、レポート、プリプリント、ピアレビュー、助成金など、13のコンテンツタイプで構成されています。メタデータは、基本的な出版物メタデータに限定されませんが、全文、資金調達とライセンス情報、引用リンク、修正、更新、撤回などに関する情報への要約とリンクを含めることもできます。メタデータは、REST APIやOAI-PMHを含む多くのAPIを通じて利用できます。この論文では、CrossRefが提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、CrossRefの研究エコシステムにおける役割と、引用データの提供の進化など、長年にわたるメタデータのキュレーションの傾向にも注目しています。 Crossrefのメタデータで使用される研究を要約し、将来のメタデータの品質と検索を改善する計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: 開いた書誌メタデータの源としてのCrossref Crossref as a source of open bibliographic metadata\\nAbstract: CrossRefの学術出版物の書誌メタデータのオープン可用性を促進するために、いくつかのイニシアチブが採用されています。 CrossRefでの6つのメタデータ要素の可用性の最新の概要を示します。参照リスト、要約、ORCID、著者の提携、資金情報、およびライセンス情報です。私たちの分析は、これらのメタデータ要素の可用性が、少なくともCrossRefで最も一般的な出版タイプであるジャーナル記事で、時間とともに改善されたことを示しています。ただし、分析では、多くの出版社が書誌メタデータの完全な開放性を実現するために追加の努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93m## Summary: Crossref provides scholarly metadata to publishers, authors, librarians, funders, and researchers.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00022\n",
      "Crossref's metadata, which includes over 106 million records and expands at an average rate of 11% a year, is a major source of scholarly data. It consists of 13 content types, including journals, conference papers, datasets, reports, preprints, peer reviews, and grants. The metadata is available through various APIs and is a valuable source for research in scientometrics, measuring the growth and impact of science, and understanding new trends in scholarly communications. This paper describes the metadata that Crossref provides, how it is collected and curated, and its role in the research ecosystem.\n",
      "\n",
      "## DOI: 10.31222/osf.io/smxe5\n",
      "This paper discusses initiatives to promote the open availability of bibliographic metadata of scholarly publications in Crossref. It presents an overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. The analysis shows that the availability of these metadata elements has improved over time, particularly for journal articles, the most common publication type in Crossref. However, it also highlights the need for additional efforts from publishers to achieve full openness of bibliographic metadata.\n",
      "\n",
      "My lady, I hope this information is helpful!\n",
      "45\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.933\n",
      "Recall: 0.933\n",
      "F1-Score: 0.933\n",
      "Accuracy: 0.933\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.99954, '10.1162/qss_a_00022'), (0.41347715, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.3145/epi.2023.mar.09\\n Title: Bibliometricsに関連するメタデータのどれが同じであり、Microsoft Academic GraphからOpenAlexに切り替えるときにどのメタデータが異なりますか？ Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\\nAbstract: Microsoft Academic Graph（MAG）の退職の発表により、非営利団体Ourresearchは、Openalexという名前で同様のリソースを提供すると発表しました。したがって、メタデータを、最新のMAGスナップショットの書誌分析と初期のオープンアレックススナップショットと比較します。実質的にMAGのすべての作品は、書誌データの出版年、ボリューム、ファーストページと最後のページ、DOI、および引用分析の重要な要素である参照の数を保存するOpenalexに転送されました。 MAGドキュメントの90％以上がOpenAlexに同等のドキュメントタイプを持っています。残りのもののうち、特にOpenalex Document Typesの再分類ジャーナルアーティクルとブックチャプターは正しいと思われ、7％以上になります。そのため、ドキュメントタイプの仕様はMAGからOpenalexに大幅に改善されました。書誌関連のメタデータの別の項目として、MAGおよびOpenalexでの紙ベースの被験者の分類を調べました。 MAGよりもOpenAlexの対象分類割り当てを含むかなり多くのドキュメントを見つけました。第1レベルと第2レベルでは、分類構造はほぼ同じです。表形式とグラフィカルな形式の両方のレベルでの対象の再分類に関するデータを提示します。現地正規化された書誌評価における豊富な被験者の再分類の結果の評価は、本論文の範囲にありません。この未解決の質問とは別に、OpenAlexは、2021年以前の出版年のMAGと同じくらいの書誌分析に少なくともまったく適しているように見えます。 With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: Openalex、Web of Science、Scopus、PubMed、およびSemantic Sc\\u200b\\u200bholarの出版物と文書タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、次の書誌データベースの出版物と文書の種類を比較および分析します：Openalex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、Pubmed。結果は、類型が個々のデータベースプロバイダー間でかなり異なる可能性があることを示しています。さらに、出版物はそれぞれのデータベースで異なる方法で分類されているため、参考文献分析に関連するドキュメントを特定するために必要な研究と非研究テキストの区別は、データソースによって異なる場合があります。この研究の焦点は、横断段階の比較に加えて、主にオープンアレックスに含まれる出版物とドキュメントの種類のカバレッジと分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n', 'DOI: 10.48550/arXiv.2404.01985\\n Title: 彼はOpenalex、Scopus、Web of Scienceのオープンアクセスカバレッジ he open access coverage of OpenAlex, Scopus and Web of Science\\nAbstract: Diamond Open Access（OA）ジャーナルは、著者と読者の両方に無料の公開モデルを提供しますが、主要な書誌データベースでのインデックスの欠如は、これらのジャーナルの取り込みを評価する際の課題を提示します。さらに、出版言語や出版国などのOAの特性は、OAジャーナルがより多様であり、地域社会にサービスを提供することを目指しているという議論を支持するためにしばしば使用されてきましたが、OAジャーナルの地理的および言語的特性に関連する経験的証拠の現在の欠如があります。 OpenAlexとオープンアクセスジャーナルのディレクトリをベンチマークとして使用して、このペーパーでは、フィールド、国、言語による科学とスコープスのWebでの著者とジャーナルの報道を通じて、ダイヤモンドと金の報道を調査します。結果は、WOSとSCOPUSでのより低いカバレッジ、およびダイヤモンドOAの局所範囲を示しています。英語のみのジャーナルのシェアは、ゴールドジャーナルの間でかなり高くなっています。高所得国は、社会科学と人文科学のダイヤモンドジャーナルを除き、すべてのドメインとジャーナルの種類で著者のシェアが最も高い。ダイヤモンドOAインデックスの現在の景観を理解することは、より包括的なOAモデルに向けて政策と実践を進めることで、学術通信ネットワークを支援することができます。 Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2401.16359', '10.3145/epi.2023.mar.09', '10.48550/arXiv.2406.15154', '10.48550/arXiv.2404.01985']\n",
      "\u001b[93mMy lady, here is a summary of the key features and limitations of OpenAlex as a bibliometric database:\n",
      "\n",
      "## Summary:\n",
      "OpenAlex is a promising open-source database of scholarly metadata, offering a free and accessible alternative to established proprietary sources like Web of Science and Scopus. It has been gaining traction as a data source for research, particularly in bibliometric analyses. However, its rapidly evolving nature and the expansion and rapid changes in its internal data raise questions about data reliability.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2401.16359\n",
      "This report compares OpenAlex's reference coverage and selected metadata with those of Web of Science and Scopus. When restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex demonstrates average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. The analysis also reveals mixed results when comparing other core metadata covered by OpenAlex, with more ORCID identifiers, fewer abstracts, and a similar number of open-access status indicators per article compared to Web of Science and Scopus.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.17663\n",
      "This study, conducted in collaboration with the OpenAlex team, compares OpenAlex to Scopus across various dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, especially at the country level. However, issues of metadata accuracy and completeness suggest that additional research is needed to fully understand and address OpenAlex's limitations.\n",
      "\n",
      "## DOI: 10.3145/epi.2023.mar.09\n",
      "With the retirement of Microsoft Academic Graph (MAG), OpenAlex was introduced as a similar resource by the non-profit organization OurResearch. This paper compares the metadata relevant to bibliometric analyses between the latest MAG snapshot and an early OpenAlex snapshot. It finds that practically all works from MAG were transferred to OpenAlex, preserving bibliographic data such as publication year, volume, first and last page, DOI, and the number of references. More than 90% of MAG documents have equivalent document types in OpenAlex, and the document type specifications have improved significantly from MAG to OpenAlex.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2406.15154\n",
      "This study compares and analyses publication and document types in various bibliographic databases, including OpenAlex, Scopus, Web of Science, Semantic Scholar, and PubMed. The results show that typologies can differ considerably between individual database providers, and the distinction between research and non-research texts can vary depending on the data source due to differences in how publications are classified. The focus of this study is primarily on the coverage and analysis of the publication and document types in OpenAlex, which is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.01985\n",
      "This paper investigates the open access coverage of OpenAlex, Scopus, and Web of Science, using OpenAlex and the Directory of Open Access Journals as benchmarks. The results indicate lower coverage in WoS and Scopus for diamond open access (OA) journals, with a local scope for diamond OA. High-income countries have the highest share of authorship in most domains and journal types, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can help advance policy and practices towards more inclusive OA models.\n",
      "45\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.999764, '10.48550/arXiv.2404.17663'), (0.9930153, '10.48550/arXiv.2401.16359'), (0.971779, '10.3145/epi.2023.mar.09'), (0.90665317, '10.48550/arXiv.2406.15154'), (0.3060083, '10.48550/arXiv.2404.01985')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/s11192-015-1765-5\\n Title: 彼の科学とスコープスのWebの記事をジャーナル：比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌メソッドは、さまざまな目的、つまり研究評価のために複数の分野で使用されます。ほとんどの書誌分析には、Thomson ReutersのWeb of Science（WOS）とElsevierのScopusなどのデータソースが共通しています。この研究の目的は、これらの2つのデータベースのジャーナル報道を説明し、何らかの分野、出版国、言語が過剰または過小評価されているかどうかを評価することです。これを行うために、WOS（13,605のジャーナル）とScopus（20,346のジャーナル）のアクティブな学術雑誌の報道と、Ulrichの広範な定期的なディレクトリ（63,013雑誌）と比較しました。結果は、研究評価のためにWOSまたはSCOPUSのいずれかを使用すると、自然科学と工学を支持するバイアス、ならびに社会科学と芸術と人文科学を損なう生物医学的研究が導入される可能性があることを示しています。同様に、英語のジャーナルは、他の言語の損害に過大評価されています。両方のデータベースはこれらのバイアスを共有していますが、カバレッジは大幅に異なります。結果として、書誌分析の結果は、使用されるデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈では、特に異なる分野、機関、国、または言語を比較する場合、WOSとSCOPUSを注意して使用する必要があることを意味します。書誌コミュニティは、フィールド固有および国家引用指数など、WOSやSCOPUSでカバーされていない科学的生産量を含む方法と指標を開発するための努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データソースの大規模な比較：Scopus、Web of Science、Dimensions、CrossRef、およびMicrosoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: Scopus、Web of Science、Dimensions、CrossRef、およびMicrosoft Academicの5つの学際的な参考文献データソースの大規模な比較を提示します。この比較では、これらのデータソースでカバーされている2008年から2017年の期間の科学文書を考慮しています。 Scopusは、他のそれぞれのデータソースとペアワイズで比較されます。まず、ドキュメントのカバレッジのデータソース間の違いを分析します。たとえば、時間の経過とともに違い、ドキュメントタイプあたりの違い、および分野あたりの違いに焦点を当てます。次に、引用リンクの完全性と精度の違いを調べます。分析に基づいて、さまざまなデータソースの長所と短所について説明します。科学文献の包括的な報道と、文献を選択するための柔軟なフィルターのセットを組み合わせることの重要性を強調しています。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: Openalex、Web of Science、Scopus、PubMed、およびSemantic Sc\\u200b\\u200bholarの出版物と文書タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、次の書誌データベースの出版物と文書の種類を比較および分析します：Openalex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、Pubmed。結果は、類型が個々のデータベースプロバイダー間でかなり異なる可能性があることを示しています。さらに、出版物はそれぞれのデータベースで異なる方法で分類されているため、参考文献分析に関連するドキュメントを特定するために必要な研究と非研究テキストの区別は、データソースによって異なる場合があります。この研究の焦点は、横断段階の比較に加えて、主にオープンアレックスに含まれる出版物とドキュメントの種類のカバレッジと分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1007/s11192-015-1765-5', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2401.16359', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154']\n",
      "\u001b[93mSummary: Web of Science (WoS) is a bibliometric database that has strengths and weaknesses when compared to other databases such as Scopus.\n",
      "\n",
      "DOI: 10.1007/s11192-015-1765-5\n",
      "WoS and Scopus are the two most common data sources for bibliometric analyses. However, they may introduce biases that favour natural sciences and engineering, as well as biomedical research, to the detriment of social sciences, arts, and humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. The bibliometric community should develop methods and indicators that include scientific output not covered by WoS or Scopus.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663\n",
      "WoS and Scopus have been the foundation for research in the science of science, despite systematically underrepresenting certain disciplines and world regions. A new inclusive database, OpenAlex, has emerged as a potential alternative, but its limitations need to be critically assessed.\n",
      "\n",
      "DOI: 10.48550/arXiv.2401.16359\n",
      "OpenAlex is a promising open-source of scholarly metadata and a competitor to established proprietary sources like WoS and Scopus. It provides data freely, allowing researchers to conduct bibliometric studies without licensing barriers. However, its rapidly evolving nature raises questions about data trustworthiness.\n",
      "\n",
      "DOI: 10.1162/qss_a_00112\n",
      "A large-scale comparison of five multidisciplinary bibliographic data sources, including WoS, highlights the importance of combining comprehensive coverage of scientific literature with flexible filters for literature selection.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.15154\n",
      "This study compares publication and document types in various bibliographic databases, including WoS. It finds that typologies can differ significantly between database providers, and the distinction between research and non-research texts can vary, impacting the identification of relevant documents for bibliometric analysis.\n",
      "\n",
      "My lady, I hope this information is helpful!\n",
      "45\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9947187, '10.1007/s11192-015-1765-5'), (0.96073127, '10.48550/arXiv.2404.17663'), (0.71721, '10.48550/arXiv.2401.16359'), (0.63793355, '10.1162/qss_a_00112'), (0.44269672, '10.48550/arXiv.2406.15154')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.1007/978-3-031-88708-6_3\\n Title: 関連性はレトリバーからジェネレーターにぼろきれに伝播されますか？ Is Relevance Propagated from Retriever to Generator in RAG?\\nAbstract: 検索拡張生成（RAG）は、通常、コレクションから取得された一連のドキュメントの形で、プロンプトの大規模な言語モデル（LLM）への一連のドキュメントの形で、質問の回答などの下流タスクのパフォーマンスを潜在的に改善するためのフレームワークです。一連のトップランクのドキュメントの関連性を最大化するという標準検索タスクの目的とは異なり、RAGシステムの目的は、ドキュメントのユーティリティがLLMプロンプトの追加コンテキスト情報の一部としてそれを含めることがダウンストリームタスクを改善するかどうかを示します。既存の研究では、知識集約型の言語タスク（KILT）のRAGコンテキストの関連性の役割を調査します。対照的に、私たちの仕事では、関連性は、情報を求めるタスクのクエリとドキュメントの間の局所的な重複の関連性に対応しています。具体的には、IRテストコレクションを利用して、局所的に関連するドキュメントで構成されるRAGコンテキストが下流のパフォーマンスの改善につながるかどうかを経験的に調査します。私たちの実験は、次の発見につながります。（a）関連性と有用性の間には小さな正の相関があります。 （b）この相関は、コンテキストサイズの増加とともに減少します（k-shotのkの値が高い）。 （c）より効果的な検索モデルは、一般に、下流のラグパフォーマンスの向上につながります。 Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n', \"DOI: 10.48550/arXiv.2505.18247\\n Title: メタゲンブレンドラグ：特殊なドメインの質問を解決するためのゼロショット精度のロックを解除する MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索された生成（RAG）は、ドメイン固有のエンタープライズデータセットとの闘いであり、しばしばファイアウォールの背後に隔離され、トレーニング前にLLMSによって見えない複雑で特殊な用語が豊富です。医学、ネットワーキング、または法律などのドメイン間のセマンティックな変動は、Ragのコンテキストの精度を妨げますが、微調整ソリューションはコストがかかり、遅く、新しいデータが出現するにつれて一般化が欠けています。微調整せずにレトリーバーでゼロショット精度を達成することは、依然として重要な課題です。メタデータの生成パイプラインとハイブリッドクエリインデックスを介してセマンティックレトリバーを強化する新しいエンタープライズ検索アプローチである「メタゲンブレンドラグ」を紹介します。重要な概念、トピック、頭字語を活用することにより、メタデータが豊富なセマンティックインデックスを作成し、ハイブリッドクエリをブーストし、微調整せずに堅牢でスケーラブルなパフォーマンスを提供します。 Biomedical PubMedqaデータセットでは、Metagenブレンドラグは82％の回収精度と77％のRAG精度を達成し、以前のゼロショットラグベンチマークをすべて上回り、そのデータセットの微調整されたモデルに匹敵し、SquadやNQのようなデータセットでも優れています。このアプローチは、特殊なドメイン全体で比類のない一般化を備えたセマンティックレトリバーを構築するための新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\", 'DOI: 10.1609/aaai.v38i16.29728\\n Title: 検索された世代の大規模な言語モデルのベンチマーク Benchmarking Large Language Models in Retrieval-Augmented Generation\\nAbstract: 検索された生成（RAG）は、大規模な言語モデル（LLM）の幻覚を緩和するための有望なアプローチです。ただし、既存の研究には、さまざまな大規模な言語モデルに対する検索された生成の影響に関する厳密な評価がありません。これにより、異なるLLMのRAGの機能における潜在的なボトルネックを特定することが困難になります。この論文では、検索された生成が大規模な言語モデルに与える影響を体系的に調査します。ノイズの堅牢性、否定的な拒否、情報統合、反事実的堅牢性など、RAG\\u200b\\u200bに必要な4つの基本能力におけるさまざまな大手言語モデルのパフォーマンスを分析します。この目的のために、英語と中国語の両方でRAG評価のための新しいコーパスである検索された生成ベンチマーク（RGB）を確立します。 RGBは、ベンチマーク内のインスタンスを、ケースを解決するために必要な前述の基本能力に基づいて、4つの個別のテストベッドに分割します。次に、RGBの6つの代表LLMを評価して、RAGを適用する際に現在のLLMの課題を診断します。評価により、LLMはある程度のノイズの堅牢性を示していますが、否定的な拒絶、情報統合、誤った情報への対処に関して依然として著しく苦労していることが明らかになりました。前述の評価の結果は、LLMにRAGを効果的に適用するために、まだかなりの旅がまだあることを示しています。 Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.13213', '10.1007/978-3-031-88708-6_3', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2505.18247', '10.1609/aaai.v38i16.29728']\n",
      "\u001b[93m## Summary: RAG (Retrieval-Augmented Generation) is a framework that incorporates external knowledge to improve the performance of downstream tasks like question answering.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2406.13213\n",
      "RAG enables the retrieval of relevant information from external sources, allowing large language models (LLMs) to answer queries over unseen document collections. However, traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence.\n",
      "\n",
      "## DOI: 10.1007/978-3-031-88708-6_3\n",
      "RAG is a framework for incorporating external knowledge, usually in the form of a set of documents, as part of a prompt to an LLM to potentially improve the performance of a downstream task, such as question answering. The objective of a RAG system is to maximise the total utility of the documents, where the utility indicates whether including it as additional contextual information improves a downstream task.\n",
      "\n",
      "## DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "This paper proposes a method called M-RAG (Metadata Retrieval-Augmented Generation) for effective search in open-domain question answering (ODQA) systems. M-RAG utilises embeddings that include metadata and employs generative models like GPT-3.5-Turbo-16K and GPT-4 for automated answer generation. Experimental results show that M-RAG exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over existing RAG methods.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2505.18247\n",
      "RAG struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialised terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision. A novel enterprise search approach called MetaGen Blended RAG is introduced to enhance semantic retrievers through a metadata generation pipeline and hybrid query indexes. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieved 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks.\n",
      "\n",
      "## DOI: 10.1609/aaai.v38i16.29728\n",
      "RAG is a promising approach for mitigating the hallucination of LLMs. This paper systematically investigates the impact of RAG on LLMs, analysing their performance in four fundamental abilities required for RAG. The evaluation reveals that LLMs exhibit a certain degree of noise robustness but still struggle with negative rejection, information integration, and dealing with false information.\n",
      "\n",
      "My lady, I hope these findings provide a comprehensive understanding of how RAG is used to improve question answering and information retrieval systems.\n",
      "45\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.956\n",
      "Recall: 0.956\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.887\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9997565, '10.48550/arXiv.2406.13213'), (0.999617, '10.1007/978-3-031-88708-6_3'), (0.9992563, '10.6109/jkiice.2023.27.12.1489'), (0.9956006, '10.48550/arXiv.2505.18247'), (0.8362874, '10.1609/aaai.v38i16.29728')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1371/journal.pbio.1002542\\n Title: 引用メトリック：正規化する方法（NOT）の入門書 Citation Metrics: A Primer on How (Not) to Normalize\\nAbstract: 引用指標は、公開された研究を評価するためにますます使用されています。課題の1つは、科学分野の違い、年齢（出版年）、ドキュメントの種類、データベースカバレッジ、およびその他の要因を説明するために、これらのメトリックを正規化するかどうか、および方法です。さまざまなアプローチを使用して、正規化の長所と短所について説明します。科学者、機関、雑誌、または国のコーパスを評価するために、複数の論文で引用指標を組み合わせる必要がある場合、および多著者の論文でクレジットを属性にしようとする場合、追加の課題が現れます。異なる引用メトリックは補完的な洞察を提供する可能性がありますが、計算の根底にある仮定を慎重に検討する必要があります。 Citation metrics are increasingly used to appraise published research. One challenge is whether and how to normalize these metrics to account for differences across scientific fields, age (year of publication), type of document, database coverage, and other factors. We discuss the pros and cons for normalizations using different approaches. Additional challenges emerge when citation metrics need to be combined across multiple papers to appraise the corpus of scientists, institutions, journals, or countries, as well as when trying to attribute credit in multiauthored papers. Different citation metrics may offer complementary insights, but one should carefully consider the assumptions that underlie their calculation.\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542']\n",
      "\u001b[93mMy lady, the main challenges in normalizing citation metrics across scientific fields include:\n",
      "\n",
      "Summary: The challenges of normalizing citation metrics include accounting for differences in scientific fields, publication year, document type, database coverage, and other factors.\n",
      "\n",
      "DOI: 10.1371/journal.pbio.1002542 - This text discusses the challenges of normalizing citation metrics, highlighting the need to consider various factors such as scientific field, age (year of publication), document type, database coverage, and others. It also mentions additional challenges that arise when combining citation metrics across multiple papers to evaluate scientists, institutions, journals, or countries, as well as attributing credit in multi-authored papers. The text emphasizes the importance of carefully considering the assumptions underlying the calculation of different citation metrics.\n",
      "45\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.600\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.99984646, '10.1371/journal.pbio.1002542')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.5281/ZENODO.13960973\\n Title: 欠落しているデータパターンを使用して、書誌データセットで誤って割り当てられた記事を検出する Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\\nAbstract: DORA宣言とCoaraは、オープンデータに基づいた書誌指標の使用を求めています。ただし、確立された学術的メタデータデータセットは閉じられており、オープンデータセットの品質はまだ徹底的に調査されていません。この論文では、欠落データパターンを使用してデータセット内のエラーを検出する方法を提示します。例として、この方法は、ETHチューリッヒに関連する出版物の所属メタデータに適用されます。これにより、一連の誤って提携した論文を特定することができます。このペーパーで導入された方法は、提携データ用に特別に設計されておらず、他のタイプのデータのエラーを検出するためにも使用できます。それは、プロバイダーとデータのユーザーに利益をもたらすことを願っている修正につながる可能性があります。 The DORA declaration and CoARA call for the use of bibliometric indicators based on open data. However, established scholarly metadata datasets are closed, and the quality of open datasets has not yet been thoroughly examined. In this paper, I present a method to detect errors in a dataset using missing data patterns. As an example, the method is applied to the affiliation metadata of publications associated with ETH Zurich. This allows me to identify a series of incorrectly affiliated papers. The method introduced in this paper is not specifically designed for affiliation data and can also be used to detect errors in other types of data. It could lead to corrections which will hopefully benefit providers as well as users of data.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: CrossRefデータのDOIエラーによる無効な引用の識別と修正 Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この作業は、CrossRefで利用可能なオープンな参考文献メタデータを分析することにより、DOIの間違いのクラスを特定し、どの出版社がそのような間違いを担当し、これらの誤ったDOIの数を自動プロセスで修正できるかを強調することを目的としています。 By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs.これらの引用は、CrossRefの関連する引用データをアップロードする責任を負う責任者の有効性と出版社を追跡することで処理しました。最後に、無効なDOIの事実上の誤りのパターンと、それらをキャッチして修正するために必要な正規表現を特定しました。この研究の結果は、無効な引用の大部分に責任を負い、および/または影響を受けた出版社は少数であることを示しています。過去の研究で提案されたDOI名エラーの分類法を拡張し、以前のアプローチよりも無効なDOIでより多くのミスをきれいにすることができる、より詳細に精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的観点からDOIの間違いの可能な理由を調査し、出版社が無効な引用データの生産の根底にある問題を特定するのに役立ちます。また、私たちが提示するDOIクリーニングメカニズムは、既存のプロセス（COCIなど）に統合して、間違ったDOIを自動的に修正することで引用を追加できます。この研究は、オープンサイエンスの原則に厳密に従っていたため、私たちの研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.5281/ZENODO.13960973', '10.1007/s11192-022-04367-w']\n",
      "\u001b[93mMy lady, there are several methods used to detect and correct errors in bibliographic datasets.\n",
      "\n",
      "## Summary:\n",
      "Missing data patterns can be used to detect errors in datasets.\n",
      "\n",
      "## DOI: 10.5281/ZENODO.13960973\n",
      "This paper presents a method to detect errors in a dataset using missing data patterns. The method is applied to the affiliation metadata of publications associated with ETH Zurich, allowing the identification of incorrectly affiliated papers. The approach is not limited to affiliation data and can be used to detect errors in other types of data too.\n",
      "\n",
      "## DOI: 10.1007/s11192-022-04367-w\n",
      "This work aims to identify classes of DOI mistakes by analysing open bibliographic metadata available in Crossref. It highlights which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. The study also identifies patterns of factual errors in invalid DOIs and the regular expressions needed to catch and correct them.\n",
      "45\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.933\n",
      "Recall: 0.933\n",
      "F1-Score: 0.933\n",
      "Accuracy: 0.933\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.99960643, '10.5281/ZENODO.13960973'), (0.6302299, '10.1007/s11192-022-04367-w')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/978-3-031-88708-6_3\\n Title: 関連性はレトリバーからジェネレーターにぼろきれに伝播されますか？ Is Relevance Propagated from Retriever to Generator in RAG?\\nAbstract: 検索拡張生成（RAG）は、通常、コレクションから取得された一連のドキュメントの形で、プロンプトの大規模な言語モデル（LLM）への一連のドキュメントの形で、質問の回答などの下流タスクのパフォーマンスを潜在的に改善するためのフレームワークです。一連のトップランクのドキュメントの関連性を最大化するという標準検索タスクの目的とは異なり、RAGシステムの目的は、ドキュメントのユーティリティがLLMプロンプトの追加コンテキスト情報の一部としてそれを含めることがダウンストリームタスクを改善するかどうかを示します。既存の研究では、知識集約型の言語タスク（KILT）のRAGコンテキストの関連性の役割を調査します。対照的に、私たちの仕事では、関連性は、情報を求めるタスクのクエリとドキュメントの間の局所的な重複の関連性に対応しています。具体的には、IRテストコレクションを利用して、局所的に関連するドキュメントで構成されるRAGコンテキストが下流のパフォーマンスの改善につながるかどうかを経験的に調査します。私たちの実験は、次の発見につながります。（a）関連性と有用性の間には小さな正の相関があります。 （b）この相関は、コンテキストサイズの増加とともに減少します（k-shotのkの値が高い）。 （c）より効果的な検索モデルは、一般に、下流のラグパフォーマンスの向上につながります。 Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n', \"DOI: 10.48550/arXiv.2404.13948\\n Title: ぼろきれの背中を壊したYPO ypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\\nAbstract: 最近の大規模な言語モデル（LLMS）の堅牢性は、さまざまなドメインや現実世界のアプリケーションで適用性が拡大するにつれてますます重要になっています。検索された生成（RAG）は、LLMの限界に対処するための有望なソリューションですが、RAGの堅牢性に関する既存の研究は、しばしば、RAGコンポーネント間の相互接続された関係またはマイナーなテキストエラーなどの実際のデータベースで一般的な潜在的な脅威を見落としています。この作業では、RAGの堅牢性を評価する際に、2つの未掘りの側面を調査します。1）低レベルの摂動を通じて騒々しい文書に対する脆弱性と2）Ragの堅牢性の全体的な評価。さらに、これらの側面をターゲットにする新しい攻撃法であるRag（\\\\ textit {garag}）に対する遺伝的攻撃を紹介します。具体的には、GARAGは各コンポーネント内の脆弱性を明らかにし、騒々しいドキュメントに対してシステム全体の機能をテストするように設計されています。 \\\\ textIT {garag}を標準のQAデータセットに適用し、多様なレトリバーとLLMを組み込んで、ragの堅牢性を検証します。実験結果は、Garagが一貫して高い攻撃の成功率を達成することを示しています。また、各コンポーネントのパフォーマンスとその相乗効果を大幅に破壊し、現実世界のぼろきれシステムを混乱させる際にマイナーなテキストの不正確さがもたらす実質的なリスクを強調しています。 The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\\\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\\n\"]\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1007/978-3-031-88708-6_3', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2404.13948']\n",
      "\u001b[93m## Summary: RAG is a framework for improving the performance of downstream tasks by incorporating external knowledge into large language models (LLMs).\n",
      "\n",
      "## DOI: 10.1007/978-3-031-88708-6_3\n",
      "RAG, or Retrieval Augmented Generation, is a framework that aims to enhance the performance of downstream tasks, such as question answering, by incorporating external knowledge into large language models (LLMs). Unlike standard retrieval tasks, which focus on maximising the relevance of top-ranked documents, RAG systems aim to maximise the utility of documents by including them as additional contextual information in LLM prompts. Existing studies investigate the role of relevance in RAG contexts for knowledge-intensive language tasks (KILT), while other work focuses on the relevance of topical overlap between queries and documents for information-seeking tasks.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2406.13213\n",
      "RAG enables the retrieval of relevant information from external knowledge sources, allowing LLMs to answer queries over previously unseen document collections. However, traditional RAG applications have been shown to perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. A new method called Multi-Meta-RAG has been introduced to improve RAG selection by using database filtering with LLM-extracted metadata. This method has been found to greatly improve results on the MultiHop-RAG benchmark.\n",
      "\n",
      "## DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain question answering (ODQA) systems. M-RAG utilises embeddings that include metadata and employs generative models such as GPT-3.5-Turbo-16K and GPT-4 for automated answer generation. Experimental results show that M-RAG exhibits up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over existing RAG methods.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.13948\n",
      "RAG is considered a promising solution for addressing the limitations of LLMs, but existing studies on RAG robustness often overlook potential threats prevalent in real-world databases, such as minor textual errors. This work investigates two underexplored aspects of RAG robustness: vulnerability to noisy documents through low-level perturbations and a holistic evaluation of RAG robustness. A novel attack method called Genetic Attack on RAG (GARAG) is introduced to target these aspects. Experimental results demonstrate that GARAG consistently achieves high attack success rates and highlights the substantial risk posed by minor textual inaccuracies in disrupting RAG systems in the real world.\n",
      "45\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.675\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.94834626, '10.1007/978-3-031-88708-6_3'), (0.5391045, '10.48550/arXiv.2406.13213'), (0.38225675, '10.6109/jkiice.2023.27.12.1489'), (0.37410447, '10.48550/arXiv.2404.13948')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00286\\n Title: 8つのフリーアクセス学術データベースの出版メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースにおけるメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000を超えるレコードのランダムなCrossRefサンプルを選択し、7つのデータベース（Dimensions、Google Scholar、Microsoft Academic、Openalex、Scilit、Semantic Sc\\u200b\\u200bholar、およびThe Lens）で検索されました。この情報、これらのフィールドの完全性レート、およびデータベース間の合意を説明するフィールドを観察するために、7つの特性（要約、アクセス、書誌情報、書誌情報、文書の種類、公開日、言語、識別子）を観察しました。結果は、アカデミック検索エンジン（Google Scholar、Microsoft Academic、およびSemantic Sc\\u200b\\u200bholar）が少ない情報を収集し、完全性が低いことを示しています。逆に、サードパーティのデータベース（寸法、OpenAlex、Scilit、およびレンズ）は、メタデータの品質が高く、完全性が高くなります。アカデミック検索エンジンには、Webをcrawって信頼できる記述データを取得する能力がないと結論付けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することから得られる情報の喪失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: メタエンハンス：大学図書館の電子論文と学位論文のメタデータの品質改善 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタルオブジェクトがデジタルライブラリインターフェイスを通じて発見されるために重要です。ただし、さまざまな理由により、デジタルオブジェクトのメタデータは、しばしば不完全で、一貫性がなく、誤った値を示します。ケーススタディとして、電子論文と論文（ETD）の7つの重要な分野を使用して、学術メタデータを自動的に検出、修正、および正規化する方法を調査します。メタエンハンスを提案します。メタエンハンスは、これらの分野の品質を改善するために最先端の人工知能方法を利用するフレームワークです。 Metaenhanceを評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 ETDを含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークでMetaenhanceをテストし、提案された方法は、7つのフィールドのうち5つで0.85から1.00の範囲のエラーとF1スコアの検出にほぼ完全なF1スコアを達成したことを発見しました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化全体のメタデータの品質の問題を特定する Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、標準的な形式でコンテキスト、技術、および管理情報を提供することにより、発見とアクセスに不可欠です。しかし、メタデータは、社会文化的表現、リソースの制約、および標準化されたシステムの間の緊張の場所でもあります。正式および非公式の介入は、品質の問題、アイデンティティを主張する政治的行為、または視界を最大化するための戦略的選択と解釈される場合があります。これに関連して、メタデータの品質、一貫性、および完全性が個人やコミュニティにどのように影響するかを理解しようとしました。記録のサンプルをレビューして、メタデータとコミュニティが互いに反対して、意図的に文化的な意味を反映する（またはそうでない）方法に起因する問題を特定して分類しました。 Metadata are crucial for discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. In this context, we sought to understand how metadata quality, consistency, and completeness impact individuals and communities. Reviewing a sample of records, we identified and classified issues stemming from how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101']\n",
      "\u001b[93mMy lady, here are the studies that examined the abstract in metadata:\n",
      "\n",
      "## Summary: Studies have examined the quality, consistency, and completeness of metadata, including the abstract field, to understand its impact on discovery and access.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00286\n",
      "This study compared the amount of metadata and the completeness degree of research publications in new academic databases. The abstract was one of the seven characteristics analysed to observe fields that describe information, the completeness rate of these fields, and the agreement among databases.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2303.17661\n",
      "The study investigated methods to automatically detect, correct, and canonicalise scholarly metadata using seven key fields of electronic theses and dissertations (ETDs) as a case study. The abstract was one of the fields used to evaluate the proposed methods, which achieved nearly perfect F1-scores in detecting and correcting errors.\n",
      "\n",
      "## DOI: 10.5860/crl.86.1.101\n",
      "This study sought to understand how metadata quality, consistency, and completeness impact individuals and communities. The abstract was one of the fields reviewed to identify and classify issues stemming from how metadata and communities interact to reflect cultural meanings.\n",
      "45\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.575\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.4213137, '10.1162/qss_a_00286'), (0.19544049, '10.48550/arXiv.2303.17661'), (0.12951279, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0\n",
      "Length of documents: 45\n",
      "reranked_documents: []\n",
      "length of reranked_documents: 0\n",
      "Retrieved DOIs: []\n",
      "\u001b[93mMy lady, could you please provide me with some context or a specific topic you are interested in? I can then offer you studies that have examined citations within that field.\n",
      "45\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.500\n",
      "Faithfulness score: 0\n",
      "Documents score: []\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.1007/s11192-023-04923-y\\n Title: Openalexの不足している機関：考えられる理由、意味、および解決策 Missing institutions in OpenAlex: possible reasons, implications, and solutions\\nAbstract: オープンサイエンスの出現では、データ品質が高いオープンデータプラットフォームが必要です。 2022年1月に開始されたグローバルな研究システムの完全に開かれたカタログとして、OpenAlexは、定量的科学研究で広く使用されている簡単なデータアクセシビリティと幅広いデータカバレッジの2つの主要な利点を特徴としています。驚くべきことに、Openalexはライデン大学のランキングの重要なデータソースとして採用されています。ただし、Openalexのジャーナル記事メタデータには、機関が欠落しているという深刻なデータ品質の問題があります。この研究では、3種類の制度情報（FII）、部分的に欠落している機関情報（PMII）、および完全に欠落している機関情報（CMII）を定義することにより、問題とその結果と解決策の考えられる理由を調査します。私たちの結果は、不足している機関の問題がOpenalexのジャーナル記事の60％以上で発生することを示しています。この問題は、特に初期のメタデータや社会科学や人文科学で広まっています。データのサブサンプルを使用して、問題の考えられる理由、歪んだ結果のリスク、および不足している機関の問題に対する可能な解決策をさらに調査します。目的は、オープンリソースのデータ品質改善の重要性を高め、定量的科学研究およびより広い文脈でのオープンリソースの責任ある使用をサポートすることです。 The advent of open science calls for open data platforms with high data quality. As a fully open catalog of the global research system launched in January 2022, OpenAlex features two main advantages of easy data accessibility and broad data coverage, which has been widely used in quantitative science studies. Remarkably, OpenAlex is adopted as an important data source for Leiden university ranking. However, there is a severe data quality problem of missing institutions in journal article metadata in OpenAlex. This study investigates the possible reasons for the problem and its consequences and solutions by defining three types of institutional information—full institutional information (FII), partially missing institutional information (PMII) and completely missing institutional information (CMII). Our results show that the problem of missing institutions occurs in more than 60% of the journal articles in OpenAlex. The problem is particularly widespread in metadata from the early years and in the social sciences and humanities. Using sub-samples of the data, we further explore the possible reasons for the problem, the risk it might represent for distorted results, and possible solutions to the problem of missing institutions. The aim is to raise the importance of data quality improvements in open resources, and thus to support the responsible use of open resources in quantitative science studies and also in broader contexts.\\n', 'DOI: 10.1590/SciELOPreprints.11205\\n Title: ユニバーサルインデックスへのオープンロードで：OpenAlexおよびOpenJournal Systems On the Open Road to Universal Indexing: OpenAlex and OpenJournal Systems\\nAbstract: この調査では、OpenAlexのオープンジャーナルシステム（JUOJS）を使用したジャーナルのインデックス作成を検証し、包括的な学術参加をサポートする2つのオープンソースソフトウェアイニシアチブを反映しています。 47,625のアクティブなJuojsのデータセットを分析することにより、これらのジャーナルの71％がOpenAlexで少なくとも1つの記事をインデックス付けされていることを明らかにします。私たちの調査結果は、OpenAlexに含まれるCrossRef doiを使用してジャーナルの97％を使用して、インデックス作成の達成におけるCrossRef DOIの中心的な役割を強調しています。ただし、この技術的依存は、特に低所得国（Juojsの47％）および非英語言語ジャーナル（Juojsの55％-64％）からのリソース制限されたジャーナル（Juojsの55％-64％）の雑誌として、より広範な構造的不平等を反映しています。私たちの研究は、学術インフラストラクチャの依存関係の理論的意味と、世界的な知識の可視性における体系的な格差を永続させる上でのその役割を強調しています。 OpenAlexのような包括的な書誌データベースでさえ、世界規模で公平な索引付けを促進するために、財務、インフラ、および言語の障壁に積極的に対処する必要があると主張します。インデックス作成メカニズム、永続的な識別子、および構造的不平等との関係を概念化することにより、この研究は、グローバルな多言語学術生態系における普遍的なインデックス作成のダイナミクスとその実現を再考するための重要なレンズを提供します。 This study examines OpenAlex’s indexing of journals using Open Journal Systems (JUOJS), reflecting two open source software initiatives supporting inclusive scholarly participation. By analyzing a dataset of 47,625 active JUOJS, we reveal that 71% of these journals have at least one article indexed in OpenAlex. Our findings underscore the central role of Crossref DOIs in achieving indexing, with 97% of the journals using Crossref DOIs included in OpenAlex. However, this technical dependency reflects broader structural inequities, as resource-limited journals, particularly those from low-income countries (47% of JUOJS) and non-English language journals (55%-64% of JUOJS), remain underrepresented. Our work highlights the theoretical implications of scholarly infrastructure dependencies and their role in perpetuating systemic disparities in global knowledge visibility. We argue that even inclusive bibliographic databases like OpenAlex must actively address financial, infrastructural, and linguistic barriers to foster equitable indexing on a global scale. By conceptualizing the relationship between indexing mechanisms, persistent identifiers, and structural inequities, this study provides a critical lens for rethinking the dynamics of universal indexing and its realization in a global, multilingual scholarly ecosystem.\\n', 'DOI: 10.48550/arXiv.2404.01985\\n Title: 彼はOpenalex、Scopus、Web of Scienceのオープンアクセスカバレッジ he open access coverage of OpenAlex, Scopus and Web of Science\\nAbstract: Diamond Open Access（OA）ジャーナルは、著者と読者の両方に無料の公開モデルを提供しますが、主要な書誌データベースでのインデックスの欠如は、これらのジャーナルの取り込みを評価する際の課題を提示します。さらに、出版言語や出版国などのOAの特性は、OAジャーナルがより多様であり、地域社会にサービスを提供することを目指しているという議論を支持するためにしばしば使用されてきましたが、OAジャーナルの地理的および言語的特性に関連する経験的証拠の現在の欠如があります。 OpenAlexとオープンアクセスジャーナルのディレクトリをベンチマークとして使用して、このペーパーでは、フィールド、国、言語による科学とスコープスのWebでの著者とジャーナルの報道を通じて、ダイヤモンドと金の報道を調査します。結果は、WOSとSCOPUSでのより低いカバレッジ、およびダイヤモンドOAの局所範囲を示しています。英語のみのジャーナルのシェアは、ゴールドジャーナルの間でかなり高くなっています。高所得国は、社会科学と人文科学のダイヤモンドジャーナルを除き、すべてのドメインとジャーナルの種類で著者のシェアが最も高い。ダイヤモンドOAインデックスの現在の景観を理解することは、より包括的なOAモデルに向けて政策と実践を進めることで、学術通信ネットワークを支援することができます。 Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2401.16359', '10.48550/arXiv.2404.17663', '10.1007/s11192-023-04923-y', '10.1590/SciELOPreprints.11205', '10.48550/arXiv.2404.01985']\n",
      "\u001b[93m## Summary: OpenAlex is an open-source, scholarly metadata platform that provides free and open data, enabling researchers to conduct bibliometric studies without licensing barriers.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2401.16359\n",
      "This report compares OpenAlex's reference coverage and selected metadata with established databases like Web of Science and Scopus. It finds that OpenAlex has comparable average source reference numbers and internal coverage rates, despite being a rapidly evolving source with expanding and changing data.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.17663\n",
      "This study examines the suitability of OpenAlex for bibliometric analyses. It concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for certain analyses, particularly at the country level. However, issues with metadata accuracy and completeness highlight the need for further research to fully understand and address OpenAlex's limitations.\n",
      "\n",
      "## DOI: 10.1007/s11192-023-04923-y\n",
      "This research focuses on the problem of missing institutions in OpenAlex's journal article metadata. It defines three types of institutional information and investigates the reasons, implications, and potential solutions for this data quality issue. The study finds that over 60% of journal articles in OpenAlex are affected, particularly in early years and in the social sciences and humanities.\n",
      "\n",
      "## DOI: 10.1590/SciELOPreprints.11205\n",
      "This investigation explores OpenAlex's indexing of journals using Open Journal Systems (JUOJS). It reveals that 71% of journals have at least one article indexed in OpenAlex and highlights the central role of Crossref DOIs in achieving indexing. The study argues that even inclusive databases like OpenAlex must actively address financial, infrastructural, and linguistic barriers to promote equitable indexing globally.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.01985\n",
      "This paper uses OpenAlex and the Directory of Open Access Journals as benchmarks to investigate the coverage of diamond and gold open access (OA) journals in Web of Science and Scopus. It finds lower coverage in WoS and Scopus and highlights the local scope of diamond OA. The study suggests that understanding the current landscape of diamond OA indexing can support the scholarly communications network in moving towards more inclusive OA models.\n",
      "\n",
      "My lady, I hope these summaries provide a comprehensive overview of OpenAlex and its various aspects.\n",
      "45\n",
      "For query: ['Tell me about OpenAlex.']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.773\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9965301, '10.48550/arXiv.2401.16359'), (0.9940428, '10.48550/arXiv.2404.17663'), (0.99040353, '10.1007/s11192-023-04923-y'), (0.9678993, '10.1590/SciELOPreprints.11205'), (0.96405166, '10.48550/arXiv.2404.01985')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: CrossRef：コミュニティ所有の学術メタデータの持続可能な供給源 Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このホワイトペーパーでは、CrossRefによって収集および利用可能になった学術的メタデータと、学術研究の生態系におけるその重要性について説明します。 1億600万件以上の記録を含み、年間平均11％のレートで拡大するCrossrefのメタデータは、出版社、著者、図書館員、資金提供者、および研究者向けの学術データの主要な情報源の1つになりました。メタデータセットは、ジャーナルやカンファレンスペーパーなどの従来のタイプだけでなく、データセット、レポート、プリプリント、ピアレビュー、助成金など、13のコンテンツタイプで構成されています。メタデータは、基本的な出版物メタデータに限定されませんが、全文、資金調達とライセンス情報、引用リンク、修正、更新、撤回などに関する情報への要約とリンクを含めることもできます。メタデータは、REST APIやOAI-PMHを含む多くのAPIを通じて利用できます。この論文では、CrossRefが提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、CrossRefの研究エコシステムにおける役割と、引用データの提供の進化など、長年にわたるメタデータのキュレーションの傾向にも注目しています。 Crossrefのメタデータで使用される研究を要約し、将来のメタデータの品質と検索を改善する計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022']\n",
      "\u001b[93mMy lady, Crossref is a sustainable source of community-owned scholarly metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "This paper describes the scholarly metadata collected and made available by Crossref, which has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. Containing over 106 million records and expanding at an average rate of 11% a year, the metadata set consists of 13 content types, including traditional types such as journals and conference papers, as well as data sets, reports, preprints, peer reviews, and grants. The metadata is available through a number of APIs, including REST API and OAI-PMH.\n",
      "45\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.556\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.8345418, '10.1162/qss_a_00022')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: オープンアレックスの言語カバレッジの評価：メタデータの精度と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: ClarivateのWeb of Science（WOS）とElsevierのScopusは、数十年にわたって書誌情報の主要なソースでした。高度にキュレーションされていますが、これらの閉鎖された独自のデータベースは、英語の出版物に大きく偏っており、研究普及における他の言語の使用を過小評価しています。 2022年に発売されたOpenalexは、包括的、包括的、オープンソースの研究情報を約束しました。すでに学者や研究機関が使用している間、そのメタデータの質は現在評価されています。この論文は、言語に関連するOpenalexのメタデータの完全性と正確性、WOSとの比較、および6,836の記事のサンプルの詳細な手動検証を通じて、この文献に貢献しています。結果は、オープンアレックスがWOSよりもはるかにバランスのとれた言語カバレッジを示すことを示しています。ただし、言語メタデータは必ずしも正確ではないため、Openalexは他の言語のそれを過小評価しながら英語の場所を過大評価します。批判的に使用すると、OpenAlexは学術出版に使用される言語の包括的かつ代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャレベルでより多くの作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8つのフリーアクセス学術データベースの出版メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースにおけるメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000を超えるレコードのランダムなCrossRefサンプルを選択し、7つのデータベース（Dimensions、Google Scholar、Microsoft Academic、Openalex、Scilit、Semantic Sc\\u200b\\u200bholar、およびThe Lens）で検索されました。この情報、これらのフィールドの完全性レート、およびデータベース間の合意を説明するフィールドを観察するために、7つの特性（要約、アクセス、書誌情報、書誌情報、文書の種類、公開日、言語、識別子）を観察しました。結果は、アカデミック検索エンジン（Google Scholar、Microsoft Academic、およびSemantic Sc\\u200b\\u200bholar）が少ない情報を収集し、完全性が低いことを示しています。逆に、サードパーティのデータベース（寸法、OpenAlex、Scilit、およびレンズ）は、メタデータの品質が高く、完全性が高くなります。アカデミック検索エンジンには、Webをcrawって信頼できる記述データを取得する能力がないと結論付けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することから得られる情報の喪失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.1162/qss_a_00286']\n",
      "\u001b[93mMy lady, here are two papers that evaluate the linguistic coverage or language-related metadata in scholarly databases:\n",
      "\n",
      "## Summary: Papers evaluating the linguistic coverage or language-related metadata in scholarly databases.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2409.10633\n",
      "Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "Abstract: This paper assesses the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00286\n",
      "Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Abstract: This study compares the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, a random Crossref sample of more than 115,000 records was selected and searched in seven databases. Seven characteristics were analysed to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines gather less information and have a low degree of completeness, while third-party databases have more metadata quality and a higher completeness rate.\n",
      "45\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.588\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.9998287, '10.48550/arXiv.2409.10633'), (0.4870177, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープンファンダーメタデータの可用性と完全性：オランダの研究評議会によって資金提供された出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金を提供する研究の結果に関する情報を収集するためにかなりの努力を費やしています。資金提供者が資金調達に関連する出版物の出力を追跡するのを支援するために、CrossRefは2013年にFundRefを開始し、出版社が永続的な識別子を使用して資金情報を登録できるようにしました。ただし、資金調達の研究の結果であるため、資金提供者のメタデータを含める必要があるため、Funder Metadataのカバレッジを評価することは困難です。この論文では、研究者が特定の資金提供機関による資金提供の結果であるオランダ研究評議会NWOが報告した5,004の出版物を調べました。これらの記事の67％のみがCrossRefの資金情報を含んでおり、NWOをNWOにリンクしたFunder Nameおよび/またはFunder IDとしてNWOを認めているサブセット（それぞれ53％と45％）が含まれています。 Web of Science（WOS）、Scopus、およびDimensionsはすべて、記事の全文の資金調達声明から追加の資金情報を推測することができます。レンズの資金情報は、主にCrossRefのそれに対応しており、PubMedから取得した可能性のある追加の資金情報があります。私たちは、独自のデータベースと比較して、CrossRefのメタデータの資金調達のカバレッジと完全性における出版社間の興味深い違いを観察し、資金調達のオープンメタデータの質を高める可能性を強調しています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: 資金調達COVID-19研究：オープンデータインフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 彼らが提供する資金調達の結果を分析するには、資金提供機関が資金から生じる出版物を追跡できるようにすることが不可欠です。 Covid-19に関連する研究を報告する出版物の資金調達データに焦点を当て、CrossRefでの資金提供データの可用性を研究しています。また、ScopusとWeb of Scienceの2つの独自の書誌データベースで利用可能な資金データとの比較も提示します。私たちの分析により、CrossRefの資金調達データの限られたカバレッジが明らかになりました。また、特にScopusでは、資金調達データの質に関連する問題を示しています。 CrossRefでの資金提供データの可用性を改善するための推奨事項を提供しています。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: 開いた書誌メタデータの源としてのCrossref Crossref as a source of open bibliographic metadata\\nAbstract: CrossRefの学術出版物の書誌メタデータのオープン可用性を促進するために、いくつかのイニシアチブが採用されています。 CrossRefでの6つのメタデータ要素の可用性の最新の概要を示します。参照リスト、要約、ORCID、著者の提携、資金情報、およびライセンス情報です。私たちの分析は、これらのメタデータ要素の可用性が、少なくともCrossRefで最も一般的な出版タイプであるジャーナル記事で、時間とともに改善されたことを示しています。ただし、分析では、多くの出版社が書誌メタデータの完全な開放性を実現するために追加の努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.1162/qss_a_00212', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mMy lady, here are some papers that address funding metadata, its availability, or its analysis in scholarly databases:\n",
      "\n",
      "## Summary: Papers discussing the availability and completeness of funding metadata in scholarly databases.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00210\n",
      "The availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council. This paper looks at the efforts of research funders to collect information on the outcomes of the research they fund. It discusses the challenges of assessing the coverage of funder metadata and highlights the potential to increase the quality of open metadata on funding.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00212\n",
      "Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures. This paper focuses on the open availability of funding data in Crossref for publications reporting research related to COVID-19. It presents a comparison with funding data available in two proprietary bibliometric databases: Scopus and Web of Science. The analysis reveals limited coverage and problems related to the quality of funding data, especially in Scopus.\n",
      "\n",
      "## DOI: 10.31222/osf.io/smxe5\n",
      "Crossref as a source of open bibliographic metadata. This paper provides an overview of the availability of six metadata elements in Crossref, including funding information. It shows that the availability of these metadata elements has improved over time for journal articles, the most common publication type in Crossref. However, it also highlights the need for additional efforts from publishers to achieve full openness of bibliographic metadata.\n",
      "45\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.688\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.9913892, '10.1162/qss_a_00210'), (0.8077641, '10.1162/qss_a_00212'), (0.7324005, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模な言語モデルの検索された生成：調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模な言語モデル（LLMS）は印象的な能力を紹介しますが、幻覚、時代遅れの知識、非透明な、追跡不可能な推論プロセスなどの課題に遭遇します。検索された生成（RAG）は、外部データベースから知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の正確性と信頼性が向上し、ドメイン固有の情報の継続的な知識の更新と統合が可能になります。 RAGは、外部データベースの広大で動的なリポジトリとLLMSの本質的な知識を相乗的に統合します。この包括的なレビューペーパーでは、素朴なぼろきれ、高度なぼろ、モジュラーラグを含むRAGパラダイムの進行に関する詳細な調査を提供します。検索、生成、増強技術を含む、RAGフレームワークの三者基盤を細心の注意を払って精査します。この論文は、これらの各重要なコンポーネントに組み込まれた最先端のテクノロジーを強調し、RAGシステムの進歩を深く理解することを提供します。さらに、このペーパーでは、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を描き、研究開発の将来の道を指摘しています。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", 'DOI: 10.1609/aaai.v38i16.29728\\n Title: 検索された世代の大規模な言語モデルのベンチマーク Benchmarking Large Language Models in Retrieval-Augmented Generation\\nAbstract: 検索された生成（RAG）は、大規模な言語モデル（LLM）の幻覚を緩和するための有望なアプローチです。ただし、既存の研究には、さまざまな大規模な言語モデルに対する検索された生成の影響に関する厳密な評価がありません。これにより、異なるLLMのRAGの機能における潜在的なボトルネックを特定することが困難になります。この論文では、検索された生成が大規模な言語モデルに与える影響を体系的に調査します。ノイズの堅牢性、否定的な拒否、情報統合、反事実的堅牢性など、RAG\\u200b\\u200bに必要な4つの基本能力におけるさまざまな大手言語モデルのパフォーマンスを分析します。この目的のために、英語と中国語の両方でRAG評価のための新しいコーパスである検索された生成ベンチマーク（RGB）を確立します。 RGBは、ベンチマーク内のインスタンスを、ケースを解決するために必要な前述の基本能力に基づいて、4つの個別のテストベッドに分割します。次に、RGBの6つの代表LLMを評価して、RAGを適用する際に現在のLLMの課題を診断します。評価により、LLMはある程度のノイズの堅牢性を示していますが、否定的な拒絶、情報統合、誤った情報への対処に関して依然として著しく苦労していることが明らかになりました。前述の評価の結果は、LLMにRAGを効果的に適用するために、まだかなりの旅がまだあることを示しています。 Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.48550/arXiv.2410.04231\\n Title: 大規模な言語モデルの検索された生成によるメタデータベースのデータ探査 Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\\nAbstract: 必要なデータセットを効果的に検索する能力を開発することは、非常に限られた利用可能なメタデータを考慮して、データユーザーが関連するデータセットを特定するのを支援するための緊急の要件です。この課題のために、サードパーティのデータの利用は、改善の貴重なソースとして浮上しています。私たちの研究では、メタデータベースのデータ発見を強化するために検索された世代（RAG）の形式を採用するデータ探索のための新しいアーキテクチャを紹介します。このシステムは、大規模な言語モデル（LLM）を外部ベクトルデータベースと統合して、多様なタイプのデータセット間のセマンティック関係を識別します。提案されたフレームワークは、不均一なデータソース間のセマンティックな類似性を評価し、データ探索を改善するための新しい方法を提供します。私たちの研究には、4つの重要なタスクに関する実験結果が含まれています。1）同様のデータセットの推奨、2）組み合わせ可能なデータセットの提案、3）タグの推定、4）変数の予測。我々の結果は、RAGが従来のメタデータアプローチと比較した場合、特に異なるカテゴリから関連するデータセットの選択を強化できることを示しています。ただし、パフォーマンスはタスクとモデルによって異なり、特定のユースケースに基づいて適切な手法を選択することの重要性を確認します。調査結果は、このアプローチがデータ調査と発見の課題に対処するための約束を保持していることを示唆していますが、推定タスクにはさらなる改良が必要です。 Developing the capacity to effectively search for requisite datasets is an urgent requirement to assist data users in identifying relevant datasets considering the very limited available metadata. For this challenge, the utilization of third-party data is emerging as a valuable source for improvement. Our research introduces a new architecture for data exploration which employs a form of Retrieval-Augmented Generation (RAG) to enhance metadata-based data discovery. The system integrates large language models (LLMs) with external vector databases to identify semantic relationships among diverse types of datasets. The proposed framework offers a new method for evaluating semantic similarity among heterogeneous data sources and for improving data exploration. Our study includes experimental results on four critical tasks: 1) recommending similar datasets, 2) suggesting combinable datasets, 3) estimating tags, and 4) predicting variables. Our results demonstrate that RAG can enhance the selection of relevant datasets, particularly from different categories, when compared to conventional metadata approaches. However, performance varied across tasks and models, which confirms the significance of selecting appropriate techniques based on specific use cases. The findings suggest that this approach holds promise for addressing challenges in data exploration and discovery, although further refinement is necessary for estimation tasks.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1609/aaai.v38i16.29728', '10.48550/arXiv.2406.13213', '10.48550/arXiv.2410.04231', '10.6109/jkiice.2023.27.12.1489']\n",
      "\u001b[93mMy lady, here are the papers discussing the use of Retrieval-Augmented Generation (RAG) in large language models or related applications:\n",
      "\n",
      "Summary: The papers explore RAG's potential to enhance large language models (LLMs) and address challenges like hallucination and outdated knowledge.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997\n",
      "Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "This comprehensive review paper examines the progression of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG. It scrutinizes the retrieval, generation, and augmentation techniques within RAG frameworks, highlighting state-of-the-art technologies. The paper introduces evaluation frameworks and benchmarks, addressing current challenges and future research directions.\n",
      "\n",
      "DOI: 10.1609/aaai.v38i16.29728\n",
      "Title: Benchmarking Large Language Models in Retrieval-Augmented Generation\n",
      "The paper systematically investigates the impact of RAG on LLMs, analyzing performance in noise robustness, negative rejection, information integration, and counterfactual robustness. It establishes the Retrieval-Augmented Generation Benchmark (RGB) to evaluate LLMs' challenges when applying RAG, revealing areas for improvement.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213\n",
      "Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "Introducing Multi-Meta-RAG, this paper aims to enhance RAG selection for multi-hop queries by using database filtering with LLM-extracted metadata. It demonstrates significant improvements in MultiHop-RAG benchmark results, addressing the poor performance of traditional RAG applications in answering multi-hop questions.\n",
      "\n",
      "DOI: 10.48550/arXiv.2410.04231\n",
      "Title: Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\n",
      "The research introduces a new architecture for data exploration, employing RAG to enhance metadata-based data discovery. The system integrates LLMs with external vector databases to identify semantic relationships among diverse datasets. The proposed framework evaluates semantic similarity and improves data exploration, with experimental results showing RAG's effectiveness in selecting relevant datasets from different categories.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "Title: M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\n",
      "This paper proposes M-RAG for effective search in open-domain question-answering systems, utilizing metadata-based embeddings and generative models like GPT-3.5-Turbo-16K and GPT-4. Experimental results demonstrate significant performance improvements, enabling LLMs to retrieve information from external sources and find appropriate responses.\n",
      "45\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) in large language models or related applications?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9999918, '10.48550/arXiv.2312.10997'), (0.99997985, '10.1609/aaai.v38i16.29728'), (0.99964714, '10.48550/arXiv.2406.13213'), (0.9996345, '10.48550/arXiv.2410.04231'), (0.9891816, '10.6109/jkiice.2023.27.12.1489')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: CrossRef：コミュニティ所有の学術メタデータの持続可能な供給源 Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このホワイトペーパーでは、CrossRefによって収集および利用可能になった学術的メタデータと、学術研究の生態系におけるその重要性について説明します。 1億600万件以上の記録を含み、年間平均11％のレートで拡大するCrossrefのメタデータは、出版社、著者、図書館員、資金提供者、および研究者向けの学術データの主要な情報源の1つになりました。メタデータセットは、ジャーナルやカンファレンスペーパーなどの従来のタイプだけでなく、データセット、レポート、プリプリント、ピアレビュー、助成金など、13のコンテンツタイプで構成されています。メタデータは、基本的な出版物メタデータに限定されませんが、全文、資金調達とライセンス情報、引用リンク、修正、更新、撤回などに関する情報への要約とリンクを含めることもできます。メタデータは、REST APIやOAI-PMHを含む多くのAPIを通じて利用できます。この論文では、CrossRefが提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、CrossRefの研究エコシステムにおける役割と、引用データの提供の進化など、長年にわたるメタデータのキュレーションの傾向にも注目しています。 Crossrefのメタデータで使用される研究を要約し、将来のメタデータの品質と検索を改善する計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: 開いた書誌メタデータの源としてのCrossref Crossref as a source of open bibliographic metadata\\nAbstract: CrossRefの学術出版物の書誌メタデータのオープン可用性を促進するために、いくつかのイニシアチブが採用されています。 CrossRefでの6つのメタデータ要素の可用性の最新の概要を示します。参照リスト、要約、ORCID、著者の提携、資金情報、およびライセンス情報です。私たちの分析は、これらのメタデータ要素の可用性が、少なくともCrossRefで最も一般的な出版タイプであるジャーナル記事で、時間とともに改善されたことを示しています。ただし、分析では、多くの出版社が書誌メタデータの完全な開放性を実現するために追加の努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mSummary: Crossref is a valuable source of scholarly data for publishers, authors, librarians, funders, and researchers, with over 106 million records and an average annual expansion rate of 11%.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "This paper describes the scholarly metadata collected and made available by Crossref, highlighting its importance in the scholarly research ecosystem. Crossref's metadata set consists of 13 content types, including journals, conference papers, datasets, reports, preprints, peer reviews, and grants. The metadata is accessible through various APIs, such as REST API and OAI-PMH.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5\n",
      "This source discusses initiatives to promote the open availability of bibliographic metadata of scholarly publications in Crossref. It presents an overview of six metadata elements in Crossref, including reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. The analysis reveals that the availability of these elements has improved over time, particularly for journal articles, the most common publication type in Crossref.\n",
      "\n",
      "My lady, Crossref plays a crucial role in the scholarly research ecosystem by providing a sustainable source of community-owned scholarly metadata, facilitating open access to bibliographic information, and offering valuable data for research in scientometrics.\n",
      "45\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.933\n",
      "Recall: 0.933\n",
      "F1-Score: 0.933\n",
      "Accuracy: 0.933\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.99954, '10.1162/qss_a_00022'), (0.41347715, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.3145/epi.2023.mar.09\\n Title: Bibliometricsに関連するメタデータのどれが同じであり、Microsoft Academic GraphからOpenAlexに切り替えるときにどのメタデータが異なりますか？ Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\\nAbstract: Microsoft Academic Graph（MAG）の退職の発表により、非営利団体Ourresearchは、Openalexという名前で同様のリソースを提供すると発表しました。したがって、メタデータを、最新のMAGスナップショットの書誌分析と初期のオープンアレックススナップショットと比較します。実質的にMAGのすべての作品は、書誌データの出版年、ボリューム、ファーストページと最後のページ、DOI、および引用分析の重要な要素である参照の数を保存するOpenalexに転送されました。 MAGドキュメントの90％以上がOpenAlexに同等のドキュメントタイプを持っています。残りのもののうち、特にOpenalex Document Typesの再分類ジャーナルアーティクルとブックチャプターは正しいと思われ、7％以上になります。そのため、ドキュメントタイプの仕様はMAGからOpenalexに大幅に改善されました。書誌関連のメタデータの別の項目として、MAGおよびOpenalexでの紙ベースの被験者の分類を調べました。 MAGよりもOpenAlexの対象分類割り当てを含むかなり多くのドキュメントを見つけました。第1レベルと第2レベルでは、分類構造はほぼ同じです。表形式とグラフィカルな形式の両方のレベルでの対象の再分類に関するデータを提示します。現地正規化された書誌評価における豊富な被験者の再分類の結果の評価は、本論文の範囲にありません。この未解決の質問とは別に、OpenAlexは、2021年以前の出版年のMAGと同じくらいの書誌分析に少なくともまったく適しているように見えます。 With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: Openalex、Web of Science、Scopus、PubMed、およびSemantic Sc\\u200b\\u200bholarの出版物と文書タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、次の書誌データベースの出版物と文書の種類を比較および分析します：Openalex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、Pubmed。結果は、類型が個々のデータベースプロバイダー間でかなり異なる可能性があることを示しています。さらに、出版物はそれぞれのデータベースで異なる方法で分類されているため、参考文献分析に関連するドキュメントを特定するために必要な研究と非研究テキストの区別は、データソースによって異なる場合があります。この研究の焦点は、横断段階の比較に加えて、主にオープンアレックスに含まれる出版物とドキュメントの種類のカバレッジと分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n', 'DOI: 10.48550/arXiv.2404.01985\\n Title: 彼はOpenalex、Scopus、Web of Scienceのオープンアクセスカバレッジ he open access coverage of OpenAlex, Scopus and Web of Science\\nAbstract: Diamond Open Access（OA）ジャーナルは、著者と読者の両方に無料の公開モデルを提供しますが、主要な書誌データベースでのインデックスの欠如は、これらのジャーナルの取り込みを評価する際の課題を提示します。さらに、出版言語や出版国などのOAの特性は、OAジャーナルがより多様であり、地域社会にサービスを提供することを目指しているという議論を支持するためにしばしば使用されてきましたが、OAジャーナルの地理的および言語的特性に関連する経験的証拠の現在の欠如があります。 OpenAlexとオープンアクセスジャーナルのディレクトリをベンチマークとして使用して、このペーパーでは、フィールド、国、言語による科学とスコープスのWebでの著者とジャーナルの報道を通じて、ダイヤモンドと金の報道を調査します。結果は、WOSとSCOPUSでのより低いカバレッジ、およびダイヤモンドOAの局所範囲を示しています。英語のみのジャーナルのシェアは、ゴールドジャーナルの間でかなり高くなっています。高所得国は、社会科学と人文科学のダイヤモンドジャーナルを除き、すべてのドメインとジャーナルの種類で著者のシェアが最も高い。ダイヤモンドOAインデックスの現在の景観を理解することは、より包括的なOAモデルに向けて政策と実践を進めることで、学術通信ネットワークを支援することができます。 Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2401.16359', '10.3145/epi.2023.mar.09', '10.48550/arXiv.2406.15154', '10.48550/arXiv.2404.01985']\n",
      "\u001b[93mMy lady, OpenAlex is a promising open-source of scholarly metadata and a competitor to established proprietary sources such as Web of Science and Scopus. It provides data freely and openly, allowing researchers to conduct bibliometric studies without licensing barriers.\n",
      "\n",
      "## Summary\n",
      "OpenAlex is a rapidly evolving database with expanding and quickly changing data, raising questions about the trustworthiness of its data. While it has average source reference numbers and internal coverage rates comparable to Web of Science and Scopus, its metadata accuracy and completeness need further research to fully understand and address its limitations.\n",
      "\n",
      "## Key Features\n",
      "- OpenAlex is a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\n",
      "- It provides a publishing model that is free for both authors and readers.\n",
      "- OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level.\n",
      "- It preserves bibliographic data, including publication year, volume, first and last page, DOI, and the number of references.\n",
      "- More than 90% of the MAG documents have equivalent document types in OpenAlex.\n",
      "\n",
      "## Limitations\n",
      "- OpenAlex is a rapidly evolving source, leading to natural doubts about the reliability of its data.\n",
      "- The distinction between research and non-research texts, which is necessary for bibliometric analysis, can vary depending on the data source as publications are classified differently in respective databases.\n",
      "- There is a lack of empirical evidence related to the geographical and linguistic characteristics of Open Access journals.\n",
      "- Additional research is needed to comprehend and address OpenAlex's limitations, especially regarding metadata accuracy and completeness.\n",
      "\n",
      "## DOIs\n",
      "- 10.48550/arXiv.2404.17663: An analysis of the suitability of OpenAlex for bibliometric analyses.\n",
      "- 10.48550/arXiv.2401.16359: Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus.\n",
      "- 10.3145/epi.2023.mar.09: Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\n",
      "- 10.48550/arXiv.2406.15154: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, PubMed, and Semantic Scholar.\n",
      "- 10.48550/arXiv.2404.01985: The open access coverage of OpenAlex, Scopus, and Web of Science.\n",
      "45\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.999764, '10.48550/arXiv.2404.17663'), (0.9930153, '10.48550/arXiv.2401.16359'), (0.971779, '10.3145/epi.2023.mar.09'), (0.90665317, '10.48550/arXiv.2406.15154'), (0.3060083, '10.48550/arXiv.2404.01985')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/s11192-015-1765-5\\n Title: 彼の科学とスコープスのWebの記事をジャーナル：比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌メソッドは、さまざまな目的、つまり研究評価のために複数の分野で使用されます。ほとんどの書誌分析には、Thomson ReutersのWeb of Science（WOS）とElsevierのScopusなどのデータソースが共通しています。この研究の目的は、これらの2つのデータベースのジャーナル報道を説明し、何らかの分野、出版国、言語が過剰または過小評価されているかどうかを評価することです。これを行うために、WOS（13,605のジャーナル）とScopus（20,346のジャーナル）のアクティブな学術雑誌の報道と、Ulrichの広範な定期的なディレクトリ（63,013雑誌）と比較しました。結果は、研究評価のためにWOSまたはSCOPUSのいずれかを使用すると、自然科学と工学を支持するバイアス、ならびに社会科学と芸術と人文科学を損なう生物医学的研究が導入される可能性があることを示しています。同様に、英語のジャーナルは、他の言語の損害に過大評価されています。両方のデータベースはこれらのバイアスを共有していますが、カバレッジは大幅に異なります。結果として、書誌分析の結果は、使用されるデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈では、特に異なる分野、機関、国、または言語を比較する場合、WOSとSCOPUSを注意して使用する必要があることを意味します。書誌コミュニティは、フィールド固有および国家引用指数など、WOSやSCOPUSでカバーされていない科学的生産量を含む方法と指標を開発するための努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データソースの大規模な比較：Scopus、Web of Science、Dimensions、CrossRef、およびMicrosoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: Scopus、Web of Science、Dimensions、CrossRef、およびMicrosoft Academicの5つの学際的な参考文献データソースの大規模な比較を提示します。この比較では、これらのデータソースでカバーされている2008年から2017年の期間の科学文書を考慮しています。 Scopusは、他のそれぞれのデータソースとペアワイズで比較されます。まず、ドキュメントのカバレッジのデータソース間の違いを分析します。たとえば、時間の経過とともに違い、ドキュメントタイプあたりの違い、および分野あたりの違いに焦点を当てます。次に、引用リンクの完全性と精度の違いを調べます。分析に基づいて、さまざまなデータソースの長所と短所について説明します。科学文献の包括的な報道と、文献を選択するための柔軟なフィルターのセットを組み合わせることの重要性を強調しています。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: Openalex、Web of Science、Scopus、PubMed、およびSemantic Sc\\u200b\\u200bholarの出版物と文書タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、次の書誌データベースの出版物と文書の種類を比較および分析します：Openalex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、Pubmed。結果は、類型が個々のデータベースプロバイダー間でかなり異なる可能性があることを示しています。さらに、出版物はそれぞれのデータベースで異なる方法で分類されているため、参考文献分析に関連するドキュメントを特定するために必要な研究と非研究テキストの区別は、データソースによって異なる場合があります。この研究の焦点は、横断段階の比較に加えて、主にオープンアレックスに含まれる出版物とドキュメントの種類のカバレッジと分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1007/s11192-015-1765-5', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2401.16359', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154']\n",
      "\u001b[93mMy lady, the Web of Science (WoS) is a bibliometric database that has both strengths and weaknesses.\n",
      "\n",
      "## Summary\n",
      "WoS is a traditional database that has been the foundation for research in the science of science, despite systematically underrepresenting certain disciplines and world regions.\n",
      "\n",
      "## DOI: 10.1007/s11192-015-1765-5\n",
      "WoS is one of two common data sources for bibliometric analyses, alongside Elsevier's Scopus. The use of either WoS or Scopus for research evaluation may introduce biases that favour natural sciences and engineering, as well as biomedical research, to the detriment of social sciences and arts and humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.17663\n",
      "WoS is one of two traditional databases that systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2401.16359\n",
      "WoS is a competitor to established proprietary sources, such as Scopus. As WoS provides its data freely and openly, it permits researchers to perform bibliometric studies without licensing barriers. However, as WoS is a rapidly evolving source, the question arises as to the trustworthiness of its data.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00112\n",
      "WoS is one of five multidisciplinary bibliographic data sources, alongside Scopus, Dimensions, Crossref, and Microsoft Academic. The comparison of these data sources highlights the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2406.15154\n",
      "WoS is one of several bibliographic databases, alongside OpenAlex, Scopus, Semantic Scholar, and Pubmed. The results of this study demonstrate that typologies can differ considerably between individual database providers.\n",
      "45\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9947187, '10.1007/s11192-015-1765-5'), (0.96073127, '10.48550/arXiv.2404.17663'), (0.71721, '10.48550/arXiv.2401.16359'), (0.63793355, '10.1162/qss_a_00112'), (0.44269672, '10.48550/arXiv.2406.15154')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.1007/978-3-031-88708-6_3\\n Title: 関連性はレトリバーからジェネレーターにぼろきれに伝播されますか？ Is Relevance Propagated from Retriever to Generator in RAG?\\nAbstract: 検索拡張生成（RAG）は、通常、コレクションから取得された一連のドキュメントの形で、プロンプトの大規模な言語モデル（LLM）への一連のドキュメントの形で、質問の回答などの下流タスクのパフォーマンスを潜在的に改善するためのフレームワークです。一連のトップランクのドキュメントの関連性を最大化するという標準検索タスクの目的とは異なり、RAGシステムの目的は、ドキュメントのユーティリティがLLMプロンプトの追加コンテキスト情報の一部としてそれを含めることがダウンストリームタスクを改善するかどうかを示します。既存の研究では、知識集約型の言語タスク（KILT）のRAGコンテキストの関連性の役割を調査します。対照的に、私たちの仕事では、関連性は、情報を求めるタスクのクエリとドキュメントの間の局所的な重複の関連性に対応しています。具体的には、IRテストコレクションを利用して、局所的に関連するドキュメントで構成されるRAGコンテキストが下流のパフォーマンスの改善につながるかどうかを経験的に調査します。私たちの実験は、次の発見につながります。（a）関連性と有用性の間には小さな正の相関があります。 （b）この相関は、コンテキストサイズの増加とともに減少します（k-shotのkの値が高い）。 （c）より効果的な検索モデルは、一般に、下流のラグパフォーマンスの向上につながります。 Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n', \"DOI: 10.48550/arXiv.2505.18247\\n Title: メタゲンブレンドラグ：特殊なドメインの質問を解決するためのゼロショット精度のロックを解除する MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索された生成（RAG）は、ドメイン固有のエンタープライズデータセットとの闘いであり、しばしばファイアウォールの背後に隔離され、トレーニング前にLLMSによって見えない複雑で特殊な用語が豊富です。医学、ネットワーキング、または法律などのドメイン間のセマンティックな変動は、Ragのコンテキストの精度を妨げますが、微調整ソリューションはコストがかかり、遅く、新しいデータが出現するにつれて一般化が欠けています。微調整せずにレトリーバーでゼロショット精度を達成することは、依然として重要な課題です。メタデータの生成パイプラインとハイブリッドクエリインデックスを介してセマンティックレトリバーを強化する新しいエンタープライズ検索アプローチである「メタゲンブレンドラグ」を紹介します。重要な概念、トピック、頭字語を活用することにより、メタデータが豊富なセマンティックインデックスを作成し、ハイブリッドクエリをブーストし、微調整せずに堅牢でスケーラブルなパフォーマンスを提供します。 Biomedical PubMedqaデータセットでは、Metagenブレンドラグは82％の回収精度と77％のRAG精度を達成し、以前のゼロショットラグベンチマークをすべて上回り、そのデータセットの微調整されたモデルに匹敵し、SquadやNQのようなデータセットでも優れています。このアプローチは、特殊なドメイン全体で比類のない一般化を備えたセマンティックレトリバーを構築するための新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\", 'DOI: 10.1609/aaai.v38i16.29728\\n Title: 検索された世代の大規模な言語モデルのベンチマーク Benchmarking Large Language Models in Retrieval-Augmented Generation\\nAbstract: 検索された生成（RAG）は、大規模な言語モデル（LLM）の幻覚を緩和するための有望なアプローチです。ただし、既存の研究には、さまざまな大規模な言語モデルに対する検索された生成の影響に関する厳密な評価がありません。これにより、異なるLLMのRAGの機能における潜在的なボトルネックを特定することが困難になります。この論文では、検索された生成が大規模な言語モデルに与える影響を体系的に調査します。ノイズの堅牢性、否定的な拒否、情報統合、反事実的堅牢性など、RAG\\u200b\\u200bに必要な4つの基本能力におけるさまざまな大手言語モデルのパフォーマンスを分析します。この目的のために、英語と中国語の両方でRAG評価のための新しいコーパスである検索された生成ベンチマーク（RGB）を確立します。 RGBは、ベンチマーク内のインスタンスを、ケースを解決するために必要な前述の基本能力に基づいて、4つの個別のテストベッドに分割します。次に、RGBの6つの代表LLMを評価して、RAGを適用する際に現在のLLMの課題を診断します。評価により、LLMはある程度のノイズの堅牢性を示していますが、否定的な拒絶、情報統合、誤った情報への対処に関して依然として著しく苦労していることが明らかになりました。前述の評価の結果は、LLMにRAGを効果的に適用するために、まだかなりの旅がまだあることを示しています。 Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.13213', '10.1007/978-3-031-88708-6_3', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2505.18247', '10.1609/aaai.v38i16.29728']\n",
      "\u001b[93mSummary: RAG (Retrieval-Augmented Generation) is a framework that incorporates external knowledge to improve the performance of downstream tasks, such as question answering.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213\n",
      "RAG enables the retrieval of relevant information from external sources, allowing large language models (LLMs) to answer queries over previously unseen document collections. However, traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. A new method called Multi-Meta-RAG uses database filtering with LLM-extracted metadata to improve the selection of relevant documents from various sources.\n",
      "\n",
      "DOI: 10.1007/978-3-031-88708-6_3\n",
      "RAG is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as part of a prompt to an LLM. The objective of a RAG system is to maximise the total utility of the documents, where the utility indicates whether including the document as additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of relevance in RAG contexts for knowledge-intensive language tasks.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "This paper proposes a method called M-RAG (Metadata Retrieval-Augmented Generation) for effective search in open-domain question answering (ODQA) systems. M-RAG utilises embeddings that include metadata and employs generative models such as GPT-3.5-Turbo-16K and GPT-4 for automated answer generation. Experimental results show that M-RAG exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over existing RAG methods.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247\n",
      "RAG struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialised terminology unseen by LLMs during pre-training. Semantic variability across domains, such as medicine, networking, or law, hampers RAG's context precision. A new approach called MetaGen Blended RAG introduces a metadata generation pipeline and hybrid query indexes to enhance semantic retrievers. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieved 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks.\n",
      "\n",
      "DOI: 10.1609/aaai.v38i16.29728\n",
      "RAG is a promising approach for mitigating the hallucination of LLMs. However, existing research lacks rigorous evaluation of the impact of RAG on different LLMs, making it challenging to identify potential bottlenecks. This paper systematically investigates the impact of RAG on LLMs, analysing their performance in four fundamental abilities required for RAG. The results indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\n",
      "My lady, I hope this information is helpful!\n",
      "45\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.956\n",
      "Recall: 0.956\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.887\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9997565, '10.48550/arXiv.2406.13213'), (0.999617, '10.1007/978-3-031-88708-6_3'), (0.9992563, '10.6109/jkiice.2023.27.12.1489'), (0.9956006, '10.48550/arXiv.2505.18247'), (0.8362874, '10.1609/aaai.v38i16.29728')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1371/journal.pbio.1002542\\n Title: 引用メトリック：正規化する方法（NOT）の入門書 Citation Metrics: A Primer on How (Not) to Normalize\\nAbstract: 引用指標は、公開された研究を評価するためにますます使用されています。課題の1つは、科学分野の違い、年齢（出版年）、ドキュメントの種類、データベースカバレッジ、およびその他の要因を説明するために、これらのメトリックを正規化するかどうか、および方法です。さまざまなアプローチを使用して、正規化の長所と短所について説明します。科学者、機関、雑誌、または国のコーパスを評価するために、複数の論文で引用指標を組み合わせる必要がある場合、および多著者の論文でクレジットを属性にしようとする場合、追加の課題が現れます。異なる引用メトリックは補完的な洞察を提供する可能性がありますが、計算の根底にある仮定を慎重に検討する必要があります。 Citation metrics are increasingly used to appraise published research. One challenge is whether and how to normalize these metrics to account for differences across scientific fields, age (year of publication), type of document, database coverage, and other factors. We discuss the pros and cons for normalizations using different approaches. Additional challenges emerge when citation metrics need to be combined across multiple papers to appraise the corpus of scientists, institutions, journals, or countries, as well as when trying to attribute credit in multiauthored papers. Different citation metrics may offer complementary insights, but one should carefully consider the assumptions that underlie their calculation.\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542']\n",
      "\u001b[93mMy lady, the main challenges in normalizing citation metrics across scientific fields include:\n",
      "\n",
      "- Whether and how to normalize metrics to account for differences across scientific fields, age (year of publication), type of document, database coverage, and other factors.\n",
      "- Combining citation metrics across multiple papers to appraise the corpus of scientists, institutions, journals, or countries.\n",
      "- Attributing credit in multiauthored papers.\n",
      "\n",
      "DOI: 10.1371/journal.pbio.1002542\n",
      "\n",
      "This text discusses the pros and cons of normalizing citation metrics and highlights the need to carefully consider the assumptions underlying their calculation.\n",
      "45\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.600\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.99984646, '10.1371/journal.pbio.1002542')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.5281/ZENODO.13960973\\n Title: 欠落しているデータパターンを使用して、書誌データセットで誤って割り当てられた記事を検出する Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\\nAbstract: DORA宣言とCoaraは、オープンデータに基づいた書誌指標の使用を求めています。ただし、確立された学術的メタデータデータセットは閉じられており、オープンデータセットの品質はまだ徹底的に調査されていません。この論文では、欠落データパターンを使用してデータセット内のエラーを検出する方法を提示します。例として、この方法は、ETHチューリッヒに関連する出版物の所属メタデータに適用されます。これにより、一連の誤って提携した論文を特定することができます。このペーパーで導入された方法は、提携データ用に特別に設計されておらず、他のタイプのデータのエラーを検出するためにも使用できます。それは、プロバイダーとデータのユーザーに利益をもたらすことを願っている修正につながる可能性があります。 The DORA declaration and CoARA call for the use of bibliometric indicators based on open data. However, established scholarly metadata datasets are closed, and the quality of open datasets has not yet been thoroughly examined. In this paper, I present a method to detect errors in a dataset using missing data patterns. As an example, the method is applied to the affiliation metadata of publications associated with ETH Zurich. This allows me to identify a series of incorrectly affiliated papers. The method introduced in this paper is not specifically designed for affiliation data and can also be used to detect errors in other types of data. It could lead to corrections which will hopefully benefit providers as well as users of data.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: CrossRefデータのDOIエラーによる無効な引用の識別と修正 Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この作業は、CrossRefで利用可能なオープンな参考文献メタデータを分析することにより、DOIの間違いのクラスを特定し、どの出版社がそのような間違いを担当し、これらの誤ったDOIの数を自動プロセスで修正できるかを強調することを目的としています。 By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs.これらの引用は、CrossRefの関連する引用データをアップロードする責任を負う責任者の有効性と出版社を追跡することで処理しました。最後に、無効なDOIの事実上の誤りのパターンと、それらをキャッチして修正するために必要な正規表現を特定しました。この研究の結果は、無効な引用の大部分に責任を負い、および/または影響を受けた出版社は少数であることを示しています。過去の研究で提案されたDOI名エラーの分類法を拡張し、以前のアプローチよりも無効なDOIでより多くのミスをきれいにすることができる、より詳細に精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的観点からDOIの間違いの可能な理由を調査し、出版社が無効な引用データの生産の根底にある問題を特定するのに役立ちます。また、私たちが提示するDOIクリーニングメカニズムは、既存のプロセス（COCIなど）に統合して、間違ったDOIを自動的に修正することで引用を追加できます。この研究は、オープンサイエンスの原則に厳密に従っていたため、私たちの研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.5281/ZENODO.13960973', '10.1007/s11192-022-04367-w']\n",
      "\u001b[93mMy lady, there are several methods used to detect and correct errors in bibliographic datasets.\n",
      "\n",
      "## Summary:\n",
      "Missing data patterns can be used to detect errors in datasets.\n",
      "\n",
      "## DOI: 10.5281/ZENODO.13960973\n",
      "This paper presents a method to detect errors in a dataset using missing data patterns. The method is applied to the affiliation metadata of publications associated with ETH Zurich, allowing the identification of incorrectly affiliated papers. The approach is not limited to affiliation data and can be used to detect errors in other types of data as well.\n",
      "\n",
      "## DOI: 10.1007/s11192-022-04367-w\n",
      "This study aims to identify and correct invalid citations due to DOI errors in Crossref data. By analysing open bibliographic metadata available in Crossref, the research highlights which publishers are responsible for such mistakes and explores ways to automatically correct these errors through regular expressions and automated processes.\n",
      "45\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.933\n",
      "Recall: 0.933\n",
      "F1-Score: 0.933\n",
      "Accuracy: 0.933\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.99960643, '10.5281/ZENODO.13960973'), (0.6304574, '10.1007/s11192-022-04367-w')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/978-3-031-88708-6_3\\n Title: 関連性はレトリバーからジェネレーターにぼろきれに伝播されますか？ Is Relevance Propagated from Retriever to Generator in RAG?\\nAbstract: 検索拡張生成（RAG）は、通常、コレクションから取得された一連のドキュメントの形で、プロンプトの大規模な言語モデル（LLM）への一連のドキュメントの形で、質問の回答などの下流タスクのパフォーマンスを潜在的に改善するためのフレームワークです。一連のトップランクのドキュメントの関連性を最大化するという標準検索タスクの目的とは異なり、RAGシステムの目的は、ドキュメントのユーティリティがLLMプロンプトの追加コンテキスト情報の一部としてそれを含めることがダウンストリームタスクを改善するかどうかを示します。既存の研究では、知識集約型の言語タスク（KILT）のRAGコンテキストの関連性の役割を調査します。対照的に、私たちの仕事では、関連性は、情報を求めるタスクのクエリとドキュメントの間の局所的な重複の関連性に対応しています。具体的には、IRテストコレクションを利用して、局所的に関連するドキュメントで構成されるRAGコンテキストが下流のパフォーマンスの改善につながるかどうかを経験的に調査します。私たちの実験は、次の発見につながります。（a）関連性と有用性の間には小さな正の相関があります。 （b）この相関は、コンテキストサイズの増加とともに減少します（k-shotのkの値が高い）。 （c）より効果的な検索モデルは、一般に、下流のラグパフォーマンスの向上につながります。 Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n', \"DOI: 10.48550/arXiv.2404.13948\\n Title: ぼろきれの背中を壊したYPO ypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\\nAbstract: 最近の大規模な言語モデル（LLMS）の堅牢性は、さまざまなドメインや現実世界のアプリケーションで適用性が拡大するにつれてますます重要になっています。検索された生成（RAG）は、LLMの限界に対処するための有望なソリューションですが、RAGの堅牢性に関する既存の研究は、しばしば、RAGコンポーネント間の相互接続された関係またはマイナーなテキストエラーなどの実際のデータベースで一般的な潜在的な脅威を見落としています。この作業では、RAGの堅牢性を評価する際に、2つの未掘りの側面を調査します。1）低レベルの摂動を通じて騒々しい文書に対する脆弱性と2）Ragの堅牢性の全体的な評価。さらに、これらの側面をターゲットにする新しい攻撃法であるRag（\\\\ textit {garag}）に対する遺伝的攻撃を紹介します。具体的には、GARAGは各コンポーネント内の脆弱性を明らかにし、騒々しいドキュメントに対してシステム全体の機能をテストするように設計されています。 \\\\ textIT {garag}を標準のQAデータセットに適用し、多様なレトリバーとLLMを組み込んで、ragの堅牢性を検証します。実験結果は、Garagが一貫して高い攻撃の成功率を達成することを示しています。また、各コンポーネントのパフォーマンスとその相乗効果を大幅に破壊し、現実世界のぼろきれシステムを混乱させる際にマイナーなテキストの不正確さがもたらす実質的なリスクを強調しています。 The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\\\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\\n\"]\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1007/978-3-031-88708-6_3', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2404.13948']\n",
      "\u001b[93m## Summary: RAG is a framework for improving the performance of downstream tasks, such as question answering, by incorporating external knowledge into large language models (LLMs).\n",
      "\n",
      "## DOI: 10.1007/978-3-031-88708-6_3\n",
      "RAG, or Retrieval Augmented Generation, is a framework that aims to enhance the performance of downstream tasks by incorporating external knowledge in the form of a set of documents retrieved from a collection. Unlike standard retrieval tasks, which focus on maximising the relevance of top-ranked documents, RAG systems aim to maximise the utility of documents, where utility indicates whether including a document as additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of relevance in RAG contexts for knowledge-intensive language tasks, while other work focuses on the relevance of topical overlap between a query and a document for information-seeking tasks.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2406.13213\n",
      "RAG enables the retrieval of relevant information from external knowledge sources, allowing LLMs to answer queries over previously unseen document collections. However, traditional RAG applications have been shown to perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. To address this, a new method called Multi-Meta-RAG has been introduced, which uses database filtering with LLM-extracted metadata to improve the RAG selection of relevant documents from various sources.\n",
      "\n",
      "## DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain question answering (ODQA) systems. M-RAG utilises embeddings that include metadata and employs generative models such as GPT-3.5-Turbo-16K and GPT-4 for automated answer generation. Through this approach, the generative models can understand the order and context of query documents through metadata. Experimental results show that M-RAG exhibits up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over existing RAG methods.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.13948\n",
      "The robustness of RAG systems has been a focus of recent research, with a particular emphasis on addressing the limitations of LLMs. Existing studies on RAG robustness often overlook interconnected relationships between RAG components or potential threats prevalent in real-world databases, such as minor textual errors. To evaluate RAG robustness, two underexplored aspects are investigated: vulnerability to noisy documents through low-level perturbations and a holistic evaluation of RAG robustness. A novel attack method called Genetic Attack on RAG (GARAG) is introduced, which targets these aspects and reveals vulnerabilities within each component, testing the overall system functionality against noisy documents. Experimental results demonstrate that GARAG consistently achieves high attack success rates, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\n",
      "45\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.675\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.94834626, '10.1007/978-3-031-88708-6_3'), (0.5391045, '10.48550/arXiv.2406.13213'), (0.38225675, '10.6109/jkiice.2023.27.12.1489'), (0.37410447, '10.48550/arXiv.2404.13948')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00286\\n Title: 8つのフリーアクセス学術データベースの出版メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースにおけるメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000を超えるレコードのランダムなCrossRefサンプルを選択し、7つのデータベース（Dimensions、Google Scholar、Microsoft Academic、Openalex、Scilit、Semantic Sc\\u200b\\u200bholar、およびThe Lens）で検索されました。この情報、これらのフィールドの完全性レート、およびデータベース間の合意を説明するフィールドを観察するために、7つの特性（要約、アクセス、書誌情報、書誌情報、文書の種類、公開日、言語、識別子）を観察しました。結果は、アカデミック検索エンジン（Google Scholar、Microsoft Academic、およびSemantic Sc\\u200b\\u200bholar）が少ない情報を収集し、完全性が低いことを示しています。逆に、サードパーティのデータベース（寸法、OpenAlex、Scilit、およびレンズ）は、メタデータの品質が高く、完全性が高くなります。アカデミック検索エンジンには、Webをcrawって信頼できる記述データを取得する能力がないと結論付けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することから得られる情報の喪失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: メタエンハンス：大学図書館の電子論文と学位論文のメタデータの品質改善 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタルオブジェクトがデジタルライブラリインターフェイスを通じて発見されるために重要です。ただし、さまざまな理由により、デジタルオブジェクトのメタデータは、しばしば不完全で、一貫性がなく、誤った値を示します。ケーススタディとして、電子論文と論文（ETD）の7つの重要な分野を使用して、学術メタデータを自動的に検出、修正、および正規化する方法を調査します。メタエンハンスを提案します。メタエンハンスは、これらの分野の品質を改善するために最先端の人工知能方法を利用するフレームワークです。 Metaenhanceを評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 ETDを含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークでMetaenhanceをテストし、提案された方法は、7つのフィールドのうち5つで0.85から1.00の範囲のエラーとF1スコアの検出にほぼ完全なF1スコアを達成したことを発見しました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化全体のメタデータの品質の問題を特定する Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、標準的な形式でコンテキスト、技術、および管理情報を提供することにより、発見とアクセスに不可欠です。しかし、メタデータは、社会文化的表現、リソースの制約、および標準化されたシステムの間の緊張の場所でもあります。正式および非公式の介入は、品質の問題、アイデンティティを主張する政治的行為、または視界を最大化するための戦略的選択と解釈される場合があります。これに関連して、メタデータの品質、一貫性、および完全性が個人やコミュニティにどのように影響するかを理解しようとしました。記録のサンプルをレビューして、メタデータとコミュニティが互いに反対して、意図的に文化的な意味を反映する（またはそうでない）方法に起因する問題を特定して分類しました。 Metadata are crucial for discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. In this context, we sought to understand how metadata quality, consistency, and completeness impact individuals and communities. Reviewing a sample of records, we identified and classified issues stemming from how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101']\n",
      "\u001b[93mMy lady, here are the studies that examined the abstract in metadata:\n",
      "\n",
      "## Summary: Three studies examined the abstract in metadata, focusing on the quality, consistency, and completeness of metadata for digital objects, across cultures, and in academic databases.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00286\n",
      "This study compared the amount of metadata and the completeness degree of research publications in new academic databases. The abstract was one of seven characteristics analysed to observe fields that describe information, the completeness rate of these fields, and the agreement among databases.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2303.17661\n",
      "This study investigated methods to automatically detect, correct, and canonicalise scholarly metadata, using seven key fields of electronic theses and dissertations as a case study. The abstract was one of these seven fields, and the study proposed MetaEnhance, a framework that utilises state-of-the-art artificial intelligence methods to improve the quality of these fields.\n",
      "\n",
      "## DOI: 10.5860/crl.86.1.101\n",
      "This study sought to understand how metadata quality, consistency, and completeness impact individuals and communities. The abstract was one of the characteristics analysed to observe how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\n",
      "45\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.575\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.42143276, '10.1162/qss_a_00286'), (0.19528697, '10.48550/arXiv.2303.17661'), (0.13006426, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0\n",
      "Length of documents: 45\n",
      "reranked_documents: []\n",
      "length of reranked_documents: 0\n",
      "Retrieved DOIs: []\n",
      "\u001b[93mMy lady, could you please provide me with some context or a specific topic you are interested in? I can then offer you studies that examined citations within that field.\n",
      "45\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.500\n",
      "Faithfulness score: 0\n",
      "Documents score: []\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.1007/s11192-023-04923-y\\n Title: Openalexの不足している機関：考えられる理由、意味、および解決策 Missing institutions in OpenAlex: possible reasons, implications, and solutions\\nAbstract: オープンサイエンスの出現では、データ品質が高いオープンデータプラットフォームが必要です。 2022年1月に開始されたグローバルな研究システムの完全に開かれたカタログとして、OpenAlexは、定量的科学研究で広く使用されている簡単なデータアクセシビリティと幅広いデータカバレッジの2つの主要な利点を特徴としています。驚くべきことに、Openalexはライデン大学のランキングの重要なデータソースとして採用されています。ただし、Openalexのジャーナル記事メタデータには、機関が欠落しているという深刻なデータ品質の問題があります。この研究では、3種類の制度情報（FII）、部分的に欠落している機関情報（PMII）、および完全に欠落している機関情報（CMII）を定義することにより、問題とその結果と解決策の考えられる理由を調査します。私たちの結果は、不足している機関の問題がOpenalexのジャーナル記事の60％以上で発生することを示しています。この問題は、特に初期のメタデータや社会科学や人文科学で広まっています。データのサブサンプルを使用して、問題の考えられる理由、歪んだ結果のリスク、および不足している機関の問題に対する可能な解決策をさらに調査します。目的は、オープンリソースのデータ品質改善の重要性を高め、定量的科学研究およびより広い文脈でのオープンリソースの責任ある使用をサポートすることです。 The advent of open science calls for open data platforms with high data quality. As a fully open catalog of the global research system launched in January 2022, OpenAlex features two main advantages of easy data accessibility and broad data coverage, which has been widely used in quantitative science studies. Remarkably, OpenAlex is adopted as an important data source for Leiden university ranking. However, there is a severe data quality problem of missing institutions in journal article metadata in OpenAlex. This study investigates the possible reasons for the problem and its consequences and solutions by defining three types of institutional information—full institutional information (FII), partially missing institutional information (PMII) and completely missing institutional information (CMII). Our results show that the problem of missing institutions occurs in more than 60% of the journal articles in OpenAlex. The problem is particularly widespread in metadata from the early years and in the social sciences and humanities. Using sub-samples of the data, we further explore the possible reasons for the problem, the risk it might represent for distorted results, and possible solutions to the problem of missing institutions. The aim is to raise the importance of data quality improvements in open resources, and thus to support the responsible use of open resources in quantitative science studies and also in broader contexts.\\n', 'DOI: 10.1590/SciELOPreprints.11205\\n Title: ユニバーサルインデックスへのオープンロードで：OpenAlexおよびOpenJournal Systems On the Open Road to Universal Indexing: OpenAlex and OpenJournal Systems\\nAbstract: この調査では、OpenAlexのオープンジャーナルシステム（JUOJS）を使用したジャーナルのインデックス作成を検証し、包括的な学術参加をサポートする2つのオープンソースソフトウェアイニシアチブを反映しています。 47,625のアクティブなJuojsのデータセットを分析することにより、これらのジャーナルの71％がOpenAlexで少なくとも1つの記事をインデックス付けされていることを明らかにします。私たちの調査結果は、OpenAlexに含まれるCrossRef doiを使用してジャーナルの97％を使用して、インデックス作成の達成におけるCrossRef DOIの中心的な役割を強調しています。ただし、この技術的依存は、特に低所得国（Juojsの47％）および非英語言語ジャーナル（Juojsの55％-64％）からのリソース制限されたジャーナル（Juojsの55％-64％）の雑誌として、より広範な構造的不平等を反映しています。私たちの研究は、学術インフラストラクチャの依存関係の理論的意味と、世界的な知識の可視性における体系的な格差を永続させる上でのその役割を強調しています。 OpenAlexのような包括的な書誌データベースでさえ、世界規模で公平な索引付けを促進するために、財務、インフラ、および言語の障壁に積極的に対処する必要があると主張します。インデックス作成メカニズム、永続的な識別子、および構造的不平等との関係を概念化することにより、この研究は、グローバルな多言語学術生態系における普遍的なインデックス作成のダイナミクスとその実現を再考するための重要なレンズを提供します。 This study examines OpenAlex’s indexing of journals using Open Journal Systems (JUOJS), reflecting two open source software initiatives supporting inclusive scholarly participation. By analyzing a dataset of 47,625 active JUOJS, we reveal that 71% of these journals have at least one article indexed in OpenAlex. Our findings underscore the central role of Crossref DOIs in achieving indexing, with 97% of the journals using Crossref DOIs included in OpenAlex. However, this technical dependency reflects broader structural inequities, as resource-limited journals, particularly those from low-income countries (47% of JUOJS) and non-English language journals (55%-64% of JUOJS), remain underrepresented. Our work highlights the theoretical implications of scholarly infrastructure dependencies and their role in perpetuating systemic disparities in global knowledge visibility. We argue that even inclusive bibliographic databases like OpenAlex must actively address financial, infrastructural, and linguistic barriers to foster equitable indexing on a global scale. By conceptualizing the relationship between indexing mechanisms, persistent identifiers, and structural inequities, this study provides a critical lens for rethinking the dynamics of universal indexing and its realization in a global, multilingual scholarly ecosystem.\\n', 'DOI: 10.48550/arXiv.2404.01985\\n Title: 彼はOpenalex、Scopus、Web of Scienceのオープンアクセスカバレッジ he open access coverage of OpenAlex, Scopus and Web of Science\\nAbstract: Diamond Open Access（OA）ジャーナルは、著者と読者の両方に無料の公開モデルを提供しますが、主要な書誌データベースでのインデックスの欠如は、これらのジャーナルの取り込みを評価する際の課題を提示します。さらに、出版言語や出版国などのOAの特性は、OAジャーナルがより多様であり、地域社会にサービスを提供することを目指しているという議論を支持するためにしばしば使用されてきましたが、OAジャーナルの地理的および言語的特性に関連する経験的証拠の現在の欠如があります。 OpenAlexとオープンアクセスジャーナルのディレクトリをベンチマークとして使用して、このペーパーでは、フィールド、国、言語による科学とスコープスのWebでの著者とジャーナルの報道を通じて、ダイヤモンドと金の報道を調査します。結果は、WOSとSCOPUSでのより低いカバレッジ、およびダイヤモンドOAの局所範囲を示しています。英語のみのジャーナルのシェアは、ゴールドジャーナルの間でかなり高くなっています。高所得国は、社会科学と人文科学のダイヤモンドジャーナルを除き、すべてのドメインとジャーナルの種類で著者のシェアが最も高い。ダイヤモンドOAインデックスの現在の景観を理解することは、より包括的なOAモデルに向けて政策と実践を進めることで、学術通信ネットワークを支援することができます。 Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2401.16359', '10.48550/arXiv.2404.17663', '10.1007/s11192-023-04923-y', '10.1590/SciELOPreprints.11205', '10.48550/arXiv.2404.01985']\n",
      "\u001b[93m## Summary: OpenAlex is an open-source, scholarly metadata platform that provides free and open access to data, enabling researchers to conduct bibliometric studies without licensing barriers.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2401.16359\n",
      "This report compares OpenAlex to established proprietary sources like Web of Science and Scopus, demonstrating its average source reference numbers and internal coverage rates comparable to both. OpenAlex's data is rapidly evolving, raising questions about its trustworthiness.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.17663\n",
      "This study, conducted with the OpenAlex team, compares OpenAlex to Scopus across various dimensions. It concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. However, issues with metadata accuracy and completeness require further research to fully understand and address OpenAlex's limitations.\n",
      "\n",
      "## DOI: 10.1007/s11192-023-04923-y\n",
      "This research investigates the severe data quality problem of missing institutions in OpenAlex's journal article metadata. It defines three types of institutional information and finds that the problem occurs in over 60% of journal articles, particularly in early years and in the social sciences and humanities.\n",
      "\n",
      "## DOI: 10.1590/SciELOPreprints.11205\n",
      "This study examines OpenAlex's indexing of journals using Open Journal Systems, revealing that 71% of journals have at least one article indexed in OpenAlex. It highlights the central role of Crossref DOIs in achieving indexing and emphasizes the need to address financial, infrastructural, and linguistic barriers to promote equitable indexing on a global scale.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.01985\n",
      "This paper investigates the coverage of diamond and gold open access (OA) journals through authorship and journal coverage in Web of Science and Scopus. It finds lower coverage in WoS and Scopus and highlights the local scope of diamond OA, with high-income countries having the highest share of authorship in most domains and journal types.\n",
      "\n",
      "My lady, I hope this information is helpful.\n",
      "45\n",
      "For query: ['Tell me about OpenAlex.']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.773\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9965301, '10.48550/arXiv.2401.16359'), (0.9940428, '10.48550/arXiv.2404.17663'), (0.99040353, '10.1007/s11192-023-04923-y'), (0.9678993, '10.1590/SciELOPreprints.11205'), (0.96405166, '10.48550/arXiv.2404.01985')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: CrossRef：コミュニティ所有の学術メタデータの持続可能な供給源 Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このホワイトペーパーでは、CrossRefによって収集および利用可能になった学術的メタデータと、学術研究の生態系におけるその重要性について説明します。 1億600万件以上の記録を含み、年間平均11％のレートで拡大するCrossrefのメタデータは、出版社、著者、図書館員、資金提供者、および研究者向けの学術データの主要な情報源の1つになりました。メタデータセットは、ジャーナルやカンファレンスペーパーなどの従来のタイプだけでなく、データセット、レポート、プリプリント、ピアレビュー、助成金など、13のコンテンツタイプで構成されています。メタデータは、基本的な出版物メタデータに限定されませんが、全文、資金調達とライセンス情報、引用リンク、修正、更新、撤回などに関する情報への要約とリンクを含めることもできます。メタデータは、REST APIやOAI-PMHを含む多くのAPIを通じて利用できます。この論文では、CrossRefが提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、CrossRefの研究エコシステムにおける役割と、引用データの提供の進化など、長年にわたるメタデータのキュレーションの傾向にも注目しています。 Crossrefのメタデータで使用される研究を要約し、将来のメタデータの品質と検索を改善する計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022']\n",
      "\u001b[93mMy lady, Crossref is a sustainable source of community-owned scholarly metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "This paper describes the scholarly metadata collected and made available by Crossref, which has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. Containing over 106 million records and expanding at an average rate of 11% a year, the metadata set consists of 13 content types, including traditional types such as journals and conference papers, as well as data sets, reports, preprints, peer reviews, and grants. The metadata is available through a number of APIs, including REST API and OAI-PMH. The paper also looks at Crossref’s role in the research ecosystem and trends in metadata curation over the years.\n",
      "45\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.556\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.8345418, '10.1162/qss_a_00022')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: オープンアレックスの言語カバレッジの評価：メタデータの精度と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: ClarivateのWeb of Science（WOS）とElsevierのScopusは、数十年にわたって書誌情報の主要なソースでした。高度にキュレーションされていますが、これらの閉鎖された独自のデータベースは、英語の出版物に大きく偏っており、研究普及における他の言語の使用を過小評価しています。 2022年に発売されたOpenalexは、包括的、包括的、オープンソースの研究情報を約束しました。すでに学者や研究機関が使用している間、そのメタデータの質は現在評価されています。この論文は、言語に関連するOpenalexのメタデータの完全性と正確性、WOSとの比較、および6,836の記事のサンプルの詳細な手動検証を通じて、この文献に貢献しています。結果は、オープンアレックスがWOSよりもはるかにバランスのとれた言語カバレッジを示すことを示しています。ただし、言語メタデータは必ずしも正確ではないため、Openalexは他の言語のそれを過小評価しながら英語の場所を過大評価します。批判的に使用すると、OpenAlexは学術出版に使用される言語の包括的かつ代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャレベルでより多くの作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8つのフリーアクセス学術データベースの出版メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースにおけるメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000を超えるレコードのランダムなCrossRefサンプルを選択し、7つのデータベース（Dimensions、Google Scholar、Microsoft Academic、Openalex、Scilit、Semantic Sc\\u200b\\u200bholar、およびThe Lens）で検索されました。この情報、これらのフィールドの完全性レート、およびデータベース間の合意を説明するフィールドを観察するために、7つの特性（要約、アクセス、書誌情報、書誌情報、文書の種類、公開日、言語、識別子）を観察しました。結果は、アカデミック検索エンジン（Google Scholar、Microsoft Academic、およびSemantic Sc\\u200b\\u200bholar）が少ない情報を収集し、完全性が低いことを示しています。逆に、サードパーティのデータベース（寸法、OpenAlex、Scilit、およびレンズ）は、メタデータの品質が高く、完全性が高くなります。アカデミック検索エンジンには、Webをcrawって信頼できる記述データを取得する能力がないと結論付けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することから得られる情報の喪失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.1162/qss_a_00286']\n",
      "\u001b[93mMy lady, here are the papers that evaluate the linguistic coverage or language-related metadata in scholarly databases:\n",
      "\n",
      "## Summary: Papers that evaluate the linguistic coverage or language-related metadata in scholarly databases.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2409.10633\n",
      "Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "Abstract: This paper assesses the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with Clarivate's Web of Science (WoS), as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00286\n",
      "Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Abstract: This study compares the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, a random Crossref sample of more than 115,000 records was selected and searched in seven databases. Seven characteristics were analyzed to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines gather less information and have a low degree of completeness. Conversely, third-party databases have more metadata quality and a higher completeness rate.\n",
      "45\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.588\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.9998287, '10.48550/arXiv.2409.10633'), (0.4870177, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープンファンダーメタデータの可用性と完全性：オランダの研究評議会によって資金提供された出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金を提供する研究の結果に関する情報を収集するためにかなりの努力を費やしています。資金提供者が資金調達に関連する出版物の出力を追跡するのを支援するために、CrossRefは2013年にFundRefを開始し、出版社が永続的な識別子を使用して資金情報を登録できるようにしました。ただし、資金調達の研究の結果であるため、資金提供者のメタデータを含める必要があるため、Funder Metadataのカバレッジを評価することは困難です。この論文では、研究者が特定の資金提供機関による資金提供の結果であるオランダ研究評議会NWOが報告した5,004の出版物を調べました。これらの記事の67％のみがCrossRefの資金情報を含んでおり、NWOをNWOにリンクしたFunder Nameおよび/またはFunder IDとしてNWOを認めているサブセット（それぞれ53％と45％）が含まれています。 Web of Science（WOS）、Scopus、およびDimensionsはすべて、記事の全文の資金調達声明から追加の資金情報を推測することができます。レンズの資金情報は、主にCrossRefのそれに対応しており、PubMedから取得した可能性のある追加の資金情報があります。私たちは、独自のデータベースと比較して、CrossRefのメタデータの資金調達のカバレッジと完全性における出版社間の興味深い違いを観察し、資金調達のオープンメタデータの質を高める可能性を強調しています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: 資金調達COVID-19研究：オープンデータインフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 彼らが提供する資金調達の結果を分析するには、資金提供機関が資金から生じる出版物を追跡できるようにすることが不可欠です。 Covid-19に関連する研究を報告する出版物の資金調達データに焦点を当て、CrossRefでの資金提供データの可用性を研究しています。また、ScopusとWeb of Scienceの2つの独自の書誌データベースで利用可能な資金データとの比較も提示します。私たちの分析により、CrossRefの資金調達データの限られたカバレッジが明らかになりました。また、特にScopusでは、資金調達データの質に関連する問題を示しています。 CrossRefでの資金提供データの可用性を改善するための推奨事項を提供しています。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: 開いた書誌メタデータの源としてのCrossref Crossref as a source of open bibliographic metadata\\nAbstract: CrossRefの学術出版物の書誌メタデータのオープン可用性を促進するために、いくつかのイニシアチブが採用されています。 CrossRefでの6つのメタデータ要素の可用性の最新の概要を示します。参照リスト、要約、ORCID、著者の提携、資金情報、およびライセンス情報です。私たちの分析は、これらのメタデータ要素の可用性が、少なくともCrossRefで最も一般的な出版タイプであるジャーナル記事で、時間とともに改善されたことを示しています。ただし、分析では、多くの出版社が書誌メタデータの完全な開放性を実現するために追加の努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.1162/qss_a_00212', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mMy lady, here are some papers that address funding metadata, its availability, or its analysis in scholarly databases:\n",
      "\n",
      "## Summary: Papers discussing the availability and completeness of funding metadata in scholarly databases.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00210\n",
      "The availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council. This paper looks at 5,004 publications reported to be the result of funding by the Dutch Research Council. It finds that only 67% of these articles contain funding information in Crossref, highlighting the potential to increase the quality of open metadata on funding.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00212\n",
      "Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures. This paper studies the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. It reveals limited coverage of funding data in Crossref and problems related to the quality of funding data, especially in Scopus.\n",
      "\n",
      "## DOI: 10.31222/osf.io/smxe5\n",
      "Crossref as a source of open bibliographic metadata. This paper presents an overview of the availability of six metadata elements in Crossref, including funding information. It shows that the availability of these metadata elements has improved over time, but also that many publishers need to make additional efforts to realise full openness of bibliographic metadata.\n",
      "45\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.688\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.9913892, '10.1162/qss_a_00210'), (0.8077641, '10.1162/qss_a_00212'), (0.7324005, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模な言語モデルの検索された生成：調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模な言語モデル（LLMS）は印象的な能力を紹介しますが、幻覚、時代遅れの知識、非透明な、追跡不可能な推論プロセスなどの課題に遭遇します。検索された生成（RAG）は、外部データベースから知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の正確性と信頼性が向上し、ドメイン固有の情報の継続的な知識の更新と統合が可能になります。 RAGは、外部データベースの広大で動的なリポジトリとLLMSの本質的な知識を相乗的に統合します。この包括的なレビューペーパーでは、素朴なぼろきれ、高度なぼろ、モジュラーラグを含むRAGパラダイムの進行に関する詳細な調査を提供します。検索、生成、増強技術を含む、RAGフレームワークの三者基盤を細心の注意を払って精査します。この論文は、これらの各重要なコンポーネントに組み込まれた最先端のテクノロジーを強調し、RAGシステムの進歩を深く理解することを提供します。さらに、このペーパーでは、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を描き、研究開発の将来の道を指摘しています。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", 'DOI: 10.1609/aaai.v38i16.29728\\n Title: 検索された世代の大規模な言語モデルのベンチマーク Benchmarking Large Language Models in Retrieval-Augmented Generation\\nAbstract: 検索された生成（RAG）は、大規模な言語モデル（LLM）の幻覚を緩和するための有望なアプローチです。ただし、既存の研究には、さまざまな大規模な言語モデルに対する検索された生成の影響に関する厳密な評価がありません。これにより、異なるLLMのRAGの機能における潜在的なボトルネックを特定することが困難になります。この論文では、検索された生成が大規模な言語モデルに与える影響を体系的に調査します。ノイズの堅牢性、否定的な拒否、情報統合、反事実的堅牢性など、RAG\\u200b\\u200bに必要な4つの基本能力におけるさまざまな大手言語モデルのパフォーマンスを分析します。この目的のために、英語と中国語の両方でRAG評価のための新しいコーパスである検索された生成ベンチマーク（RGB）を確立します。 RGBは、ベンチマーク内のインスタンスを、ケースを解決するために必要な前述の基本能力に基づいて、4つの個別のテストベッドに分割します。次に、RGBの6つの代表LLMを評価して、RAGを適用する際に現在のLLMの課題を診断します。評価により、LLMはある程度のノイズの堅牢性を示していますが、否定的な拒絶、情報統合、誤った情報への対処に関して依然として著しく苦労していることが明らかになりました。前述の評価の結果は、LLMにRAGを効果的に適用するために、まだかなりの旅がまだあることを示しています。 Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.48550/arXiv.2410.04231\\n Title: 大規模な言語モデルの検索された生成によるメタデータベースのデータ探査 Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\\nAbstract: 必要なデータセットを効果的に検索する能力を開発することは、非常に限られた利用可能なメタデータを考慮して、データユーザーが関連するデータセットを特定するのを支援するための緊急の要件です。この課題のために、サードパーティのデータの利用は、改善の貴重なソースとして浮上しています。私たちの研究では、メタデータベースのデータ発見を強化するために検索された世代（RAG）の形式を採用するデータ探索のための新しいアーキテクチャを紹介します。このシステムは、大規模な言語モデル（LLM）を外部ベクトルデータベースと統合して、多様なタイプのデータセット間のセマンティック関係を識別します。提案されたフレームワークは、不均一なデータソース間のセマンティックな類似性を評価し、データ探索を改善するための新しい方法を提供します。私たちの研究には、4つの重要なタスクに関する実験結果が含まれています。1）同様のデータセットの推奨、2）組み合わせ可能なデータセットの提案、3）タグの推定、4）変数の予測。我々の結果は、RAGが従来のメタデータアプローチと比較した場合、特に異なるカテゴリから関連するデータセットの選択を強化できることを示しています。ただし、パフォーマンスはタスクとモデルによって異なり、特定のユースケースに基づいて適切な手法を選択することの重要性を確認します。調査結果は、このアプローチがデータ調査と発見の課題に対処するための約束を保持していることを示唆していますが、推定タスクにはさらなる改良が必要です。 Developing the capacity to effectively search for requisite datasets is an urgent requirement to assist data users in identifying relevant datasets considering the very limited available metadata. For this challenge, the utilization of third-party data is emerging as a valuable source for improvement. Our research introduces a new architecture for data exploration which employs a form of Retrieval-Augmented Generation (RAG) to enhance metadata-based data discovery. The system integrates large language models (LLMs) with external vector databases to identify semantic relationships among diverse types of datasets. The proposed framework offers a new method for evaluating semantic similarity among heterogeneous data sources and for improving data exploration. Our study includes experimental results on four critical tasks: 1) recommending similar datasets, 2) suggesting combinable datasets, 3) estimating tags, and 4) predicting variables. Our results demonstrate that RAG can enhance the selection of relevant datasets, particularly from different categories, when compared to conventional metadata approaches. However, performance varied across tasks and models, which confirms the significance of selecting appropriate techniques based on specific use cases. The findings suggest that this approach holds promise for addressing challenges in data exploration and discovery, although further refinement is necessary for estimation tasks.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1609/aaai.v38i16.29728', '10.48550/arXiv.2406.13213', '10.48550/arXiv.2410.04231', '10.6109/jkiice.2023.27.12.1489']\n",
      "\u001b[93mMy lady, here are some papers discussing the use of Retrieval-Augmented Generation (RAG) in large language models or related applications:\n",
      "\n",
      "Summary: A range of papers discuss the use of RAG in large language models, highlighting its potential to enhance accuracy and credibility, particularly for knowledge-intensive tasks.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997\n",
      "Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "This comprehensive review paper examines the progression of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG. It scrutinises the tripartite foundation of RAG frameworks, emphasising state-of-the-art technologies in retrieval, generation, and augmentation techniques. The paper introduces up-to-date evaluation frameworks and benchmarks, addressing challenges and suggesting future research directions.\n",
      "\n",
      "DOI: 10.1609/aaai.v38i16.29728\n",
      "Title: Benchmarking Large Language Models in Retrieval-Augmented Generation\n",
      "This paper systematically investigates the impact of RAG on large language models, analysing performance in four fundamental abilities: noise robustness, negative rejection, information integration, and counterfactual robustness. It establishes the Retrieval-Augmented Generation Benchmark (RGB) for RAG evaluation in English and Chinese. The study reveals that while LLMs exhibit noise robustness, they struggle with negative rejection, information integration, and handling false information, indicating a need for further development.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213\n",
      "Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "This paper introduces Multi-Meta-RAG, a method that uses database filtering with LLM-extracted metadata to improve RAG selection of relevant documents for multi-hop queries. Traditional RAG applications have been shown to perform poorly in answering such queries, and Multi-Meta-RAG aims to address this by enhancing the retrieval of relevant information from various sources.\n",
      "\n",
      "DOI: 10.48550/arXiv.2410.04231\n",
      "Title: Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\n",
      "This research introduces a new architecture for data exploration, employing RAG to enhance metadata-based data discovery. The system integrates LLMs with external vector databases to identify semantic relationships among diverse datasets. The proposed framework evaluates semantic similarity and improves data exploration, with experimental results showing RAG's ability to enhance the selection of relevant datasets, particularly from different categories.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "Title: M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\n",
      "This paper proposes M-RAG, a method for effective search in open-domain question-answering systems. It utilises embeddings with metadata and generative models (GPT-3.5, GPT-4) to understand query document order and context. By incorporating source information, M-RAG enhances answer accuracy, allowing LLMs to retrieve information from external sources and provide appropriate responses. Experimental results demonstrate significant performance improvements compared to existing RAG methods and external inference systems.\n",
      "45\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) in large language models or related applications?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9999918, '10.48550/arXiv.2312.10997'), (0.99997985, '10.1609/aaai.v38i16.29728'), (0.99964714, '10.48550/arXiv.2406.13213'), (0.9996345, '10.48550/arXiv.2410.04231'), (0.9891816, '10.6109/jkiice.2023.27.12.1489')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: CrossRef：コミュニティ所有の学術メタデータの持続可能な供給源 Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このホワイトペーパーでは、CrossRefによって収集および利用可能になった学術的メタデータと、学術研究の生態系におけるその重要性について説明します。 1億600万件以上の記録を含み、年間平均11％のレートで拡大するCrossrefのメタデータは、出版社、著者、図書館員、資金提供者、および研究者向けの学術データの主要な情報源の1つになりました。メタデータセットは、ジャーナルやカンファレンスペーパーなどの従来のタイプだけでなく、データセット、レポート、プリプリント、ピアレビュー、助成金など、13のコンテンツタイプで構成されています。メタデータは、基本的な出版物メタデータに限定されませんが、全文、資金調達とライセンス情報、引用リンク、修正、更新、撤回などに関する情報への要約とリンクを含めることもできます。メタデータは、REST APIやOAI-PMHを含む多くのAPIを通じて利用できます。この論文では、CrossRefが提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、CrossRefの研究エコシステムにおける役割と、引用データの提供の進化など、長年にわたるメタデータのキュレーションの傾向にも注目しています。 Crossrefのメタデータで使用される研究を要約し、将来のメタデータの品質と検索を改善する計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: 開いた書誌メタデータの源としてのCrossref Crossref as a source of open bibliographic metadata\\nAbstract: CrossRefの学術出版物の書誌メタデータのオープン可用性を促進するために、いくつかのイニシアチブが採用されています。 CrossRefでの6つのメタデータ要素の可用性の最新の概要を示します。参照リスト、要約、ORCID、著者の提携、資金情報、およびライセンス情報です。私たちの分析は、これらのメタデータ要素の可用性が、少なくともCrossRefで最も一般的な出版タイプであるジャーナル記事で、時間とともに改善されたことを示しています。ただし、分析では、多くの出版社が書誌メタデータの完全な開放性を実現するために追加の努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mSummary: Crossref is a valuable source of scholarly data for publishers, authors, librarians, funders, and researchers, with over 106 million records and an average annual growth rate of 11%.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "Crossref's metadata set consists of 13 content types, including journals, conference papers, datasets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication information but can also include abstracts, full text, funding and license details, citation links, and information on corrections, updates, and retractions. This scale and breadth make Crossref a valuable source for research in scientometrics, measuring the growth and impact of science, and understanding new trends in scholarly communications.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5\n",
      "Crossref promotes the open availability of bibliographic metadata for scholarly publications. Six key metadata elements are available in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Analysis shows that the availability of these elements has improved over time, particularly for journal articles, the most common publication type in Crossref. However, many publishers need to make further efforts to achieve full openness of bibliographic metadata.\n",
      "45\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.933\n",
      "Recall: 0.933\n",
      "F1-Score: 0.933\n",
      "Accuracy: 0.933\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.99954, '10.1162/qss_a_00022'), (0.41347715, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.3145/epi.2023.mar.09\\n Title: Bibliometricsに関連するメタデータのどれが同じであり、Microsoft Academic GraphからOpenAlexに切り替えるときにどのメタデータが異なりますか？ Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\\nAbstract: Microsoft Academic Graph（MAG）の退職の発表により、非営利団体Ourresearchは、Openalexという名前で同様のリソースを提供すると発表しました。したがって、メタデータを、最新のMAGスナップショットの書誌分析と初期のオープンアレックススナップショットと比較します。実質的にMAGのすべての作品は、書誌データの出版年、ボリューム、ファーストページと最後のページ、DOI、および引用分析の重要な要素である参照の数を保存するOpenalexに転送されました。 MAGドキュメントの90％以上がOpenAlexに同等のドキュメントタイプを持っています。残りのもののうち、特にOpenalex Document Typesの再分類ジャーナルアーティクルとブックチャプターは正しいと思われ、7％以上になります。そのため、ドキュメントタイプの仕様はMAGからOpenalexに大幅に改善されました。書誌関連のメタデータの別の項目として、MAGおよびOpenalexでの紙ベースの被験者の分類を調べました。 MAGよりもOpenAlexの対象分類割り当てを含むかなり多くのドキュメントを見つけました。第1レベルと第2レベルでは、分類構造はほぼ同じです。表形式とグラフィカルな形式の両方のレベルでの対象の再分類に関するデータを提示します。現地正規化された書誌評価における豊富な被験者の再分類の結果の評価は、本論文の範囲にありません。この未解決の質問とは別に、OpenAlexは、2021年以前の出版年のMAGと同じくらいの書誌分析に少なくともまったく適しているように見えます。 With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: Openalex、Web of Science、Scopus、PubMed、およびSemantic Sc\\u200b\\u200bholarの出版物と文書タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、次の書誌データベースの出版物と文書の種類を比較および分析します：Openalex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、Pubmed。結果は、類型が個々のデータベースプロバイダー間でかなり異なる可能性があることを示しています。さらに、出版物はそれぞれのデータベースで異なる方法で分類されているため、参考文献分析に関連するドキュメントを特定するために必要な研究と非研究テキストの区別は、データソースによって異なる場合があります。この研究の焦点は、横断段階の比較に加えて、主にオープンアレックスに含まれる出版物とドキュメントの種類のカバレッジと分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n', 'DOI: 10.48550/arXiv.2404.01985\\n Title: 彼はOpenalex、Scopus、Web of Scienceのオープンアクセスカバレッジ he open access coverage of OpenAlex, Scopus and Web of Science\\nAbstract: Diamond Open Access（OA）ジャーナルは、著者と読者の両方に無料の公開モデルを提供しますが、主要な書誌データベースでのインデックスの欠如は、これらのジャーナルの取り込みを評価する際の課題を提示します。さらに、出版言語や出版国などのOAの特性は、OAジャーナルがより多様であり、地域社会にサービスを提供することを目指しているという議論を支持するためにしばしば使用されてきましたが、OAジャーナルの地理的および言語的特性に関連する経験的証拠の現在の欠如があります。 OpenAlexとオープンアクセスジャーナルのディレクトリをベンチマークとして使用して、このペーパーでは、フィールド、国、言語による科学とスコープスのWebでの著者とジャーナルの報道を通じて、ダイヤモンドと金の報道を調査します。結果は、WOSとSCOPUSでのより低いカバレッジ、およびダイヤモンドOAの局所範囲を示しています。英語のみのジャーナルのシェアは、ゴールドジャーナルの間でかなり高くなっています。高所得国は、社会科学と人文科学のダイヤモンドジャーナルを除き、すべてのドメインとジャーナルの種類で著者のシェアが最も高い。ダイヤモンドOAインデックスの現在の景観を理解することは、より包括的なOAモデルに向けて政策と実践を進めることで、学術通信ネットワークを支援することができます。 Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2401.16359', '10.3145/epi.2023.mar.09', '10.48550/arXiv.2406.15154', '10.48550/arXiv.2404.01985']\n",
      "\u001b[93mMy lady, here is a summary of the key features and limitations of OpenAlex as a bibliometric database:\n",
      "\n",
      "## Summary:\n",
      "OpenAlex is a promising open-source database of scholarly metadata, offering a free alternative to established proprietary sources like Web of Science and Scopus. It provides data openly, allowing researchers to conduct bibliometric studies without licensing barriers. However, as a rapidly evolving source with expanding and quickly changing data, questions arise about its data reliability.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2401.16359\n",
      "This report compares OpenAlex's reference coverage and selected metadata with Web of Science and Scopus. When restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex demonstrates average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. The analysis also reveals mixed results when comparing other core metadata covered by OpenAlex, capturing more ORCID identifiers, fewer abstracts, and a similar number of open-access status indicators per article.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.17663\n",
      "This study compares OpenAlex with Scopus across various dimensions. It concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. However, issues with metadata accuracy and completeness highlight the need for further research to fully understand and address OpenAlex's limitations.\n",
      "\n",
      "## DOI: 10.3145/epi.2023.mar.09\n",
      "With the retirement of Microsoft Academic Graph (MAG), OpenAlex was introduced as a similar resource. This paper compares the metadata of the latest MAG snapshot with an early OpenAlex snapshot. It finds that OpenAlex preserves bibliographic data, including publication year, volume, first and last page, DOI, and the number of references. More than 90% of MAG documents have equivalent document types in OpenAlex, and the document type specifications have improved significantly. The paper also examines subject classification in MAG and OpenAlex, finding more documents with subject classification assignments in OpenAlex.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2406.15154\n",
      "This study compares publication and document types in OpenAlex, Scopus, Web of Science, Semantic Scholar, and PubMed. The results show that typologies can differ considerably between database providers, and the distinction between research and non-research texts can vary, impacting the identification of relevant documents for bibliometric analysis. The study focuses on the coverage and analysis of publications and document types in OpenAlex, highlighting its growing importance as a free alternative for bibliometric analyses.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.01985\n",
      "This paper investigates the open access coverage of OpenAlex, Scopus, and Web of Science. It uses OpenAlex and the Directory of Open Access Journals as benchmarks to examine the coverage of diamond and gold through authorship and journal coverage in Web of Science and Scopus by field, country, and language. The results indicate lower coverage in WoS and Scopus for diamond OA and a local scope for diamond OA.\n",
      "45\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.999764, '10.48550/arXiv.2404.17663'), (0.9930153, '10.48550/arXiv.2401.16359'), (0.971779, '10.3145/epi.2023.mar.09'), (0.90665317, '10.48550/arXiv.2406.15154'), (0.3060083, '10.48550/arXiv.2404.01985')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/s11192-015-1765-5\\n Title: 彼の科学とスコープスのWebの記事をジャーナル：比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌メソッドは、さまざまな目的、つまり研究評価のために複数の分野で使用されます。ほとんどの書誌分析には、Thomson ReutersのWeb of Science（WOS）とElsevierのScopusなどのデータソースが共通しています。この研究の目的は、これらの2つのデータベースのジャーナル報道を説明し、何らかの分野、出版国、言語が過剰または過小評価されているかどうかを評価することです。これを行うために、WOS（13,605のジャーナル）とScopus（20,346のジャーナル）のアクティブな学術雑誌の報道と、Ulrichの広範な定期的なディレクトリ（63,013雑誌）と比較しました。結果は、研究評価のためにWOSまたはSCOPUSのいずれかを使用すると、自然科学と工学を支持するバイアス、ならびに社会科学と芸術と人文科学を損なう生物医学的研究が導入される可能性があることを示しています。同様に、英語のジャーナルは、他の言語の損害に過大評価されています。両方のデータベースはこれらのバイアスを共有していますが、カバレッジは大幅に異なります。結果として、書誌分析の結果は、使用されるデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈では、特に異なる分野、機関、国、または言語を比較する場合、WOSとSCOPUSを注意して使用する必要があることを意味します。書誌コミュニティは、フィールド固有および国家引用指数など、WOSやSCOPUSでカバーされていない科学的生産量を含む方法と指標を開発するための努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データソースの大規模な比較：Scopus、Web of Science、Dimensions、CrossRef、およびMicrosoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: Scopus、Web of Science、Dimensions、CrossRef、およびMicrosoft Academicの5つの学際的な参考文献データソースの大規模な比較を提示します。この比較では、これらのデータソースでカバーされている2008年から2017年の期間の科学文書を考慮しています。 Scopusは、他のそれぞれのデータソースとペアワイズで比較されます。まず、ドキュメントのカバレッジのデータソース間の違いを分析します。たとえば、時間の経過とともに違い、ドキュメントタイプあたりの違い、および分野あたりの違いに焦点を当てます。次に、引用リンクの完全性と精度の違いを調べます。分析に基づいて、さまざまなデータソースの長所と短所について説明します。科学文献の包括的な報道と、文献を選択するための柔軟なフィルターのセットを組み合わせることの重要性を強調しています。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: Openalex、Web of Science、Scopus、PubMed、およびSemantic Sc\\u200b\\u200bholarの出版物と文書タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、次の書誌データベースの出版物と文書の種類を比較および分析します：Openalex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、Pubmed。結果は、類型が個々のデータベースプロバイダー間でかなり異なる可能性があることを示しています。さらに、出版物はそれぞれのデータベースで異なる方法で分類されているため、参考文献分析に関連するドキュメントを特定するために必要な研究と非研究テキストの区別は、データソースによって異なる場合があります。この研究の焦点は、横断段階の比較に加えて、主にオープンアレックスに含まれる出版物とドキュメントの種類のカバレッジと分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1007/s11192-015-1765-5', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2401.16359', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154']\n",
      "\u001b[93mSummary: Web of Science (WoS) is a bibliometric database that has been the foundation for research in the science of science, despite its systematic underrepresentation of certain disciplines and world regions.\n",
      "\n",
      "DOI: 10.1007/s11192-015-1765-5\n",
      "The use of WoS for research evaluation may introduce biases that favour natural sciences and engineering, as well as biomedical research, to the detriment of social sciences and arts and humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. Both WoS and Scopus share these biases, but their coverage differs substantially. As a result, the outcomes of bibliometric analyses may vary depending on the database used.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663\n",
      "WoS, along with Scopus, has been the foundation for research in the science of science, even though these traditional databases systematically underrepresent certain disciplines and world regions.\n",
      "\n",
      "DOI: 10.48550/arXiv.2401.16359\n",
      "WoS is a rapidly evolving source, and the data contained within is expanding and also quickly changing, which naturally raises questions about the trustworthiness of its data.\n",
      "\n",
      "DOI: 10.1162/qss_a_00112\n",
      "This study emphasises the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.15154\n",
      "Typologies can differ considerably between individual database providers, and the distinction between research and non-research texts can vary depending on the data source because publications are classified differently in the respective databases.\n",
      "\n",
      "My lady, I hope this information is helpful.\n",
      "45\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9947187, '10.1007/s11192-015-1765-5'), (0.96073127, '10.48550/arXiv.2404.17663'), (0.71721, '10.48550/arXiv.2401.16359'), (0.63793355, '10.1162/qss_a_00112'), (0.44269672, '10.48550/arXiv.2406.15154')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.1007/978-3-031-88708-6_3\\n Title: 関連性はレトリバーからジェネレーターにぼろきれに伝播されますか？ Is Relevance Propagated from Retriever to Generator in RAG?\\nAbstract: 検索拡張生成（RAG）は、通常、コレクションから取得された一連のドキュメントの形で、プロンプトの大規模な言語モデル（LLM）への一連のドキュメントの形で、質問の回答などの下流タスクのパフォーマンスを潜在的に改善するためのフレームワークです。一連のトップランクのドキュメントの関連性を最大化するという標準検索タスクの目的とは異なり、RAGシステムの目的は、ドキュメントのユーティリティがLLMプロンプトの追加コンテキスト情報の一部としてそれを含めることがダウンストリームタスクを改善するかどうかを示します。既存の研究では、知識集約型の言語タスク（KILT）のRAGコンテキストの関連性の役割を調査します。対照的に、私たちの仕事では、関連性は、情報を求めるタスクのクエリとドキュメントの間の局所的な重複の関連性に対応しています。具体的には、IRテストコレクションを利用して、局所的に関連するドキュメントで構成されるRAGコンテキストが下流のパフォーマンスの改善につながるかどうかを経験的に調査します。私たちの実験は、次の発見につながります。（a）関連性と有用性の間には小さな正の相関があります。 （b）この相関は、コンテキストサイズの増加とともに減少します（k-shotのkの値が高い）。 （c）より効果的な検索モデルは、一般に、下流のラグパフォーマンスの向上につながります。 Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n', \"DOI: 10.48550/arXiv.2505.18247\\n Title: メタゲンブレンドラグ：特殊なドメインの質問を解決するためのゼロショット精度のロックを解除する MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索された生成（RAG）は、ドメイン固有のエンタープライズデータセットとの闘いであり、しばしばファイアウォールの背後に隔離され、トレーニング前にLLMSによって見えない複雑で特殊な用語が豊富です。医学、ネットワーキング、または法律などのドメイン間のセマンティックな変動は、Ragのコンテキストの精度を妨げますが、微調整ソリューションはコストがかかり、遅く、新しいデータが出現するにつれて一般化が欠けています。微調整せずにレトリーバーでゼロショット精度を達成することは、依然として重要な課題です。メタデータの生成パイプラインとハイブリッドクエリインデックスを介してセマンティックレトリバーを強化する新しいエンタープライズ検索アプローチである「メタゲンブレンドラグ」を紹介します。重要な概念、トピック、頭字語を活用することにより、メタデータが豊富なセマンティックインデックスを作成し、ハイブリッドクエリをブーストし、微調整せずに堅牢でスケーラブルなパフォーマンスを提供します。 Biomedical PubMedqaデータセットでは、Metagenブレンドラグは82％の回収精度と77％のRAG精度を達成し、以前のゼロショットラグベンチマークをすべて上回り、そのデータセットの微調整されたモデルに匹敵し、SquadやNQのようなデータセットでも優れています。このアプローチは、特殊なドメイン全体で比類のない一般化を備えたセマンティックレトリバーを構築するための新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\", 'DOI: 10.1609/aaai.v38i16.29728\\n Title: 検索された世代の大規模な言語モデルのベンチマーク Benchmarking Large Language Models in Retrieval-Augmented Generation\\nAbstract: 検索された生成（RAG）は、大規模な言語モデル（LLM）の幻覚を緩和するための有望なアプローチです。ただし、既存の研究には、さまざまな大規模な言語モデルに対する検索された生成の影響に関する厳密な評価がありません。これにより、異なるLLMのRAGの機能における潜在的なボトルネックを特定することが困難になります。この論文では、検索された生成が大規模な言語モデルに与える影響を体系的に調査します。ノイズの堅牢性、否定的な拒否、情報統合、反事実的堅牢性など、RAG\\u200b\\u200bに必要な4つの基本能力におけるさまざまな大手言語モデルのパフォーマンスを分析します。この目的のために、英語と中国語の両方でRAG評価のための新しいコーパスである検索された生成ベンチマーク（RGB）を確立します。 RGBは、ベンチマーク内のインスタンスを、ケースを解決するために必要な前述の基本能力に基づいて、4つの個別のテストベッドに分割します。次に、RGBの6つの代表LLMを評価して、RAGを適用する際に現在のLLMの課題を診断します。評価により、LLMはある程度のノイズの堅牢性を示していますが、否定的な拒絶、情報統合、誤った情報への対処に関して依然として著しく苦労していることが明らかになりました。前述の評価の結果は、LLMにRAGを効果的に適用するために、まだかなりの旅がまだあることを示しています。 Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.13213', '10.1007/978-3-031-88708-6_3', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2505.18247', '10.1609/aaai.v38i16.29728']\n",
      "\u001b[93mSummary: RAG (Retrieval-Augmented Generation) is a framework that incorporates external knowledge to improve the performance of downstream tasks like question answering.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213\n",
      "RAG enables the retrieval of relevant information from external sources, allowing large language models (LLMs) to answer queries over unseen document collections. However, traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. A new method called Multi-Meta-RAG uses database filtering with LLM-extracted metadata to improve the selection of relevant documents from various sources.\n",
      "\n",
      "DOI: 10.1007/978-3-031-88708-6_3\n",
      "RAG is a framework for incorporating external knowledge, usually in the form of a set of documents, as part of a prompt to an LLM. The objective of a RAG system is to maximise the total utility of the documents, where the utility indicates whether including the document as additional contextual information improves a downstream task. Existing studies investigate the role of relevance in RAG for knowledge-intensive language tasks.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "This paper proposes a method called M-RAG (Metadata Retrieval-Augmented Generation) for effective search in open-domain question answering systems. M-RAG utilises embeddings with metadata and generative models like GPT-3.5-Turbo-16K and GPT-4 for automated answer generation. Experimental results show a 46% performance improvement compared to an external inference ODQA system and a 6% improvement over existing RAG methods.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247\n",
      "RAG struggles with domain-specific enterprise datasets, often isolated behind firewalls and containing complex, specialised terminology unseen by LLMs during pre-training. Semantic variability across domains hampers RAG's context precision, and fine-tuning solutions are costly and lack generalisation. A new approach called MetaGen Blended RAG enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieved 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks.\n",
      "\n",
      "DOI: 10.1609/aaai.v38i16.29728\n",
      "RAG is a promising approach for mitigating hallucinations in LLMs. This paper systematically investigates the impact of RAG on LLMs, analysing their performance in four fundamental abilities required for RAG. The evaluation reveals that LLMs exhibit noise robustness but struggle with negative rejection, information integration, and dealing with false information.\n",
      "My lady, I hope these findings provide a comprehensive understanding of how RAG improves question answering and information retrieval systems.\n",
      "45\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.956\n",
      "Recall: 0.956\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.887\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9997565, '10.48550/arXiv.2406.13213'), (0.999617, '10.1007/978-3-031-88708-6_3'), (0.9992563, '10.6109/jkiice.2023.27.12.1489'), (0.9956006, '10.48550/arXiv.2505.18247'), (0.8362874, '10.1609/aaai.v38i16.29728')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1371/journal.pbio.1002542\\n Title: 引用メトリック：正規化する方法（NOT）の入門書 Citation Metrics: A Primer on How (Not) to Normalize\\nAbstract: 引用指標は、公開された研究を評価するためにますます使用されています。課題の1つは、科学分野の違い、年齢（出版年）、ドキュメントの種類、データベースカバレッジ、およびその他の要因を説明するために、これらのメトリックを正規化するかどうか、および方法です。さまざまなアプローチを使用して、正規化の長所と短所について説明します。科学者、機関、雑誌、または国のコーパスを評価するために、複数の論文で引用指標を組み合わせる必要がある場合、および多著者の論文でクレジットを属性にしようとする場合、追加の課題が現れます。異なる引用メトリックは補完的な洞察を提供する可能性がありますが、計算の根底にある仮定を慎重に検討する必要があります。 Citation metrics are increasingly used to appraise published research. One challenge is whether and how to normalize these metrics to account for differences across scientific fields, age (year of publication), type of document, database coverage, and other factors. We discuss the pros and cons for normalizations using different approaches. Additional challenges emerge when citation metrics need to be combined across multiple papers to appraise the corpus of scientists, institutions, journals, or countries, as well as when trying to attribute credit in multiauthored papers. Different citation metrics may offer complementary insights, but one should carefully consider the assumptions that underlie their calculation.\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542']\n",
      "\u001b[93mMy lady, the main challenges in normalizing citation metrics across scientific fields include:\n",
      "\n",
      "- **Differences across scientific fields**: Citation metrics need to account for variations in scientific disciplines, age (year of publication), document type, database coverage, and other factors.\n",
      "- **Combining metrics across multiple papers**: When citation metrics are combined to evaluate the corpus of scientists, institutions, journals, or countries, additional challenges arise.\n",
      "- **Multiauthored papers**: Attributing credit in papers with multiple authors can be complex.\n",
      "- **Complementary insights**: Different citation metrics may offer complementary insights, but one must carefully consider the assumptions underlying their calculation.\n",
      "\n",
      "DOI: 10.1371/journal.pbio.1002542\n",
      "\n",
      "This text discusses the challenges of normalizing citation metrics and provides insights into the pros and cons of different normalization approaches.\n",
      "45\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.600\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.99984646, '10.1371/journal.pbio.1002542')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.5281/ZENODO.13960973\\n Title: 欠落しているデータパターンを使用して、書誌データセットで誤って割り当てられた記事を検出する Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\\nAbstract: DORA宣言とCoaraは、オープンデータに基づいた書誌指標の使用を求めています。ただし、確立された学術的メタデータデータセットは閉じられており、オープンデータセットの品質はまだ徹底的に調査されていません。この論文では、欠落データパターンを使用してデータセット内のエラーを検出する方法を提示します。例として、この方法は、ETHチューリッヒに関連する出版物の所属メタデータに適用されます。これにより、一連の誤って提携した論文を特定することができます。このペーパーで導入された方法は、提携データ用に特別に設計されておらず、他のタイプのデータのエラーを検出するためにも使用できます。それは、プロバイダーとデータのユーザーに利益をもたらすことを願っている修正につながる可能性があります。 The DORA declaration and CoARA call for the use of bibliometric indicators based on open data. However, established scholarly metadata datasets are closed, and the quality of open datasets has not yet been thoroughly examined. In this paper, I present a method to detect errors in a dataset using missing data patterns. As an example, the method is applied to the affiliation metadata of publications associated with ETH Zurich. This allows me to identify a series of incorrectly affiliated papers. The method introduced in this paper is not specifically designed for affiliation data and can also be used to detect errors in other types of data. It could lead to corrections which will hopefully benefit providers as well as users of data.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: CrossRefデータのDOIエラーによる無効な引用の識別と修正 Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この作業は、CrossRefで利用可能なオープンな参考文献メタデータを分析することにより、DOIの間違いのクラスを特定し、どの出版社がそのような間違いを担当し、これらの誤ったDOIの数を自動プロセスで修正できるかを強調することを目的としています。 By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs.これらの引用は、CrossRefの関連する引用データをアップロードする責任を負う責任者の有効性と出版社を追跡することで処理しました。最後に、無効なDOIの事実上の誤りのパターンと、それらをキャッチして修正するために必要な正規表現を特定しました。この研究の結果は、無効な引用の大部分に責任を負い、および/または影響を受けた出版社は少数であることを示しています。過去の研究で提案されたDOI名エラーの分類法を拡張し、以前のアプローチよりも無効なDOIでより多くのミスをきれいにすることができる、より詳細に精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的観点からDOIの間違いの可能な理由を調査し、出版社が無効な引用データの生産の根底にある問題を特定するのに役立ちます。また、私たちが提示するDOIクリーニングメカニズムは、既存のプロセス（COCIなど）に統合して、間違ったDOIを自動的に修正することで引用を追加できます。この研究は、オープンサイエンスの原則に厳密に従っていたため、私たちの研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.5281/ZENODO.13960973', '10.1007/s11192-022-04367-w']\n",
      "\u001b[93mMy lady, here are some methods used to detect and correct errors in bibliographic datasets:\n",
      "\n",
      "## Summary:\n",
      "Missing data patterns can be used to detect errors in datasets.\n",
      "\n",
      "## DOI: 10.5281/ZENODO.13960973\n",
      "The DORA declaration and CoARA call for the use of bibliometric indicators based on open data. However, established scholarly metadata datasets are closed, and the quality of open datasets has not yet been thoroughly examined. This paper presents a method to detect errors in a dataset using missing data patterns. For example, the method is applied to the affiliation metadata of publications associated with ETH Zurich, allowing the identification of incorrectly affiliated papers. The method can also be used to detect errors in other types of data, potentially leading to corrections that benefit both data providers and users.\n",
      "\n",
      "## DOI: 10.1007/s11192-022-04367-w\n",
      "This study aims to identify classes of DOI mistakes by analysing open bibliographic metadata available in Crossref. It highlights which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. The study used a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years. The citations were retrieved from the January 2021 Crossref dump and processed by tracking their validity and the publishers responsible for uploading the related citation data in Crossref. The study identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes showed that only a few publishers were responsible for and/or affected by the majority of invalid citations. The study extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. The DOI cleaning mechanism presented could be integrated into existing processes to add citations by automatically correcting wrong DOIs.\n",
      "45\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.933\n",
      "Recall: 0.933\n",
      "F1-Score: 0.933\n",
      "Accuracy: 0.933\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.99960643, '10.5281/ZENODO.13960973'), (0.6302299, '10.1007/s11192-022-04367-w')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/978-3-031-88708-6_3\\n Title: 関連性はレトリバーからジェネレーターにぼろきれに伝播されますか？ Is Relevance Propagated from Retriever to Generator in RAG?\\nAbstract: 検索拡張生成（RAG）は、通常、コレクションから取得された一連のドキュメントの形で、プロンプトの大規模な言語モデル（LLM）への一連のドキュメントの形で、質問の回答などの下流タスクのパフォーマンスを潜在的に改善するためのフレームワークです。一連のトップランクのドキュメントの関連性を最大化するという標準検索タスクの目的とは異なり、RAGシステムの目的は、ドキュメントのユーティリティがLLMプロンプトの追加コンテキスト情報の一部としてそれを含めることがダウンストリームタスクを改善するかどうかを示します。既存の研究では、知識集約型の言語タスク（KILT）のRAGコンテキストの関連性の役割を調査します。対照的に、私たちの仕事では、関連性は、情報を求めるタスクのクエリとドキュメントの間の局所的な重複の関連性に対応しています。具体的には、IRテストコレクションを利用して、局所的に関連するドキュメントで構成されるRAGコンテキストが下流のパフォーマンスの改善につながるかどうかを経験的に調査します。私たちの実験は、次の発見につながります。（a）関連性と有用性の間には小さな正の相関があります。 （b）この相関は、コンテキストサイズの増加とともに減少します（k-shotのkの値が高い）。 （c）より効果的な検索モデルは、一般に、下流のラグパフォーマンスの向上につながります。 Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n', \"DOI: 10.48550/arXiv.2404.13948\\n Title: ぼろきれの背中を壊したYPO ypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\\nAbstract: 最近の大規模な言語モデル（LLMS）の堅牢性は、さまざまなドメインや現実世界のアプリケーションで適用性が拡大するにつれてますます重要になっています。検索された生成（RAG）は、LLMの限界に対処するための有望なソリューションですが、RAGの堅牢性に関する既存の研究は、しばしば、RAGコンポーネント間の相互接続された関係またはマイナーなテキストエラーなどの実際のデータベースで一般的な潜在的な脅威を見落としています。この作業では、RAGの堅牢性を評価する際に、2つの未掘りの側面を調査します。1）低レベルの摂動を通じて騒々しい文書に対する脆弱性と2）Ragの堅牢性の全体的な評価。さらに、これらの側面をターゲットにする新しい攻撃法であるRag（\\\\ textit {garag}）に対する遺伝的攻撃を紹介します。具体的には、GARAGは各コンポーネント内の脆弱性を明らかにし、騒々しいドキュメントに対してシステム全体の機能をテストするように設計されています。 \\\\ textIT {garag}を標準のQAデータセットに適用し、多様なレトリバーとLLMを組み込んで、ragの堅牢性を検証します。実験結果は、Garagが一貫して高い攻撃の成功率を達成することを示しています。また、各コンポーネントのパフォーマンスとその相乗効果を大幅に破壊し、現実世界のぼろきれシステムを混乱させる際にマイナーなテキストの不正確さがもたらす実質的なリスクを強調しています。 The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\\\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\\n\"]\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1007/978-3-031-88708-6_3', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2404.13948']\n",
      "\u001b[93mMy lady, RAG, or Retrieval Augmented Generation, is a framework for incorporating external knowledge into a large language model (LLM) to improve the performance of downstream tasks such as question answering.\n",
      "\n",
      "RAG enables the retrieval of relevant information from external knowledge sources, allowing LLMs to answer queries over previously unseen document collections. However, traditional RAG applications have been shown to perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence.\n",
      "\n",
      "To address this, a new method called Multi-Meta-RAG has been introduced, which uses database filtering with LLM-extracted metadata to improve the selection of relevant documents from various sources. This method has been found to greatly improve results on the MultiHop-RAG benchmark.\n",
      "\n",
      "In addition, a method called Metadata Retrieval-Augmented Generation (M-RAG) has been proposed for effective search in open-domain question answering (ODQA) systems. M-RAG utilises embeddings that include metadata and employs generative models such as GPT-3.5-Turbo-16K and GPT-4 for automated answer generation. Experimental results have shown that M-RAG exhibits up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over existing RAG methods.\n",
      "\n",
      "However, the robustness of RAG has been a subject of investigation, as existing studies often overlook the interconnected relationships between RAG components or potential threats prevalent in real-world databases, such as minor textual errors. A novel attack method called Genetic Attack on RAG (GARAG) has been introduced to target these aspects and test the overall system functionality against noisy documents. Experimental results have shown that GARAG consistently achieves high attack success rates, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\n",
      "\n",
      "DOI: 10.1007/978-3-031-88708-6_3\n",
      "DOI: 10.48550/arXiv.2406.13213\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "DOI: 10.48550/arXiv.2404.13948\n",
      "45\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.675\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.94834626, '10.1007/978-3-031-88708-6_3'), (0.5391045, '10.48550/arXiv.2406.13213'), (0.38225675, '10.6109/jkiice.2023.27.12.1489'), (0.37410447, '10.48550/arXiv.2404.13948')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00286\\n Title: 8つのフリーアクセス学術データベースの出版メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースにおけるメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000を超えるレコードのランダムなCrossRefサンプルを選択し、7つのデータベース（Dimensions、Google Scholar、Microsoft Academic、Openalex、Scilit、Semantic Sc\\u200b\\u200bholar、およびThe Lens）で検索されました。この情報、これらのフィールドの完全性レート、およびデータベース間の合意を説明するフィールドを観察するために、7つの特性（要約、アクセス、書誌情報、書誌情報、文書の種類、公開日、言語、識別子）を観察しました。結果は、アカデミック検索エンジン（Google Scholar、Microsoft Academic、およびSemantic Sc\\u200b\\u200bholar）が少ない情報を収集し、完全性が低いことを示しています。逆に、サードパーティのデータベース（寸法、OpenAlex、Scilit、およびレンズ）は、メタデータの品質が高く、完全性が高くなります。アカデミック検索エンジンには、Webをcrawって信頼できる記述データを取得する能力がないと結論付けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することから得られる情報の喪失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: メタエンハンス：大学図書館の電子論文と学位論文のメタデータの品質改善 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタルオブジェクトがデジタルライブラリインターフェイスを通じて発見されるために重要です。ただし、さまざまな理由により、デジタルオブジェクトのメタデータは、しばしば不完全で、一貫性がなく、誤った値を示します。ケーススタディとして、電子論文と論文（ETD）の7つの重要な分野を使用して、学術メタデータを自動的に検出、修正、および正規化する方法を調査します。メタエンハンスを提案します。メタエンハンスは、これらの分野の品質を改善するために最先端の人工知能方法を利用するフレームワークです。 Metaenhanceを評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 ETDを含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークでMetaenhanceをテストし、提案された方法は、7つのフィールドのうち5つで0.85から1.00の範囲のエラーとF1スコアの検出にほぼ完全なF1スコアを達成したことを発見しました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化全体のメタデータの品質の問題を特定する Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、標準的な形式でコンテキスト、技術、および管理情報を提供することにより、発見とアクセスに不可欠です。しかし、メタデータは、社会文化的表現、リソースの制約、および標準化されたシステムの間の緊張の場所でもあります。正式および非公式の介入は、品質の問題、アイデンティティを主張する政治的行為、または視界を最大化するための戦略的選択と解釈される場合があります。これに関連して、メタデータの品質、一貫性、および完全性が個人やコミュニティにどのように影響するかを理解しようとしました。記録のサンプルをレビューして、メタデータとコミュニティが互いに反対して、意図的に文化的な意味を反映する（またはそうでない）方法に起因する問題を特定して分類しました。 Metadata are crucial for discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. In this context, we sought to understand how metadata quality, consistency, and completeness impact individuals and communities. Reviewing a sample of records, we identified and classified issues stemming from how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101']\n",
      "\u001b[93mMy lady, here are the studies that examined the abstract in metadata:\n",
      "\n",
      "## Summary: Three studies examined the abstract in metadata, focusing on metadata quality, completeness, and improvement.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00286\n",
      "This study compared the amount of metadata and the completeness degree of research publications in new academic databases. The abstract was one of the seven characteristics analysed to observe fields that describe information, the completeness rate of these fields, and the agreement among databases.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2303.17661\n",
      "The abstract discusses the importance of metadata quality for digital objects to be discovered through digital library interfaces. The study proposes MetaEnhance, a framework that utilises state-of-the-art artificial intelligence methods to improve the quality of metadata in seven key fields of electronic theses and dissertations.\n",
      "\n",
      "## DOI: 10.5860/crl.86.1.101\n",
      "This study sought to understand how metadata quality, consistency, and completeness impact individuals and communities. The abstract was used to review a sample of records, identifying and classifying issues stemming from how metadata and communities intentionally reflect (or not) cultural meanings.\n",
      "45\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.575\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.4213137, '10.1162/qss_a_00286'), (0.19544049, '10.48550/arXiv.2303.17661'), (0.12951279, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0\n",
      "Length of documents: 45\n",
      "reranked_documents: []\n",
      "length of reranked_documents: 0\n",
      "Retrieved DOIs: []\n",
      "\u001b[93mMy lady, could you please provide me with some context or a specific topic you are interested in? I can then assist you in finding studies that examined citations within that field.\n",
      "45\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.500\n",
      "Faithfulness score: 0\n",
      "Documents score: []\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.1007/s11192-023-04923-y\\n Title: Openalexの不足している機関：考えられる理由、意味、および解決策 Missing institutions in OpenAlex: possible reasons, implications, and solutions\\nAbstract: オープンサイエンスの出現では、データ品質が高いオープンデータプラットフォームが必要です。 2022年1月に開始されたグローバルな研究システムの完全に開かれたカタログとして、OpenAlexは、定量的科学研究で広く使用されている簡単なデータアクセシビリティと幅広いデータカバレッジの2つの主要な利点を特徴としています。驚くべきことに、Openalexはライデン大学のランキングの重要なデータソースとして採用されています。ただし、Openalexのジャーナル記事メタデータには、機関が欠落しているという深刻なデータ品質の問題があります。この研究では、3種類の制度情報（FII）、部分的に欠落している機関情報（PMII）、および完全に欠落している機関情報（CMII）を定義することにより、問題とその結果と解決策の考えられる理由を調査します。私たちの結果は、不足している機関の問題がOpenalexのジャーナル記事の60％以上で発生することを示しています。この問題は、特に初期のメタデータや社会科学や人文科学で広まっています。データのサブサンプルを使用して、問題の考えられる理由、歪んだ結果のリスク、および不足している機関の問題に対する可能な解決策をさらに調査します。目的は、オープンリソースのデータ品質改善の重要性を高め、定量的科学研究およびより広い文脈でのオープンリソースの責任ある使用をサポートすることです。 The advent of open science calls for open data platforms with high data quality. As a fully open catalog of the global research system launched in January 2022, OpenAlex features two main advantages of easy data accessibility and broad data coverage, which has been widely used in quantitative science studies. Remarkably, OpenAlex is adopted as an important data source for Leiden university ranking. However, there is a severe data quality problem of missing institutions in journal article metadata in OpenAlex. This study investigates the possible reasons for the problem and its consequences and solutions by defining three types of institutional information—full institutional information (FII), partially missing institutional information (PMII) and completely missing institutional information (CMII). Our results show that the problem of missing institutions occurs in more than 60% of the journal articles in OpenAlex. The problem is particularly widespread in metadata from the early years and in the social sciences and humanities. Using sub-samples of the data, we further explore the possible reasons for the problem, the risk it might represent for distorted results, and possible solutions to the problem of missing institutions. The aim is to raise the importance of data quality improvements in open resources, and thus to support the responsible use of open resources in quantitative science studies and also in broader contexts.\\n', 'DOI: 10.1590/SciELOPreprints.11205\\n Title: ユニバーサルインデックスへのオープンロードで：OpenAlexおよびOpenJournal Systems On the Open Road to Universal Indexing: OpenAlex and OpenJournal Systems\\nAbstract: この調査では、OpenAlexのオープンジャーナルシステム（JUOJS）を使用したジャーナルのインデックス作成を検証し、包括的な学術参加をサポートする2つのオープンソースソフトウェアイニシアチブを反映しています。 47,625のアクティブなJuojsのデータセットを分析することにより、これらのジャーナルの71％がOpenAlexで少なくとも1つの記事をインデックス付けされていることを明らかにします。私たちの調査結果は、OpenAlexに含まれるCrossRef doiを使用してジャーナルの97％を使用して、インデックス作成の達成におけるCrossRef DOIの中心的な役割を強調しています。ただし、この技術的依存は、特に低所得国（Juojsの47％）および非英語言語ジャーナル（Juojsの55％-64％）からのリソース制限されたジャーナル（Juojsの55％-64％）の雑誌として、より広範な構造的不平等を反映しています。私たちの研究は、学術インフラストラクチャの依存関係の理論的意味と、世界的な知識の可視性における体系的な格差を永続させる上でのその役割を強調しています。 OpenAlexのような包括的な書誌データベースでさえ、世界規模で公平な索引付けを促進するために、財務、インフラ、および言語の障壁に積極的に対処する必要があると主張します。インデックス作成メカニズム、永続的な識別子、および構造的不平等との関係を概念化することにより、この研究は、グローバルな多言語学術生態系における普遍的なインデックス作成のダイナミクスとその実現を再考するための重要なレンズを提供します。 This study examines OpenAlex’s indexing of journals using Open Journal Systems (JUOJS), reflecting two open source software initiatives supporting inclusive scholarly participation. By analyzing a dataset of 47,625 active JUOJS, we reveal that 71% of these journals have at least one article indexed in OpenAlex. Our findings underscore the central role of Crossref DOIs in achieving indexing, with 97% of the journals using Crossref DOIs included in OpenAlex. However, this technical dependency reflects broader structural inequities, as resource-limited journals, particularly those from low-income countries (47% of JUOJS) and non-English language journals (55%-64% of JUOJS), remain underrepresented. Our work highlights the theoretical implications of scholarly infrastructure dependencies and their role in perpetuating systemic disparities in global knowledge visibility. We argue that even inclusive bibliographic databases like OpenAlex must actively address financial, infrastructural, and linguistic barriers to foster equitable indexing on a global scale. By conceptualizing the relationship between indexing mechanisms, persistent identifiers, and structural inequities, this study provides a critical lens for rethinking the dynamics of universal indexing and its realization in a global, multilingual scholarly ecosystem.\\n', 'DOI: 10.48550/arXiv.2404.01985\\n Title: 彼はOpenalex、Scopus、Web of Scienceのオープンアクセスカバレッジ he open access coverage of OpenAlex, Scopus and Web of Science\\nAbstract: Diamond Open Access（OA）ジャーナルは、著者と読者の両方に無料の公開モデルを提供しますが、主要な書誌データベースでのインデックスの欠如は、これらのジャーナルの取り込みを評価する際の課題を提示します。さらに、出版言語や出版国などのOAの特性は、OAジャーナルがより多様であり、地域社会にサービスを提供することを目指しているという議論を支持するためにしばしば使用されてきましたが、OAジャーナルの地理的および言語的特性に関連する経験的証拠の現在の欠如があります。 OpenAlexとオープンアクセスジャーナルのディレクトリをベンチマークとして使用して、このペーパーでは、フィールド、国、言語による科学とスコープスのWebでの著者とジャーナルの報道を通じて、ダイヤモンドと金の報道を調査します。結果は、WOSとSCOPUSでのより低いカバレッジ、およびダイヤモンドOAの局所範囲を示しています。英語のみのジャーナルのシェアは、ゴールドジャーナルの間でかなり高くなっています。高所得国は、社会科学と人文科学のダイヤモンドジャーナルを除き、すべてのドメインとジャーナルの種類で著者のシェアが最も高い。ダイヤモンドOAインデックスの現在の景観を理解することは、より包括的なOAモデルに向けて政策と実践を進めることで、学術通信ネットワークを支援することができます。 Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2401.16359', '10.48550/arXiv.2404.17663', '10.1007/s11192-023-04923-y', '10.1590/SciELOPreprints.11205', '10.48550/arXiv.2404.01985']\n",
      "\u001b[93m## Summary: OpenAlex is an open-source platform for scholarly metadata, offering easy data accessibility and broad coverage.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2401.16359\n",
      "This report compares OpenAlex's reference coverage and metadata to established databases like Web of Science and Scopus. It finds that OpenAlex has comparable average source reference numbers and internal coverage rates. However, the rapidly evolving nature of OpenAlex raises questions about data trustworthiness.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.17663\n",
      "This study assesses OpenAlex's suitability for bibliometric analyses. It concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for certain analyses, particularly at the country level. However, issues with metadata accuracy and completeness require further research to fully understand and address OpenAlex's limitations.\n",
      "\n",
      "## DOI: 10.1007/s11192-023-04923-y\n",
      "This research explores the problem of missing institutions in OpenAlex's journal article metadata. It defines three types of institutional information and finds that over 60% of journal articles in OpenAlex lack institutional data. The issue is particularly prevalent in early metadata and the social sciences and humanities.\n",
      "\n",
      "## DOI: 10.1590/SciELOPreprints.11205\n",
      "This study examines OpenAlex's indexing of journals using Open Journal Systems (JUOJS). It reveals that 71% of journals have at least one article indexed in OpenAlex and highlights the central role of Crossref DOIs in achieving indexing. However, the technical dependency reflects broader structural inequities, particularly for resource-limited journals from low-income countries and non-English language journals.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.01985\n",
      "This paper investigates the open access coverage of OpenAlex, Scopus, and Web of Science. It uses OpenAlex and the Directory of Open Access Journals as benchmarks to explore the coverage of diamond and gold journals through authorship and journal coverage. The results show lower coverage in WoS and Scopus and a local scope for diamond open access.\n",
      "\n",
      "My lady, I hope these insights into OpenAlex are of use to you.\n",
      "45\n",
      "For query: ['Tell me about OpenAlex.']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.773\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9965301, '10.48550/arXiv.2401.16359'), (0.9940428, '10.48550/arXiv.2404.17663'), (0.99040353, '10.1007/s11192-023-04923-y'), (0.9678993, '10.1590/SciELOPreprints.11205'), (0.96405166, '10.48550/arXiv.2404.01985')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: CrossRef：コミュニティ所有の学術メタデータの持続可能な供給源 Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このホワイトペーパーでは、CrossRefによって収集および利用可能になった学術的メタデータと、学術研究の生態系におけるその重要性について説明します。 1億600万件以上の記録を含み、年間平均11％のレートで拡大するCrossrefのメタデータは、出版社、著者、図書館員、資金提供者、および研究者向けの学術データの主要な情報源の1つになりました。メタデータセットは、ジャーナルやカンファレンスペーパーなどの従来のタイプだけでなく、データセット、レポート、プリプリント、ピアレビュー、助成金など、13のコンテンツタイプで構成されています。メタデータは、基本的な出版物メタデータに限定されませんが、全文、資金調達とライセンス情報、引用リンク、修正、更新、撤回などに関する情報への要約とリンクを含めることもできます。メタデータは、REST APIやOAI-PMHを含む多くのAPIを通じて利用できます。この論文では、CrossRefが提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、CrossRefの研究エコシステムにおける役割と、引用データの提供の進化など、長年にわたるメタデータのキュレーションの傾向にも注目しています。 Crossrefのメタデータで使用される研究を要約し、将来のメタデータの品質と検索を改善する計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022']\n",
      "\u001b[93mMy lady, Crossref is a sustainable source of community-owned scholarly metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref's metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including traditional types such as journals and conference papers, as well as data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata but can also include abstracts and links to full text, funding and license information, citation links, and information about corrections, updates, and retractions. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications.\n",
      "45\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.556\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.8345418, '10.1162/qss_a_00022')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: オープンアレックスの言語カバレッジの評価：メタデータの精度と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: ClarivateのWeb of Science（WOS）とElsevierのScopusは、数十年にわたって書誌情報の主要なソースでした。高度にキュレーションされていますが、これらの閉鎖された独自のデータベースは、英語の出版物に大きく偏っており、研究普及における他の言語の使用を過小評価しています。 2022年に発売されたOpenalexは、包括的、包括的、オープンソースの研究情報を約束しました。すでに学者や研究機関が使用している間、そのメタデータの質は現在評価されています。この論文は、言語に関連するOpenalexのメタデータの完全性と正確性、WOSとの比較、および6,836の記事のサンプルの詳細な手動検証を通じて、この文献に貢献しています。結果は、オープンアレックスがWOSよりもはるかにバランスのとれた言語カバレッジを示すことを示しています。ただし、言語メタデータは必ずしも正確ではないため、Openalexは他の言語のそれを過小評価しながら英語の場所を過大評価します。批判的に使用すると、OpenAlexは学術出版に使用される言語の包括的かつ代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャレベルでより多くの作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8つのフリーアクセス学術データベースの出版メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースにおけるメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000を超えるレコードのランダムなCrossRefサンプルを選択し、7つのデータベース（Dimensions、Google Scholar、Microsoft Academic、Openalex、Scilit、Semantic Sc\\u200b\\u200bholar、およびThe Lens）で検索されました。この情報、これらのフィールドの完全性レート、およびデータベース間の合意を説明するフィールドを観察するために、7つの特性（要約、アクセス、書誌情報、書誌情報、文書の種類、公開日、言語、識別子）を観察しました。結果は、アカデミック検索エンジン（Google Scholar、Microsoft Academic、およびSemantic Sc\\u200b\\u200bholar）が少ない情報を収集し、完全性が低いことを示しています。逆に、サードパーティのデータベース（寸法、OpenAlex、Scilit、およびレンズ）は、メタデータの品質が高く、完全性が高くなります。アカデミック検索エンジンには、Webをcrawって信頼できる記述データを取得する能力がないと結論付けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することから得られる情報の喪失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.1162/qss_a_00286']\n",
      "\u001b[93mMy lady, here are two papers that evaluate the linguistic coverage or language-related metadata in scholarly databases:\n",
      "\n",
      "## Summary: Papers evaluating the linguistic coverage of OpenAlex and the completeness of publication metadata in eight free-access scholarly databases.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2409.10633\n",
      "Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "Abstract: This paper assesses the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with Clarivate's Web of Science (WoS), as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00286\n",
      "Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Abstract: This study compares the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, the authors selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases. Seven characteristics were analyzed to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines gather less information and have a low degree of completeness, while third-party databases have more metadata quality and a higher completeness rate.\n",
      "45\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.588\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.9998287, '10.48550/arXiv.2409.10633'), (0.4870177, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープンファンダーメタデータの可用性と完全性：オランダの研究評議会によって資金提供された出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金を提供する研究の結果に関する情報を収集するためにかなりの努力を費やしています。資金提供者が資金調達に関連する出版物の出力を追跡するのを支援するために、CrossRefは2013年にFundRefを開始し、出版社が永続的な識別子を使用して資金情報を登録できるようにしました。ただし、資金調達の研究の結果であるため、資金提供者のメタデータを含める必要があるため、Funder Metadataのカバレッジを評価することは困難です。この論文では、研究者が特定の資金提供機関による資金提供の結果であるオランダ研究評議会NWOが報告した5,004の出版物を調べました。これらの記事の67％のみがCrossRefの資金情報を含んでおり、NWOをNWOにリンクしたFunder Nameおよび/またはFunder IDとしてNWOを認めているサブセット（それぞれ53％と45％）が含まれています。 Web of Science（WOS）、Scopus、およびDimensionsはすべて、記事の全文の資金調達声明から追加の資金情報を推測することができます。レンズの資金情報は、主にCrossRefのそれに対応しており、PubMedから取得した可能性のある追加の資金情報があります。私たちは、独自のデータベースと比較して、CrossRefのメタデータの資金調達のカバレッジと完全性における出版社間の興味深い違いを観察し、資金調達のオープンメタデータの質を高める可能性を強調しています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: 資金調達COVID-19研究：オープンデータインフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 彼らが提供する資金調達の結果を分析するには、資金提供機関が資金から生じる出版物を追跡できるようにすることが不可欠です。 Covid-19に関連する研究を報告する出版物の資金調達データに焦点を当て、CrossRefでの資金提供データの可用性を研究しています。また、ScopusとWeb of Scienceの2つの独自の書誌データベースで利用可能な資金データとの比較も提示します。私たちの分析により、CrossRefの資金調達データの限られたカバレッジが明らかになりました。また、特にScopusでは、資金調達データの質に関連する問題を示しています。 CrossRefでの資金提供データの可用性を改善するための推奨事項を提供しています。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: 開いた書誌メタデータの源としてのCrossref Crossref as a source of open bibliographic metadata\\nAbstract: CrossRefの学術出版物の書誌メタデータのオープン可用性を促進するために、いくつかのイニシアチブが採用されています。 CrossRefでの6つのメタデータ要素の可用性の最新の概要を示します。参照リスト、要約、ORCID、著者の提携、資金情報、およびライセンス情報です。私たちの分析は、これらのメタデータ要素の可用性が、少なくともCrossRefで最も一般的な出版タイプであるジャーナル記事で、時間とともに改善されたことを示しています。ただし、分析では、多くの出版社が書誌メタデータの完全な開放性を実現するために追加の努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.1162/qss_a_00212', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mMy lady, here are some papers that address funding metadata, its availability, or its analysis in scholarly databases:\n",
      "\n",
      "## Summary: Papers discussing the availability and completeness of funding metadata in scholarly databases.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00210\n",
      "The availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council. This paper looks at 5,004 publications reported to be the result of funding by the Dutch Research Council. It finds that only 67% of these articles contain funding information in Crossref, with a subset acknowledging the Dutch Research Council as the funder. The paper also observes interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00212\n",
      "Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures. This paper studies the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. It presents a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. The analysis reveals limited coverage of funding data in Crossref and problems related to the quality of funding data, especially in Scopus. The paper offers recommendations for improving the open availability of funding data in Crossref.\n",
      "\n",
      "## DOI: 10.31222/osf.io/smxe5\n",
      "Crossref as a source of open bibliographic metadata. This paper presents an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. The analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, it also shows that many publishers need to make additional efforts to realise full openness of bibliographic metadata.\n",
      "45\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.688\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.9913892, '10.1162/qss_a_00210'), (0.8077641, '10.1162/qss_a_00212'), (0.7324005, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模な言語モデルの検索された生成：調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模な言語モデル（LLMS）は印象的な能力を紹介しますが、幻覚、時代遅れの知識、非透明な、追跡不可能な推論プロセスなどの課題に遭遇します。検索された生成（RAG）は、外部データベースから知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の正確性と信頼性が向上し、ドメイン固有の情報の継続的な知識の更新と統合が可能になります。 RAGは、外部データベースの広大で動的なリポジトリとLLMSの本質的な知識を相乗的に統合します。この包括的なレビューペーパーでは、素朴なぼろきれ、高度なぼろ、モジュラーラグを含むRAGパラダイムの進行に関する詳細な調査を提供します。検索、生成、増強技術を含む、RAGフレームワークの三者基盤を細心の注意を払って精査します。この論文は、これらの各重要なコンポーネントに組み込まれた最先端のテクノロジーを強調し、RAGシステムの進歩を深く理解することを提供します。さらに、このペーパーでは、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を描き、研究開発の将来の道を指摘しています。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", 'DOI: 10.1609/aaai.v38i16.29728\\n Title: 検索された世代の大規模な言語モデルのベンチマーク Benchmarking Large Language Models in Retrieval-Augmented Generation\\nAbstract: 検索された生成（RAG）は、大規模な言語モデル（LLM）の幻覚を緩和するための有望なアプローチです。ただし、既存の研究には、さまざまな大規模な言語モデルに対する検索された生成の影響に関する厳密な評価がありません。これにより、異なるLLMのRAGの機能における潜在的なボトルネックを特定することが困難になります。この論文では、検索された生成が大規模な言語モデルに与える影響を体系的に調査します。ノイズの堅牢性、否定的な拒否、情報統合、反事実的堅牢性など、RAG\\u200b\\u200bに必要な4つの基本能力におけるさまざまな大手言語モデルのパフォーマンスを分析します。この目的のために、英語と中国語の両方でRAG評価のための新しいコーパスである検索された生成ベンチマーク（RGB）を確立します。 RGBは、ベンチマーク内のインスタンスを、ケースを解決するために必要な前述の基本能力に基づいて、4つの個別のテストベッドに分割します。次に、RGBの6つの代表LLMを評価して、RAGを適用する際に現在のLLMの課題を診断します。評価により、LLMはある程度のノイズの堅牢性を示していますが、否定的な拒絶、情報統合、誤った情報への対処に関して依然として著しく苦労していることが明らかになりました。前述の評価の結果は、LLMにRAGを効果的に適用するために、まだかなりの旅がまだあることを示しています。 Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.48550/arXiv.2410.04231\\n Title: 大規模な言語モデルの検索された生成によるメタデータベースのデータ探査 Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\\nAbstract: 必要なデータセットを効果的に検索する能力を開発することは、非常に限られた利用可能なメタデータを考慮して、データユーザーが関連するデータセットを特定するのを支援するための緊急の要件です。この課題のために、サードパーティのデータの利用は、改善の貴重なソースとして浮上しています。私たちの研究では、メタデータベースのデータ発見を強化するために検索された世代（RAG）の形式を採用するデータ探索のための新しいアーキテクチャを紹介します。このシステムは、大規模な言語モデル（LLM）を外部ベクトルデータベースと統合して、多様なタイプのデータセット間のセマンティック関係を識別します。提案されたフレームワークは、不均一なデータソース間のセマンティックな類似性を評価し、データ探索を改善するための新しい方法を提供します。私たちの研究には、4つの重要なタスクに関する実験結果が含まれています。1）同様のデータセットの推奨、2）組み合わせ可能なデータセットの提案、3）タグの推定、4）変数の予測。我々の結果は、RAGが従来のメタデータアプローチと比較した場合、特に異なるカテゴリから関連するデータセットの選択を強化できることを示しています。ただし、パフォーマンスはタスクとモデルによって異なり、特定のユースケースに基づいて適切な手法を選択することの重要性を確認します。調査結果は、このアプローチがデータ調査と発見の課題に対処するための約束を保持していることを示唆していますが、推定タスクにはさらなる改良が必要です。 Developing the capacity to effectively search for requisite datasets is an urgent requirement to assist data users in identifying relevant datasets considering the very limited available metadata. For this challenge, the utilization of third-party data is emerging as a valuable source for improvement. Our research introduces a new architecture for data exploration which employs a form of Retrieval-Augmented Generation (RAG) to enhance metadata-based data discovery. The system integrates large language models (LLMs) with external vector databases to identify semantic relationships among diverse types of datasets. The proposed framework offers a new method for evaluating semantic similarity among heterogeneous data sources and for improving data exploration. Our study includes experimental results on four critical tasks: 1) recommending similar datasets, 2) suggesting combinable datasets, 3) estimating tags, and 4) predicting variables. Our results demonstrate that RAG can enhance the selection of relevant datasets, particularly from different categories, when compared to conventional metadata approaches. However, performance varied across tasks and models, which confirms the significance of selecting appropriate techniques based on specific use cases. The findings suggest that this approach holds promise for addressing challenges in data exploration and discovery, although further refinement is necessary for estimation tasks.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1609/aaai.v38i16.29728', '10.48550/arXiv.2406.13213', '10.48550/arXiv.2410.04231', '10.6109/jkiice.2023.27.12.1489']\n",
      "\u001b[93mMy lady, here are the papers discussing the use of Retrieval-Augmented Generation (RAG) in large language models or related applications:\n",
      "\n",
      "Summary: The papers explore RAG's potential to enhance large language models (LLMs) and address challenges like hallucination and outdated knowledge.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997\n",
      "Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "This comprehensive review paper examines the progression of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG. It scrutinizes the RAG framework's tripartite foundation, highlighting state-of-the-art technologies in retrieval, generation, and augmentation techniques. The paper introduces evaluation frameworks and benchmarks, addressing current challenges and future research directions.\n",
      "\n",
      "DOI: 10.1609/aaai.v38i16.29728\n",
      "Title: Benchmarking Large Language Models in Retrieval-Augmented Generation\n",
      "The paper systematically investigates RAG's impact on LLMs, analyzing performance in noise robustness, negative rejection, information integration, and counterfactual robustness. It establishes the Retrieval-Augmented Generation Benchmark (RGB) to evaluate LLMs' challenges in applying RAG, revealing areas for improvement in dealing with false information and negative rejection.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213\n",
      "Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "Introducing Multi-Meta-RAG, the paper aims to enhance RAG selection for relevant documents from various sources for multi-hop queries. By using database filtering with LLM-extracted metadata, it improves performance in answering complex questions, demonstrating significant results on the MultiHop-RAG benchmark.\n",
      "\n",
      "DOI: 10.48550/arXiv.2410.04231\n",
      "Title: Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\n",
      "The research introduces a new architecture for data exploration, employing RAG to enhance metadata-based data discovery. The system integrates LLMs with external vector databases to identify semantic relationships among diverse datasets. Experimental results show RAG's effectiveness in selecting relevant datasets, particularly from different categories, compared to conventional metadata approaches.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "Title: M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\n",
      "Proposing the M-RAG method, the paper aims to improve open-domain question-answering systems. By utilizing metadata-based embeddings and generative models like GPT-3.5-Turbo-16K and GPT-4, M-RAG enhances answer accuracy and enables LLMs to retrieve information from external sources. Experimental results demonstrate significant performance improvements over existing RAG methods.\n",
      "45\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) in large language models or related applications?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9999918, '10.48550/arXiv.2312.10997'), (0.99997985, '10.1609/aaai.v38i16.29728'), (0.99964714, '10.48550/arXiv.2406.13213'), (0.9996345, '10.48550/arXiv.2410.04231'), (0.9891816, '10.6109/jkiice.2023.27.12.1489')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: CrossRef：コミュニティ所有の学術メタデータの持続可能な供給源 Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このホワイトペーパーでは、CrossRefによって収集および利用可能になった学術的メタデータと、学術研究の生態系におけるその重要性について説明します。 1億600万件以上の記録を含み、年間平均11％のレートで拡大するCrossrefのメタデータは、出版社、著者、図書館員、資金提供者、および研究者向けの学術データの主要な情報源の1つになりました。メタデータセットは、ジャーナルやカンファレンスペーパーなどの従来のタイプだけでなく、データセット、レポート、プリプリント、ピアレビュー、助成金など、13のコンテンツタイプで構成されています。メタデータは、基本的な出版物メタデータに限定されませんが、全文、資金調達とライセンス情報、引用リンク、修正、更新、撤回などに関する情報への要約とリンクを含めることもできます。メタデータは、REST APIやOAI-PMHを含む多くのAPIを通じて利用できます。この論文では、CrossRefが提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、CrossRefの研究エコシステムにおける役割と、引用データの提供の進化など、長年にわたるメタデータのキュレーションの傾向にも注目しています。 Crossrefのメタデータで使用される研究を要約し、将来のメタデータの品質と検索を改善する計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: 開いた書誌メタデータの源としてのCrossref Crossref as a source of open bibliographic metadata\\nAbstract: CrossRefの学術出版物の書誌メタデータのオープン可用性を促進するために、いくつかのイニシアチブが採用されています。 CrossRefでの6つのメタデータ要素の可用性の最新の概要を示します。参照リスト、要約、ORCID、著者の提携、資金情報、およびライセンス情報です。私たちの分析は、これらのメタデータ要素の可用性が、少なくともCrossRefで最も一般的な出版タイプであるジャーナル記事で、時間とともに改善されたことを示しています。ただし、分析では、多くの出版社が書誌メタデータの完全な開放性を実現するために追加の努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mSummary: Crossref is a valuable source of scholarly data for publishers, authors, librarians, funders, and researchers.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "Crossref's metadata, which includes over 106 million records and expands at an average rate of 11% a year, is a major source of scholarly data. The metadata set consists of 13 content types, including journals, conference papers, datasets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication information but can also include abstracts, full text, funding and license information, citation links, and information about corrections, updates, and retractions. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5\n",
      "Crossref promotes the open availability of bibliographic metadata of scholarly publications. Six metadata elements are available in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. The availability of these metadata elements has improved over time, particularly for journal articles, the most common publication type in Crossref. However, many publishers need to make additional efforts to achieve full openness of bibliographic metadata.\n",
      "\n",
      "Addressed, my lady.\n",
      "45\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.933\n",
      "Recall: 0.933\n",
      "F1-Score: 0.933\n",
      "Accuracy: 0.933\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.99954, '10.1162/qss_a_00022'), (0.41347715, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.3145/epi.2023.mar.09\\n Title: Bibliometricsに関連するメタデータのどれが同じであり、Microsoft Academic GraphからOpenAlexに切り替えるときにどのメタデータが異なりますか？ Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\\nAbstract: Microsoft Academic Graph（MAG）の退職の発表により、非営利団体Ourresearchは、Openalexという名前で同様のリソースを提供すると発表しました。したがって、メタデータを、最新のMAGスナップショットの書誌分析と初期のオープンアレックススナップショットと比較します。実質的にMAGのすべての作品は、書誌データの出版年、ボリューム、ファーストページと最後のページ、DOI、および引用分析の重要な要素である参照の数を保存するOpenalexに転送されました。 MAGドキュメントの90％以上がOpenAlexに同等のドキュメントタイプを持っています。残りのもののうち、特にOpenalex Document Typesの再分類ジャーナルアーティクルとブックチャプターは正しいと思われ、7％以上になります。そのため、ドキュメントタイプの仕様はMAGからOpenalexに大幅に改善されました。書誌関連のメタデータの別の項目として、MAGおよびOpenalexでの紙ベースの被験者の分類を調べました。 MAGよりもOpenAlexの対象分類割り当てを含むかなり多くのドキュメントを見つけました。第1レベルと第2レベルでは、分類構造はほぼ同じです。表形式とグラフィカルな形式の両方のレベルでの対象の再分類に関するデータを提示します。現地正規化された書誌評価における豊富な被験者の再分類の結果の評価は、本論文の範囲にありません。この未解決の質問とは別に、OpenAlexは、2021年以前の出版年のMAGと同じくらいの書誌分析に少なくともまったく適しているように見えます。 With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: Openalex、Web of Science、Scopus、PubMed、およびSemantic Sc\\u200b\\u200bholarの出版物と文書タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、次の書誌データベースの出版物と文書の種類を比較および分析します：Openalex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、Pubmed。結果は、類型が個々のデータベースプロバイダー間でかなり異なる可能性があることを示しています。さらに、出版物はそれぞれのデータベースで異なる方法で分類されているため、参考文献分析に関連するドキュメントを特定するために必要な研究と非研究テキストの区別は、データソースによって異なる場合があります。この研究の焦点は、横断段階の比較に加えて、主にオープンアレックスに含まれる出版物とドキュメントの種類のカバレッジと分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n', 'DOI: 10.48550/arXiv.2404.01985\\n Title: 彼はOpenalex、Scopus、Web of Scienceのオープンアクセスカバレッジ he open access coverage of OpenAlex, Scopus and Web of Science\\nAbstract: Diamond Open Access（OA）ジャーナルは、著者と読者の両方に無料の公開モデルを提供しますが、主要な書誌データベースでのインデックスの欠如は、これらのジャーナルの取り込みを評価する際の課題を提示します。さらに、出版言語や出版国などのOAの特性は、OAジャーナルがより多様であり、地域社会にサービスを提供することを目指しているという議論を支持するためにしばしば使用されてきましたが、OAジャーナルの地理的および言語的特性に関連する経験的証拠の現在の欠如があります。 OpenAlexとオープンアクセスジャーナルのディレクトリをベンチマークとして使用して、このペーパーでは、フィールド、国、言語による科学とスコープスのWebでの著者とジャーナルの報道を通じて、ダイヤモンドと金の報道を調査します。結果は、WOSとSCOPUSでのより低いカバレッジ、およびダイヤモンドOAの局所範囲を示しています。英語のみのジャーナルのシェアは、ゴールドジャーナルの間でかなり高くなっています。高所得国は、社会科学と人文科学のダイヤモンドジャーナルを除き、すべてのドメインとジャーナルの種類で著者のシェアが最も高い。ダイヤモンドOAインデックスの現在の景観を理解することは、より包括的なOAモデルに向けて政策と実践を進めることで、学術通信ネットワークを支援することができます。 Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2401.16359', '10.3145/epi.2023.mar.09', '10.48550/arXiv.2406.15154', '10.48550/arXiv.2404.01985']\n",
      "\u001b[93mMy lady, here is a summary of the key features and limitations of OpenAlex as a bibliometric database:\n",
      "\n",
      "## Summary:\n",
      "OpenAlex is a promising open-source database of scholarly metadata, offering free and open access to data for bibliometric studies. It has been compared favourably to established databases like Scopus and Web of Science, particularly at the country level. However, its rapidly evolving nature and the expansion of its internal data raise questions about data trustworthiness. Additionally, issues with metadata accuracy and completeness have been identified, requiring further research to fully understand and address these limitations.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.17663\n",
      "This study compares OpenAlex to Scopus across various dimensions. It concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, especially at the country level. However, it highlights the need for additional research to comprehend and address OpenAlex's limitations, particularly regarding metadata accuracy and completeness.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2401.16359\n",
      "This report investigates the reference coverage and selected metadata within OpenAlex, Web of Science, and Scopus. It finds that OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases. The report also analyses metadata by journal, revealing mixed results and capturing more ORCID identifiers, fewer abstracts, and a similar number of open-access status indicators per article compared to Web of Science and Scopus.\n",
      "\n",
      "## DOI: 10.3145/epi.2023.mar.09\n",
      "With the retirement of Microsoft Academic Graph (MAG), OpenAlex was introduced as a similar resource. This study compares the metadata between MAG and OpenAlex, finding that OpenAlex preserves bibliographic data, including publication year, volume, first and last page, DOI, and the number of references. It also notes that OpenAlex has improved document type specifications and subject classification assignments compared to MAG.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2406.15154\n",
      "This research compares publication and document types in various bibliographic databases, including OpenAlex, Scopus, Web of Science, Semantic Scholar, and PubMed. It highlights that typologies can differ considerably between database providers, and the distinction between research and non-research texts can vary, impacting the identification of relevant documents for bibliometric analysis. The study focuses on the coverage and analysis of publications and document types in OpenAlex, given its growing importance as a free alternative to established proprietary providers for bibliometric analyses.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.01985\n",
      "This paper investigates the open access coverage of OpenAlex, Scopus, and Web of Science. It uses OpenAlex and the Directory of Open Access Journals as benchmarks to study the coverage of diamond and gold journals through authorship and journal coverage in Web of Science and Scopus by field, country, and language. The results show lower coverage in WoS and Scopus for diamond OA journals, with a local scope and a higher share of English-only journals among gold journals.\n",
      "45\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.999764, '10.48550/arXiv.2404.17663'), (0.9930153, '10.48550/arXiv.2401.16359'), (0.971779, '10.3145/epi.2023.mar.09'), (0.90665317, '10.48550/arXiv.2406.15154'), (0.3060083, '10.48550/arXiv.2404.01985')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/s11192-015-1765-5\\n Title: 彼の科学とスコープスのWebの記事をジャーナル：比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌メソッドは、さまざまな目的、つまり研究評価のために複数の分野で使用されます。ほとんどの書誌分析には、Thomson ReutersのWeb of Science（WOS）とElsevierのScopusなどのデータソースが共通しています。この研究の目的は、これらの2つのデータベースのジャーナル報道を説明し、何らかの分野、出版国、言語が過剰または過小評価されているかどうかを評価することです。これを行うために、WOS（13,605のジャーナル）とScopus（20,346のジャーナル）のアクティブな学術雑誌の報道と、Ulrichの広範な定期的なディレクトリ（63,013雑誌）と比較しました。結果は、研究評価のためにWOSまたはSCOPUSのいずれかを使用すると、自然科学と工学を支持するバイアス、ならびに社会科学と芸術と人文科学を損なう生物医学的研究が導入される可能性があることを示しています。同様に、英語のジャーナルは、他の言語の損害に過大評価されています。両方のデータベースはこれらのバイアスを共有していますが、カバレッジは大幅に異なります。結果として、書誌分析の結果は、使用されるデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈では、特に異なる分野、機関、国、または言語を比較する場合、WOSとSCOPUSを注意して使用する必要があることを意味します。書誌コミュニティは、フィールド固有および国家引用指数など、WOSやSCOPUSでカバーされていない科学的生産量を含む方法と指標を開発するための努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データソースの大規模な比較：Scopus、Web of Science、Dimensions、CrossRef、およびMicrosoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: Scopus、Web of Science、Dimensions、CrossRef、およびMicrosoft Academicの5つの学際的な参考文献データソースの大規模な比較を提示します。この比較では、これらのデータソースでカバーされている2008年から2017年の期間の科学文書を考慮しています。 Scopusは、他のそれぞれのデータソースとペアワイズで比較されます。まず、ドキュメントのカバレッジのデータソース間の違いを分析します。たとえば、時間の経過とともに違い、ドキュメントタイプあたりの違い、および分野あたりの違いに焦点を当てます。次に、引用リンクの完全性と精度の違いを調べます。分析に基づいて、さまざまなデータソースの長所と短所について説明します。科学文献の包括的な報道と、文献を選択するための柔軟なフィルターのセットを組み合わせることの重要性を強調しています。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: Openalex、Web of Science、Scopus、PubMed、およびSemantic Sc\\u200b\\u200bholarの出版物と文書タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、次の書誌データベースの出版物と文書の種類を比較および分析します：Openalex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、Pubmed。結果は、類型が個々のデータベースプロバイダー間でかなり異なる可能性があることを示しています。さらに、出版物はそれぞれのデータベースで異なる方法で分類されているため、参考文献分析に関連するドキュメントを特定するために必要な研究と非研究テキストの区別は、データソースによって異なる場合があります。この研究の焦点は、横断段階の比較に加えて、主にオープンアレックスに含まれる出版物とドキュメントの種類のカバレッジと分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1007/s11192-015-1765-5', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2401.16359', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154']\n",
      "\u001b[93mMy lady, the Web of Science (WoS) is a bibliometric database that has both strengths and weaknesses.\n",
      "\n",
      "## Summary:\n",
      "WoS is a valuable resource for research evaluation, but it may introduce biases favouring certain disciplines and languages.\n",
      "\n",
      "## DOI: 10.1007/s11192-015-1765-5\n",
      "WoS, along with Scopus, has been the foundation for research in the science of science. However, these traditional databases have been found to systematically underrepresent certain disciplines and world regions. The use of WoS for research evaluation may introduce biases that favour natural sciences and engineering, as well as biomedical research, to the detriment of social sciences and arts and humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. These biases are shared by both databases, but their coverage differs substantially. As a result, the outcomes of bibliometric analyses may vary depending on the database used.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2401.16359\n",
      "WoS is a competitor to established proprietary sources, such as Scopus. It provides data freely and openly, allowing researchers to perform bibliometric studies without licensing barriers. However, as WoS is a rapidly evolving source, questions arise regarding the trustworthiness of its data, as the data contained within is expanding and quickly changing.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.17663\n",
      "WoS, along with Scopus, has been criticised for systematically underrepresenting certain disciplines and world regions. In response, new inclusive databases, such as OpenAlex, have emerged. OpenAlex is a promising open source of scholarly metadata and has been found to be a reliable alternative for some analyses, particularly at the country level. However, issues of metadata accuracy and completeness suggest that additional research is needed to fully comprehend and address OpenAlex's limitations.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2406.15154\n",
      "WoS, along with other bibliographic databases, has been found to have typologies that differ considerably between individual database providers. The distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source, as publications are classified differently in the respective databases.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00112\n",
      "WoS is one of five multidisciplinary bibliographic data sources, along with Scopus, Dimensions, Crossref, and Microsoft Academic. The comparison of these data sources highlights the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\n",
      "45\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9947187, '10.1007/s11192-015-1765-5'), (0.96073127, '10.48550/arXiv.2404.17663'), (0.71721, '10.48550/arXiv.2401.16359'), (0.63793355, '10.1162/qss_a_00112'), (0.44269672, '10.48550/arXiv.2406.15154')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.1007/978-3-031-88708-6_3\\n Title: 関連性はレトリバーからジェネレーターにぼろきれに伝播されますか？ Is Relevance Propagated from Retriever to Generator in RAG?\\nAbstract: 検索拡張生成（RAG）は、通常、コレクションから取得された一連のドキュメントの形で、プロンプトの大規模な言語モデル（LLM）への一連のドキュメントの形で、質問の回答などの下流タスクのパフォーマンスを潜在的に改善するためのフレームワークです。一連のトップランクのドキュメントの関連性を最大化するという標準検索タスクの目的とは異なり、RAGシステムの目的は、ドキュメントのユーティリティがLLMプロンプトの追加コンテキスト情報の一部としてそれを含めることがダウンストリームタスクを改善するかどうかを示します。既存の研究では、知識集約型の言語タスク（KILT）のRAGコンテキストの関連性の役割を調査します。対照的に、私たちの仕事では、関連性は、情報を求めるタスクのクエリとドキュメントの間の局所的な重複の関連性に対応しています。具体的には、IRテストコレクションを利用して、局所的に関連するドキュメントで構成されるRAGコンテキストが下流のパフォーマンスの改善につながるかどうかを経験的に調査します。私たちの実験は、次の発見につながります。（a）関連性と有用性の間には小さな正の相関があります。 （b）この相関は、コンテキストサイズの増加とともに減少します（k-shotのkの値が高い）。 （c）より効果的な検索モデルは、一般に、下流のラグパフォーマンスの向上につながります。 Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n', \"DOI: 10.48550/arXiv.2505.18247\\n Title: メタゲンブレンドラグ：特殊なドメインの質問を解決するためのゼロショット精度のロックを解除する MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索された生成（RAG）は、ドメイン固有のエンタープライズデータセットとの闘いであり、しばしばファイアウォールの背後に隔離され、トレーニング前にLLMSによって見えない複雑で特殊な用語が豊富です。医学、ネットワーキング、または法律などのドメイン間のセマンティックな変動は、Ragのコンテキストの精度を妨げますが、微調整ソリューションはコストがかかり、遅く、新しいデータが出現するにつれて一般化が欠けています。微調整せずにレトリーバーでゼロショット精度を達成することは、依然として重要な課題です。メタデータの生成パイプラインとハイブリッドクエリインデックスを介してセマンティックレトリバーを強化する新しいエンタープライズ検索アプローチである「メタゲンブレンドラグ」を紹介します。重要な概念、トピック、頭字語を活用することにより、メタデータが豊富なセマンティックインデックスを作成し、ハイブリッドクエリをブーストし、微調整せずに堅牢でスケーラブルなパフォーマンスを提供します。 Biomedical PubMedqaデータセットでは、Metagenブレンドラグは82％の回収精度と77％のRAG精度を達成し、以前のゼロショットラグベンチマークをすべて上回り、そのデータセットの微調整されたモデルに匹敵し、SquadやNQのようなデータセットでも優れています。このアプローチは、特殊なドメイン全体で比類のない一般化を備えたセマンティックレトリバーを構築するための新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\", 'DOI: 10.1609/aaai.v38i16.29728\\n Title: 検索された世代の大規模な言語モデルのベンチマーク Benchmarking Large Language Models in Retrieval-Augmented Generation\\nAbstract: 検索された生成（RAG）は、大規模な言語モデル（LLM）の幻覚を緩和するための有望なアプローチです。ただし、既存の研究には、さまざまな大規模な言語モデルに対する検索された生成の影響に関する厳密な評価がありません。これにより、異なるLLMのRAGの機能における潜在的なボトルネックを特定することが困難になります。この論文では、検索された生成が大規模な言語モデルに与える影響を体系的に調査します。ノイズの堅牢性、否定的な拒否、情報統合、反事実的堅牢性など、RAG\\u200b\\u200bに必要な4つの基本能力におけるさまざまな大手言語モデルのパフォーマンスを分析します。この目的のために、英語と中国語の両方でRAG評価のための新しいコーパスである検索された生成ベンチマーク（RGB）を確立します。 RGBは、ベンチマーク内のインスタンスを、ケースを解決するために必要な前述の基本能力に基づいて、4つの個別のテストベッドに分割します。次に、RGBの6つの代表LLMを評価して、RAGを適用する際に現在のLLMの課題を診断します。評価により、LLMはある程度のノイズの堅牢性を示していますが、否定的な拒絶、情報統合、誤った情報への対処に関して依然として著しく苦労していることが明らかになりました。前述の評価の結果は、LLMにRAGを効果的に適用するために、まだかなりの旅がまだあることを示しています。 Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.13213', '10.1007/978-3-031-88708-6_3', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2505.18247', '10.1609/aaai.v38i16.29728']\n",
      "\u001b[93mMy lady, RAG, or Retrieval-Augmented Generation, is a framework that incorporates external knowledge, typically in the form of a set of documents retrieved from a collection, as part of a prompt to a large language model (LLM). This approach has the potential to improve the performance of downstream tasks, such as question answering.\n",
      "\n",
      "RAG enables the retrieval of relevant information from external sources, allowing LLMs to answer queries over previously unseen document collections. It aims to maximise the total utility of documents, indicating whether including them as additional contextual information in an LLM prompt improves a downstream task.\n",
      "\n",
      "However, traditional RAG applications have been found to perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. To address this, new methods have been introduced, such as Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the selection of relevant documents from various sources.\n",
      "\n",
      "Additionally, RAG has been applied to open-domain question answering (ODQA) systems, with methods like M-RAG utilising metadata retrieval to enhance performance. RAG has also been explored in specialised domains, with approaches like MetaGen Blended RAG aiming to unlock zero-shot precision for specialised domain question answering.\n",
      "\n",
      "In terms of benchmarking, RAG has been studied as a promising approach for mitigating the hallucination of LLMs. However, existing research lacks rigorous evaluation of RAG's impact on different LLMs, making it challenging to identify potential bottlenecks.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213\n",
      "DOI: 10.1007/978-3-031-88708-6_3\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "DOI: 10.48550/arXiv.2505.18247\n",
      "DOI: 10.1609/aaai.v38i16.29728\n",
      "45\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.956\n",
      "Recall: 0.956\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.887\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9997565, '10.48550/arXiv.2406.13213'), (0.999617, '10.1007/978-3-031-88708-6_3'), (0.9992563, '10.6109/jkiice.2023.27.12.1489'), (0.9956006, '10.48550/arXiv.2505.18247'), (0.8362874, '10.1609/aaai.v38i16.29728')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1371/journal.pbio.1002542\\n Title: 引用メトリック：正規化する方法（NOT）の入門書 Citation Metrics: A Primer on How (Not) to Normalize\\nAbstract: 引用指標は、公開された研究を評価するためにますます使用されています。課題の1つは、科学分野の違い、年齢（出版年）、ドキュメントの種類、データベースカバレッジ、およびその他の要因を説明するために、これらのメトリックを正規化するかどうか、および方法です。さまざまなアプローチを使用して、正規化の長所と短所について説明します。科学者、機関、雑誌、または国のコーパスを評価するために、複数の論文で引用指標を組み合わせる必要がある場合、および多著者の論文でクレジットを属性にしようとする場合、追加の課題が現れます。異なる引用メトリックは補完的な洞察を提供する可能性がありますが、計算の根底にある仮定を慎重に検討する必要があります。 Citation metrics are increasingly used to appraise published research. One challenge is whether and how to normalize these metrics to account for differences across scientific fields, age (year of publication), type of document, database coverage, and other factors. We discuss the pros and cons for normalizations using different approaches. Additional challenges emerge when citation metrics need to be combined across multiple papers to appraise the corpus of scientists, institutions, journals, or countries, as well as when trying to attribute credit in multiauthored papers. Different citation metrics may offer complementary insights, but one should carefully consider the assumptions that underlie their calculation.\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542']\n",
      "\u001b[93mMy lady, the main challenges in normalizing citation metrics across scientific fields include:\n",
      "\n",
      "- Whether and how to normalize these metrics to account for differences across scientific fields, age (year of publication), type of document, database coverage, and other factors.\n",
      "- Combining citation metrics across multiple papers to appraise the corpus of scientists, institutions, journals, or countries.\n",
      "- Attributing credit in multiauthored papers.\n",
      "\n",
      "DOI: 10.1371/journal.pbio.1002542\n",
      "\n",
      "This text discusses the pros and cons of normalizing citation metrics using different approaches, highlighting the need to carefully consider the assumptions underlying their calculation.\n",
      "45\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.600\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.99984646, '10.1371/journal.pbio.1002542')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.5281/ZENODO.13960973\\n Title: 欠落しているデータパターンを使用して、書誌データセットで誤って割り当てられた記事を検出する Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\\nAbstract: DORA宣言とCoaraは、オープンデータに基づいた書誌指標の使用を求めています。ただし、確立された学術的メタデータデータセットは閉じられており、オープンデータセットの品質はまだ徹底的に調査されていません。この論文では、欠落データパターンを使用してデータセット内のエラーを検出する方法を提示します。例として、この方法は、ETHチューリッヒに関連する出版物の所属メタデータに適用されます。これにより、一連の誤って提携した論文を特定することができます。このペーパーで導入された方法は、提携データ用に特別に設計されておらず、他のタイプのデータのエラーを検出するためにも使用できます。それは、プロバイダーとデータのユーザーに利益をもたらすことを願っている修正につながる可能性があります。 The DORA declaration and CoARA call for the use of bibliometric indicators based on open data. However, established scholarly metadata datasets are closed, and the quality of open datasets has not yet been thoroughly examined. In this paper, I present a method to detect errors in a dataset using missing data patterns. As an example, the method is applied to the affiliation metadata of publications associated with ETH Zurich. This allows me to identify a series of incorrectly affiliated papers. The method introduced in this paper is not specifically designed for affiliation data and can also be used to detect errors in other types of data. It could lead to corrections which will hopefully benefit providers as well as users of data.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: CrossRefデータのDOIエラーによる無効な引用の識別と修正 Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この作業は、CrossRefで利用可能なオープンな参考文献メタデータを分析することにより、DOIの間違いのクラスを特定し、どの出版社がそのような間違いを担当し、これらの誤ったDOIの数を自動プロセスで修正できるかを強調することを目的としています。 By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs.これらの引用は、CrossRefの関連する引用データをアップロードする責任を負う責任者の有効性と出版社を追跡することで処理しました。最後に、無効なDOIの事実上の誤りのパターンと、それらをキャッチして修正するために必要な正規表現を特定しました。この研究の結果は、無効な引用の大部分に責任を負い、および/または影響を受けた出版社は少数であることを示しています。過去の研究で提案されたDOI名エラーの分類法を拡張し、以前のアプローチよりも無効なDOIでより多くのミスをきれいにすることができる、より詳細に精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的観点からDOIの間違いの可能な理由を調査し、出版社が無効な引用データの生産の根底にある問題を特定するのに役立ちます。また、私たちが提示するDOIクリーニングメカニズムは、既存のプロセス（COCIなど）に統合して、間違ったDOIを自動的に修正することで引用を追加できます。この研究は、オープンサイエンスの原則に厳密に従っていたため、私たちの研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.5281/ZENODO.13960973', '10.1007/s11192-022-04367-w']\n",
      "\u001b[93mMy lady, there are several methods used to detect and correct errors in bibliographic datasets.\n",
      "\n",
      "## Summary:\n",
      "Missing data patterns can be used to detect errors in datasets.\n",
      "\n",
      "## DOI: 10.5281/ZENODO.13960973\n",
      "This paper presents a method to detect errors in a dataset using missing data patterns. For example, the method is applied to the affiliation metadata of publications associated with ETH Zurich, allowing the identification of incorrectly affiliated papers. The method can also be used to detect errors in other types of data, potentially leading to corrections that benefit data providers and users.\n",
      "\n",
      "## DOI: 10.1007/s11192-022-04367-w\n",
      "This study aims to identify and correct invalid citations due to DOI errors in Crossref data. By analysing open bibliographic metadata available in Crossref, the research highlights which publishers are responsible for such mistakes and how many incorrect DOIs can be corrected through automatic processes. The study also proposes an extension to the taxonomy of DOI name errors and defines more detailed regular expressions to clean a higher number of mistakes in invalid DOIs.\n",
      "45\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.933\n",
      "Recall: 0.933\n",
      "F1-Score: 0.933\n",
      "Accuracy: 0.933\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.99960643, '10.5281/ZENODO.13960973'), (0.6302299, '10.1007/s11192-022-04367-w')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/978-3-031-88708-6_3\\n Title: 関連性はレトリバーからジェネレーターにぼろきれに伝播されますか？ Is Relevance Propagated from Retriever to Generator in RAG?\\nAbstract: 検索拡張生成（RAG）は、通常、コレクションから取得された一連のドキュメントの形で、プロンプトの大規模な言語モデル（LLM）への一連のドキュメントの形で、質問の回答などの下流タスクのパフォーマンスを潜在的に改善するためのフレームワークです。一連のトップランクのドキュメントの関連性を最大化するという標準検索タスクの目的とは異なり、RAGシステムの目的は、ドキュメントのユーティリティがLLMプロンプトの追加コンテキスト情報の一部としてそれを含めることがダウンストリームタスクを改善するかどうかを示します。既存の研究では、知識集約型の言語タスク（KILT）のRAGコンテキストの関連性の役割を調査します。対照的に、私たちの仕事では、関連性は、情報を求めるタスクのクエリとドキュメントの間の局所的な重複の関連性に対応しています。具体的には、IRテストコレクションを利用して、局所的に関連するドキュメントで構成されるRAGコンテキストが下流のパフォーマンスの改善につながるかどうかを経験的に調査します。私たちの実験は、次の発見につながります。（a）関連性と有用性の間には小さな正の相関があります。 （b）この相関は、コンテキストサイズの増加とともに減少します（k-shotのkの値が高い）。 （c）より効果的な検索モデルは、一般に、下流のラグパフォーマンスの向上につながります。 Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n', \"DOI: 10.48550/arXiv.2404.13948\\n Title: ぼろきれの背中を壊したYPO ypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\\nAbstract: 最近の大規模な言語モデル（LLMS）の堅牢性は、さまざまなドメインや現実世界のアプリケーションで適用性が拡大するにつれてますます重要になっています。検索された生成（RAG）は、LLMの限界に対処するための有望なソリューションですが、RAGの堅牢性に関する既存の研究は、しばしば、RAGコンポーネント間の相互接続された関係またはマイナーなテキストエラーなどの実際のデータベースで一般的な潜在的な脅威を見落としています。この作業では、RAGの堅牢性を評価する際に、2つの未掘りの側面を調査します。1）低レベルの摂動を通じて騒々しい文書に対する脆弱性と2）Ragの堅牢性の全体的な評価。さらに、これらの側面をターゲットにする新しい攻撃法であるRag（\\\\ textit {garag}）に対する遺伝的攻撃を紹介します。具体的には、GARAGは各コンポーネント内の脆弱性を明らかにし、騒々しいドキュメントに対してシステム全体の機能をテストするように設計されています。 \\\\ textIT {garag}を標準のQAデータセットに適用し、多様なレトリバーとLLMを組み込んで、ragの堅牢性を検証します。実験結果は、Garagが一貫して高い攻撃の成功率を達成することを示しています。また、各コンポーネントのパフォーマンスとその相乗効果を大幅に破壊し、現実世界のぼろきれシステムを混乱させる際にマイナーなテキストの不正確さがもたらす実質的なリスクを強調しています。 The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\\\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\\n\"]\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1007/978-3-031-88708-6_3', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2404.13948']\n",
      "\u001b[93mMy lady, RAG, or Retrieval Augmented Generation, is a framework for incorporating external knowledge into a large language model (LLM) to improve the performance of downstream tasks, such as question answering. RAG enables LLMs to answer queries over previously unseen document collections by retrieving relevant information from external knowledge sources.\n",
      "\n",
      "The objective of a RAG system is to maximise the total utility of a set of documents, where the utility of a document indicates whether including it as additional contextual information in an LLM prompt improves a downstream task. This differs from the standard retrieval task's objective of maximising the relevance of a set of top-ranked documents.\n",
      "\n",
      "While RAG is a promising solution for addressing the limitations of LLMs, existing studies on the robustness of RAG often overlook potential threats prevalent in real-world databases, such as minor textual errors. To address this, a novel attack method called the Genetic Attack on RAG (GARAG) has been introduced, which targets two underexplored aspects of RAG robustness: vulnerability to noisy documents through low-level perturbations and a holistic evaluation of RAG robustness.\n",
      "\n",
      "Experimental results have shown that GARAG consistently achieves high attack success rates, significantly devastating the performance of each component and their synergy. This highlights the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.13948\n",
      "Title: Ypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\n",
      "DOI: 10.48550/arXiv.2406.13213\n",
      "Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "Title: M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\n",
      "DOI: 10.1007/978-3-031-88708-6_3\n",
      "Title: Is Relevance Propagated from Retriever to Generator in RAG?\n",
      "45\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.675\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.94834626, '10.1007/978-3-031-88708-6_3'), (0.5391045, '10.48550/arXiv.2406.13213'), (0.38225675, '10.6109/jkiice.2023.27.12.1489'), (0.37410447, '10.48550/arXiv.2404.13948')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00286\\n Title: 8つのフリーアクセス学術データベースの出版メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースにおけるメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000を超えるレコードのランダムなCrossRefサンプルを選択し、7つのデータベース（Dimensions、Google Scholar、Microsoft Academic、Openalex、Scilit、Semantic Sc\\u200b\\u200bholar、およびThe Lens）で検索されました。この情報、これらのフィールドの完全性レート、およびデータベース間の合意を説明するフィールドを観察するために、7つの特性（要約、アクセス、書誌情報、書誌情報、文書の種類、公開日、言語、識別子）を観察しました。結果は、アカデミック検索エンジン（Google Scholar、Microsoft Academic、およびSemantic Sc\\u200b\\u200bholar）が少ない情報を収集し、完全性が低いことを示しています。逆に、サードパーティのデータベース（寸法、OpenAlex、Scilit、およびレンズ）は、メタデータの品質が高く、完全性が高くなります。アカデミック検索エンジンには、Webをcrawって信頼できる記述データを取得する能力がないと結論付けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することから得られる情報の喪失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: メタエンハンス：大学図書館の電子論文と学位論文のメタデータの品質改善 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタルオブジェクトがデジタルライブラリインターフェイスを通じて発見されるために重要です。ただし、さまざまな理由により、デジタルオブジェクトのメタデータは、しばしば不完全で、一貫性がなく、誤った値を示します。ケーススタディとして、電子論文と論文（ETD）の7つの重要な分野を使用して、学術メタデータを自動的に検出、修正、および正規化する方法を調査します。メタエンハンスを提案します。メタエンハンスは、これらの分野の品質を改善するために最先端の人工知能方法を利用するフレームワークです。 Metaenhanceを評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 ETDを含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークでMetaenhanceをテストし、提案された方法は、7つのフィールドのうち5つで0.85から1.00の範囲のエラーとF1スコアの検出にほぼ完全なF1スコアを達成したことを発見しました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化全体のメタデータの品質の問題を特定する Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、標準的な形式でコンテキスト、技術、および管理情報を提供することにより、発見とアクセスに不可欠です。しかし、メタデータは、社会文化的表現、リソースの制約、および標準化されたシステムの間の緊張の場所でもあります。正式および非公式の介入は、品質の問題、アイデンティティを主張する政治的行為、または視界を最大化するための戦略的選択と解釈される場合があります。これに関連して、メタデータの品質、一貫性、および完全性が個人やコミュニティにどのように影響するかを理解しようとしました。記録のサンプルをレビューして、メタデータとコミュニティが互いに反対して、意図的に文化的な意味を反映する（またはそうでない）方法に起因する問題を特定して分類しました。 Metadata are crucial for discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. In this context, we sought to understand how metadata quality, consistency, and completeness impact individuals and communities. Reviewing a sample of records, we identified and classified issues stemming from how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101']\n",
      "\u001b[93mMy lady, I have located three studies that examined the abstract in metadata.\n",
      "\n",
      "## Summary: Three studies examined the abstract in metadata, focusing on metadata quality, completeness, and improvement methods.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00286\n",
      "This study compared the amount of metadata and the completeness degree of research publications in new academic databases. It analysed seven characteristics, including the abstract, to observe fields that describe information, the completeness rate of these fields, and the agreement among databases. The results showed that academic search engines gather less information and have a low degree of completeness, while third-party databases have more metadata quality and a higher completeness rate.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2303.17661\n",
      "This study investigated methods to automatically detect, correct, and canonicalise scholarly metadata, using seven key fields of electronic theses and dissertations as a case study. It proposed MetaEnhance, a framework that utilises state-of-the-art artificial intelligence methods to improve the quality of these fields. The study found that the proposed methods achieved nearly perfect F1-scores in detecting and correcting errors.\n",
      "\n",
      "## DOI: 10.5860/crl.86.1.101\n",
      "This study sought to understand how metadata quality, consistency, and completeness impact individuals and communities. It reviewed a sample of records to identify and classify issues stemming from how metadata and communities intentionally reflect (or not) cultural meanings. The study highlighted that metadata is crucial for discovery and access but also a site of tension between sociocultural representations, resource constraints, and standardised systems.\n",
      "45\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.575\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.4213137, '10.1162/qss_a_00286'), (0.19544049, '10.48550/arXiv.2303.17661'), (0.12951279, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0\n",
      "Length of documents: 45\n",
      "reranked_documents: []\n",
      "length of reranked_documents: 0\n",
      "Retrieved DOIs: []\n",
      "\u001b[93mMy lady, could you please provide me with some context or a specific topic you are interested in? I can then assist you in finding studies that examined citations within that field.\n",
      "45\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.500\n",
      "Faithfulness score: 0\n",
      "Documents score: []\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.1007/s11192-023-04923-y\\n Title: Openalexの不足している機関：考えられる理由、意味、および解決策 Missing institutions in OpenAlex: possible reasons, implications, and solutions\\nAbstract: オープンサイエンスの出現では、データ品質が高いオープンデータプラットフォームが必要です。 2022年1月に開始されたグローバルな研究システムの完全に開かれたカタログとして、OpenAlexは、定量的科学研究で広く使用されている簡単なデータアクセシビリティと幅広いデータカバレッジの2つの主要な利点を特徴としています。驚くべきことに、Openalexはライデン大学のランキングの重要なデータソースとして採用されています。ただし、Openalexのジャーナル記事メタデータには、機関が欠落しているという深刻なデータ品質の問題があります。この研究では、3種類の制度情報（FII）、部分的に欠落している機関情報（PMII）、および完全に欠落している機関情報（CMII）を定義することにより、問題とその結果と解決策の考えられる理由を調査します。私たちの結果は、不足している機関の問題がOpenalexのジャーナル記事の60％以上で発生することを示しています。この問題は、特に初期のメタデータや社会科学や人文科学で広まっています。データのサブサンプルを使用して、問題の考えられる理由、歪んだ結果のリスク、および不足している機関の問題に対する可能な解決策をさらに調査します。目的は、オープンリソースのデータ品質改善の重要性を高め、定量的科学研究およびより広い文脈でのオープンリソースの責任ある使用をサポートすることです。 The advent of open science calls for open data platforms with high data quality. As a fully open catalog of the global research system launched in January 2022, OpenAlex features two main advantages of easy data accessibility and broad data coverage, which has been widely used in quantitative science studies. Remarkably, OpenAlex is adopted as an important data source for Leiden university ranking. However, there is a severe data quality problem of missing institutions in journal article metadata in OpenAlex. This study investigates the possible reasons for the problem and its consequences and solutions by defining three types of institutional information—full institutional information (FII), partially missing institutional information (PMII) and completely missing institutional information (CMII). Our results show that the problem of missing institutions occurs in more than 60% of the journal articles in OpenAlex. The problem is particularly widespread in metadata from the early years and in the social sciences and humanities. Using sub-samples of the data, we further explore the possible reasons for the problem, the risk it might represent for distorted results, and possible solutions to the problem of missing institutions. The aim is to raise the importance of data quality improvements in open resources, and thus to support the responsible use of open resources in quantitative science studies and also in broader contexts.\\n', 'DOI: 10.1590/SciELOPreprints.11205\\n Title: ユニバーサルインデックスへのオープンロードで：OpenAlexおよびOpenJournal Systems On the Open Road to Universal Indexing: OpenAlex and OpenJournal Systems\\nAbstract: この調査では、OpenAlexのオープンジャーナルシステム（JUOJS）を使用したジャーナルのインデックス作成を検証し、包括的な学術参加をサポートする2つのオープンソースソフトウェアイニシアチブを反映しています。 47,625のアクティブなJuojsのデータセットを分析することにより、これらのジャーナルの71％がOpenAlexで少なくとも1つの記事をインデックス付けされていることを明らかにします。私たちの調査結果は、OpenAlexに含まれるCrossRef doiを使用してジャーナルの97％を使用して、インデックス作成の達成におけるCrossRef DOIの中心的な役割を強調しています。ただし、この技術的依存は、特に低所得国（Juojsの47％）および非英語言語ジャーナル（Juojsの55％-64％）からのリソース制限されたジャーナル（Juojsの55％-64％）の雑誌として、より広範な構造的不平等を反映しています。私たちの研究は、学術インフラストラクチャの依存関係の理論的意味と、世界的な知識の可視性における体系的な格差を永続させる上でのその役割を強調しています。 OpenAlexのような包括的な書誌データベースでさえ、世界規模で公平な索引付けを促進するために、財務、インフラ、および言語の障壁に積極的に対処する必要があると主張します。インデックス作成メカニズム、永続的な識別子、および構造的不平等との関係を概念化することにより、この研究は、グローバルな多言語学術生態系における普遍的なインデックス作成のダイナミクスとその実現を再考するための重要なレンズを提供します。 This study examines OpenAlex’s indexing of journals using Open Journal Systems (JUOJS), reflecting two open source software initiatives supporting inclusive scholarly participation. By analyzing a dataset of 47,625 active JUOJS, we reveal that 71% of these journals have at least one article indexed in OpenAlex. Our findings underscore the central role of Crossref DOIs in achieving indexing, with 97% of the journals using Crossref DOIs included in OpenAlex. However, this technical dependency reflects broader structural inequities, as resource-limited journals, particularly those from low-income countries (47% of JUOJS) and non-English language journals (55%-64% of JUOJS), remain underrepresented. Our work highlights the theoretical implications of scholarly infrastructure dependencies and their role in perpetuating systemic disparities in global knowledge visibility. We argue that even inclusive bibliographic databases like OpenAlex must actively address financial, infrastructural, and linguistic barriers to foster equitable indexing on a global scale. By conceptualizing the relationship between indexing mechanisms, persistent identifiers, and structural inequities, this study provides a critical lens for rethinking the dynamics of universal indexing and its realization in a global, multilingual scholarly ecosystem.\\n', 'DOI: 10.48550/arXiv.2404.01985\\n Title: 彼はOpenalex、Scopus、Web of Scienceのオープンアクセスカバレッジ he open access coverage of OpenAlex, Scopus and Web of Science\\nAbstract: Diamond Open Access（OA）ジャーナルは、著者と読者の両方に無料の公開モデルを提供しますが、主要な書誌データベースでのインデックスの欠如は、これらのジャーナルの取り込みを評価する際の課題を提示します。さらに、出版言語や出版国などのOAの特性は、OAジャーナルがより多様であり、地域社会にサービスを提供することを目指しているという議論を支持するためにしばしば使用されてきましたが、OAジャーナルの地理的および言語的特性に関連する経験的証拠の現在の欠如があります。 OpenAlexとオープンアクセスジャーナルのディレクトリをベンチマークとして使用して、このペーパーでは、フィールド、国、言語による科学とスコープスのWebでの著者とジャーナルの報道を通じて、ダイヤモンドと金の報道を調査します。結果は、WOSとSCOPUSでのより低いカバレッジ、およびダイヤモンドOAの局所範囲を示しています。英語のみのジャーナルのシェアは、ゴールドジャーナルの間でかなり高くなっています。高所得国は、社会科学と人文科学のダイヤモンドジャーナルを除き、すべてのドメインとジャーナルの種類で著者のシェアが最も高い。ダイヤモンドOAインデックスの現在の景観を理解することは、より包括的なOAモデルに向けて政策と実践を進めることで、学術通信ネットワークを支援することができます。 Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2401.16359', '10.48550/arXiv.2404.17663', '10.1007/s11192-023-04923-y', '10.1590/SciELOPreprints.11205', '10.48550/arXiv.2404.01985']\n",
      "\u001b[93m## Summary: OpenAlex is a promising open-source database of scholarly metadata, offering easy data accessibility and broad coverage.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2401.16359\n",
      "This report compares OpenAlex to established proprietary sources like Web of Science and Scopus, demonstrating its average source reference numbers and internal coverage rates comparable to both. OpenAlex provides data freely, enabling bibliometric studies without licensing barriers. However, its rapidly evolving nature raises questions about data trustworthiness.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.17663\n",
      "This study, conducted with the OpenAlex team, compares OpenAlex to Scopus across various dimensions. It concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. However, issues with metadata accuracy and completeness require further research to fully understand and address OpenAlex's limitations.\n",
      "\n",
      "## DOI: 10.1007/s11192-023-04923-y\n",
      "This research investigates the severe data quality problem of missing institutions in OpenAlex's journal article metadata. It defines three types of institutional information and finds that the problem occurs in over 60% of journal articles, particularly in early years and in the social sciences and humanities. The aim is to improve data quality in open resources and support their responsible use.\n",
      "\n",
      "## DOI: 10.1590/SciELOPreprints.11205\n",
      "This study examines OpenAlex's indexing of journals using Open Journal Systems (JUOJS), reflecting two open-source software initiatives. It reveals that 71% of journals have at least one article indexed in OpenAlex and highlights the central role of Crossref DOIs in achieving indexing. However, this technical dependency reflects broader structural inequities, particularly for resource-limited journals from low-income countries and non-English language journals.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.01985\n",
      "This paper investigates the coverage of diamond and gold open access (OA) journals through authorship and journal coverage in Web of Science and Scopus. It uses OpenAlex and the Directory of Open Access Journals as benchmarks. The results show lower coverage in WoS and Scopus and the local scope of diamond OA. The study argues that understanding the current landscape of diamond OA indexing can aid the scholarly communications network in advancing towards more inclusive OA models.\n",
      "\n",
      "My lady, I hope these insights into OpenAlex are of use to you.\n",
      "45\n",
      "For query: ['Tell me about OpenAlex.']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.773\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9965301, '10.48550/arXiv.2401.16359'), (0.9940428, '10.48550/arXiv.2404.17663'), (0.99040353, '10.1007/s11192-023-04923-y'), (0.9678993, '10.1590/SciELOPreprints.11205'), (0.96405166, '10.48550/arXiv.2404.01985')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: CrossRef：コミュニティ所有の学術メタデータの持続可能な供給源 Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このホワイトペーパーでは、CrossRefによって収集および利用可能になった学術的メタデータと、学術研究の生態系におけるその重要性について説明します。 1億600万件以上の記録を含み、年間平均11％のレートで拡大するCrossrefのメタデータは、出版社、著者、図書館員、資金提供者、および研究者向けの学術データの主要な情報源の1つになりました。メタデータセットは、ジャーナルやカンファレンスペーパーなどの従来のタイプだけでなく、データセット、レポート、プリプリント、ピアレビュー、助成金など、13のコンテンツタイプで構成されています。メタデータは、基本的な出版物メタデータに限定されませんが、全文、資金調達とライセンス情報、引用リンク、修正、更新、撤回などに関する情報への要約とリンクを含めることもできます。メタデータは、REST APIやOAI-PMHを含む多くのAPIを通じて利用できます。この論文では、CrossRefが提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、CrossRefの研究エコシステムにおける役割と、引用データの提供の進化など、長年にわたるメタデータのキュレーションの傾向にも注目しています。 Crossrefのメタデータで使用される研究を要約し、将来のメタデータの品質と検索を改善する計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022']\n",
      "\u001b[93mMy lady, Crossref is a sustainable source of community-owned scholarly metadata. It has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers.\n",
      "\n",
      "Crossref's metadata set consists of 13 content types, including traditional types like journals and conference papers, as well as data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication information but can also include abstracts and links to full text, funding and license details, citation links, and information about corrections, updates, and retractions.\n",
      "\n",
      "With over 106 million records and expanding at an average rate of 11% annually, Crossref's metadata is a valuable source for research in scientometrics, measuring the growth and impact of science, and understanding new trends in scholarly communications. The metadata is accessible through various APIs, including REST API and OAI-PMH.\n",
      "\n",
      "This paper, with the DOI 10.1162/qss_a_00022, describes the scholarly metadata collected and made available by Crossref, its importance in the scholarly research ecosystem, and the trends in metadata curation over the years. It also summarises the research used in Crossref's metadata and outlines plans to enhance metadata quality and retrieval in the future.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "This paper provides an in-depth analysis of Crossref's role in the research ecosystem, detailing the types of metadata it provides, how it is collected and curated, and its evolution as a source of citation data.\n",
      "45\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.822\n",
      "Recall: 0.822\n",
      "F1-Score: 0.822\n",
      "Accuracy: 0.822\n",
      "Balanced accuracy: 0.556\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.8345418, '10.1162/qss_a_00022')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: オープンアレックスの言語カバレッジの評価：メタデータの精度と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: ClarivateのWeb of Science（WOS）とElsevierのScopusは、数十年にわたって書誌情報の主要なソースでした。高度にキュレーションされていますが、これらの閉鎖された独自のデータベースは、英語の出版物に大きく偏っており、研究普及における他の言語の使用を過小評価しています。 2022年に発売されたOpenalexは、包括的、包括的、オープンソースの研究情報を約束しました。すでに学者や研究機関が使用している間、そのメタデータの質は現在評価されています。この論文は、言語に関連するOpenalexのメタデータの完全性と正確性、WOSとの比較、および6,836の記事のサンプルの詳細な手動検証を通じて、この文献に貢献しています。結果は、オープンアレックスがWOSよりもはるかにバランスのとれた言語カバレッジを示すことを示しています。ただし、言語メタデータは必ずしも正確ではないため、Openalexは他の言語のそれを過小評価しながら英語の場所を過大評価します。批判的に使用すると、OpenAlexは学術出版に使用される言語の包括的かつ代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャレベルでより多くの作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8つのフリーアクセス学術データベースの出版メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースにおけるメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000を超えるレコードのランダムなCrossRefサンプルを選択し、7つのデータベース（Dimensions、Google Scholar、Microsoft Academic、Openalex、Scilit、Semantic Sc\\u200b\\u200bholar、およびThe Lens）で検索されました。この情報、これらのフィールドの完全性レート、およびデータベース間の合意を説明するフィールドを観察するために、7つの特性（要約、アクセス、書誌情報、書誌情報、文書の種類、公開日、言語、識別子）を観察しました。結果は、アカデミック検索エンジン（Google Scholar、Microsoft Academic、およびSemantic Sc\\u200b\\u200bholar）が少ない情報を収集し、完全性が低いことを示しています。逆に、サードパーティのデータベース（寸法、OpenAlex、Scilit、およびレンズ）は、メタデータの品質が高く、完全性が高くなります。アカデミック検索エンジンには、Webをcrawって信頼できる記述データを取得する能力がないと結論付けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することから得られる情報の喪失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.1162/qss_a_00286']\n",
      "\u001b[93mMy lady, here are the papers that evaluate the linguistic coverage or language-related metadata in scholarly databases:\n",
      "\n",
      "## Summary: Papers that evaluate the linguistic coverage or language-related metadata in scholarly databases.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2409.10633\n",
      "Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "Abstract: This paper assesses the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with Clarivate's Web of Science (WoS), as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00286\n",
      "Title: Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "Abstract: This study compares the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, a random Crossref sample of more than 115,000 records was selected and searched in seven databases. Seven characteristics were analyzed to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines gather less information and have a low degree of completeness. Conversely, third-party databases have more metadata quality and a higher completeness rate.\n",
      "45\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.588\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.9998287, '10.48550/arXiv.2409.10633'), (0.4870177, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープンファンダーメタデータの可用性と完全性：オランダの研究評議会によって資金提供された出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金を提供する研究の結果に関する情報を収集するためにかなりの努力を費やしています。資金提供者が資金調達に関連する出版物の出力を追跡するのを支援するために、CrossRefは2013年にFundRefを開始し、出版社が永続的な識別子を使用して資金情報を登録できるようにしました。ただし、資金調達の研究の結果であるため、資金提供者のメタデータを含める必要があるため、Funder Metadataのカバレッジを評価することは困難です。この論文では、研究者が特定の資金提供機関による資金提供の結果であるオランダ研究評議会NWOが報告した5,004の出版物を調べました。これらの記事の67％のみがCrossRefの資金情報を含んでおり、NWOをNWOにリンクしたFunder Nameおよび/またはFunder IDとしてNWOを認めているサブセット（それぞれ53％と45％）が含まれています。 Web of Science（WOS）、Scopus、およびDimensionsはすべて、記事の全文の資金調達声明から追加の資金情報を推測することができます。レンズの資金情報は、主にCrossRefのそれに対応しており、PubMedから取得した可能性のある追加の資金情報があります。私たちは、独自のデータベースと比較して、CrossRefのメタデータの資金調達のカバレッジと完全性における出版社間の興味深い違いを観察し、資金調達のオープンメタデータの質を高める可能性を強調しています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: 資金調達COVID-19研究：オープンデータインフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 彼らが提供する資金調達の結果を分析するには、資金提供機関が資金から生じる出版物を追跡できるようにすることが不可欠です。 Covid-19に関連する研究を報告する出版物の資金調達データに焦点を当て、CrossRefでの資金提供データの可用性を研究しています。また、ScopusとWeb of Scienceの2つの独自の書誌データベースで利用可能な資金データとの比較も提示します。私たちの分析により、CrossRefの資金調達データの限られたカバレッジが明らかになりました。また、特にScopusでは、資金調達データの質に関連する問題を示しています。 CrossRefでの資金提供データの可用性を改善するための推奨事項を提供しています。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: 開いた書誌メタデータの源としてのCrossref Crossref as a source of open bibliographic metadata\\nAbstract: CrossRefの学術出版物の書誌メタデータのオープン可用性を促進するために、いくつかのイニシアチブが採用されています。 CrossRefでの6つのメタデータ要素の可用性の最新の概要を示します。参照リスト、要約、ORCID、著者の提携、資金情報、およびライセンス情報です。私たちの分析は、これらのメタデータ要素の可用性が、少なくともCrossRefで最も一般的な出版タイプであるジャーナル記事で、時間とともに改善されたことを示しています。ただし、分析では、多くの出版社が書誌メタデータの完全な開放性を実現するために追加の努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.1162/qss_a_00212', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mMy lady, here are some papers that address funding metadata, its availability, or its analysis in scholarly databases:\n",
      "\n",
      "## Summary: Papers exploring the availability and completeness of funding metadata in scholarly databases.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00210\n",
      "The availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council. This paper looks at 5,004 publications reported to be the result of funding by the Dutch Research Council. It finds that only 67% of these articles contain funding information in Crossref, highlighting the potential to increase the quality of open metadata on funding.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00212\n",
      "Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures. This paper studies the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. It reveals limited coverage of funding data in Crossref and problems related to the quality of funding data, especially in Scopus.\n",
      "\n",
      "## DOI: 10.31222/osf.io/smxe5\n",
      "Crossref as a source of open bibliographic metadata. This paper presents an up-to-date overview of the availability of six metadata elements in Crossref, including funding information. It shows that the availability of these metadata elements has improved over time, but many publishers need to make additional efforts to realise full openness of bibliographic metadata.\n",
      "45\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.688\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.9913892, '10.1162/qss_a_00210'), (0.8077641, '10.1162/qss_a_00212'), (0.7324005, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模な言語モデルの検索された生成：調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模な言語モデル（LLMS）は印象的な能力を紹介しますが、幻覚、時代遅れの知識、非透明な、追跡不可能な推論プロセスなどの課題に遭遇します。検索された生成（RAG）は、外部データベースから知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の正確性と信頼性が向上し、ドメイン固有の情報の継続的な知識の更新と統合が可能になります。 RAGは、外部データベースの広大で動的なリポジトリとLLMSの本質的な知識を相乗的に統合します。この包括的なレビューペーパーでは、素朴なぼろきれ、高度なぼろ、モジュラーラグを含むRAGパラダイムの進行に関する詳細な調査を提供します。検索、生成、増強技術を含む、RAGフレームワークの三者基盤を細心の注意を払って精査します。この論文は、これらの各重要なコンポーネントに組み込まれた最先端のテクノロジーを強調し、RAGシステムの進歩を深く理解することを提供します。さらに、このペーパーでは、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を描き、研究開発の将来の道を指摘しています。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", 'DOI: 10.1609/aaai.v38i16.29728\\n Title: 検索された世代の大規模な言語モデルのベンチマーク Benchmarking Large Language Models in Retrieval-Augmented Generation\\nAbstract: 検索された生成（RAG）は、大規模な言語モデル（LLM）の幻覚を緩和するための有望なアプローチです。ただし、既存の研究には、さまざまな大規模な言語モデルに対する検索された生成の影響に関する厳密な評価がありません。これにより、異なるLLMのRAGの機能における潜在的なボトルネックを特定することが困難になります。この論文では、検索された生成が大規模な言語モデルに与える影響を体系的に調査します。ノイズの堅牢性、否定的な拒否、情報統合、反事実的堅牢性など、RAG\\u200b\\u200bに必要な4つの基本能力におけるさまざまな大手言語モデルのパフォーマンスを分析します。この目的のために、英語と中国語の両方でRAG評価のための新しいコーパスである検索された生成ベンチマーク（RGB）を確立します。 RGBは、ベンチマーク内のインスタンスを、ケースを解決するために必要な前述の基本能力に基づいて、4つの個別のテストベッドに分割します。次に、RGBの6つの代表LLMを評価して、RAGを適用する際に現在のLLMの課題を診断します。評価により、LLMはある程度のノイズの堅牢性を示していますが、否定的な拒絶、情報統合、誤った情報への対処に関して依然として著しく苦労していることが明らかになりました。前述の評価の結果は、LLMにRAGを効果的に適用するために、まだかなりの旅がまだあることを示しています。 Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.48550/arXiv.2410.04231\\n Title: 大規模な言語モデルの検索された生成によるメタデータベースのデータ探査 Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\\nAbstract: 必要なデータセットを効果的に検索する能力を開発することは、非常に限られた利用可能なメタデータを考慮して、データユーザーが関連するデータセットを特定するのを支援するための緊急の要件です。この課題のために、サードパーティのデータの利用は、改善の貴重なソースとして浮上しています。私たちの研究では、メタデータベースのデータ発見を強化するために検索された世代（RAG）の形式を採用するデータ探索のための新しいアーキテクチャを紹介します。このシステムは、大規模な言語モデル（LLM）を外部ベクトルデータベースと統合して、多様なタイプのデータセット間のセマンティック関係を識別します。提案されたフレームワークは、不均一なデータソース間のセマンティックな類似性を評価し、データ探索を改善するための新しい方法を提供します。私たちの研究には、4つの重要なタスクに関する実験結果が含まれています。1）同様のデータセットの推奨、2）組み合わせ可能なデータセットの提案、3）タグの推定、4）変数の予測。我々の結果は、RAGが従来のメタデータアプローチと比較した場合、特に異なるカテゴリから関連するデータセットの選択を強化できることを示しています。ただし、パフォーマンスはタスクとモデルによって異なり、特定のユースケースに基づいて適切な手法を選択することの重要性を確認します。調査結果は、このアプローチがデータ調査と発見の課題に対処するための約束を保持していることを示唆していますが、推定タスクにはさらなる改良が必要です。 Developing the capacity to effectively search for requisite datasets is an urgent requirement to assist data users in identifying relevant datasets considering the very limited available metadata. For this challenge, the utilization of third-party data is emerging as a valuable source for improvement. Our research introduces a new architecture for data exploration which employs a form of Retrieval-Augmented Generation (RAG) to enhance metadata-based data discovery. The system integrates large language models (LLMs) with external vector databases to identify semantic relationships among diverse types of datasets. The proposed framework offers a new method for evaluating semantic similarity among heterogeneous data sources and for improving data exploration. Our study includes experimental results on four critical tasks: 1) recommending similar datasets, 2) suggesting combinable datasets, 3) estimating tags, and 4) predicting variables. Our results demonstrate that RAG can enhance the selection of relevant datasets, particularly from different categories, when compared to conventional metadata approaches. However, performance varied across tasks and models, which confirms the significance of selecting appropriate techniques based on specific use cases. The findings suggest that this approach holds promise for addressing challenges in data exploration and discovery, although further refinement is necessary for estimation tasks.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.1609/aaai.v38i16.29728', '10.48550/arXiv.2406.13213', '10.48550/arXiv.2410.04231', '10.6109/jkiice.2023.27.12.1489']\n",
      "\u001b[93mMy lady, here are the papers that discuss the use of Retrieval-Augmented Generation (RAG) in large language models or related applications:\n",
      "\n",
      "## Summary:\n",
      "A range of papers discuss the use of RAG in large language models (LLMs), highlighting its potential to mitigate hallucinations and improve accuracy and credibility, particularly for knowledge-intensive tasks.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2312.10997\n",
      "This comprehensive review paper examines the progression of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG. It scrutinises the tripartite foundation of RAG frameworks, encompassing retrieval, generation, and augmentation techniques. The paper emphasises state-of-the-art technologies embedded in these components, providing an understanding of RAG system advancements. It also introduces evaluation frameworks and benchmarks, addressing current challenges and suggesting future research directions.\n",
      "\n",
      "## DOI: 10.1609/aaai.v38i16.29728\n",
      "This paper systematically investigates the impact of RAG on LLMs, analysing performance in four fundamental abilities required for RAG: noise robustness, negative rejection, information integration, and counterfactual robustness. It establishes the Retrieval-Augmented Generation Benchmark (RGB) for RAG evaluation in English and Chinese. The paper evaluates six representative LLMs on RGB, diagnosing challenges in applying RAG to current LLMs. Results indicate a need for further development to effectively apply RAG to LLMs.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2406.13213\n",
      "Introducing a new method called Multi-Meta-RAG, this paper aims to improve RAG for multi-hop queries using database filtering with LLM-extracted metadata. Multi-Meta-RAG enhances the RAG selection of relevant documents from various sources, improving performance on the MultiHop-RAG benchmark. The code for this method is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2410.04231\n",
      "Focusing on data exploration, this research introduces a new architecture employing RAG to enhance metadata-based data discovery. The system integrates LLMs with external vector databases to identify semantic relationships among diverse datasets. The proposed framework evaluates semantic similarity and improves data exploration. Experimental results on four critical tasks demonstrate RAG's ability to enhance the selection of relevant datasets, particularly from different categories, compared to conventional metadata approaches.\n",
      "\n",
      "## DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "Proposing a method called Metadata Retrieval-Augmented Generation (M-RAG), this paper aims to enhance open-domain question answering (ODQA) systems. M-RAG utilises embeddings with metadata and generative models (GPT-3.5-Turbo-16K, GPT-4) for automated answer generation. Experimental results show up to a 46% performance improvement compared to external inference ODQA systems and a 6% improvement over existing RAG methods.\n",
      "45\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) in large language models or related applications?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9999918, '10.48550/arXiv.2312.10997'), (0.99997985, '10.1609/aaai.v38i16.29728'), (0.99964714, '10.48550/arXiv.2406.13213'), (0.9996345, '10.48550/arXiv.2410.04231'), (0.9891816, '10.6109/jkiice.2023.27.12.1489')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: CrossRef：コミュニティ所有の学術メタデータの持続可能な供給源 Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このホワイトペーパーでは、CrossRefによって収集および利用可能になった学術的メタデータと、学術研究の生態系におけるその重要性について説明します。 1億600万件以上の記録を含み、年間平均11％のレートで拡大するCrossrefのメタデータは、出版社、著者、図書館員、資金提供者、および研究者向けの学術データの主要な情報源の1つになりました。メタデータセットは、ジャーナルやカンファレンスペーパーなどの従来のタイプだけでなく、データセット、レポート、プリプリント、ピアレビュー、助成金など、13のコンテンツタイプで構成されています。メタデータは、基本的な出版物メタデータに限定されませんが、全文、資金調達とライセンス情報、引用リンク、修正、更新、撤回などに関する情報への要約とリンクを含めることもできます。メタデータは、REST APIやOAI-PMHを含む多くのAPIを通じて利用できます。この論文では、CrossRefが提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、CrossRefの研究エコシステムにおける役割と、引用データの提供の進化など、長年にわたるメタデータのキュレーションの傾向にも注目しています。 Crossrefのメタデータで使用される研究を要約し、将来のメタデータの品質と検索を改善する計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: 開いた書誌メタデータの源としてのCrossref Crossref as a source of open bibliographic metadata\\nAbstract: CrossRefの学術出版物の書誌メタデータのオープン可用性を促進するために、いくつかのイニシアチブが採用されています。 CrossRefでの6つのメタデータ要素の可用性の最新の概要を示します。参照リスト、要約、ORCID、著者の提携、資金情報、およびライセンス情報です。私たちの分析は、これらのメタデータ要素の可用性が、少なくともCrossRefで最も一般的な出版タイプであるジャーナル記事で、時間とともに改善されたことを示しています。ただし、分析では、多くの出版社が書誌メタデータの完全な開放性を実現するために追加の努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mSummary: Crossref is a valuable source of scholarly data for publishers, authors, librarians, funders, and researchers.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "Crossref's metadata, which includes over 106 million records and expands at an average rate of 11% a year, is a major source of scholarly data for various stakeholders in the research ecosystem. The metadata set consists of 13 content types, covering traditional types like journals and conference papers, as well as data sets, reports, preprints, peer reviews, and grants. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5\n",
      "Crossref promotes the open availability of bibliographic metadata of scholarly publications. An analysis of six metadata elements in Crossref—reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information—reveals improved availability over time, particularly for journal articles, the most common publication type in Crossref. However, many publishers need to make additional efforts to achieve full openness of bibliographic metadata.\n",
      "\n",
      "My lady, I hope this information is helpful!\n",
      "45\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.933\n",
      "Recall: 0.933\n",
      "F1-Score: 0.933\n",
      "Accuracy: 0.933\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.99954, '10.1162/qss_a_00022'), (0.41347715, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.3145/epi.2023.mar.09\\n Title: Bibliometricsに関連するメタデータのどれが同じであり、Microsoft Academic GraphからOpenAlexに切り替えるときにどのメタデータが異なりますか？ Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\\nAbstract: Microsoft Academic Graph（MAG）の退職の発表により、非営利団体Ourresearchは、Openalexという名前で同様のリソースを提供すると発表しました。したがって、メタデータを、最新のMAGスナップショットの書誌分析と初期のオープンアレックススナップショットと比較します。実質的にMAGのすべての作品は、書誌データの出版年、ボリューム、ファーストページと最後のページ、DOI、および引用分析の重要な要素である参照の数を保存するOpenalexに転送されました。 MAGドキュメントの90％以上がOpenAlexに同等のドキュメントタイプを持っています。残りのもののうち、特にOpenalex Document Typesの再分類ジャーナルアーティクルとブックチャプターは正しいと思われ、7％以上になります。そのため、ドキュメントタイプの仕様はMAGからOpenalexに大幅に改善されました。書誌関連のメタデータの別の項目として、MAGおよびOpenalexでの紙ベースの被験者の分類を調べました。 MAGよりもOpenAlexの対象分類割り当てを含むかなり多くのドキュメントを見つけました。第1レベルと第2レベルでは、分類構造はほぼ同じです。表形式とグラフィカルな形式の両方のレベルでの対象の再分類に関するデータを提示します。現地正規化された書誌評価における豊富な被験者の再分類の結果の評価は、本論文の範囲にありません。この未解決の質問とは別に、OpenAlexは、2021年以前の出版年のMAGと同じくらいの書誌分析に少なくともまったく適しているように見えます。 With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: Openalex、Web of Science、Scopus、PubMed、およびSemantic Sc\\u200b\\u200bholarの出版物と文書タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、次の書誌データベースの出版物と文書の種類を比較および分析します：Openalex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、Pubmed。結果は、類型が個々のデータベースプロバイダー間でかなり異なる可能性があることを示しています。さらに、出版物はそれぞれのデータベースで異なる方法で分類されているため、参考文献分析に関連するドキュメントを特定するために必要な研究と非研究テキストの区別は、データソースによって異なる場合があります。この研究の焦点は、横断段階の比較に加えて、主にオープンアレックスに含まれる出版物とドキュメントの種類のカバレッジと分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n', 'DOI: 10.48550/arXiv.2404.01985\\n Title: 彼はOpenalex、Scopus、Web of Scienceのオープンアクセスカバレッジ he open access coverage of OpenAlex, Scopus and Web of Science\\nAbstract: Diamond Open Access（OA）ジャーナルは、著者と読者の両方に無料の公開モデルを提供しますが、主要な書誌データベースでのインデックスの欠如は、これらのジャーナルの取り込みを評価する際の課題を提示します。さらに、出版言語や出版国などのOAの特性は、OAジャーナルがより多様であり、地域社会にサービスを提供することを目指しているという議論を支持するためにしばしば使用されてきましたが、OAジャーナルの地理的および言語的特性に関連する経験的証拠の現在の欠如があります。 OpenAlexとオープンアクセスジャーナルのディレクトリをベンチマークとして使用して、このペーパーでは、フィールド、国、言語による科学とスコープスのWebでの著者とジャーナルの報道を通じて、ダイヤモンドと金の報道を調査します。結果は、WOSとSCOPUSでのより低いカバレッジ、およびダイヤモンドOAの局所範囲を示しています。英語のみのジャーナルのシェアは、ゴールドジャーナルの間でかなり高くなっています。高所得国は、社会科学と人文科学のダイヤモンドジャーナルを除き、すべてのドメインとジャーナルの種類で著者のシェアが最も高い。ダイヤモンドOAインデックスの現在の景観を理解することは、より包括的なOAモデルに向けて政策と実践を進めることで、学術通信ネットワークを支援することができます。 Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2401.16359', '10.3145/epi.2023.mar.09', '10.48550/arXiv.2406.15154', '10.48550/arXiv.2404.01985']\n",
      "\u001b[93mMy lady, here is a summary of the key features and limitations of OpenAlex as a bibliometric database:\n",
      "\n",
      "## Summary:\n",
      "OpenAlex is a promising open-source database of scholarly metadata, offering free and open access to data for bibliometric studies. It is a rapidly evolving source with expanding and quickly changing data, raising questions about its trustworthiness. OpenAlex has been compared to established databases like Scopus and Web of Science, demonstrating average source reference numbers and internal coverage rates comparable to both. However, its limitations, particularly regarding metadata accuracy and completeness, require further research for a comprehensive understanding.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.17663\n",
      "This study compares OpenAlex to Scopus across various dimensions. It concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, especially at the country level. However, issues with metadata accuracy and completeness highlight the need for additional research to fully grasp and address OpenAlex's limitations.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2401.16359\n",
      "This report investigates the reference coverage and selected metadata within OpenAlex, Web of Science, and Scopus. It finds that OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases. The analysis also reveals mixed results when comparing other core metadata covered by OpenAlex, broken down by journal.\n",
      "\n",
      "## DOI: 10.3145/epi.2023.mar.09\n",
      "With the retirement of Microsoft Academic Graph (MAG), OpenAlex was introduced as a similar resource. This study compares the metadata between the latest MAG snapshot and an early OpenAlex snapshot. It finds that OpenAlex preserves bibliographic data, including publication year, volume, first and last page, DOI, and the number of references. More than 90% of MAG documents have equivalent document types in OpenAlex, and the document type specifications have improved significantly. OpenAlex also has more documents with subject classification assignments than MAG.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2406.15154\n",
      "This research compares and analyses publication and document types in OpenAlex, Scopus, Web of Science, Semantic Scholar, and PubMed. The results show that typologies can differ considerably between database providers, and the distinction between research and non-research texts can vary, affecting the identification of relevant documents for bibliometric analysis. The study focuses on the coverage and analysis of publications and document types in OpenAlex, highlighting its growing importance as a free alternative to established proprietary providers.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.01985\n",
      "This paper investigates the open access coverage of OpenAlex, Scopus, and Web of Science. It uses OpenAlex and the Directory of Open Access Journals as benchmarks to study the coverage of diamond and gold journals through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. The results show lower coverage in WoS and Scopus for diamond OA, with a local scope and a higher share of English-only journals among gold journals.\n",
      "45\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.999764, '10.48550/arXiv.2404.17663'), (0.9930153, '10.48550/arXiv.2401.16359'), (0.971779, '10.3145/epi.2023.mar.09'), (0.90665317, '10.48550/arXiv.2406.15154'), (0.3060083, '10.48550/arXiv.2404.01985')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/s11192-015-1765-5\\n Title: 彼の科学とスコープスのWebの記事をジャーナル：比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌メソッドは、さまざまな目的、つまり研究評価のために複数の分野で使用されます。ほとんどの書誌分析には、Thomson ReutersのWeb of Science（WOS）とElsevierのScopusなどのデータソースが共通しています。この研究の目的は、これらの2つのデータベースのジャーナル報道を説明し、何らかの分野、出版国、言語が過剰または過小評価されているかどうかを評価することです。これを行うために、WOS（13,605のジャーナル）とScopus（20,346のジャーナル）のアクティブな学術雑誌の報道と、Ulrichの広範な定期的なディレクトリ（63,013雑誌）と比較しました。結果は、研究評価のためにWOSまたはSCOPUSのいずれかを使用すると、自然科学と工学を支持するバイアス、ならびに社会科学と芸術と人文科学を損なう生物医学的研究が導入される可能性があることを示しています。同様に、英語のジャーナルは、他の言語の損害に過大評価されています。両方のデータベースはこれらのバイアスを共有していますが、カバレッジは大幅に異なります。結果として、書誌分析の結果は、使用されるデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈では、特に異なる分野、機関、国、または言語を比較する場合、WOSとSCOPUSを注意して使用する必要があることを意味します。書誌コミュニティは、フィールド固有および国家引用指数など、WOSやSCOPUSでカバーされていない科学的生産量を含む方法と指標を開発するための努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: 書誌分析のためのオープンアレックスの適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: ScopusとWeb of Scienceは、これらの従来のデータベースが特定の分野と世界地域を体系的に過小評価していたにもかかわらず、科学の研究の基盤となっています。これに応じて、新しい包括的データベース、特にOpenAlexが登場しました。多くの研究がデータソースとしてOpenAlexを使用し始めていますが、その制限を批判的に評価する人はほとんどいません。 Openalexチームと協力して実施されたこの研究は、Openalexを多くの次元にわたってScopusと比較することにより、このギャップに対処します。分析では、OpenalexはScopusのスーパーセットであり、特に国レベルでの一部の分析には信頼できる代替手段になる可能性があると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、Openalexの制限を完全に理解し、対処するために追加の研究が必要であることを示しています。これを行うには、より制約されたデータベースではまったく可能ではない分析を含む、より広い分析セットでOpenalexを自信を持って使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science and Scopusと比較したOpenalexの参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: Openalexは、学術的メタデータの有望なオープンソースであり、Web of ScienceやScopusなどの確立された独自の情報源の競争相手です。 OpenAlexはデータを自由かつ公然と提供するため、研究者は障壁をライセンスすることなくコミュニティで再現できる書誌研究を実施することができます。ただし、OpenAlexは急速に進化するソースであり、内部に含まれるデータが拡大し、急速に変化しているため、そのデータの信頼性に関しては自然に疑問が生じます。このレポートでは、各データベース内の参照カバレッジと選択されたメタデータを調査し、それらを互いに比較して、書誌におけるこの未解決の質問に対処するのに役立ちます。大規模な研究では、3つのデータベースすべてが共有する1680万人の最近の出版物のクリーン化されたデータセットに制限されている場合、OpenAlexは科学とSCOPUSの両方に匹敵する平均ソース参照番号と内部カバレッジ率を持っていることを実証します。さらに、科学のWeb、Web of Science and Scopus by Journalのメタデータを分析し、Openalexと比較して科学とScopusのWebのソース参照カウントの分布に類似していることがわかります。また、OpenAlexで覆われた他のコアメタデータの比較は、ジャーナルによって分割されたときに混合結果を示し、より多くのORCID識別子、より少ないアブストラクト、および科学のWebとScopusの両方と比較した場合、記事ごとに同様の数のオープンアクセスステータスインジケーターをキャプチャすることを示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データソースの大規模な比較：Scopus、Web of Science、Dimensions、CrossRef、およびMicrosoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: Scopus、Web of Science、Dimensions、CrossRef、およびMicrosoft Academicの5つの学際的な参考文献データソースの大規模な比較を提示します。この比較では、これらのデータソースでカバーされている2008年から2017年の期間の科学文書を考慮しています。 Scopusは、他のそれぞれのデータソースとペアワイズで比較されます。まず、ドキュメントのカバレッジのデータソース間の違いを分析します。たとえば、時間の経過とともに違い、ドキュメントタイプあたりの違い、および分野あたりの違いに焦点を当てます。次に、引用リンクの完全性と精度の違いを調べます。分析に基づいて、さまざまなデータソースの長所と短所について説明します。科学文献の包括的な報道と、文献を選択するための柔軟なフィルターのセットを組み合わせることの重要性を強調しています。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: Openalex、Web of Science、Scopus、PubMed、およびSemantic Sc\\u200b\\u200bholarの出版物と文書タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、次の書誌データベースの出版物と文書の種類を比較および分析します：Openalex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、Pubmed。結果は、類型が個々のデータベースプロバイダー間でかなり異なる可能性があることを示しています。さらに、出版物はそれぞれのデータベースで異なる方法で分類されているため、参考文献分析に関連するドキュメントを特定するために必要な研究と非研究テキストの区別は、データソースによって異なる場合があります。この研究の焦点は、横断段階の比較に加えて、主にオープンアレックスに含まれる出版物とドキュメントの種類のカバレッジと分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1007/s11192-015-1765-5', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2401.16359', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154']\n",
      "\u001b[93mSummary: Web of Science (WoS) is a bibliometric database that has been the foundation for research in the science of science, despite its systematic underrepresentation of certain disciplines and world regions.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663\n",
      "Scopus and WoS have been the foundation for research in the science of science, even though these traditional databases systematically underrepresent certain disciplines and world regions.\n",
      "\n",
      "DOI: 10.48550/arXiv.2401.16359\n",
      "WoS is a rapidly evolving source, and the data contained within is expanding and quickly changing, which raises questions about the trustworthiness of its data.\n",
      "\n",
      "DOI: 10.1007/s11192-015-1765-5\n",
      "Most bibliometric analyses have in common their data sources: Thomson Reuters’ WoS and Elsevier’s Scopus. The results of bibliometric analyses may vary depending on the database used.\n",
      "\n",
      "DOI: 10.1162/qss_a_00112\n",
      "The strengths and weaknesses of different data sources are discussed in this study, which emphasises the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.15154\n",
      "Typologies can differ considerably between individual database providers, and the distinction between research and non-research texts can vary depending on the data source.\n",
      "45\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9947187, '10.1007/s11192-015-1765-5'), (0.96073127, '10.48550/arXiv.2404.17663'), (0.71721, '10.48550/arXiv.2401.16359'), (0.63793355, '10.1162/qss_a_00112'), (0.44269672, '10.48550/arXiv.2406.15154')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.1007/978-3-031-88708-6_3\\n Title: 関連性はレトリバーからジェネレーターにぼろきれに伝播されますか？ Is Relevance Propagated from Retriever to Generator in RAG?\\nAbstract: 検索拡張生成（RAG）は、通常、コレクションから取得された一連のドキュメントの形で、プロンプトの大規模な言語モデル（LLM）への一連のドキュメントの形で、質問の回答などの下流タスクのパフォーマンスを潜在的に改善するためのフレームワークです。一連のトップランクのドキュメントの関連性を最大化するという標準検索タスクの目的とは異なり、RAGシステムの目的は、ドキュメントのユーティリティがLLMプロンプトの追加コンテキスト情報の一部としてそれを含めることがダウンストリームタスクを改善するかどうかを示します。既存の研究では、知識集約型の言語タスク（KILT）のRAGコンテキストの関連性の役割を調査します。対照的に、私たちの仕事では、関連性は、情報を求めるタスクのクエリとドキュメントの間の局所的な重複の関連性に対応しています。具体的には、IRテストコレクションを利用して、局所的に関連するドキュメントで構成されるRAGコンテキストが下流のパフォーマンスの改善につながるかどうかを経験的に調査します。私たちの実験は、次の発見につながります。（a）関連性と有用性の間には小さな正の相関があります。 （b）この相関は、コンテキストサイズの増加とともに減少します（k-shotのkの値が高い）。 （c）より効果的な検索モデルは、一般に、下流のラグパフォーマンスの向上につながります。 Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n', \"DOI: 10.48550/arXiv.2505.18247\\n Title: メタゲンブレンドラグ：特殊なドメインの質問を解決するためのゼロショット精度のロックを解除する MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索された生成（RAG）は、ドメイン固有のエンタープライズデータセットとの闘いであり、しばしばファイアウォールの背後に隔離され、トレーニング前にLLMSによって見えない複雑で特殊な用語が豊富です。医学、ネットワーキング、または法律などのドメイン間のセマンティックな変動は、Ragのコンテキストの精度を妨げますが、微調整ソリューションはコストがかかり、遅く、新しいデータが出現するにつれて一般化が欠けています。微調整せずにレトリーバーでゼロショット精度を達成することは、依然として重要な課題です。メタデータの生成パイプラインとハイブリッドクエリインデックスを介してセマンティックレトリバーを強化する新しいエンタープライズ検索アプローチである「メタゲンブレンドラグ」を紹介します。重要な概念、トピック、頭字語を活用することにより、メタデータが豊富なセマンティックインデックスを作成し、ハイブリッドクエリをブーストし、微調整せずに堅牢でスケーラブルなパフォーマンスを提供します。 Biomedical PubMedqaデータセットでは、Metagenブレンドラグは82％の回収精度と77％のRAG精度を達成し、以前のゼロショットラグベンチマークをすべて上回り、そのデータセットの微調整されたモデルに匹敵し、SquadやNQのようなデータセットでも優れています。このアプローチは、特殊なドメイン全体で比類のない一般化を備えたセマンティックレトリバーを構築するための新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\", 'DOI: 10.1609/aaai.v38i16.29728\\n Title: 検索された世代の大規模な言語モデルのベンチマーク Benchmarking Large Language Models in Retrieval-Augmented Generation\\nAbstract: 検索された生成（RAG）は、大規模な言語モデル（LLM）の幻覚を緩和するための有望なアプローチです。ただし、既存の研究には、さまざまな大規模な言語モデルに対する検索された生成の影響に関する厳密な評価がありません。これにより、異なるLLMのRAGの機能における潜在的なボトルネックを特定することが困難になります。この論文では、検索された生成が大規模な言語モデルに与える影響を体系的に調査します。ノイズの堅牢性、否定的な拒否、情報統合、反事実的堅牢性など、RAG\\u200b\\u200bに必要な4つの基本能力におけるさまざまな大手言語モデルのパフォーマンスを分析します。この目的のために、英語と中国語の両方でRAG評価のための新しいコーパスである検索された生成ベンチマーク（RGB）を確立します。 RGBは、ベンチマーク内のインスタンスを、ケースを解決するために必要な前述の基本能力に基づいて、4つの個別のテストベッドに分割します。次に、RGBの6つの代表LLMを評価して、RAGを適用する際に現在のLLMの課題を診断します。評価により、LLMはある程度のノイズの堅牢性を示していますが、否定的な拒絶、情報統合、誤った情報への対処に関して依然として著しく苦労していることが明らかになりました。前述の評価の結果は、LLMにRAGを効果的に適用するために、まだかなりの旅がまだあることを示しています。 Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.13213', '10.1007/978-3-031-88708-6_3', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2505.18247', '10.1609/aaai.v38i16.29728']\n",
      "\u001b[93mSummary: RAG (Retrieval-Augmented Generation) is a framework that incorporates external knowledge to improve the performance of downstream tasks, such as question answering.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213\n",
      "RAG enables the retrieval of relevant information from external knowledge sources, allowing large language models (LLMs) to answer queries over previously unseen document collections. However, traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. A new method called Multi-Meta-RAG uses database filtering with LLM-extracted metadata to improve the RAG selection of relevant documents from various sources.\n",
      "\n",
      "DOI: 10.1007/978-3-031-88708-6_3\n",
      "RAG is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as part of a prompt to an LLM. The objective of a RAG system is to maximise the total utility of the documents, where the utility indicates whether including the document as additional contextual information in an LLM prompt improves a downstream task.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "This paper proposes a method called M-RAG (Metadata Retrieval-Augmented Generation) for effective search in open-domain question answering (ODQA) systems. M-RAG utilises embeddings that include metadata and employs generative models such as GPT-3.5-Turbo-16K and GPT-4 for automated answer generation. Experimental results show that M-RAG exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247\n",
      "RAG struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialised terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision. A new approach called MetaGen Blended RAG introduces a metadata generation pipeline and hybrid query indexes to enhance semantic retrievers. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks.\n",
      "\n",
      "DOI: 10.1609/aaai.v38i16.29728\n",
      "RAG is a promising approach for mitigating the hallucination of LLMs. However, existing research lacks rigorous evaluation of the impact of RAG on different LLMs. This paper systematically investigates the impact of RAG on LLMs, analysing their performance in four fundamental abilities required for RAG. The results indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\n",
      "My lady, I hope this information is helpful!\n",
      "45\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.956\n",
      "Recall: 0.956\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.887\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9997565, '10.48550/arXiv.2406.13213'), (0.999617, '10.1007/978-3-031-88708-6_3'), (0.9992563, '10.6109/jkiice.2023.27.12.1489'), (0.9956006, '10.48550/arXiv.2505.18247'), (0.8362874, '10.1609/aaai.v38i16.29728')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1371/journal.pbio.1002542\\n Title: 引用メトリック：正規化する方法（NOT）の入門書 Citation Metrics: A Primer on How (Not) to Normalize\\nAbstract: 引用指標は、公開された研究を評価するためにますます使用されています。課題の1つは、科学分野の違い、年齢（出版年）、ドキュメントの種類、データベースカバレッジ、およびその他の要因を説明するために、これらのメトリックを正規化するかどうか、および方法です。さまざまなアプローチを使用して、正規化の長所と短所について説明します。科学者、機関、雑誌、または国のコーパスを評価するために、複数の論文で引用指標を組み合わせる必要がある場合、および多著者の論文でクレジットを属性にしようとする場合、追加の課題が現れます。異なる引用メトリックは補完的な洞察を提供する可能性がありますが、計算の根底にある仮定を慎重に検討する必要があります。 Citation metrics are increasingly used to appraise published research. One challenge is whether and how to normalize these metrics to account for differences across scientific fields, age (year of publication), type of document, database coverage, and other factors. We discuss the pros and cons for normalizations using different approaches. Additional challenges emerge when citation metrics need to be combined across multiple papers to appraise the corpus of scientists, institutions, journals, or countries, as well as when trying to attribute credit in multiauthored papers. Different citation metrics may offer complementary insights, but one should carefully consider the assumptions that underlie their calculation.\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542']\n",
      "\u001b[93mSummary: Normalizing citation metrics across scientific fields is challenging due to differences in scientific fields, age (year of publication), document type, database coverage, and other factors.\n",
      "\n",
      "DOI: 10.1371/journal.pbio.1002542\n",
      "Citation metrics are increasingly used to evaluate published research. One challenge is whether and how to normalize these metrics to account for differences across scientific fields, age (year of publication), type of document, database coverage, and other factors. The article discusses the pros and cons of different normalization approaches. Additional challenges arise when citation metrics need to be combined across multiple papers to evaluate the corpus of scientists, institutions, journals, or countries, and when trying to attribute credit in multi-authored papers. Different citation metrics may provide complementary insights, but one should carefully consider the assumptions underlying their calculation.\n",
      "\n",
      "My lady.\n",
      "45\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.600\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.99984646, '10.1371/journal.pbio.1002542')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.5281/ZENODO.13960973\\n Title: 欠落しているデータパターンを使用して、書誌データセットで誤って割り当てられた記事を検出する Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\\nAbstract: DORA宣言とCoaraは、オープンデータに基づいた書誌指標の使用を求めています。ただし、確立された学術的メタデータデータセットは閉じられており、オープンデータセットの品質はまだ徹底的に調査されていません。この論文では、欠落データパターンを使用してデータセット内のエラーを検出する方法を提示します。例として、この方法は、ETHチューリッヒに関連する出版物の所属メタデータに適用されます。これにより、一連の誤って提携した論文を特定することができます。このペーパーで導入された方法は、提携データ用に特別に設計されておらず、他のタイプのデータのエラーを検出するためにも使用できます。それは、プロバイダーとデータのユーザーに利益をもたらすことを願っている修正につながる可能性があります。 The DORA declaration and CoARA call for the use of bibliometric indicators based on open data. However, established scholarly metadata datasets are closed, and the quality of open datasets has not yet been thoroughly examined. In this paper, I present a method to detect errors in a dataset using missing data patterns. As an example, the method is applied to the affiliation metadata of publications associated with ETH Zurich. This allows me to identify a series of incorrectly affiliated papers. The method introduced in this paper is not specifically designed for affiliation data and can also be used to detect errors in other types of data. It could lead to corrections which will hopefully benefit providers as well as users of data.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: CrossRefデータのDOIエラーによる無効な引用の識別と修正 Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この作業は、CrossRefで利用可能なオープンな参考文献メタデータを分析することにより、DOIの間違いのクラスを特定し、どの出版社がそのような間違いを担当し、これらの誤ったDOIの数を自動プロセスで修正できるかを強調することを目的としています。 By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs.これらの引用は、CrossRefの関連する引用データをアップロードする責任を負う責任者の有効性と出版社を追跡することで処理しました。最後に、無効なDOIの事実上の誤りのパターンと、それらをキャッチして修正するために必要な正規表現を特定しました。この研究の結果は、無効な引用の大部分に責任を負い、および/または影響を受けた出版社は少数であることを示しています。過去の研究で提案されたDOI名エラーの分類法を拡張し、以前のアプローチよりも無効なDOIでより多くのミスをきれいにすることができる、より詳細に精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的観点からDOIの間違いの可能な理由を調査し、出版社が無効な引用データの生産の根底にある問題を特定するのに役立ちます。また、私たちが提示するDOIクリーニングメカニズムは、既存のプロセス（COCIなど）に統合して、間違ったDOIを自動的に修正することで引用を追加できます。この研究は、オープンサイエンスの原則に厳密に従っていたため、私たちの研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.5281/ZENODO.13960973', '10.1007/s11192-022-04367-w']\n",
      "\u001b[93mMy lady, there are several methods used to detect and correct errors in bibliographic datasets.\n",
      "\n",
      "## Summary:\n",
      "Missing data patterns can be used to detect errors in datasets.\n",
      "\n",
      "## DOI: 10.5281/ZENODO.13960973\n",
      "This paper presents a method to detect errors in a dataset using missing data patterns. For example, the method is applied to the affiliation metadata of publications associated with ETH Zurich, allowing the identification of incorrectly affiliated papers. The method can also be used to detect errors in other types of data, potentially leading to corrections that benefit data providers and users.\n",
      "\n",
      "## DOI: 10.1007/s11192-022-04367-w\n",
      "This study aims to identify and correct invalid citations due to DOI errors in Crossref data. By analysing open bibliographic metadata available in Crossref, the research highlights which publishers are responsible for such mistakes and how many incorrect DOIs can be corrected through automatic processes. The study also proposes an extension to the taxonomy of DOI name errors and defines more detailed regular expressions to clean a higher number of mistakes in invalid DOIs.\n",
      "45\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.933\n",
      "Recall: 0.933\n",
      "F1-Score: 0.933\n",
      "Accuracy: 0.933\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.99960643, '10.5281/ZENODO.13960973'), (0.6302299, '10.1007/s11192-022-04367-w')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/978-3-031-88708-6_3\\n Title: 関連性はレトリバーからジェネレーターにぼろきれに伝播されますか？ Is Relevance Propagated from Retriever to Generator in RAG?\\nAbstract: 検索拡張生成（RAG）は、通常、コレクションから取得された一連のドキュメントの形で、プロンプトの大規模な言語モデル（LLM）への一連のドキュメントの形で、質問の回答などの下流タスクのパフォーマンスを潜在的に改善するためのフレームワークです。一連のトップランクのドキュメントの関連性を最大化するという標準検索タスクの目的とは異なり、RAGシステムの目的は、ドキュメントのユーティリティがLLMプロンプトの追加コンテキスト情報の一部としてそれを含めることがダウンストリームタスクを改善するかどうかを示します。既存の研究では、知識集約型の言語タスク（KILT）のRAGコンテキストの関連性の役割を調査します。対照的に、私たちの仕事では、関連性は、情報を求めるタスクのクエリとドキュメントの間の局所的な重複の関連性に対応しています。具体的には、IRテストコレクションを利用して、局所的に関連するドキュメントで構成されるRAGコンテキストが下流のパフォーマンスの改善につながるかどうかを経験的に調査します。私たちの実験は、次の発見につながります。（a）関連性と有用性の間には小さな正の相関があります。 （b）この相関は、コンテキストサイズの増加とともに減少します（k-shotのkの値が高い）。 （c）より効果的な検索モデルは、一般に、下流のラグパフォーマンスの向上につながります。 Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG：メタデータ検索の高等世代によるオープンドメインの質問応答を強化します M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: このホワイトペーパーでは、1つ以上のドキュメントのオープンドメイン質問応答（ODQA）システムで効果的な検索のために、メタデータ検索の高等発電（M-RAG）と呼ばれる方法を提案し、そのパフォーマンスを比較します。これを達成するために、メタデータを含む埋め込みを利用し、自動回答生成にGPT-3.5-Turbo-16KやGPT-4などの生成モデルを使用します。このアプローチを通じて、生成モデル（GPT-3.5、GPT-4）は、メタデータを介したクエリドキュメントの順序とコンテキストを理解することができます。さらに、迅速なエンジニアリングを通じてソース情報と元のテキスト要件を組み込むことにより、問題回答（QA）のソース属性機能をアクティブにし、それにより回答の精度を向上させます。この論文の結果として、LLMが持たない情報は外部ソースから取得でき、適切な応答を見つけることができます。実験結果は、この方法が同じ外部推論ODQAシステムと比較して最大46％のパフォーマンス改善を示し、既存のRAGメソッドよりも6％の改善を示したことを示しています。 This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n', \"DOI: 10.48550/arXiv.2404.13948\\n Title: ぼろきれの背中を壊したYPO ypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\\nAbstract: 最近の大規模な言語モデル（LLMS）の堅牢性は、さまざまなドメインや現実世界のアプリケーションで適用性が拡大するにつれてますます重要になっています。検索された生成（RAG）は、LLMの限界に対処するための有望なソリューションですが、RAGの堅牢性に関する既存の研究は、しばしば、RAGコンポーネント間の相互接続された関係またはマイナーなテキストエラーなどの実際のデータベースで一般的な潜在的な脅威を見落としています。この作業では、RAGの堅牢性を評価する際に、2つの未掘りの側面を調査します。1）低レベルの摂動を通じて騒々しい文書に対する脆弱性と2）Ragの堅牢性の全体的な評価。さらに、これらの側面をターゲットにする新しい攻撃法であるRag（\\\\ textit {garag}）に対する遺伝的攻撃を紹介します。具体的には、GARAGは各コンポーネント内の脆弱性を明らかにし、騒々しいドキュメントに対してシステム全体の機能をテストするように設計されています。 \\\\ textIT {garag}を標準のQAデータセットに適用し、多様なレトリバーとLLMを組み込んで、ragの堅牢性を検証します。実験結果は、Garagが一貫して高い攻撃の成功率を達成することを示しています。また、各コンポーネントのパフォーマンスとその相乗効果を大幅に破壊し、現実世界のぼろきれシステムを混乱させる際にマイナーなテキストの不正確さがもたらす実質的なリスクを強調しています。 The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\\\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\\n\"]\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1007/978-3-031-88708-6_3', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2404.13948']\n",
      "\u001b[93mMy lady, RAG, or Retrieval Augmented Generation, is a framework that incorporates external knowledge to improve the performance of downstream tasks, such as question answering. It does this by retrieving relevant information from an external knowledge source and feeding it to a large language model (LLM) as additional context.\n",
      "\n",
      "The goal of a RAG system is to maximise the utility of documents, where utility indicates whether including a document as part of the LLM prompt improves a downstream task. This differs from standard retrieval tasks, which aim to maximise the relevance of a set of top-ranked documents.\n",
      "\n",
      "RAG has been found to enhance open-domain question answering (ODQA) systems, with one method, M-RAG, demonstrating up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over existing RAG methods.\n",
      "\n",
      "However, RAG's robustness has been questioned, with studies highlighting its vulnerability to noisy documents and the potential risks posed by minor textual inaccuracies.\n",
      "\n",
      "DOI: 10.1007/978-3-031-88708-6_3\n",
      "DOI: 10.48550/arXiv.2406.13213\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "DOI: 10.48550/arXiv.2404.13948\n",
      "45\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.889\n",
      "Recall: 0.889\n",
      "F1-Score: 0.889\n",
      "Accuracy: 0.889\n",
      "Balanced accuracy: 0.675\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.94834626, '10.1007/978-3-031-88708-6_3'), (0.5391045, '10.48550/arXiv.2406.13213'), (0.38225675, '10.6109/jkiice.2023.27.12.1489'), (0.37410447, '10.48550/arXiv.2404.13948')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13\n",
      "\u001b[95m!!!!! All Done!!!!!\n"
     ]
    }
   ],
   "source": [
    "#run the test from here\n",
    "\n",
    "\n",
    "#***** Begin chat session *****\n",
    "# set directory path\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_jats\"\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_multi_lang\"\n",
    "\n",
    "# read documents and dois from the directory path\n",
    "documents_with_doi = read_documents_with_doi(directory_path)\n",
    "documents = [doc[0].split('\\n')[1:] for doc in documents_with_doi]\n",
    "print(f\"Length of documents: {len(documents)}\")\n",
    "print(f\"Length of corpus: {len(documents_with_doi)}\")\n",
    "\n",
    "# Countdown function\n",
    "def countdown(seconds:int)->None:\n",
    "    # Loop until seconds is 0\n",
    "    while seconds > 0:\n",
    "        print(Fore.LIGHTMAGENTA_EX + f\"{seconds}\", end='      \\r')  # Print current countdown value\n",
    "        time.sleep(1)  # Wait for 1 second\n",
    "        seconds -= 1  # Decrease seconds by 1\n",
    "    print(\"The time has come!\")  # Countdown finished message\n",
    "\n",
    "def evaluate_retrieval(retrieved_dois, ground_truth, response, query:str,reranked_DOIs_with_score_end)->Dict:\n",
    "    corpus_doi_list = []\n",
    "    #corpus_list is a global variable in rag_pipeline()\n",
    "    for each in range(len(documents_with_doi)):\n",
    "        #a = documents_with_doi[each].get('doi',\"\")\n",
    "        a = documents_with_doi[each].split(\"\\n\")[0].lstrip(\"DOI: \")\n",
    "        corpus_doi_list.append(a)\n",
    "    print(len(corpus_doi_list))\n",
    "\n",
    "    def compare_lists(list1, list2, list3):\n",
    "        for val in list1:\n",
    "            if val in list2:\n",
    "                list3.append(1)\n",
    "            else:\n",
    "                list3.append(0)\n",
    "\n",
    "    #set y_true so that len(y_true)==len(corpus_doi_list)\n",
    "    y_true = []\n",
    "    compare_lists(corpus_doi_list,ground_truth,y_true)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = []\n",
    "    compare_lists(corpus_doi_list,retrieved_dois,y_pred)\n",
    "\n",
    "\n",
    "    # calculate metrics - could also use sklearn.metrics functions such as precision_score, but this is easier to read\n",
    "    precision = precision_score(y_true, y_pred, average=\"micro\")\n",
    "    recall = recall_score(y_true, y_pred,average=\"micro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    faithfulness_score = 0\n",
    "    for each in retrieved_dois:\n",
    "        if each in response.message.content[0].text:\n",
    "            faithfulness_score+=1\n",
    "        else:\n",
    "            faithfulness_score+=0\n",
    "\n",
    "        \n",
    "    return {\n",
    "        'Query':f\"{query}\",\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\":accuracy,\n",
    "        \"Balanced accuracy\":balanced_accuracy,\n",
    "        \"Faithfulness score\":faithfulness_score,\n",
    "        \"Documents score\":str(reranked_DOIs_with_score_end),\n",
    "        \"Response\":response.message.content[0].text\n",
    "    }\n",
    "\n",
    "def print_results(retrieved_dois, ground_truth, response, query:str, reranked_DOIs_with_score_end)->Dict:\n",
    "    \"\"\"\n",
    "    Prints a nicely ordered set of results from evalaute_retrieval()\n",
    "    \"\"\"\n",
    "\n",
    "    results = evaluate_retrieval(retrieved_dois, ground_truth, response, query, reranked_DOIs_with_score_end)\n",
    "    print(f\"For query: {results['Query']}:\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"F1-Score: {results['F1-Score']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    print(f\"Balanced accuracy: {results['Balanced accuracy']:.3f}\")\n",
    "    print(f\"Faithfulness score: {results['Faithfulness score']}\")\n",
    "    print(f\"Documents score: {results['Documents score']}\")\n",
    "    return results\n",
    "\n",
    "#print_results()\n",
    "\n",
    "def cohere_test_loop(query:str,ground_truth:List[str]):\n",
    "\n",
    "    # set top_k\n",
    "    top_k = 5\n",
    "    #set threshold \n",
    "    threshold = 0.10\n",
    "\n",
    "    response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,query,top_k,threshold)\n",
    "    \n",
    "    # Extract DOIs from retrieved documents\n",
    "    retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "    print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "    # Display the response\n",
    "    print(Fore.LIGHTYELLOW_EX + f\"{response.message.content[0].text}\")\n",
    "\n",
    "    new_result = print_results(retrieved_dois, ground_truth, response, query, reranked_DOIs_with_score_end)\n",
    "    # add the new result to the df\n",
    "    results_df.loc[len(results_df)] = new_result\n",
    "\n",
    "    #save the queries and responses to separate dataframe to be manually annontated\n",
    "    answer_relevance_df = results_df[['Query','Response']].copy(deep=True)\n",
    "\n",
    "    # save out answer_relevance_df\n",
    "    filename=\"analysis/dense_answer_relevance_results.xlsx\"\n",
    "    answer_relevance_df.to_excel(filename)\n",
    "\n",
    "    filename = \"analysis/dense_analysis_results.xlsx\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    results_df.to_excel(filename)\n",
    "\n",
    "    # rate limit functions\n",
    "    seconds = 10\n",
    "    print(Fore.LIGHTRED_EX + f\"Waiting for {seconds} seconds...\")\n",
    "    countdown(seconds)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "#golden_set_df_test['Response\\nDense'] = golden_set_df_test.apply(lambda x: test_loop(x.query,x.ground_truth), axis=1)\n",
    "golden_set_df_query = golden_set_df['query'].to_list()\n",
    "golden_set_df_ground_truth = golden_set_df['ground_truth'].to_list()\n",
    "\n",
    "loop_length = 5\n",
    "while loop_length:\n",
    "    for i in range(len(golden_set_df_query)):\n",
    "        \n",
    "        cohere_test_loop([golden_set_df_query[i]],golden_set_df_ground_truth[i])\n",
    "        print(Fore.LIGHTCYAN_EX + f\"Working on row: {i}\")\n",
    "    loop_length = loop_length-1\n",
    "\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"!!!!! All Done!!!!!\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
