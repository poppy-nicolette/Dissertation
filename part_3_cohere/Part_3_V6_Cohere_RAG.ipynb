{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere RAG with dense retriever and ReRank model\n",
    "- references: https://docs.cohere.com/v2/docs/rag-complete-example\n",
    "- [ ] check performance with SciFact\n",
    "<br>\n",
    "“This work was supported by compute credits from a Cohere Labs Research Grant, these grants are designed to support academic partners conducting research with the goal of releasing scientific artifacts and data for good projects.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import cohere\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import sys\n",
    "from colorama import Fore, Style, Back\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mall is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "# load secret from local .env file\n",
    "def get_key():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    _key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "    #verify if it worked\n",
    "    if _key is not None:\n",
    "        print(Fore.GREEN + \"all is good, beautiful!\")\n",
    "        return _key\n",
    "    else:\n",
    "        print(Fore.LIGHTRED_EX + \"API Key is missing\")\n",
    "\n",
    "# initilize client\n",
    "co = cohere.ClientV2(get_key())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0]\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append(f\"{doi} {text}\\n\")\n",
    "    return documents_with_doi\n",
    "\n",
    "# initialize empty search query\n",
    "search_queries = []\n",
    "# Embed the documents\n",
    "def document_embed(documents:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the documents from a list provided from read_documents_with_doi()\n",
    "    NOTE: Change the model based on the test condition\n",
    "    \"\"\"\n",
    "    doc_emb = co.embed(\n",
    "        #model=\"embed-v4.0\",\n",
    "        model=\"embed-english-v3.0\",\n",
    "        input_type=\"search_document\",\n",
    "        texts=[doc for doc in documents],\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return doc_emb\n",
    "\n",
    "# Embed the search query\n",
    "def query_embed(search_queries:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the query from a list provided in search_queries variable\n",
    "    NOTE: change model depending on test condition\n",
    "    \"\"\"\n",
    "    query_emb = co.embed(\n",
    "        #model=\"embed-v4.0\",\n",
    "        model=\"embed-english-v3.0\",\n",
    "        input_type=\"search_query\",\n",
    "        texts=search_queries,\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return query_emb\n",
    "\n",
    "# retrieve top_k and compute similarity using dot product\n",
    "def retrieve_top_k(top_k, query_embedded, documents_embedded, documents)->List[str]:\n",
    "    \"\"\"\n",
    "    returns the top_k documents based on dot product similarity\n",
    "    \"\"\"\n",
    "\n",
    "    scores = np.dot(query_embedded, np.transpose(documents_embedded))[0]#ordered list!\n",
    "    # takes top scores, and returns sorted list and returns indices sliced by top_k\n",
    "    max_idx = np.argsort(-scores)[:top_k]\n",
    "    # returns documents by index\n",
    "    retrieved_docs = [documents[item] for item in max_idx]\n",
    "    # returns a list of documents\n",
    "    return retrieved_docs\n",
    "\n",
    "def rerank_documents(retrieved_documents,search_queries,threshold,top_k)->List[str]:\n",
    "    \"\"\"\n",
    "    takes retrieved_documents as input along with search_queries and runs them through the \n",
    "    rerank model from cohere for semantic similarity. \n",
    "\n",
    "    top_n = top_k\n",
    "    Limits those returned by a threshold score. this is to reduce those that are irrelevant.\n",
    "\n",
    "    NOTE: change the model based on the test condition\n",
    "    \"\"\"\n",
    "    # Rerank the documents\n",
    "    results = co.rerank(\n",
    "        #model=\"rerank-v3.5\",\n",
    "        model=\"rerank-english-v3.0\",\n",
    "        query=search_queries[0],\n",
    "        documents=[doc for doc in retrieved_documents],\n",
    "        top_n=top_k,\n",
    "        max_tokens_per_doc=4096,# defaults to 4096\n",
    "    )\n",
    "\n",
    "    # Display the reranking results\n",
    "    #for idx, result in enumerate(results.results):\n",
    "    #    print(f\"Rank: {idx+1}\")\n",
    "    #    print(f\"Score: {result.relevance_score}\")\n",
    "    #    print(f\"Document: {retrieved_documents[result.index]}\\n\")\n",
    "\n",
    "    #returns only those over threshold\n",
    "    reranked_docs = [\n",
    "        retrieved_documents[result.index] for result in results.results if result.relevance_score >=threshold\n",
    "    ]\n",
    "    reranked_with_score = [(result.relevance_score, retrieved_documents[result.index].split(\"\\n\")[0].strip(\"DOI: \")) for result in results.results if result.relevance_score >=threshold]\n",
    "\n",
    "    print(f\"reranked_documents: {reranked_docs}\")\n",
    "    print(f\"length of reranked_documents: {len(reranked_docs)}\")\n",
    "\n",
    "    return reranked_docs, reranked_with_score\n",
    "\n",
    "def cohere_rag_pipeline(directory_path,search_queries,top_k,threshold):\n",
    "\n",
    "    # retrieve documents from directory\n",
    "    documents = read_documents_with_doi(directory_path)\n",
    "    print(f\"Length of documents: {len(documents)}\")\n",
    "\n",
    "    # randomize order of documents\n",
    "    random.shuffle(documents)\n",
    "\n",
    "    # embed the documents\n",
    "    documents_embedded = document_embed(documents)\n",
    "\n",
    "    #embed the query:\n",
    "    query_embedded = query_embed(search_queries)\n",
    "\n",
    "    # retrieve the top_k documents\n",
    "    retrieved_documents = retrieve_top_k(top_k, query_embedded, documents_embedded, documents)\n",
    "\n",
    "    # rerank the documents using the Rerank model from Cohere\n",
    "    reranked_documents, reranked_DOIs_with_score = rerank_documents(retrieved_documents,search_queries,threshold,top_k)\n",
    "    # set system instructions\n",
    "    instructions = \"\"\"\n",
    "                    You are an academic research assistant.\n",
    "                    You must include the DOI in your response.\n",
    "                    If there is no content provided, ask for a different question.\n",
    "                    Please structure your response like this:\n",
    "                    Summary: summary statement here. \n",
    "                    DOI: summary of the text associated with this DOI.\n",
    "                    Address me as, 'my lady'.\n",
    "                    \"\"\"\n",
    "    # create messages to model\n",
    "    messages = [{\"role\":\"user\",\n",
    "                \"content\": search_queries[0]},\n",
    "                {\"role\":\"system\",\n",
    "                \"content\":instructions}]\n",
    "\n",
    "    # Generate the response NOTE: change the model for the test condition!\n",
    "    resp = co.chat(\n",
    "        model=\"command-a-03-2025\",\n",
    "        messages=messages,\n",
    "        documents=reranked_documents,\n",
    "    )\n",
    "\n",
    "    return resp, reranked_documents, reranked_DOIs_with_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#  run here to test functions avove\n",
    "# ****** Pipeline ********\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.1\n",
    "\n",
    "response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "# Display the response\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"{response.message.content[0].text}\")\n",
    "print(Fore.LIGHTCYAN_EX + f\"------\\nReranked documents:\")\n",
    "for doc in reranked_documents_end:\n",
    "    print(doc)\n",
    "\n",
    "# Display the citations and source documents\n",
    "if response.message.citations:\n",
    "    print(Fore.LIGHTYELLOW_EX + \"\\nCITATIONS:\")\n",
    "    for citation in response.message.citations:\n",
    "        print(f\"source text: {citation.text},\\nsource: {citation.sources[0].document.get('content').split(\"\\n\")[0]}\\n------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# Analysis\n",
    "Precision, recall, accuracy, F1 scores and faithfulness\n",
    "## Precision, recall, F1 score\n",
    "### references\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "- https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from colorama import Fore, Back, Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## automated version \n",
    "Currently Works!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Documents score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query, Precision, Recall, F1-Score, Accuracy, Balanced accuracy, Faithfulness score, Documents score, Response]\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial dataframe to capture results from each query and results\n",
    "#ONLY DO THIS AT THE BEGINNING OF THE ANALYSIS PROCEDURE, OTHERWISE, IT WILL ERASE THE PREVIOUS RESULTS!!\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Query','Precision','Recall','F1-Score','Accuracy', 'Balanced accuracy', 'Faithfulness score', 'Documents score', 'Response'])\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m Golder set loaded!\n"
     ]
    }
   ],
   "source": [
    "golden_set_df = pd.read_excel(\"golden_set.xlsx\")\n",
    "#golden_set_df_test = golden_set_df.head(3)\n",
    "#golden_set_df\n",
    "print(Fore.LIGHTGREEN_EX + f\" Golder set loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of documents: 96\n",
      "Length of corpus: 96\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1109/ADL.1998.670425', '10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101']\n",
      "\u001b[93mSummary: Two studies examined the abstract in metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - This study compared the amount of metadata and the completeness degree of research publications in new academic databases. A quantitative approach was used to select a random Crossref sample of more than 115,000 records, which was then searched in seven databases. Seven characteristics were analyzed, including the abstract, to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661 - This study investigated methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. The proposed framework, MetaEnhance, utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. Although the abstract was not explicitly mentioned, the study focused on improving metadata quality, which could potentially include abstract metadata.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need more details, please let me know.\n",
      "96\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.927\n",
      "Recall: 0.927\n",
      "F1-Score: 0.927\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.6092336, '10.1109/ADL.1998.670425'), (0.3526475, '10.1162/qss_a_00286'), (0.23633993, '10.48550/arXiv.2303.17661'), (0.111627996, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1108/JD-10-2022-0234\\n Title: すべての学術分野にわたる引用と参照の習慣の分析: 書誌の参照と引用の実践におけるアプローチと傾向 An analysis of citing and referencing habits across all scholarly disciplines: approaches and trends in bibliographic referencing and citing practices\\nAbstract: この研究で、著者らは、学術文献における引用および参照の誤りについて現在考えられる原因を特定し、スウィートランド氏が1989年の論文で提供したスナップショットから何かが変わったかどうかを比較したいと考えている。,著者らは、27の主題分野にわたる147のジャーナルに掲載された729件の論文から、参考要素、すなわち書誌的参照、言及、引用、およびそれぞれの本文中の参照ポインタを分析した。,分析の結果は、書誌的事項が指摘された。著者らの知る限り、この研究は、Sweetland (1989) 以来、文献における参照および引用の慣行における誤りを分析したものとしては、最近入手可能な最良のものである。 In this study, the authors want to identify current possible causes for citing and referencing errors in scholarly literature to compare if something changed from the snapshot provided by Sweetland in his 1989 paper.,The authors analysed reference elements, i.e. bibliographic references, mentions, quotations and respective in-text reference pointers, from 729 articles published in 147 journals across the 27 subject areas.,The outcomes of the analysis pointed out that bibliographic errors have been perpetuated for decades and that their possible causes have increased, despite the encouraged use of technological facilities, i.e. the reference managers.,As far as the authors know, the study is the best recent available analysis of errors in referencing and citing practices in the literature since Sweetland (1989).\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1108/JD-10-2022-0234']\n",
      "\u001b[93mSummary: Two studies examined citations.\n",
      "DOI: 10.1108/JD-10-2022-0234 analysed reference elements, i.e. bibliographic references, mentions, quotations and respective in-text reference pointers, from 729 articles published in 147 journals across 27 subject areas. The outcomes of the analysis pointed out that bibliographic errors have been perpetuated for decades and that their possible causes have increased, despite the encouraged use of technological facilities, i.e. the reference managers. The study is the best recent available analysis of errors in referencing and citing practices in the literature since Sweetland (1989).\n",
      "96\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 0.958\n",
      "Recall: 0.958\n",
      "F1-Score: 0.958\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.600\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.208018, '10.1108/JD-10-2022-0234')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mSummary: Crossref is a sustainable source of community-owned scholarly metadata and open bibliographic metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022 - Crossref is a major source of scholarly data for publishers, authors, librarians, funders and researchers. It contains over 106 million records and expands at an average rate of 11% a year. The metadata set consists of 13 content types, including traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref is a source of open bibliographic metadata. Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. An up-to-date overview of the availability of six metadata elements in Crossref is presented: reference lists, abstracts, ORCIDs, author affiliations, funding information and license information. The analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realise full openness of bibliographic metadata.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.94468933, '10.1162/qss_a_00022'), (0.28150436, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.1162/qss_a_00286']\n",
      "\u001b[93mSummary: Two papers evaluate the linguistic coverage or language-related metadata in scholarly databases.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 evaluates the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 compares the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, a random Crossref sample of more than 115,000 records was selected, which was then searched in seven databases. Seven characteristics were analyzed to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases.\n",
      "\n",
      "My lady, are there any other questions I can help with?\n",
      "96\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.595\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.9992792, '10.48550/arXiv.2409.10633'), (0.4369223, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: COVID-19 研究への資金提供: オープン データ インフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 資金提供機関が提供する資金の成果を分析するには、資金提供によって得られた出版物を追跡できることが不可欠です。私たちは、Crossref での資金調達データのオープンな利用可能性を調査し、新型コロナウイルス感染症 (COVID-19) に関連する研究を報告する出版物の資金調達データに焦点を当てています。また、Scopus と Web of Science という 2 つの独自の書誌データベースで利用可能な資金調達データとの比較も示します。私たちの分析では、Crossref の資金調達データの範囲が限られていることが明らかになりました。また、特に Scopus における資金調達データの品質に関連する問題も示しています。 Crossref での資金調達データのオープンな可用性を向上させるための推奨事項を提供します。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1162/qss_a_00212', '10.1162/qss_a_00286']\n",
      "\u001b[93mSummary: Four papers address funding metadata, its availability, or its analysis in scholarly databases.\n",
      "\n",
      "DOI: 10.1162/qss_a_00210 - This paper investigates the availability and completeness of open funder metadata, specifically focusing on publications funded by the Dutch Research Council (NWO). It examines 5,004 publications reported as NWO-funded and finds that only 67% contain funding information in Crossref, with varying degrees of acknowledgment of NWO as the funder. The study also compares funding metadata in Crossref with proprietary databases like Web of Science, Scopus, and Dimensions, highlighting differences in coverage and completeness.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This paper explores Crossref as a source of open bibliographic metadata, including funding information. It provides an overview of the availability of six metadata elements in Crossref, showing improvements over time for journal articles. However, it also highlights the need for publishers to make further efforts to achieve full openness of bibliographic metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00212 - This study focuses on funding data for COVID-19 research, analyzing its open availability in Crossref and comparing it with proprietary databases like Scopus and Web of Science. The analysis reveals limited coverage and quality issues in funding data, particularly in Scopus, and offers recommendations for improving open funding data availability in Crossref.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - This research compares the completeness of publication metadata in eight free-access scholarly databases, including Dimensions, Google Scholar, and The Lens. It finds that third-party databases have higher metadata quality and completeness compared to academic search engines, which struggle with retrieving reliable descriptive data. The study highlights the loss of information in third-party databases due to integrating different sources.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need additional assistance, please let me know.\n",
      "96\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.979\n",
      "Recall: 0.979\n",
      "F1-Score: 0.979\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.870\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.98469365, '10.1162/qss_a_00210'), (0.6974029, '10.31222/osf.io/smxe5'), (0.6575384, '10.1162/qss_a_00212'), (0.6167627, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.48550/arXiv.2406.13213', '10.1007/s44427-025-00006-3', '10.6109/jkiice.2023.27.12.1489', '10.18653/v1/2024.eacl-demo.16']\n",
      "\u001b[93mSummary: I found four papers that discuss the use of Retrieval-Augmented Generation (RAG) in applications.\n",
      "\n",
      "DOI: 10.1016/j.caeai.2025.100417\n",
      "This paper discusses the use of RAG in education, specifically in interactive learning systems, educational content generation and assessment, and large-scale deployment in educational ecosystems.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213\n",
      "This paper introduces a new method called Multi-Meta-RAG, which improves RAG for multi-hop queries using database filtering with LLM-extracted metadata.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3\n",
      "This paper evaluates the use of open-source LLMs in RAG systems for academic and structured knowledge tasks, specifically in the context of diploma theses abstracts.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "This paper proposes a Metadata Retrieval-Augmented Generation (M-RAG) method for open-domain question answering, which shows significant performance improvements over conventional RAG methods.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need more details, please let me know.\n",
      "96\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) when used in applications?']:\n",
      "Precision: 0.917\n",
      "Recall: 0.917\n",
      "F1-Score: 0.917\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.99998486, '10.1016/j.caeai.2025.100417'), (0.9986749, '10.48550/arXiv.2406.13213'), (0.9975656, '10.1007/s44427-025-00006-3'), (0.9943141, '10.6109/jkiice.2023.27.12.1489'), (0.9914225, '10.18653/v1/2024.eacl-demo.16')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mSummary: Crossref is a sustainable source of community-owned scholarly metadata that is collected and made available to publishers, authors, librarians, funders, and researchers.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "\n",
      "Crossref is a sustainable source of community-owned scholarly metadata that is collected and made available to publishers, authors, librarians, funders, and researchers. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.99954873, '10.1162/qss_a_00022'), (0.7735587, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science および Scopus と比較した OpenAlex の参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: OpenAlex は、学術メタデータの有望なオープン ソースであり、Web of Science や Scopus などの確立された独自のソースの競合相手です。 OpenAlex はデータを自由かつオープンに提供するため、研究者はライセンスの壁なしにコミュニティ内で再現できる書誌学的研究を実行できます。ただし、OpenAlex は急速に進化するソースであり、そこに含まれるデータは拡大し、また急速に変化しているため、そのデータの信頼性に関する疑問が当然生じます。このレポートでは、各データベース内の参照範囲と選択されたメタデータを調査し、それらを相互に比較して、書誌計量学におけるこの未解決の質問に対処します。私たちの大規模調査では、3 つのデータベースすべてで共有される 1,680 万件の最近の出版物のクリーンなデータセットに限定した場合、OpenAlex は Web of Science と Scopus の両方に匹敵する平均的なソース参照番号と内部カバー率を持っていることを実証しました。さらに、OpenAlex、Web of Science、および Scopus のメタデータをジャーナルごとに分析し、OpenAlex と比較して、Web of Science と Scopus のソース参照数の分布に類似性があることがわかりました。また、OpenAlex がカバーする他のコア メタデータの比較では、ジャーナルごとに分類するとさまざまな結果が示され、Web of Science と Scopus の両方と比較した場合、論文あたりの ORCID 識別子がより多く、抄録が少なく、論文あたりのオープン アクセス ステータス インジケーターの数が同程度であることも示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: OpenAlex、Web of Science、Scopus、Pubmed、Semantic Sc\\u200b\\u200bholar における出版物とドキュメント タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、OpenAlex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、PubMed の書誌データベースの出版物と文書の種類を比較および分析します。この結果は、類型が個々のデータベース プロバイダー間で大きく異なる可能性があることを示しています。さらに、書誌学的分析に関連する文書を特定するために必要な研究文書と非研究文書の区別は、データソースによって異なる場合があります。これは、出版物が各データベースで異なる方法で分類されているためです。 OpenAlex は、図書館や大学における書誌学的分析のための確立された独自のプロバイダーに代わる無料の代替手段としてますます重要になっているため、データベース間の比較に加えて、この研究の焦点は主に OpenAlex に含まれる出版物と文書タイプの範囲と分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n', 'DOI: 10.48550/ARXIV.2205.01833\\n Title: OpenAlex: 学術作品、著者、会場、機関、コンセプトの完全にオープンなインデックス OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts\\nAbstract: OpenAlex は、廃止された Microsoft Academic Graph (MAG) を置き換えるために開始された、新しい完全にオープンな Scientific Knowledge Graph (SKG) です。 2 億 900 万件の作品 (雑誌記事、書籍など) のメタデータが含まれています。 2013M の明確な著者。 124,000 の会場 (ジャーナルやオンライン リポジトリなど、作品をホストする場所)。 109,000の機関。および 65,000 個のウィキデータの概念 (自動化された階層型マルチタグ分類子を介して作品にリンクされています)。データセットは、Web ベースの GUI、完全なデータ ダンプ、および大容量 REST API を介して完全かつ自由に利用できます。このリソースは現在鋭意開発中であり、将来の作業により、引用情報の精度と範囲、著者/機関の解析と重複排除が改善される予定です。 OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2508.18620', '10.48550/arXiv.2401.16359', '10.48550/arXiv.2406.15154', '10.48550/ARXIV.2205.01833']\n",
      "\u001b[93mSummary: OpenAlex is a new bibliographic database that is open and inclusive, with extensive coverage. It is a promising alternative to established proprietary sources such as Web of Science and Scopus. However, there are concerns about the quality and trustworthiness of its metadata, which may hinder its broader adoption.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. However, issues of metadata accuracy and completeness require additional research to fully comprehend and address OpenAlex's limitations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 - OpenAlex's openness and extensive coverage are notable, but concerns about metadata quality may hinder its adoption. This study aims to assess metadata quality in OpenAlex and Web of Science, focusing on document type, publication year, language, and number of authors.\n",
      "\n",
      "DOI: 10.48550/arXiv.2401.16359 - OpenAlex is a promising open source of scholarly metadata, but its rapid evolution raises questions about data trustworthiness. A large-scale study demonstrates that OpenAlex has comparable average source reference numbers and internal coverage rates to Web of Science and Scopus, but mixed results in other core metadata comparisons.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.15154 - OpenAlex is becoming an important free alternative to established proprietary providers for bibliometric analyses. This study compares publication and document types in OpenAlex, Scopus, Web of Science, Semantic Scholar, and PubMed, highlighting differences in typologies and classification methods.\n",
      "\n",
      "DOI: 10.48550/ARXIV.2205.01833 - OpenAlex is a new, fully-open scientific knowledge graph containing metadata for millions of works, authors, venues, institutions, and concepts. The dataset is fully and freely available, and future work will improve citation information and author/institution parsing.\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions or need additional information, please let me know.\n",
      "96\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.99971753, '10.48550/arXiv.2404.17663'), (0.99565166, '10.48550/arXiv.2508.18620'), (0.9932811, '10.48550/arXiv.2401.16359'), (0.97090924, '10.48550/arXiv.2406.15154'), (0.8957053, '10.48550/ARXIV.2205.01833')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1007/s11192-015-1765-5\\n Title: Web of Science と Scopus のジャーナルの報道: 比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌学的手法は、研究の評価など、さまざまな目的で複数の分野で使用されています。ほとんどの書誌学的分析には、トムソン・ロイターの Web of Science (WoS) とエルゼビアの Scopus というデータ ソースが共通しています。この調査の目的は、これら 2 つのデータベースのジャーナルの対象範囲を説明し、特定の分野、出版国、言語が過小評価されているかどうかを評価することです。これを行うために、WoS (13,605 ジャーナル) および Scopus (20,346 ジャーナル) のアクティブな学術ジャーナルの範囲を、ウルリッヒの広範な定期刊行物ディレクトリ (63,013 ジャーナル) と比較しました。結果は、WoS または Scopus を研究評価に使用すると、社会科学、芸術、人文科学に不利益をもたらす、自然科学と工学、生物医学研究に有利なバイアスを導入する可能性があることを示しています。同様に、英語のジャーナルが過大評価され、他の言語に損害を与えています。どちらのデータベースもこれらのバイアスを共有していますが、その範囲は大きく異なります。そのため、書誌情報分析の結果は使用するデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈において、特に異なる分野、機関、国、または言語を比較する場合には、WoS と Scopus を慎重に使用する必要があることを示唆しています。書誌学コミュニティは、分野固有の引用インデックスや全国的な引用インデックスなど、WoS や Scopus ではカバーされていない科学的成果を含む手法や指標を開発する努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: OpenAlex、Web of Science、Scopus、Pubmed、Semantic Sc\\u200b\\u200bholar における出版物とドキュメント タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、OpenAlex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、PubMed の書誌データベースの出版物と文書の種類を比較および分析します。この結果は、類型が個々のデータベース プロバイダー間で大きく異なる可能性があることを示しています。さらに、書誌学的分析に関連する文書を特定するために必要な研究文書と非研究文書の区別は、データソースによって異なる場合があります。これは、出版物が各データベースで異なる方法で分類されているためです。 OpenAlex は、図書館や大学における書誌学的分析のための確立された独自のプロバイダーに代わる無料の代替手段としてますます重要になっているため、データベース間の比較に加えて、この研究の焦点は主に OpenAlex に含まれる出版物と文書タイプの範囲と分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2508.18620', '10.1007/s11192-015-1765-5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154']\n",
      "\u001b[93mSummary: Web of Science (WoS) is a traditional bibliometric database that has been the foundation for research in the science of science. However, it has been criticised for systematically underrepresenting certain disciplines and world regions.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663\n",
      "\n",
      "I'm sorry, my lady, but I was unable to find any further information on the strengths and weaknesses of WoS. Would you like me to answer a different question?\n",
      "96\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.9980882, '10.48550/arXiv.2508.18620'), (0.99554896, '10.1007/s11192-015-1765-5'), (0.95565146, '10.48550/arXiv.2404.17663'), (0.6040486, '10.1162/qss_a_00112'), (0.3408142, '10.48550/arXiv.2406.15154')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.48550/arXiv.2502.15005\\n Title: 研究トピックに関する自然言語クエリを知識組織システムと結び付けるための Socratic RAG アプローチ A Socratic RAG Approach to Connect Natural Language Queries on Research Topics with Knowledge Organization Systems\\nAbstract: この論文では、研究トピックに関する自然言語クエリを正確で機械解釈可能な意味エンティティにマッピングする検索拡張生成 (RAG) エージェントを提案します。私たちのアプローチは、RAG とソクラテス対話を組み合わせて、研究トピックに対するユーザーの直感的な理解を確立された知識組織システム (KOS) に合わせます。提案されたアプローチは、「小さなセマンティクス」（ドメイン固有の KOS 構造）と「大きなセマンティクス」（広範な書誌情報リポジトリ）を効果的に橋渡しし、複雑な学術分類をよりアクセスしやすくします。このような薬剤は広範囲に使用される可能性があります。 CollabNext というサンプル アプリケーションを使って説明します。これは、人、組織、研究トピックを結び付ける個人中心のナレッジ グラフです。さらに、現在の科学システムでは歴史的に目に見えない存在となっていた人々の可視性を高めるために、アプリケーションの設計がどのように HBCU と新進の研究者に意図的に焦点を当てているかについて説明します。 In this paper, we propose a Retrieval Augmented Generation (RAG) agent that maps natural language queries about research topics to precise, machine-interpretable semantic entities. Our approach combines RAG with Socratic dialogue to align a user\\'s intuitive understanding of research topics with established Knowledge Organization Systems (KOSs). The proposed approach will effectively bridge \"little semantics\" (domain-specific KOS structures) with \"big semantics\" (broad bibliometric repositories), making complex academic taxonomies more accessible. Such agents have the potential for broad use. We illustrate with a sample application called CollabNext, which is a person-centric knowledge graph connecting people, organizations, and research topics. We further describe how the application design has an intentional focus on HBCUs and emerging researchers to raise visibility of people historically rendered invisible in the current science system.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.1145/3637528.3671470', '10.48550/arXiv.2502.15005', '10.18653/v1/2024.eacl-demo.16']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is used to improve question answering and information retrieval systems by retrieving relevant information from external knowledge sources, enabling large language models (LLMs) to answer queries over previously unseen document collections.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - Multi-Meta-RAG is a new method that uses database filtering with LLM-extracted metadata to improve the RAG selection of relevant documents from various sources, relevant to the question.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - M-RAG is a method that utilizes embeddings with metadata and generative models to generate automated responses, enabling the generative model to understand the order and context of the document through metadata and answer it.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470 - RAG Meeting LLMs is a survey that comprehensively reviews existing research studies in Retrieval-Augmented Large Language Models (RA-LLMs), covering three primary technical perspectives and discussing current limitations and several promising directions for future research.\n",
      "\n",
      "DOI: 10.48550/arXiv.2502.15005 - Socratic RAG Approach is a proposed RAG agent that maps natural language queries about research topics to precise, machine-interpretable semantic entities, combining RAG with Socratic dialogue to align a user's intuitive understanding of research topics with established Knowledge Organization Systems (KOSs).\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16 - RAGAs is a framework for reference-free evaluation of RAG pipelines, introducing a suite of metrics that can evaluate different dimensions of RAG architectures without relying on ground truth human annotations.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need more details, please let me know.\n",
      "96\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.99971086, '10.48550/arXiv.2406.13213'), (0.99860066, '10.6109/jkiice.2023.27.12.1489'), (0.96741027, '10.1145/3637528.3671470'), (0.96518475, '10.48550/arXiv.2502.15005'), (0.92121595, '10.18653/v1/2024.eacl-demo.16')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: []\n",
      "length of reranked_documents: 0\n",
      "Retrieved DOIs: []\n",
      "\u001b[93mMy lady, normalizing citation metrics across scientific fields is a complex task due to several inherent challenges. Below is a structured response to your query:\n",
      "\n",
      "**Summary:** Normalizing citation metrics across scientific fields is challenging due to differences in publication cultures, citation practices, field-specific impact factors, and varying journal traditions. These factors make it difficult to establish a universal standard for comparing scholarly impact across disciplines.\n",
      "\n",
      "**DOI:** 10.1126/science.1175862  \n",
      "This article by Seglen (1997) in *Science* highlights the variability in citation patterns across different scientific fields, emphasizing that citation counts are heavily influenced by field-specific practices and traditions. The study underscores the difficulty of creating a one-size-fits-all metric for comparing scholarly impact across disciplines.\n",
      "\n",
      "**Additional Context:**  \n",
      "1. **Publication Culture:** Fields like biomedicine tend to produce more papers and citations per researcher compared to humanities or social sciences, where monographs and book chapters are more common.  \n",
      "2. **Citation Practices:** Some fields (e.g., mathematics) cite fewer references per paper, while others (e.g., biology) cite extensively, making raw citation counts incomparable.  \n",
      "3. **Field-Specific Impact Factors:** Journals in high-citation fields (e.g., physics) naturally have higher impact factors, skewing comparisons with low-citation fields (e.g., arts).  \n",
      "4. **Time Lag:** Breakthroughs in some fields (e.g., computer science) are recognized quickly, while others (e.g., humanities) may take years to accrue citations.  \n",
      "\n",
      "These challenges necessitate field-normalized metrics like the **Field-Weighted Citation Impact (FWCI)** or **Journal Normalized Citation Score**, which account for disciplinary differences. However, even these approaches have limitations, as they rely on accurate field categorization and citation databases.\n",
      "\n",
      "If you'd like further details or a different perspective, my lady, please let me know!\n",
      "96\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.500\n",
      "Faithfulness score: 0\n",
      "Documents score: []\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1007/s11192-022-04367-w', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: I'm sorry, my lady, I could not find any information about methods used to detect and correct errors in bibliographic datasets.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w discusses identifying and correcting invalid citations due to DOI errors in Crossref data.\n",
      "\n",
      "DOI: 10.5281/ZENODO.6188748 discusses classes of errors in DOI names and an evaluation dataset.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.595\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.5874699, '10.1007/s11192-022-04367-w'), (0.34224167, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.1145/3681780.3697252\\n Title: 文変換と検索拡張生成 (RAG) による書誌分析の自動化: 影響力の高い都市研究のためのカスタマイズされた文献の特徴付けのための意味論的および文脈検索のパイロット研究 Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG): A Pilot Study in Semantic and Contextual Search for Customized Literature Characterization for High-Impact Urban Research\\nAbstract: 書誌学的分析は、都市科学、特にネイチャー ポートフォリオなどの影響力の大きいジャーナルにおける研究の傾向、範囲、影響を理解するために不可欠です。ただし、キーワード検索や基本的な NLP テクニックに依存する従来の方法では、記事のタイトルやキーワードに明示的に記載されていない貴重な洞察を発見できないことがよくあります。これらのアプローチでは、意味検索や文脈の理解を実行できないため、トピックの分類や研究の特徴付けにおける有効性が制限されます。このペーパーでは、生成 AI モデル、特にトランスフォーマーと検索拡張生成 (RAG) を活用して書誌学的分析を自動化および強化することで、これらの制限に対処します。私たちは、ベクトル データベース、センテンス トランスフォーマー、ガウス混合モデル (GMM)、検索エージェント、および大規模言語モデル (LLM) を統合する技術ワークフローを開発し、カスタマイズされたプロンプト テンプレートを使用した文脈検索、トピックのランキング、および研究の特徴付けを可能にしました。過去 10 年間に Nature Communications に掲載された都市科学関連の 223 件の論文を分析したパイロット研究では、影響力の高いジャーナルの論文の質、範囲、特徴に関する洞察力に富んだ要約統計を生成する際の私たちのアプローチの有効性が明らかになりました。この研究は、都市研究における書誌学的分析と知識検索を強化するための新しいパラダイムを導入し、AI エージェントを研究の評価と理解を進めるための強力なツールとして位置づけています。 Bibliometric analysis is essential for understanding research trends, scope, and impact in urban science, especially in high-impact journals, such Nature Portfolios. However, traditional methods, relying on keyword searches and basic NLP techniques, often fail to uncover valuable insights not explicitly stated in article titles or keywords. These approaches are unable to perform semantic searches and contextual understanding, limiting their effectiveness in classifying topics and characterizing studies. In this paper, we address these limitations by leveraging Generative AI models, specifically transformers and Retrieval-Augmented Generation (RAG), to automate and enhance bibliometric analysis. We developed a technical workflow that integrates a vector database, Sentence Transformers, a Gaussian Mixture Model (GMM), Retrieval Agent, and Large Language Models (LLMs) to enable contextual search, topic ranking, and characterization of research using customized prompt templates. A pilot study analyzing 223 urban science-related articles published in Nature Communications over the past decade highlights the effectiveness of our approach in generating insightful summary statistics on the quality, scope, and characteristics of papers in high-impact journals. This study introduces a new paradigm for enhancing bibliometric analysis and knowledge retrieval in urban research, positioning an AI agent as a powerful tool for advancing research evaluation and understanding.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', \"DOI: 10.48550/arXiv.2510.11195\\n Title: RAG-Pull: コード生成のための RAG システムに対する知覚できない攻撃 RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation\\nAbstract: 検索拡張生成 (RAG) は、LLM 応答の信頼性と信頼性を高め、モデルの再トレーニングの必要性を排除することで幻覚を軽減します。これは、LLM のコンテキストに外部データを追加することによって行われます。私たちは、隠れた UTF 文字をクエリまたは外部コード リポジトリに挿入し、検索を悪意のあるコードにリダイレクトして、モデルの安全性の調整を破る新しいクラスのブラック ボックス攻撃である RAG-Pull を開発しました。クエリとコードの摂動だけでは、検索が攻撃者が制御するスニペットにシフトする可能性がある一方、クエリとターゲットの摂動を組み合わせるとほぼ完璧な成功が得られることが観察されています。これらのスニペットが取得されると、リモート コード実行や SQL インジェクションなどの悪用可能な脆弱性が生じます。 RAG-Pull の最小限の摂動は、モデルの安全性の調整を変更し、安全でないコードへの優先順位を高める可能性があるため、LLM に対する新しい種類の攻撃を可能にします。 Retrieval-Augmented Generation (RAG) increases the reliability and trustworthiness of the LLM response and reduces hallucination by eliminating the need for model retraining. It does so by adding external data into the LLM's context. We develop a new class of black-box attack, RAG-Pull, that inserts hidden UTF characters into queries or external code repositories, redirecting retrieval toward malicious code, thereby breaking the models' safety alignment. We observe that query and code perturbations alone can shift retrieval toward attacker-controlled snippets, while combined query-and-target perturbations achieve near-perfect success. Once retrieved, these snippets introduce exploitable vulnerabilities such as remote code execution and SQL injection. RAG-Pull's minimal perturbations can alter the model's safety alignment and increase preference towards unsafe code, therefore opening up a new class of attacks on LLMs.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2406.13213', '10.1145/3681780.3697252', '10.1007/s44427-025-00006-3', '10.48550/arXiv.2510.11195']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a system that enhances language models by incorporating external knowledge sources.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16: RAG systems are composed of a retrieval and an LLM-based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213: RAG enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, traditional RAG applications perform poorly in answering multi-hop questions.\n",
      "\n",
      "DOI: 10.1145/3681780.3697252: RAG is used to automate and enhance bibliometric analysis, addressing limitations of traditional methods by leveraging Generative AI models.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3: RAG systems enhance language models by incorporating external knowledge sources. However, evaluating their effectiveness remains a challenge.\n",
      "\n",
      "DOI: 10.48550/arXiv.2510.11195: RAG increases the reliability and trustworthiness of the LLM response and reduces hallucination by eliminating the need for model retraining.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.917\n",
      "Recall: 0.917\n",
      "F1-Score: 0.917\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9552357, '10.18653/v1/2024.eacl-demo.16'), (0.6951311, '10.48550/arXiv.2406.13213'), (0.63906056, '10.1145/3681780.3697252'), (0.55823666, '10.1007/s44427-025-00006-3'), (0.46128976, '10.48550/arXiv.2510.11195')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1109/ADL.1998.670425', '10.1177/09610006241239080', '10.1145/2964909', '10.5860/crl.86.1.101']\n",
      "\u001b[93mSummary: Metadata quality assessment is a critical area of research, with challenges including the lack of consensus on definitions and measurements of quality dimensions, and the need for flexible assessment tools. Advancements include the development of automated quality assessment frameworks, the identification of key quality dimensions, and the exploration of metadata quality issues across cultures.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - This study emphasizes the importance of metadata quality research and highlights the need to improve the flexibility of metadata quality assessment tools. It identifies completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness as commonly used dimensions in metadata quality assessment but notes the lack of consensus on their definitions and measurements.\n",
      "\n",
      "DOI: 10.1145/2964909 - This work aims to develop a generic metadata quality assessment framework for open data portals, addressing concerns about the quality of data sources and corresponding metadata. It defines several quality metrics that can be evaluated automatically and efficiently, contributing to a more complete understanding of metadata quality issues.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 - This study explores metadata quality issues across cultures, identifying 32 unique issues and classifying them into 5 categories. It highlights the impact of metadata quality, consistency, and completeness on individuals and communities, emphasizing the need to consider sociocultural representations and resource constraints in metadata quality assessment.\n",
      "\n",
      "My lady, I hope this response addresses your question. If you have any further inquiries or would like more details on a specific aspect, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in metadata quality assessment?']:\n",
      "Precision: 0.906\n",
      "Recall: 0.906\n",
      "F1-Score: 0.906\n",
      "Accuracy: 0.906\n",
      "Balanced accuracy: 0.478\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.93546456, '10.1109/ADL.1998.670425'), (0.92905515, '10.1177/09610006241239080'), (0.85034436, '10.1145/2964909'), (0.20291664, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101', '10.1177/09610006241239080', '10.1145/2964909']\n",
      "\u001b[93mSummary: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661 - Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 - Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\n",
      "\n",
      "DOI: 10.1145/2964909 - The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\n",
      "\n",
      "My lady, I hope this answers your question.\n",
      "96\n",
      "For query: ['What are the key challenges and potential solutions for improving metadata quality?']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.795\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.9813107, '10.48550/arXiv.2303.17661'), (0.42161134, '10.5860/crl.86.1.101'), (0.35064372, '10.1177/09610006241239080'), (0.23196201, '10.1145/2964909')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 14 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.1007/s44427-025-00006-3', '10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3: This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. The authors generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, they evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16: The authors introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of RAG pipelines. They discuss the challenges of evaluating RAG architectures, which include assessing the ability of the retrieval system to identify relevant context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213: This paper introduces Multi-Meta-RAG, a new method that uses database filtering with LLM-extracted metadata to improve RAG selection of relevant documents for multi-hop queries. The authors found that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489: The authors propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system. Their method utilizes embeddings with metadata and generative models to generate automated responses, resulting in a performance improvement of up to 46% compared to the same external inference ODQA system.\n",
      "\n",
      "My lady, I hope this summary of the key challenges and advancements in RAG systems is helpful. If you have any further questions or would like more information on a specific aspect, please don't hesitate to ask.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in Retrieval-Augmented Generation (RAG) systems?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.9996923, '10.1016/j.caeai.2025.100417'), (0.99849296, '10.1007/s44427-025-00006-3'), (0.99836427, '10.18653/v1/2024.eacl-demo.16'), (0.9962638, '10.48550/arXiv.2406.13213'), (0.9233162, '10.6109/jkiice.2023.27.12.1489')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 15 in loop: 5\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1109/ADL.1998.670425', '10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101']\n",
      "\u001b[93mSummary: Two studies examined the abstract in metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - This study compared the amount of metadata and the completeness degree of research publications in new academic databases. Seven characteristics were analyzed, including the abstract.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 - This study sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, 32 unique issues were identified, including the abstract.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.927\n",
      "Recall: 0.927\n",
      "F1-Score: 0.927\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.6092336, '10.1109/ADL.1998.670425'), (0.3526475, '10.1162/qss_a_00286'), (0.23633993, '10.48550/arXiv.2303.17661'), (0.111627996, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1108/JD-10-2022-0234\\n Title: すべての学術分野にわたる引用と参照の習慣の分析: 書誌の参照と引用の実践におけるアプローチと傾向 An analysis of citing and referencing habits across all scholarly disciplines: approaches and trends in bibliographic referencing and citing practices\\nAbstract: この研究で、著者らは、学術文献における引用および参照の誤りについて現在考えられる原因を特定し、スウィートランド氏が1989年の論文で提供したスナップショットから何かが変わったかどうかを比較したいと考えている。,著者らは、27の主題分野にわたる147のジャーナルに掲載された729件の論文から、参考要素、すなわち書誌的参照、言及、引用、およびそれぞれの本文中の参照ポインタを分析した。,分析の結果は、書誌的事項が指摘された。著者らの知る限り、この研究は、Sweetland (1989) 以来、文献における参照および引用の慣行における誤りを分析したものとしては、最近入手可能な最良のものである。 In this study, the authors want to identify current possible causes for citing and referencing errors in scholarly literature to compare if something changed from the snapshot provided by Sweetland in his 1989 paper.,The authors analysed reference elements, i.e. bibliographic references, mentions, quotations and respective in-text reference pointers, from 729 articles published in 147 journals across the 27 subject areas.,The outcomes of the analysis pointed out that bibliographic errors have been perpetuated for decades and that their possible causes have increased, despite the encouraged use of technological facilities, i.e. the reference managers.,As far as the authors know, the study is the best recent available analysis of errors in referencing and citing practices in the literature since Sweetland (1989).\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1108/JD-10-2022-0234']\n",
      "\u001b[93mSummary: This study examined citing and referencing habits across all scholarly disciplines.\n",
      "DOI: 10.1108/JD-10-2022-0234\n",
      "\n",
      "The study analysed reference elements, i.e. bibliographic references, mentions, quotations and respective in-text reference pointers, from 729 articles published in 147 journals across the 27 subject areas.\n",
      "\n",
      "The outcomes of the analysis pointed out that bibliographic errors have been perpetuated for decades and that their possible causes have increased, despite the encouraged use of technological facilities, i.e. the reference managers.\n",
      "\n",
      "As far as the authors know, the study is the best recent available analysis of errors in referencing and citing practices in the literature since Sweetland (1989).\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "96\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 0.958\n",
      "Recall: 0.958\n",
      "F1-Score: 0.958\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.600\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.208018, '10.1108/JD-10-2022-0234')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mSummary: Crossref is a sustainable source of community-owned scholarly metadata and a source of open bibliographic metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022 - Crossref is a major source of scholarly data for publishers, authors, librarians, funders, and researchers. It contains over 106 million records and is expanding at an average rate of 11% a year. The metadata set consists of 13 content types, including traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref is a source of open bibliographic metadata. Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. An analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.94468933, '10.1162/qss_a_00022'), (0.28150436, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.1162/qss_a_00286']\n",
      "\u001b[93mSummary: Two papers evaluate the linguistic coverage or language-related metadata in scholarly databases.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633\n",
      "Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "\n",
      "DOI: 10.1162/qss_a_00286\n",
      "Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "\n",
      "Can I help you with anything else, my lady?\n",
      "96\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.595\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.9992792, '10.48550/arXiv.2409.10633'), (0.4369223, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: COVID-19 研究への資金提供: オープン データ インフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 資金提供機関が提供する資金の成果を分析するには、資金提供によって得られた出版物を追跡できることが不可欠です。私たちは、Crossref での資金調達データのオープンな利用可能性を調査し、新型コロナウイルス感染症 (COVID-19) に関連する研究を報告する出版物の資金調達データに焦点を当てています。また、Scopus と Web of Science という 2 つの独自の書誌データベースで利用可能な資金調達データとの比較も示します。私たちの分析では、Crossref の資金調達データの範囲が限られていることが明らかになりました。また、特に Scopus における資金調達データの品質に関連する問題も示しています。 Crossref での資金調達データのオープンな可用性を向上させるための推奨事項を提供します。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1162/qss_a_00212', '10.1162/qss_a_00286']\n",
      "\u001b[93mSummary: Three papers address funding metadata, its availability, or its analysis in scholarly databases.\n",
      "\n",
      "DOI: 10.1162/qss_a_00210 - This paper looks at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This paper presents an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information.\n",
      "\n",
      "DOI: 10.1162/qss_a_00212 - This paper studies the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19.\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "96\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.979\n",
      "Recall: 0.979\n",
      "F1-Score: 0.979\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.870\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.98469365, '10.1162/qss_a_00210'), (0.6974029, '10.31222/osf.io/smxe5'), (0.6575384, '10.1162/qss_a_00212'), (0.6167627, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.48550/arXiv.2406.13213', '10.1007/s44427-025-00006-3', '10.6109/jkiice.2023.27.12.1489', '10.18653/v1/2024.eacl-demo.16']\n",
      "\u001b[93mI'm sorry, my lady, I am unable to answer your question.\n",
      "\n",
      "Is there anything else I can help you with?\n",
      "96\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) when used in applications?']:\n",
      "Precision: 0.917\n",
      "Recall: 0.917\n",
      "F1-Score: 0.917\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 0\n",
      "Documents score: [(0.99998486, '10.1016/j.caeai.2025.100417'), (0.9986749, '10.48550/arXiv.2406.13213'), (0.9975656, '10.1007/s44427-025-00006-3'), (0.9943141, '10.6109/jkiice.2023.27.12.1489'), (0.9914225, '10.18653/v1/2024.eacl-demo.16')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mSummary: Crossref is a sustainable source of community-owned scholarly metadata that is collected and made available for use in the scholarly research ecosystem.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022 - Crossref has over 106 million records and is expanding at an average rate of 11% a year. It has become a major source of scholarly data for publishers, authors, librarians, funders, and researchers.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref is a source of open bibliographic metadata and several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications.\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "96\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.99954873, '10.1162/qss_a_00022'), (0.7735587, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science および Scopus と比較した OpenAlex の参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: OpenAlex は、学術メタデータの有望なオープン ソースであり、Web of Science や Scopus などの確立された独自のソースの競合相手です。 OpenAlex はデータを自由かつオープンに提供するため、研究者はライセンスの壁なしにコミュニティ内で再現できる書誌学的研究を実行できます。ただし、OpenAlex は急速に進化するソースであり、そこに含まれるデータは拡大し、また急速に変化しているため、そのデータの信頼性に関する疑問が当然生じます。このレポートでは、各データベース内の参照範囲と選択されたメタデータを調査し、それらを相互に比較して、書誌計量学におけるこの未解決の質問に対処します。私たちの大規模調査では、3 つのデータベースすべてで共有される 1,680 万件の最近の出版物のクリーンなデータセットに限定した場合、OpenAlex は Web of Science と Scopus の両方に匹敵する平均的なソース参照番号と内部カバー率を持っていることを実証しました。さらに、OpenAlex、Web of Science、および Scopus のメタデータをジャーナルごとに分析し、OpenAlex と比較して、Web of Science と Scopus のソース参照数の分布に類似性があることがわかりました。また、OpenAlex がカバーする他のコア メタデータの比較では、ジャーナルごとに分類するとさまざまな結果が示され、Web of Science と Scopus の両方と比較した場合、論文あたりの ORCID 識別子がより多く、抄録が少なく、論文あたりのオープン アクセス ステータス インジケーターの数が同程度であることも示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: OpenAlex、Web of Science、Scopus、Pubmed、Semantic Sc\\u200b\\u200bholar における出版物とドキュメント タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、OpenAlex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、PubMed の書誌データベースの出版物と文書の種類を比較および分析します。この結果は、類型が個々のデータベース プロバイダー間で大きく異なる可能性があることを示しています。さらに、書誌学的分析に関連する文書を特定するために必要な研究文書と非研究文書の区別は、データソースによって異なる場合があります。これは、出版物が各データベースで異なる方法で分類されているためです。 OpenAlex は、図書館や大学における書誌学的分析のための確立された独自のプロバイダーに代わる無料の代替手段としてますます重要になっているため、データベース間の比較に加えて、この研究の焦点は主に OpenAlex に含まれる出版物と文書タイプの範囲と分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n', 'DOI: 10.48550/ARXIV.2205.01833\\n Title: OpenAlex: 学術作品、著者、会場、機関、コンセプトの完全にオープンなインデックス OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts\\nAbstract: OpenAlex は、廃止された Microsoft Academic Graph (MAG) を置き換えるために開始された、新しい完全にオープンな Scientific Knowledge Graph (SKG) です。 2 億 900 万件の作品 (雑誌記事、書籍など) のメタデータが含まれています。 2013M の明確な著者。 124,000 の会場 (ジャーナルやオンライン リポジトリなど、作品をホストする場所)。 109,000の機関。および 65,000 個のウィキデータの概念 (自動化された階層型マルチタグ分類子を介して作品にリンクされています)。データセットは、Web ベースの GUI、完全なデータ ダンプ、および大容量 REST API を介して完全かつ自由に利用できます。このリソースは現在鋭意開発中であり、将来の作業により、引用情報の精度と範囲、著者/機関の解析と重複排除が改善される予定です。 OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2508.18620', '10.48550/arXiv.2401.16359', '10.48550/arXiv.2406.15154', '10.48550/ARXIV.2205.01833']\n",
      "\u001b[93mSummary: OpenAlex is a new, open bibliometric database that has gained popularity for its extensive coverage and accessibility. However, concerns about metadata quality and rapid evolution have been raised, requiring further research to fully understand its limitations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - OpenAlex is a superset of Scopus, offering a reliable alternative for some analyses, particularly at the country level. However, metadata accuracy and completeness issues require additional research to fully comprehend and address its limitations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 - OpenAlex's metadata quality is a concern, particularly regarding document type, publication year, language, and author count discrepancies. Addressing these issues is crucial for its broader adoption in research and evaluation.\n",
      "\n",
      "DOI: 10.48550/arXiv.2401.16359 - OpenAlex has comparable average source reference numbers and internal coverage rates to Web of Science and Scopus, but its rapid evolution raises questions about data trustworthiness.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.15154 - OpenAlex is becoming an important free alternative to established proprietary providers, but publication and document type classifications may vary across databases.\n",
      "\n",
      "DOI: 10.48550/ARXIV.2205.01833 - OpenAlex is a fully-open scientific knowledge graph containing metadata for 209 million works, with ongoing development to improve citation information accuracy and author/institution parsing.\n",
      "\n",
      "My lady, I hope this summary provides a comprehensive overview of OpenAlex's key features and limitations as a bibliometric database. If you have any further questions or would like more details on specific aspects, please let me know.\n",
      "96\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.99971753, '10.48550/arXiv.2404.17663'), (0.99565166, '10.48550/arXiv.2508.18620'), (0.9932811, '10.48550/arXiv.2401.16359'), (0.97090924, '10.48550/arXiv.2406.15154'), (0.8957053, '10.48550/ARXIV.2205.01833')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1007/s11192-015-1765-5\\n Title: Web of Science と Scopus のジャーナルの報道: 比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌学的手法は、研究の評価など、さまざまな目的で複数の分野で使用されています。ほとんどの書誌学的分析には、トムソン・ロイターの Web of Science (WoS) とエルゼビアの Scopus というデータ ソースが共通しています。この調査の目的は、これら 2 つのデータベースのジャーナルの対象範囲を説明し、特定の分野、出版国、言語が過小評価されているかどうかを評価することです。これを行うために、WoS (13,605 ジャーナル) および Scopus (20,346 ジャーナル) のアクティブな学術ジャーナルの範囲を、ウルリッヒの広範な定期刊行物ディレクトリ (63,013 ジャーナル) と比較しました。結果は、WoS または Scopus を研究評価に使用すると、社会科学、芸術、人文科学に不利益をもたらす、自然科学と工学、生物医学研究に有利なバイアスを導入する可能性があることを示しています。同様に、英語のジャーナルが過大評価され、他の言語に損害を与えています。どちらのデータベースもこれらのバイアスを共有していますが、その範囲は大きく異なります。そのため、書誌情報分析の結果は使用するデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈において、特に異なる分野、機関、国、または言語を比較する場合には、WoS と Scopus を慎重に使用する必要があることを示唆しています。書誌学コミュニティは、分野固有の引用インデックスや全国的な引用インデックスなど、WoS や Scopus ではカバーされていない科学的成果を含む手法や指標を開発する努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: OpenAlex、Web of Science、Scopus、Pubmed、Semantic Sc\\u200b\\u200bholar における出版物とドキュメント タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、OpenAlex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、PubMed の書誌データベースの出版物と文書の種類を比較および分析します。この結果は、類型が個々のデータベース プロバイダー間で大きく異なる可能性があることを示しています。さらに、書誌学的分析に関連する文書を特定するために必要な研究文書と非研究文書の区別は、データソースによって異なる場合があります。これは、出版物が各データベースで異なる方法で分類されているためです。 OpenAlex は、図書館や大学における書誌学的分析のための確立された独自のプロバイダーに代わる無料の代替手段としてますます重要になっているため、データベース間の比較に加えて、この研究の焦点は主に OpenAlex に含まれる出版物と文書タイプの範囲と分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2508.18620', '10.1007/s11192-015-1765-5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154']\n",
      "\u001b[93mSummary: I'm sorry, my lady, I couldn't find any information about the strengths and weaknesses of Web of Science (WoS) as a bibliometric database.\n",
      "\n",
      "However, I did find information about the suitability of OpenAlex for bibliometric analyses and a comparison of bibliographic data sources, including Web of Science.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 discusses the suitability of OpenAlex for bibliometric analyses and compares it with Scopus. The study concludes that OpenAlex is a reliable alternative for some analyses, particularly at the country level. However, it also highlights issues with metadata accuracy and completeness, suggesting that additional research is needed to fully comprehend and address OpenAlex's limitations.\n",
      "\n",
      "DOI: 10.1162/qss_a_00112 presents a large-scale comparison of five multidisciplinary bibliographic data sources, including Web of Science. The study emphasizes the importance of combining comprehensive coverage of scientific literature with flexible filters for selecting literature. However, it does not specifically discuss the strengths and weaknesses of Web of Science.\n",
      "\n",
      "My lady, would you like me to assist you with another question?\n",
      "96\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.9980882, '10.48550/arXiv.2508.18620'), (0.99554896, '10.1007/s11192-015-1765-5'), (0.95565146, '10.48550/arXiv.2404.17663'), (0.6040486, '10.1162/qss_a_00112'), (0.3408142, '10.48550/arXiv.2406.15154')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.48550/arXiv.2502.15005\\n Title: 研究トピックに関する自然言語クエリを知識組織システムと結び付けるための Socratic RAG アプローチ A Socratic RAG Approach to Connect Natural Language Queries on Research Topics with Knowledge Organization Systems\\nAbstract: この論文では、研究トピックに関する自然言語クエリを正確で機械解釈可能な意味エンティティにマッピングする検索拡張生成 (RAG) エージェントを提案します。私たちのアプローチは、RAG とソクラテス対話を組み合わせて、研究トピックに対するユーザーの直感的な理解を確立された知識組織システム (KOS) に合わせます。提案されたアプローチは、「小さなセマンティクス」（ドメイン固有の KOS 構造）と「大きなセマンティクス」（広範な書誌情報リポジトリ）を効果的に橋渡しし、複雑な学術分類をよりアクセスしやすくします。このような薬剤は広範囲に使用される可能性があります。 CollabNext というサンプル アプリケーションを使って説明します。これは、人、組織、研究トピックを結び付ける個人中心のナレッジ グラフです。さらに、現在の科学システムでは歴史的に目に見えない存在となっていた人々の可視性を高めるために、アプリケーションの設計がどのように HBCU と新進の研究者に意図的に焦点を当てているかについて説明します。 In this paper, we propose a Retrieval Augmented Generation (RAG) agent that maps natural language queries about research topics to precise, machine-interpretable semantic entities. Our approach combines RAG with Socratic dialogue to align a user\\'s intuitive understanding of research topics with established Knowledge Organization Systems (KOSs). The proposed approach will effectively bridge \"little semantics\" (domain-specific KOS structures) with \"big semantics\" (broad bibliometric repositories), making complex academic taxonomies more accessible. Such agents have the potential for broad use. We illustrate with a sample application called CollabNext, which is a person-centric knowledge graph connecting people, organizations, and research topics. We further describe how the application design has an intentional focus on HBCUs and emerging researchers to raise visibility of people historically rendered invisible in the current science system.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.1145/3637528.3671470', '10.48550/arXiv.2502.15005', '10.18653/v1/2024.eacl-demo.16']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a technique that improves the performance of question answering and information retrieval systems by retrieving relevant information from external knowledge sources and enabling large language models (LLMs) to answer queries over previously unseen document collections.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - This paper introduces a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of relevant documents from various sources, relevant to the question.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - This paper proposes a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470 - This survey comprehensively reviews existing research studies in Retrieval-Augmented Large Language Models (RA-LLMs), covering three primary technical perspectives and discussing current limitations and several promising directions for future research.\n",
      "\n",
      "DOI: 10.48550/arXiv.2502.15005 - This paper proposes a RAG agent that maps natural language queries about research topics to precise, machine-interpretable semantic entities by combining RAG with Socratic dialogue.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16 - This paper introduces RAGAs, a framework for reference-free evaluation of RAG pipelines, which can evaluate different dimensions of RAG architectures without relying on ground truth human annotations.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please don't hesitate to ask.\n",
      "96\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.99971086, '10.48550/arXiv.2406.13213'), (0.99860066, '10.6109/jkiice.2023.27.12.1489'), (0.96741027, '10.1145/3637528.3671470'), (0.96518475, '10.48550/arXiv.2502.15005'), (0.92121595, '10.18653/v1/2024.eacl-demo.16')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: []\n",
      "length of reranked_documents: 0\n",
      "Retrieved DOIs: []\n",
      "\u001b[93mMy lady, normalizing citation metrics across scientific fields is a complex task due to several inherent challenges. Below is a structured response to your query:\n",
      "\n",
      "**Summary:** The main challenges in normalizing citation metrics across scientific fields include differences in publication and citation practices, varying journal cultures, field-specific citation rates, and the impact of disciplinary norms on metric interpretation.\n",
      "\n",
      "**DOI:** 10.1073/pnas.1616297114  \n",
      "This article, \"The Leiden Manifesto for Research Metrics,\" discusses the importance of considering field-specific differences when applying citation metrics. It highlights that fields vary in their publication rates, citation practices, and the types of journals they use, making direct comparisons between disciplines problematic. The authors argue for a more nuanced approach to research evaluation that takes these differences into account.\n",
      "\n",
      "**Additional Challenges:**\n",
      "1. **Publication Practices:** Some fields, like biomedicine, publish more frequently and in larger volumes compared to fields like mathematics or humanities, where publications are fewer but often more impactful.\n",
      "2. **Citation Rates:** Citation density varies widely across disciplines. For example, physics and mathematics tend to have lower citation rates per paper compared to molecular biology or clinical medicine.\n",
      "3. **Journal Culture:** The prestige and impact of journals differ across fields, and some disciplines rely more heavily on conference proceedings or books, which are not always captured in citation databases.\n",
      "4. **Time to Impact:** The time it takes for research to be cited varies by field. Fast-paced fields like computer science may see rapid citation accumulation, whereas humanities research may take years to gain recognition.\n",
      "\n",
      "These challenges underscore the need for field-normalized metrics, such as those provided by the **Field-Weighted Citation Impact (FWCI)**, which adjusts citation counts based on disciplinary averages.\n",
      "\n",
      "If you'd like further details or a different perspective, my lady, please let me know!\n",
      "96\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.500\n",
      "Faithfulness score: 0\n",
      "Documents score: []\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1007/s11192-022-04367-w', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: I'm sorry, my lady, I am unable to answer your question.\n",
      "DOI: 10.1007/s11192-022-04367-w and 10.5281/ZENODO.6188748 both discuss methods for identifying and correcting errors in bibliographic datasets, but do not specify what these methods are.\n",
      "96\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.595\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.5874699, '10.1007/s11192-022-04367-w'), (0.34224167, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.1145/3681780.3697252\\n Title: 文変換と検索拡張生成 (RAG) による書誌分析の自動化: 影響力の高い都市研究のためのカスタマイズされた文献の特徴付けのための意味論的および文脈検索のパイロット研究 Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG): A Pilot Study in Semantic and Contextual Search for Customized Literature Characterization for High-Impact Urban Research\\nAbstract: 書誌学的分析は、都市科学、特にネイチャー ポートフォリオなどの影響力の大きいジャーナルにおける研究の傾向、範囲、影響を理解するために不可欠です。ただし、キーワード検索や基本的な NLP テクニックに依存する従来の方法では、記事のタイトルやキーワードに明示的に記載されていない貴重な洞察を発見できないことがよくあります。これらのアプローチでは、意味検索や文脈の理解を実行できないため、トピックの分類や研究の特徴付けにおける有効性が制限されます。このペーパーでは、生成 AI モデル、特にトランスフォーマーと検索拡張生成 (RAG) を活用して書誌学的分析を自動化および強化することで、これらの制限に対処します。私たちは、ベクトル データベース、センテンス トランスフォーマー、ガウス混合モデル (GMM)、検索エージェント、および大規模言語モデル (LLM) を統合する技術ワークフローを開発し、カスタマイズされたプロンプト テンプレートを使用した文脈検索、トピックのランキング、および研究の特徴付けを可能にしました。過去 10 年間に Nature Communications に掲載された都市科学関連の 223 件の論文を分析したパイロット研究では、影響力の高いジャーナルの論文の質、範囲、特徴に関する洞察力に富んだ要約統計を生成する際の私たちのアプローチの有効性が明らかになりました。この研究は、都市研究における書誌学的分析と知識検索を強化するための新しいパラダイムを導入し、AI エージェントを研究の評価と理解を進めるための強力なツールとして位置づけています。 Bibliometric analysis is essential for understanding research trends, scope, and impact in urban science, especially in high-impact journals, such Nature Portfolios. However, traditional methods, relying on keyword searches and basic NLP techniques, often fail to uncover valuable insights not explicitly stated in article titles or keywords. These approaches are unable to perform semantic searches and contextual understanding, limiting their effectiveness in classifying topics and characterizing studies. In this paper, we address these limitations by leveraging Generative AI models, specifically transformers and Retrieval-Augmented Generation (RAG), to automate and enhance bibliometric analysis. We developed a technical workflow that integrates a vector database, Sentence Transformers, a Gaussian Mixture Model (GMM), Retrieval Agent, and Large Language Models (LLMs) to enable contextual search, topic ranking, and characterization of research using customized prompt templates. A pilot study analyzing 223 urban science-related articles published in Nature Communications over the past decade highlights the effectiveness of our approach in generating insightful summary statistics on the quality, scope, and characteristics of papers in high-impact journals. This study introduces a new paradigm for enhancing bibliometric analysis and knowledge retrieval in urban research, positioning an AI agent as a powerful tool for advancing research evaluation and understanding.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', \"DOI: 10.48550/arXiv.2510.11195\\n Title: RAG-Pull: コード生成のための RAG システムに対する知覚できない攻撃 RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation\\nAbstract: 検索拡張生成 (RAG) は、LLM 応答の信頼性と信頼性を高め、モデルの再トレーニングの必要性を排除することで幻覚を軽減します。これは、LLM のコンテキストに外部データを追加することによって行われます。私たちは、隠れた UTF 文字をクエリまたは外部コード リポジトリに挿入し、検索を悪意のあるコードにリダイレクトして、モデルの安全性の調整を破る新しいクラスのブラック ボックス攻撃である RAG-Pull を開発しました。クエリとコードの摂動だけでは、検索が攻撃者が制御するスニペットにシフトする可能性がある一方、クエリとターゲットの摂動を組み合わせるとほぼ完璧な成功が得られることが観察されています。これらのスニペットが取得されると、リモート コード実行や SQL インジェクションなどの悪用可能な脆弱性が生じます。 RAG-Pull の最小限の摂動は、モデルの安全性の調整を変更し、安全でないコードへの優先順位を高める可能性があるため、LLM に対する新しい種類の攻撃を可能にします。 Retrieval-Augmented Generation (RAG) increases the reliability and trustworthiness of the LLM response and reduces hallucination by eliminating the need for model retraining. It does so by adding external data into the LLM's context. We develop a new class of black-box attack, RAG-Pull, that inserts hidden UTF characters into queries or external code repositories, redirecting retrieval toward malicious code, thereby breaking the models' safety alignment. We observe that query and code perturbations alone can shift retrieval toward attacker-controlled snippets, while combined query-and-target perturbations achieve near-perfect success. Once retrieved, these snippets introduce exploitable vulnerabilities such as remote code execution and SQL injection. RAG-Pull's minimal perturbations can alter the model's safety alignment and increase preference towards unsafe code, therefore opening up a new class of attacks on LLMs.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2406.13213', '10.1145/3681780.3697252', '10.1007/s44427-025-00006-3', '10.48550/arXiv.2510.11195']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a system that enhances language models by incorporating external knowledge sources.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16 - RAG systems are composed of a retrieval and an LLM-based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - RAG enables retrieval of relevant information from an external knowledge source, allowing large language models (LLMs) to answer queries over previously unseen document collections.\n",
      "\n",
      "DOI: 10.1145/3681780.3697252 - RAG is used to automate and enhance bibliometric analysis, addressing limitations of traditional methods by leveraging Generative AI models, specifically transformers.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3 - RAG systems have gained significant attention for their ability to enhance language models, but evaluating their effectiveness remains a challenge.\n",
      "\n",
      "DOI: 10.48550/arXiv.2510.11195 - RAG increases the reliability and trustworthiness of LLM responses and reduces hallucination by eliminating the need for model retraining.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.917\n",
      "Recall: 0.917\n",
      "F1-Score: 0.917\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9552357, '10.18653/v1/2024.eacl-demo.16'), (0.6951311, '10.48550/arXiv.2406.13213'), (0.63906056, '10.1145/3681780.3697252'), (0.55823666, '10.1007/s44427-025-00006-3'), (0.46128976, '10.48550/arXiv.2510.11195')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1109/ADL.1998.670425', '10.1177/09610006241239080', '10.1145/2964909', '10.5860/crl.86.1.101']\n",
      "\u001b[93mSummary: The key challenges in metadata quality assessment include the lack of consensus on the exact definition and measurement of commonly used dimensions such as completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness. Advancements include the development of automated quality assessment frameworks and the application of qualitative and quantitative content analysis techniques to evaluate metadata records.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - This study highlights the importance of metadata quality assessment, identifying commonly used dimensions and emphasizing the need for further investigation into less commonly assessed quality dimensions. It also notes the prominence of digital repositories and open government data as research subjects.\n",
      "\n",
      "DOI: 10.1145/2964909 - This work focuses on developing a generic metadata quality assessment framework for open data portals, addressing concerns about data searchability, discoverability, and usability. It introduces automated quality metrics and reports findings from monitoring a large set of open data portals.\n",
      "\n",
      "DOI: 10.1109/ADL.1998.670425 - This research discusses the application of qualitative and quantitative content analysis techniques to assess metadata records, concluding that different types of metadata may require different evaluation criteria and procedures.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need more details, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in metadata quality assessment?']:\n",
      "Precision: 0.906\n",
      "Recall: 0.906\n",
      "F1-Score: 0.906\n",
      "Accuracy: 0.906\n",
      "Balanced accuracy: 0.478\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.93546456, '10.1109/ADL.1998.670425'), (0.92905515, '10.1177/09610006241239080'), (0.85034436, '10.1145/2964909'), (0.20291664, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101', '10.1177/09610006241239080', '10.1145/2964909']\n",
      "\u001b[93mSummary: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661 - Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. Methods to automatically detect, correct, and canonicalize scholarly metadata are investigated, using seven key fields of electronic theses and dissertations (ETDs) as a case study. A framework called MetaEnhance is proposed, which utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, a metadata quality evaluation benchmark containing 500 ETDs is compiled, and the proposed methods achieve nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 - Metadata serves discovery and access by providing contextual, technical, and administrative information in a standard form. However, metadata is also a site of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. A review of 427 records identifies 32 unique issues and classifies them into 5 categories to better explain how metadata and communities interact to reflect cultural meanings.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. A scoping review aims to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The review finds that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions.\n",
      "\n",
      "DOI: 10.1145/2964909 - The Open Data movement has become a driver for publicly available data on the Web. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. A generic metadata quality assessment framework is developed for various Open Data portals, and several quality metrics are defined, which can be evaluated automatically and efficiently. Findings are reported based on monitoring a set of over 260 Open Data portals with 1.1M datasets.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need more details, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and potential solutions for improving metadata quality?']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.795\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.9813107, '10.48550/arXiv.2303.17661'), (0.42161134, '10.5860/crl.86.1.101'), (0.35064372, '10.1177/09610006241239080'), (0.23196201, '10.1145/2964909')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 14 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.1007/s44427-025-00006-3', '10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) systems face challenges in evaluating their effectiveness, particularly in assessing both retrieval and generation components independently and in combination. However, advancements like RAGAs and Multi-Meta-RAG have been introduced to address these challenges, offering automated evaluation frameworks and improved methods for handling multi-hop queries.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3 - This study evaluates open-source LLMs in RAG systems using a benchmark dataset derived from diploma theses abstracts, highlighting the strengths and weaknesses of different models and retrieval strategies.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16 - Introduces RAGAs, a framework for reference-free evaluation of RAG pipelines, addressing the challenge of evaluating RAG architectures by introducing metrics that assess retrieval, generation, and overall quality without human annotations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - Presents Multi-Meta-RAG, a method that improves RAG for multi-hop queries by using database filtering with LLM-extracted metadata, significantly enhancing performance on the MultiHop-RAG benchmark.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - Proposes M-RAG, a method for enhancing open-domain question answering using metadata retrieval-augmented generation, demonstrating significant performance improvements over conventional RAG methods.\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions or need additional information, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in Retrieval-Augmented Generation (RAG) systems?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.9996923, '10.1016/j.caeai.2025.100417'), (0.99849296, '10.1007/s44427-025-00006-3'), (0.99836427, '10.18653/v1/2024.eacl-demo.16'), (0.9962638, '10.48550/arXiv.2406.13213'), (0.9233162, '10.6109/jkiice.2023.27.12.1489')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 15 in loop: 4\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1109/ADL.1998.670425', '10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101']\n",
      "\u001b[93mSummary: Two studies examined the abstract in metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286\n",
      "This study compared the amount of metadata and the completeness degree of research publications in new academic databases. Seven characteristics were analyzed, including the abstract.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101\n",
      "This study sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, 32 unique issues were identified and classified into 5 categories.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.927\n",
      "Recall: 0.927\n",
      "F1-Score: 0.927\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.6092336, '10.1109/ADL.1998.670425'), (0.3526475, '10.1162/qss_a_00286'), (0.23633993, '10.48550/arXiv.2303.17661'), (0.111627996, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1108/JD-10-2022-0234\\n Title: すべての学術分野にわたる引用と参照の習慣の分析: 書誌の参照と引用の実践におけるアプローチと傾向 An analysis of citing and referencing habits across all scholarly disciplines: approaches and trends in bibliographic referencing and citing practices\\nAbstract: この研究で、著者らは、学術文献における引用および参照の誤りについて現在考えられる原因を特定し、スウィートランド氏が1989年の論文で提供したスナップショットから何かが変わったかどうかを比較したいと考えている。,著者らは、27の主題分野にわたる147のジャーナルに掲載された729件の論文から、参考要素、すなわち書誌的参照、言及、引用、およびそれぞれの本文中の参照ポインタを分析した。,分析の結果は、書誌的事項が指摘された。著者らの知る限り、この研究は、Sweetland (1989) 以来、文献における参照および引用の慣行における誤りを分析したものとしては、最近入手可能な最良のものである。 In this study, the authors want to identify current possible causes for citing and referencing errors in scholarly literature to compare if something changed from the snapshot provided by Sweetland in his 1989 paper.,The authors analysed reference elements, i.e. bibliographic references, mentions, quotations and respective in-text reference pointers, from 729 articles published in 147 journals across the 27 subject areas.,The outcomes of the analysis pointed out that bibliographic errors have been perpetuated for decades and that their possible causes have increased, despite the encouraged use of technological facilities, i.e. the reference managers.,As far as the authors know, the study is the best recent available analysis of errors in referencing and citing practices in the literature since Sweetland (1989).\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1108/JD-10-2022-0234']\n",
      "\u001b[93mSummary: This study examined citing and referencing habits across all scholarly disciplines.\n",
      "DOI: 10.1108/JD-10-2022-0234.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 0.958\n",
      "Recall: 0.958\n",
      "F1-Score: 0.958\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.600\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.208018, '10.1108/JD-10-2022-0234')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mSummary: Crossref is a sustainable source of community-owned scholarly metadata and open bibliographic metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022 - Crossref is a sustainable source of community-owned scholarly metadata. It contains over 106 million records and expands at an average rate of 11% a year. It has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref is a source of open bibliographic metadata. Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.94468933, '10.1162/qss_a_00022'), (0.28150436, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.1162/qss_a_00286']\n",
      "\u001b[93mSummary: Two papers evaluate the linguistic coverage or language-related metadata in scholarly databases.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633\n",
      "Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n",
      "\n",
      "DOI: 10.1162/qss_a_00286\n",
      "Completeness degree of publication metadata in eight free-access scholarly databases\n",
      "\n",
      "Can I help you with anything else, my lady?\n",
      "96\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.595\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.9992792, '10.48550/arXiv.2409.10633'), (0.4369223, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: COVID-19 研究への資金提供: オープン データ インフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 資金提供機関が提供する資金の成果を分析するには、資金提供によって得られた出版物を追跡できることが不可欠です。私たちは、Crossref での資金調達データのオープンな利用可能性を調査し、新型コロナウイルス感染症 (COVID-19) に関連する研究を報告する出版物の資金調達データに焦点を当てています。また、Scopus と Web of Science という 2 つの独自の書誌データベースで利用可能な資金調達データとの比較も示します。私たちの分析では、Crossref の資金調達データの範囲が限られていることが明らかになりました。また、特に Scopus における資金調達データの品質に関連する問題も示しています。 Crossref での資金調達データのオープンな可用性を向上させるための推奨事項を提供します。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1162/qss_a_00212', '10.1162/qss_a_00286']\n",
      "\u001b[93mSummary: There are three papers that address funding metadata, its availability, or its analysis in scholarly databases.\n",
      "\n",
      "DOI: 10.1162/qss_a_00210 - This paper addresses the availability and completeness of open funder metadata, using a case study of publications funded by the Dutch Research Council.\n",
      "\n",
      "DOI: 10.1162/qss_a_00212 - This paper provides an analysis of funding for COVID-19 research, using open data infrastructures and comparing funding data available in Scopus and Web of Science.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - This paper compares the completeness of publication metadata in eight free-access scholarly databases, including Dimensions, Google Scholar, and Microsoft Academic.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.979\n",
      "Recall: 0.979\n",
      "F1-Score: 0.979\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.870\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.98469365, '10.1162/qss_a_00210'), (0.6974029, '10.31222/osf.io/smxe5'), (0.6575384, '10.1162/qss_a_00212'), (0.6167627, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.48550/arXiv.2406.13213', '10.1007/s44427-025-00006-3', '10.6109/jkiice.2023.27.12.1489', '10.18653/v1/2024.eacl-demo.16']\n",
      "\u001b[93mI'm sorry, my lady, I couldn't find any papers that discuss the use of Retrieval-Augmented Generation (RAG) when used in applications.\n",
      "96\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) when used in applications?']:\n",
      "Precision: 0.917\n",
      "Recall: 0.917\n",
      "F1-Score: 0.917\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 0\n",
      "Documents score: [(0.99998486, '10.1016/j.caeai.2025.100417'), (0.9986749, '10.48550/arXiv.2406.13213'), (0.9975656, '10.1007/s44427-025-00006-3'), (0.9943141, '10.6109/jkiice.2023.27.12.1489'), (0.9914225, '10.18653/v1/2024.eacl-demo.16')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mSummary: Crossref is a sustainable source of community-owned scholarly metadata that is collected and made available to publishers, authors, librarians, funders, and researchers.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "\n",
      "Crossref is a sustainable source of community-owned scholarly metadata that is collected and made available to publishers, authors, librarians, funders, and researchers. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data. The metadata set consists of 13 content types, including traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH.\n",
      "\n",
      "My lady, I hope this information is helpful. Is there anything else I can assist you with?\n",
      "96\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.99954873, '10.1162/qss_a_00022'), (0.7735587, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science および Scopus と比較した OpenAlex の参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: OpenAlex は、学術メタデータの有望なオープン ソースであり、Web of Science や Scopus などの確立された独自のソースの競合相手です。 OpenAlex はデータを自由かつオープンに提供するため、研究者はライセンスの壁なしにコミュニティ内で再現できる書誌学的研究を実行できます。ただし、OpenAlex は急速に進化するソースであり、そこに含まれるデータは拡大し、また急速に変化しているため、そのデータの信頼性に関する疑問が当然生じます。このレポートでは、各データベース内の参照範囲と選択されたメタデータを調査し、それらを相互に比較して、書誌計量学におけるこの未解決の質問に対処します。私たちの大規模調査では、3 つのデータベースすべてで共有される 1,680 万件の最近の出版物のクリーンなデータセットに限定した場合、OpenAlex は Web of Science と Scopus の両方に匹敵する平均的なソース参照番号と内部カバー率を持っていることを実証しました。さらに、OpenAlex、Web of Science、および Scopus のメタデータをジャーナルごとに分析し、OpenAlex と比較して、Web of Science と Scopus のソース参照数の分布に類似性があることがわかりました。また、OpenAlex がカバーする他のコア メタデータの比較では、ジャーナルごとに分類するとさまざまな結果が示され、Web of Science と Scopus の両方と比較した場合、論文あたりの ORCID 識別子がより多く、抄録が少なく、論文あたりのオープン アクセス ステータス インジケーターの数が同程度であることも示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: OpenAlex、Web of Science、Scopus、Pubmed、Semantic Sc\\u200b\\u200bholar における出版物とドキュメント タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、OpenAlex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、PubMed の書誌データベースの出版物と文書の種類を比較および分析します。この結果は、類型が個々のデータベース プロバイダー間で大きく異なる可能性があることを示しています。さらに、書誌学的分析に関連する文書を特定するために必要な研究文書と非研究文書の区別は、データソースによって異なる場合があります。これは、出版物が各データベースで異なる方法で分類されているためです。 OpenAlex は、図書館や大学における書誌学的分析のための確立された独自のプロバイダーに代わる無料の代替手段としてますます重要になっているため、データベース間の比較に加えて、この研究の焦点は主に OpenAlex に含まれる出版物と文書タイプの範囲と分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n', 'DOI: 10.48550/ARXIV.2205.01833\\n Title: OpenAlex: 学術作品、著者、会場、機関、コンセプトの完全にオープンなインデックス OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts\\nAbstract: OpenAlex は、廃止された Microsoft Academic Graph (MAG) を置き換えるために開始された、新しい完全にオープンな Scientific Knowledge Graph (SKG) です。 2 億 900 万件の作品 (雑誌記事、書籍など) のメタデータが含まれています。 2013M の明確な著者。 124,000 の会場 (ジャーナルやオンライン リポジトリなど、作品をホストする場所)。 109,000の機関。および 65,000 個のウィキデータの概念 (自動化された階層型マルチタグ分類子を介して作品にリンクされています)。データセットは、Web ベースの GUI、完全なデータ ダンプ、および大容量 REST API を介して完全かつ自由に利用できます。このリソースは現在鋭意開発中であり、将来の作業により、引用情報の精度と範囲、著者/機関の解析と重複排除が改善される予定です。 OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2508.18620', '10.48550/arXiv.2401.16359', '10.48550/arXiv.2406.15154', '10.48550/ARXIV.2205.01833']\n",
      "\u001b[93mSummary: OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.\n",
      "\n",
      "DOI: 10.48550/ARXIV.2205.01833\n",
      "\n",
      "I hope this information is helpful, my lady.\n",
      "96\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.99971753, '10.48550/arXiv.2404.17663'), (0.99565166, '10.48550/arXiv.2508.18620'), (0.9932811, '10.48550/arXiv.2401.16359'), (0.97090924, '10.48550/arXiv.2406.15154'), (0.8957053, '10.48550/ARXIV.2205.01833')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1007/s11192-015-1765-5\\n Title: Web of Science と Scopus のジャーナルの報道: 比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌学的手法は、研究の評価など、さまざまな目的で複数の分野で使用されています。ほとんどの書誌学的分析には、トムソン・ロイターの Web of Science (WoS) とエルゼビアの Scopus というデータ ソースが共通しています。この調査の目的は、これら 2 つのデータベースのジャーナルの対象範囲を説明し、特定の分野、出版国、言語が過小評価されているかどうかを評価することです。これを行うために、WoS (13,605 ジャーナル) および Scopus (20,346 ジャーナル) のアクティブな学術ジャーナルの範囲を、ウルリッヒの広範な定期刊行物ディレクトリ (63,013 ジャーナル) と比較しました。結果は、WoS または Scopus を研究評価に使用すると、社会科学、芸術、人文科学に不利益をもたらす、自然科学と工学、生物医学研究に有利なバイアスを導入する可能性があることを示しています。同様に、英語のジャーナルが過大評価され、他の言語に損害を与えています。どちらのデータベースもこれらのバイアスを共有していますが、その範囲は大きく異なります。そのため、書誌情報分析の結果は使用するデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈において、特に異なる分野、機関、国、または言語を比較する場合には、WoS と Scopus を慎重に使用する必要があることを示唆しています。書誌学コミュニティは、分野固有の引用インデックスや全国的な引用インデックスなど、WoS や Scopus ではカバーされていない科学的成果を含む手法や指標を開発する努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: OpenAlex、Web of Science、Scopus、Pubmed、Semantic Sc\\u200b\\u200bholar における出版物とドキュメント タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、OpenAlex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、PubMed の書誌データベースの出版物と文書の種類を比較および分析します。この結果は、類型が個々のデータベース プロバイダー間で大きく異なる可能性があることを示しています。さらに、書誌学的分析に関連する文書を特定するために必要な研究文書と非研究文書の区別は、データソースによって異なる場合があります。これは、出版物が各データベースで異なる方法で分類されているためです。 OpenAlex は、図書館や大学における書誌学的分析のための確立された独自のプロバイダーに代わる無料の代替手段としてますます重要になっているため、データベース間の比較に加えて、この研究の焦点は主に OpenAlex に含まれる出版物と文書タイプの範囲と分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2508.18620', '10.1007/s11192-015-1765-5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154']\n",
      "\u001b[93mSummary: I'm sorry, my lady, I couldn't find any information about the strengths and weaknesses of Web of Science (WoS) as a bibliometric database.\n",
      "\n",
      "However, I did find the following information about WoS:\n",
      "\n",
      "- WoS was the main supporting infrastructure for bibliometric research for over 30 years until new competitors emerged.\n",
      "- WoS, along with Scopus, may introduce biases in research evaluation, favouring Natural Sciences, Engineering, and Biomedical Research over Social Sciences, Arts, and Humanities.\n",
      "- English-language journals are overrepresented in WoS, which may disadvantage other languages.\n",
      "- WoS and Scopus share these biases, but their coverage differs substantially, which can lead to varying results in bibliometric analyses.\n",
      "- WoS, along with other traditional databases like Scopus, systematically underrepresent certain disciplines and world regions.\n",
      "\n",
      "Unfortunately, I couldn't find a direct comparison of WoS's strengths and weaknesses as a bibliometric database.\n",
      "\n",
      "If you'd like, I can try to answer a different question or provide more information on a specific aspect of WoS.\n",
      "96\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 0\n",
      "Documents score: [(0.9980882, '10.48550/arXiv.2508.18620'), (0.99554896, '10.1007/s11192-015-1765-5'), (0.95565146, '10.48550/arXiv.2404.17663'), (0.6040486, '10.1162/qss_a_00112'), (0.3408142, '10.48550/arXiv.2406.15154')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.48550/arXiv.2502.15005\\n Title: 研究トピックに関する自然言語クエリを知識組織システムと結び付けるための Socratic RAG アプローチ A Socratic RAG Approach to Connect Natural Language Queries on Research Topics with Knowledge Organization Systems\\nAbstract: この論文では、研究トピックに関する自然言語クエリを正確で機械解釈可能な意味エンティティにマッピングする検索拡張生成 (RAG) エージェントを提案します。私たちのアプローチは、RAG とソクラテス対話を組み合わせて、研究トピックに対するユーザーの直感的な理解を確立された知識組織システム (KOS) に合わせます。提案されたアプローチは、「小さなセマンティクス」（ドメイン固有の KOS 構造）と「大きなセマンティクス」（広範な書誌情報リポジトリ）を効果的に橋渡しし、複雑な学術分類をよりアクセスしやすくします。このような薬剤は広範囲に使用される可能性があります。 CollabNext というサンプル アプリケーションを使って説明します。これは、人、組織、研究トピックを結び付ける個人中心のナレッジ グラフです。さらに、現在の科学システムでは歴史的に目に見えない存在となっていた人々の可視性を高めるために、アプリケーションの設計がどのように HBCU と新進の研究者に意図的に焦点を当てているかについて説明します。 In this paper, we propose a Retrieval Augmented Generation (RAG) agent that maps natural language queries about research topics to precise, machine-interpretable semantic entities. Our approach combines RAG with Socratic dialogue to align a user\\'s intuitive understanding of research topics with established Knowledge Organization Systems (KOSs). The proposed approach will effectively bridge \"little semantics\" (domain-specific KOS structures) with \"big semantics\" (broad bibliometric repositories), making complex academic taxonomies more accessible. Such agents have the potential for broad use. We illustrate with a sample application called CollabNext, which is a person-centric knowledge graph connecting people, organizations, and research topics. We further describe how the application design has an intentional focus on HBCUs and emerging researchers to raise visibility of people historically rendered invisible in the current science system.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.1145/3637528.3671470', '10.48550/arXiv.2502.15005', '10.18653/v1/2024.eacl-demo.16']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a technique that improves the performance of question answering and information retrieval systems by retrieving relevant information from external knowledge sources and enabling large language models (LLMs) to answer queries over previously unseen document collections.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - This paper introduces a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of relevant documents from various sources, relevant to the question.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - This paper proposes a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470 - This survey provides a comprehensive review of existing research studies on Retrieval-Augmented Large Language Models (RA-LLMs), covering three primary technical perspectives and discussing current limitations and promising directions for future research.\n",
      "\n",
      "DOI: 10.48550/arXiv.2502.15005 - This paper proposes a RAG agent that maps natural language queries about research topics to precise, machine-interpretable semantic entities by combining RAG with Socratic dialogue.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16 - This paper introduces RAGAs, a framework for reference-free evaluation of RAG pipelines, which introduces a suite of metrics to evaluate different dimensions of RAG architectures without relying on ground truth human annotations.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or would like more details on a specific DOI, please let me know.\n",
      "96\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.99971086, '10.48550/arXiv.2406.13213'), (0.99860066, '10.6109/jkiice.2023.27.12.1489'), (0.96741027, '10.1145/3637528.3671470'), (0.96518475, '10.48550/arXiv.2502.15005'), (0.92121595, '10.18653/v1/2024.eacl-demo.16')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: []\n",
      "length of reranked_documents: 0\n",
      "Retrieved DOIs: []\n",
      "\u001b[93mMy lady, normalizing citation metrics across scientific fields is a complex task due to several inherent challenges. Below is a structured response to your query:\n",
      "\n",
      "**Summary:** The main challenges in normalizing citation metrics across scientific fields include differences in publication and citation practices, varying journal cultures, field-specific citation rates, and the impact of discipline size and age. These factors make it difficult to create a universal standard for comparing citation metrics across disciplines.\n",
      "\n",
      "**DOI:** 10.1126/science.1175862  \n",
      "This article by Seglen (1997) in *Science* highlights the variability in citation patterns across different scientific fields, emphasizing that citation rates are influenced by field-specific practices, such as the average number of references per paper and the speed of publication. The study underscores the difficulty of directly comparing citation metrics without accounting for these differences.\n",
      "\n",
      "**DOI:** 10.7717/peerj.846  \n",
      "In this *PeerJ* article by Bornmann et al. (2019), the authors discuss the challenges of field normalization in bibliometrics. They argue that traditional methods, such as journal-based normalization, are insufficient due to the heterogeneity of citation cultures across disciplines. The paper suggests more sophisticated approaches, such as using reference-based or field-specific citation percentiles, to improve normalization.\n",
      "\n",
      "**DOI:** 10.1002/asi.21340  \n",
      "Waltman and Traag (2020) in the *Journal of the Association for Information Science and Technology* explore the limitations of current normalization methods. They highlight issues such as the granularity of field classifications and the dynamic nature of scientific disciplines, which complicate efforts to create fair and accurate cross-field comparisons of citation metrics.\n",
      "\n",
      "These challenges necessitate the development of more nuanced and adaptive methods for normalizing citation metrics to ensure equitable comparisons across diverse scientific fields.\n",
      "96\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.500\n",
      "Faithfulness score: 0\n",
      "Documents score: []\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1007/s11192-022-04367-w', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: The methods used to detect and correct errors in bibliographic datasets include the analysis of open bibliographic metadata available in Crossref, the use of a list of invalid cited DOIs gathered by OpenCitations, and the identification of patterns of factual errors in the invalid DOIs.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w - This study aims to identify classes of DOI mistakes by analyzing open bibliographic metadata in Crossref, tracking publisher responsibility, and correcting errors through automatic processes. It extends previous DOI error taxonomies and defines more elaborate regular expressions for improved error correction.\n",
      "\n",
      "DOI: 10.5281/ZENODO.6188748 - This dataset evaluates the methodology presented in the article \"Identifying and correcting invalid citations due to DOI errors in Crossref data.\" It includes 193 randomly selected corrected citations, extracted using a Python script available on GitHub.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need more details, please let me know.\n",
      "96\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.595\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.5874699, '10.1007/s11192-022-04367-w'), (0.34224167, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.1145/3681780.3697252\\n Title: 文変換と検索拡張生成 (RAG) による書誌分析の自動化: 影響力の高い都市研究のためのカスタマイズされた文献の特徴付けのための意味論的および文脈検索のパイロット研究 Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG): A Pilot Study in Semantic and Contextual Search for Customized Literature Characterization for High-Impact Urban Research\\nAbstract: 書誌学的分析は、都市科学、特にネイチャー ポートフォリオなどの影響力の大きいジャーナルにおける研究の傾向、範囲、影響を理解するために不可欠です。ただし、キーワード検索や基本的な NLP テクニックに依存する従来の方法では、記事のタイトルやキーワードに明示的に記載されていない貴重な洞察を発見できないことがよくあります。これらのアプローチでは、意味検索や文脈の理解を実行できないため、トピックの分類や研究の特徴付けにおける有効性が制限されます。このペーパーでは、生成 AI モデル、特にトランスフォーマーと検索拡張生成 (RAG) を活用して書誌学的分析を自動化および強化することで、これらの制限に対処します。私たちは、ベクトル データベース、センテンス トランスフォーマー、ガウス混合モデル (GMM)、検索エージェント、および大規模言語モデル (LLM) を統合する技術ワークフローを開発し、カスタマイズされたプロンプト テンプレートを使用した文脈検索、トピックのランキング、および研究の特徴付けを可能にしました。過去 10 年間に Nature Communications に掲載された都市科学関連の 223 件の論文を分析したパイロット研究では、影響力の高いジャーナルの論文の質、範囲、特徴に関する洞察力に富んだ要約統計を生成する際の私たちのアプローチの有効性が明らかになりました。この研究は、都市研究における書誌学的分析と知識検索を強化するための新しいパラダイムを導入し、AI エージェントを研究の評価と理解を進めるための強力なツールとして位置づけています。 Bibliometric analysis is essential for understanding research trends, scope, and impact in urban science, especially in high-impact journals, such Nature Portfolios. However, traditional methods, relying on keyword searches and basic NLP techniques, often fail to uncover valuable insights not explicitly stated in article titles or keywords. These approaches are unable to perform semantic searches and contextual understanding, limiting their effectiveness in classifying topics and characterizing studies. In this paper, we address these limitations by leveraging Generative AI models, specifically transformers and Retrieval-Augmented Generation (RAG), to automate and enhance bibliometric analysis. We developed a technical workflow that integrates a vector database, Sentence Transformers, a Gaussian Mixture Model (GMM), Retrieval Agent, and Large Language Models (LLMs) to enable contextual search, topic ranking, and characterization of research using customized prompt templates. A pilot study analyzing 223 urban science-related articles published in Nature Communications over the past decade highlights the effectiveness of our approach in generating insightful summary statistics on the quality, scope, and characteristics of papers in high-impact journals. This study introduces a new paradigm for enhancing bibliometric analysis and knowledge retrieval in urban research, positioning an AI agent as a powerful tool for advancing research evaluation and understanding.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', \"DOI: 10.48550/arXiv.2510.11195\\n Title: RAG-Pull: コード生成のための RAG システムに対する知覚できない攻撃 RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation\\nAbstract: 検索拡張生成 (RAG) は、LLM 応答の信頼性と信頼性を高め、モデルの再トレーニングの必要性を排除することで幻覚を軽減します。これは、LLM のコンテキストに外部データを追加することによって行われます。私たちは、隠れた UTF 文字をクエリまたは外部コード リポジトリに挿入し、検索を悪意のあるコードにリダイレクトして、モデルの安全性の調整を破る新しいクラスのブラック ボックス攻撃である RAG-Pull を開発しました。クエリとコードの摂動だけでは、検索が攻撃者が制御するスニペットにシフトする可能性がある一方、クエリとターゲットの摂動を組み合わせるとほぼ完璧な成功が得られることが観察されています。これらのスニペットが取得されると、リモート コード実行や SQL インジェクションなどの悪用可能な脆弱性が生じます。 RAG-Pull の最小限の摂動は、モデルの安全性の調整を変更し、安全でないコードへの優先順位を高める可能性があるため、LLM に対する新しい種類の攻撃を可能にします。 Retrieval-Augmented Generation (RAG) increases the reliability and trustworthiness of the LLM response and reduces hallucination by eliminating the need for model retraining. It does so by adding external data into the LLM's context. We develop a new class of black-box attack, RAG-Pull, that inserts hidden UTF characters into queries or external code repositories, redirecting retrieval toward malicious code, thereby breaking the models' safety alignment. We observe that query and code perturbations alone can shift retrieval toward attacker-controlled snippets, while combined query-and-target perturbations achieve near-perfect success. Once retrieved, these snippets introduce exploitable vulnerabilities such as remote code execution and SQL injection. RAG-Pull's minimal perturbations can alter the model's safety alignment and increase preference towards unsafe code, therefore opening up a new class of attacks on LLMs.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2406.13213', '10.1145/3681780.3697252', '10.1007/s44427-025-00006-3', '10.48550/arXiv.2510.11195']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a system that enhances language models by incorporating external knowledge sources.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16 - RAG systems are composed of a retrieval and an LLM-based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - RAG enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, traditional RAG applications perform poorly in answering multi-hop questions.\n",
      "\n",
      "DOI: 10.1145/3681780.3697252 - RAG is used to automate and enhance bibliometric analysis, addressing limitations of traditional methods by leveraging Generative AI models, specifically transformers, to enable contextual search, topic ranking, and characterization of research.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3 - RAG systems have gained significant attention for their ability to enhance language models, but evaluating their effectiveness remains a challenge. This study aims to provide a comprehensive evaluation of open-source LLMs within a RAG system, focusing on retrieval effectiveness and answer quality.\n",
      "\n",
      "DOI: 10.48550/arXiv.2510.11195 - RAG increases the reliability and trustworthiness of LLM responses and reduces hallucination by eliminating the need for model retraining. However, it is vulnerable to a new class of black-box attacks, RAG-Pull, which can redirect retrieval toward malicious code.\n",
      "\n",
      "My lady, is there anything else I can assist you with?\n",
      "96\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.917\n",
      "Recall: 0.917\n",
      "F1-Score: 0.917\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9552357, '10.18653/v1/2024.eacl-demo.16'), (0.6951311, '10.48550/arXiv.2406.13213'), (0.63906056, '10.1145/3681780.3697252'), (0.55823666, '10.1007/s44427-025-00006-3'), (0.46128976, '10.48550/arXiv.2510.11195')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1109/ADL.1998.670425', '10.1177/09610006241239080', '10.1145/2964909', '10.5860/crl.86.1.101']\n",
      "\u001b[93mSummary: The key challenges and advancements in metadata quality assessment revolve around standardisation, automation, and cultural considerations.\n",
      "\n",
      "DOI: 10.1109/ADL.1998.670425 discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As part of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 explores the dimensions of metadata quality assessment through a scoping review of 55 studies. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions, but there is no consensus on their exact definition and measurement. The study emphasises the need to improve the flexibility of metadata quality assessment tools.\n",
      "\n",
      "DOI: 10.1145/2964909 focuses on the automated quality assessment of metadata across open data portals. The authors developed a generic metadata quality assessment framework by mapping specific metadata of three widely used portal software frameworks to a standardised schema. They defined several quality metrics that can be evaluated automatically and reported findings based on monitoring over 260 open data portals.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 identifies metadata quality issues across cultures. By reviewing a non-random sample of 427 records, the authors identified 32 unique issues and classified them into 5 categories to better understand how metadata quality, consistency, and completeness impact individuals and communities.\n",
      "\n",
      "My lady, these studies highlight the ongoing efforts to standardise metadata quality assessment, automate evaluation processes, and address cultural considerations in metadata quality. The challenges include lack of consensus on definitions and measurements, while advancements are seen in the development of generic frameworks and automated tools.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in metadata quality assessment?']:\n",
      "Precision: 0.906\n",
      "Recall: 0.906\n",
      "F1-Score: 0.906\n",
      "Accuracy: 0.906\n",
      "Balanced accuracy: 0.478\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.93546456, '10.1109/ADL.1998.670425'), (0.92905515, '10.1177/09610006241239080'), (0.85034436, '10.1145/2964909'), (0.20291664, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101', '10.1177/09610006241239080', '10.1145/2964909']\n",
      "\u001b[93mSummary: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661 - Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 - Metadata serves discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment.\n",
      "\n",
      "DOI: 10.1145/2964909 - With the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and potential solutions for improving metadata quality?']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.795\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.9813107, '10.48550/arXiv.2303.17661'), (0.42161134, '10.5860/crl.86.1.101'), (0.35064372, '10.1177/09610006241239080'), (0.23196201, '10.1145/2964909')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 14 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.1007/s44427-025-00006-3', '10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) systems face challenges in evaluating their effectiveness due to the need to assess both retrieval and generation components independently and in combination. However, advancements like RAGAs and Multi-Meta-RAG have been introduced to address these challenges, offering automated evaluation frameworks and improved performance for multi-hop queries.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3: This study evaluates open-source LLMs in RAG systems using a benchmark dataset derived from diploma theses abstracts, highlighting the strengths and weaknesses of different models and retrieval strategies.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16: Introduces RAGAs, a framework for reference-free evaluation of RAG pipelines, addressing the challenges of evaluating RAG architectures by introducing a suite of metrics that can evaluate different dimensions without relying on ground truth human annotations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213: Introduces Multi-Meta-RAG, a new method that uses database filtering with LLM-extracted metadata to improve RAG selection of relevant documents for multi-hop queries, significantly improving results on the MultiHop-RAG benchmark.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or would like more details on a specific aspect of RAG systems, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in Retrieval-Augmented Generation (RAG) systems?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.9996923, '10.1016/j.caeai.2025.100417'), (0.99849296, '10.1007/s44427-025-00006-3'), (0.99836427, '10.18653/v1/2024.eacl-demo.16'), (0.9962638, '10.48550/arXiv.2406.13213'), (0.9233162, '10.6109/jkiice.2023.27.12.1489')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 15 in loop: 3\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1109/ADL.1998.670425', '10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101']\n",
      "\u001b[93mSummary: Two studies examined the abstract in metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286\n",
      "This study compared the amount of metadata and the completeness degree of research publications in new academic databases. Seven characteristics were analyzed, including abstracts, to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101\n",
      "This study sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, 32 unique issues were identified and classified into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.927\n",
      "Recall: 0.927\n",
      "F1-Score: 0.927\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.6092336, '10.1109/ADL.1998.670425'), (0.3526475, '10.1162/qss_a_00286'), (0.23633993, '10.48550/arXiv.2303.17661'), (0.111627996, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1108/JD-10-2022-0234\\n Title: すべての学術分野にわたる引用と参照の習慣の分析: 書誌の参照と引用の実践におけるアプローチと傾向 An analysis of citing and referencing habits across all scholarly disciplines: approaches and trends in bibliographic referencing and citing practices\\nAbstract: この研究で、著者らは、学術文献における引用および参照の誤りについて現在考えられる原因を特定し、スウィートランド氏が1989年の論文で提供したスナップショットから何かが変わったかどうかを比較したいと考えている。,著者らは、27の主題分野にわたる147のジャーナルに掲載された729件の論文から、参考要素、すなわち書誌的参照、言及、引用、およびそれぞれの本文中の参照ポインタを分析した。,分析の結果は、書誌的事項が指摘された。著者らの知る限り、この研究は、Sweetland (1989) 以来、文献における参照および引用の慣行における誤りを分析したものとしては、最近入手可能な最良のものである。 In this study, the authors want to identify current possible causes for citing and referencing errors in scholarly literature to compare if something changed from the snapshot provided by Sweetland in his 1989 paper.,The authors analysed reference elements, i.e. bibliographic references, mentions, quotations and respective in-text reference pointers, from 729 articles published in 147 journals across the 27 subject areas.,The outcomes of the analysis pointed out that bibliographic errors have been perpetuated for decades and that their possible causes have increased, despite the encouraged use of technological facilities, i.e. the reference managers.,As far as the authors know, the study is the best recent available analysis of errors in referencing and citing practices in the literature since Sweetland (1989).\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1108/JD-10-2022-0234']\n",
      "\u001b[93mSummary: This study and Sweetland (1989) examined citations.\n",
      "DOI: 10.1108/JD-10-2022-0234\n",
      "\n",
      "My lady, I hope this answers your question.\n",
      "96\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 0.958\n",
      "Recall: 0.958\n",
      "F1-Score: 0.958\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.600\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.208018, '10.1108/JD-10-2022-0234')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mSummary: Crossref is a sustainable source of community-owned scholarly metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022 - Crossref is a major source of scholarly data for publishers, authors, librarians, funders, and researchers. It contains over 106 million records and expands at an average rate of 11% a year. The metadata set consists of 13 content types, including traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref is a source of open bibliographic metadata. Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. An up-to-date overview of the availability of six metadata elements in Crossref is presented: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. The analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.94468933, '10.1162/qss_a_00022'), (0.28150436, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.1162/qss_a_00286']\n",
      "\u001b[93mSummary: There are two papers that evaluate the linguistic coverage or language-related metadata in scholarly databases.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633 evaluates the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 compares the amount of metadata and the completeness degree of research publications in new academic databases, including language-related metadata.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.595\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.9992792, '10.48550/arXiv.2409.10633'), (0.4369223, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: COVID-19 研究への資金提供: オープン データ インフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 資金提供機関が提供する資金の成果を分析するには、資金提供によって得られた出版物を追跡できることが不可欠です。私たちは、Crossref での資金調達データのオープンな利用可能性を調査し、新型コロナウイルス感染症 (COVID-19) に関連する研究を報告する出版物の資金調達データに焦点を当てています。また、Scopus と Web of Science という 2 つの独自の書誌データベースで利用可能な資金調達データとの比較も示します。私たちの分析では、Crossref の資金調達データの範囲が限られていることが明らかになりました。また、特に Scopus における資金調達データの品質に関連する問題も示しています。 Crossref での資金調達データのオープンな可用性を向上させるための推奨事項を提供します。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1162/qss_a_00212', '10.1162/qss_a_00286']\n",
      "\u001b[93mSummary: Four papers address funding metadata, its availability, or its analysis in scholarly databases.\n",
      "\n",
      "DOI: 10.1162/qss_a_00210 addresses the availability and completeness of open funder metadata, with a case study for publications funded by the Dutch Research Council.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 addresses Crossref as a source of open bibliographic metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00212 addresses funding COVID-19 research, with insights from an exploratory analysis using open data infrastructures.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 addresses the completeness degree of publication metadata in eight free-access scholarly databases.\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "96\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.979\n",
      "Recall: 0.979\n",
      "F1-Score: 0.979\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.870\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.98469365, '10.1162/qss_a_00210'), (0.6974029, '10.31222/osf.io/smxe5'), (0.6575384, '10.1162/qss_a_00212'), (0.6167627, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.48550/arXiv.2406.13213', '10.1007/s44427-025-00006-3', '10.6109/jkiice.2023.27.12.1489', '10.18653/v1/2024.eacl-demo.16']\n",
      "\u001b[93mI'm sorry, my lady, I am unable to answer your question.\n",
      "96\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) when used in applications?']:\n",
      "Precision: 0.917\n",
      "Recall: 0.917\n",
      "F1-Score: 0.917\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 0\n",
      "Documents score: [(0.99998486, '10.1016/j.caeai.2025.100417'), (0.9986749, '10.48550/arXiv.2406.13213'), (0.9975656, '10.1007/s44427-025-00006-3'), (0.9943141, '10.6109/jkiice.2023.27.12.1489'), (0.9914225, '10.18653/v1/2024.eacl-demo.16')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mSummary: Crossref is a community-owned source of scholarly metadata that is available to publishers, authors, librarians, funders, and researchers.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022 - Crossref is a community-owned source of scholarly metadata that is available to publishers, authors, librarians, funders, and researchers.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref is a community-owned source of scholarly metadata that is available to publishers, authors, librarians, funders, and researchers.\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "96\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.99954873, '10.1162/qss_a_00022'), (0.7735587, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science および Scopus と比較した OpenAlex の参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: OpenAlex は、学術メタデータの有望なオープン ソースであり、Web of Science や Scopus などの確立された独自のソースの競合相手です。 OpenAlex はデータを自由かつオープンに提供するため、研究者はライセンスの壁なしにコミュニティ内で再現できる書誌学的研究を実行できます。ただし、OpenAlex は急速に進化するソースであり、そこに含まれるデータは拡大し、また急速に変化しているため、そのデータの信頼性に関する疑問が当然生じます。このレポートでは、各データベース内の参照範囲と選択されたメタデータを調査し、それらを相互に比較して、書誌計量学におけるこの未解決の質問に対処します。私たちの大規模調査では、3 つのデータベースすべてで共有される 1,680 万件の最近の出版物のクリーンなデータセットに限定した場合、OpenAlex は Web of Science と Scopus の両方に匹敵する平均的なソース参照番号と内部カバー率を持っていることを実証しました。さらに、OpenAlex、Web of Science、および Scopus のメタデータをジャーナルごとに分析し、OpenAlex と比較して、Web of Science と Scopus のソース参照数の分布に類似性があることがわかりました。また、OpenAlex がカバーする他のコア メタデータの比較では、ジャーナルごとに分類するとさまざまな結果が示され、Web of Science と Scopus の両方と比較した場合、論文あたりの ORCID 識別子がより多く、抄録が少なく、論文あたりのオープン アクセス ステータス インジケーターの数が同程度であることも示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: OpenAlex、Web of Science、Scopus、Pubmed、Semantic Sc\\u200b\\u200bholar における出版物とドキュメント タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、OpenAlex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、PubMed の書誌データベースの出版物と文書の種類を比較および分析します。この結果は、類型が個々のデータベース プロバイダー間で大きく異なる可能性があることを示しています。さらに、書誌学的分析に関連する文書を特定するために必要な研究文書と非研究文書の区別は、データソースによって異なる場合があります。これは、出版物が各データベースで異なる方法で分類されているためです。 OpenAlex は、図書館や大学における書誌学的分析のための確立された独自のプロバイダーに代わる無料の代替手段としてますます重要になっているため、データベース間の比較に加えて、この研究の焦点は主に OpenAlex に含まれる出版物と文書タイプの範囲と分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n', 'DOI: 10.48550/ARXIV.2205.01833\\n Title: OpenAlex: 学術作品、著者、会場、機関、コンセプトの完全にオープンなインデックス OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts\\nAbstract: OpenAlex は、廃止された Microsoft Academic Graph (MAG) を置き換えるために開始された、新しい完全にオープンな Scientific Knowledge Graph (SKG) です。 2 億 900 万件の作品 (雑誌記事、書籍など) のメタデータが含まれています。 2013M の明確な著者。 124,000 の会場 (ジャーナルやオンライン リポジトリなど、作品をホストする場所)。 109,000の機関。および 65,000 個のウィキデータの概念 (自動化された階層型マルチタグ分類子を介して作品にリンクされています)。データセットは、Web ベースの GUI、完全なデータ ダンプ、および大容量 REST API を介して完全かつ自由に利用できます。このリソースは現在鋭意開発中であり、将来の作業により、引用情報の精度と範囲、著者/機関の解析と重複排除が改善される予定です。 OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2508.18620', '10.48550/arXiv.2401.16359', '10.48550/arXiv.2406.15154', '10.48550/ARXIV.2205.01833']\n",
      "\u001b[93mSummary: OpenAlex is a newly launched bibliographic database that is open and has extensive coverage.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620 - OpenAlex is a bibliographic database launched in 2022 that is open and has extensive coverage. It reduces or eliminates barriers to accessing bibliometric data, however, there are concerns about the quality of its metadata.\n",
      "\n",
      "DOI: 10.48550/arXiv.2401.16359 - OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.15154 - OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\n",
      "\n",
      "DOI: 10.48550/ARXIV.2205.01833 - OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "96\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.99971753, '10.48550/arXiv.2404.17663'), (0.99565166, '10.48550/arXiv.2508.18620'), (0.9932811, '10.48550/arXiv.2401.16359'), (0.97090924, '10.48550/arXiv.2406.15154'), (0.8957053, '10.48550/ARXIV.2205.01833')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1007/s11192-015-1765-5\\n Title: Web of Science と Scopus のジャーナルの報道: 比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌学的手法は、研究の評価など、さまざまな目的で複数の分野で使用されています。ほとんどの書誌学的分析には、トムソン・ロイターの Web of Science (WoS) とエルゼビアの Scopus というデータ ソースが共通しています。この調査の目的は、これら 2 つのデータベースのジャーナルの対象範囲を説明し、特定の分野、出版国、言語が過小評価されているかどうかを評価することです。これを行うために、WoS (13,605 ジャーナル) および Scopus (20,346 ジャーナル) のアクティブな学術ジャーナルの範囲を、ウルリッヒの広範な定期刊行物ディレクトリ (63,013 ジャーナル) と比較しました。結果は、WoS または Scopus を研究評価に使用すると、社会科学、芸術、人文科学に不利益をもたらす、自然科学と工学、生物医学研究に有利なバイアスを導入する可能性があることを示しています。同様に、英語のジャーナルが過大評価され、他の言語に損害を与えています。どちらのデータベースもこれらのバイアスを共有していますが、その範囲は大きく異なります。そのため、書誌情報分析の結果は使用するデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈において、特に異なる分野、機関、国、または言語を比較する場合には、WoS と Scopus を慎重に使用する必要があることを示唆しています。書誌学コミュニティは、分野固有の引用インデックスや全国的な引用インデックスなど、WoS や Scopus ではカバーされていない科学的成果を含む手法や指標を開発する努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: OpenAlex、Web of Science、Scopus、Pubmed、Semantic Sc\\u200b\\u200bholar における出版物とドキュメント タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、OpenAlex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、PubMed の書誌データベースの出版物と文書の種類を比較および分析します。この結果は、類型が個々のデータベース プロバイダー間で大きく異なる可能性があることを示しています。さらに、書誌学的分析に関連する文書を特定するために必要な研究文書と非研究文書の区別は、データソースによって異なる場合があります。これは、出版物が各データベースで異なる方法で分類されているためです。 OpenAlex は、図書館や大学における書誌学的分析のための確立された独自のプロバイダーに代わる無料の代替手段としてますます重要になっているため、データベース間の比較に加えて、この研究の焦点は主に OpenAlex に含まれる出版物と文書タイプの範囲と分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2508.18620', '10.1007/s11192-015-1765-5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154']\n",
      "\u001b[93mSummary: Web of Science (WoS) is a bibliometric database that has been used for over 30 years and is commonly used in bibliometric analyses. However, it has been found to have biases towards certain fields and languages, which can impact the results of research evaluations.\n",
      "\n",
      "DOI: 10.1007/s11192-015-1765-5 - This study compares the journal coverage of WoS and Scopus and finds that both databases have biases towards Natural Sciences, Engineering, and Biomedical Research, as well as English-language journals. These biases can disadvantage other fields and languages, and the results of bibliometric analyses may vary depending on the database used.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663 - This study compares OpenAlex and Scopus and finds that traditional databases like WoS systematically underrepresent certain disciplines and world regions. OpenAlex is found to be a reliable alternative for some analyses, but further research is needed to fully understand its limitations.\n",
      "\n",
      "My lady, I hope this information helps. Please let me know if you have any further questions.\n",
      "96\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.9980882, '10.48550/arXiv.2508.18620'), (0.99554896, '10.1007/s11192-015-1765-5'), (0.95565146, '10.48550/arXiv.2404.17663'), (0.6040486, '10.1162/qss_a_00112'), (0.3408142, '10.48550/arXiv.2406.15154')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.48550/arXiv.2502.15005\\n Title: 研究トピックに関する自然言語クエリを知識組織システムと結び付けるための Socratic RAG アプローチ A Socratic RAG Approach to Connect Natural Language Queries on Research Topics with Knowledge Organization Systems\\nAbstract: この論文では、研究トピックに関する自然言語クエリを正確で機械解釈可能な意味エンティティにマッピングする検索拡張生成 (RAG) エージェントを提案します。私たちのアプローチは、RAG とソクラテス対話を組み合わせて、研究トピックに対するユーザーの直感的な理解を確立された知識組織システム (KOS) に合わせます。提案されたアプローチは、「小さなセマンティクス」（ドメイン固有の KOS 構造）と「大きなセマンティクス」（広範な書誌情報リポジトリ）を効果的に橋渡しし、複雑な学術分類をよりアクセスしやすくします。このような薬剤は広範囲に使用される可能性があります。 CollabNext というサンプル アプリケーションを使って説明します。これは、人、組織、研究トピックを結び付ける個人中心のナレッジ グラフです。さらに、現在の科学システムでは歴史的に目に見えない存在となっていた人々の可視性を高めるために、アプリケーションの設計がどのように HBCU と新進の研究者に意図的に焦点を当てているかについて説明します。 In this paper, we propose a Retrieval Augmented Generation (RAG) agent that maps natural language queries about research topics to precise, machine-interpretable semantic entities. Our approach combines RAG with Socratic dialogue to align a user\\'s intuitive understanding of research topics with established Knowledge Organization Systems (KOSs). The proposed approach will effectively bridge \"little semantics\" (domain-specific KOS structures) with \"big semantics\" (broad bibliometric repositories), making complex academic taxonomies more accessible. Such agents have the potential for broad use. We illustrate with a sample application called CollabNext, which is a person-centric knowledge graph connecting people, organizations, and research topics. We further describe how the application design has an intentional focus on HBCUs and emerging researchers to raise visibility of people historically rendered invisible in the current science system.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.1145/3637528.3671470', '10.48550/arXiv.2502.15005', '10.18653/v1/2024.eacl-demo.16']\n",
      "\u001b[93mSummary: Retrieval-augmented generation (RAG) is used to improve question answering and information retrieval systems by providing reliable and up-to-date external knowledge, enhancing the quality of generated content, and reducing the risk of hallucinations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - RAG enables large language models (LLMs) to answer queries over previously unseen document collections by retrieving relevant information from external knowledge sources. However, traditional RAG applications perform poorly in answering multi-hop questions. A new method called Multi-Meta-RAG is introduced, which uses database filtering with LLM-extracted metadata to improve the selection of relevant documents, leading to better performance on multi-hop queries.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - The M-RAG method enhances open-domain question answering (ODQA) systems by utilizing embeddings with metadata and generative models to generate automated responses. This approach allows the model to understand the order and context of documents through metadata, increasing the accuracy of answers. Experiments show a significant performance improvement compared to conventional RAG methods.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470 - RAG is highlighted as a powerful technique in AI, offering reliable and up-to-date external knowledge to assist generative AI in producing high-quality outputs. The emergence of Retrieval-Augmented Large Language Models (RA-LLMs) leverages external knowledge bases to enhance the quality of LLM-generated content, addressing limitations such as hallucinations and out-of-date internal knowledge.\n",
      "\n",
      "DOI: 10.48550/arXiv.2502.15005 - A proposed RAG agent maps natural language queries about research topics to precise, machine-interpretable semantic entities, bridging the gap between domain-specific knowledge organization systems (KOSs) and broad bibliometric repositories. This approach makes complex academic taxonomies more accessible and has potential for broad use in connecting people, organizations, and research topics.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16 - The RAGA framework is introduced for the automated evaluation of RAG pipelines, addressing the challenges of assessing RAG systems' ability to retrieve relevant context, faithfully utilize it, and generate high-quality responses. RAGA provides a suite of metrics for evaluation without relying on human annotations, contributing to faster evaluation cycles of RAG architectures.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need more details, feel free to ask.\n",
      "96\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.99971086, '10.48550/arXiv.2406.13213'), (0.99860066, '10.6109/jkiice.2023.27.12.1489'), (0.96741027, '10.1145/3637528.3671470'), (0.96518475, '10.48550/arXiv.2502.15005'), (0.92121595, '10.18653/v1/2024.eacl-demo.16')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: []\n",
      "length of reranked_documents: 0\n",
      "Retrieved DOIs: []\n",
      "\u001b[93mMy lady, normalizing citation metrics across scientific fields is a complex task due to several inherent challenges. Below is a structured response to your query:\n",
      "\n",
      "**Summary:** Normalizing citation metrics across scientific fields is challenging due to differences in publication cultures, citation practices, field-specific impact factors, and varying publication rates. These factors make it difficult to establish a universal standard for comparing scholarly impact across disciplines.\n",
      "\n",
      "**DOI:** 10.1126/science.1175862  \n",
      "This article by Seglen (1997) in *Science* highlights the variability in citation patterns across different scientific fields, emphasizing that citation counts are heavily influenced by field-specific practices and publication norms. The study underscores the difficulty of directly comparing citation metrics without accounting for these differences.\n",
      "\n",
      "**Additional Challenges:**\n",
      "1. **Publication Culture:** Fields like biomedicine tend to produce more citations per paper compared to mathematics or humanities, where monographs or books may be more influential.\n",
      "2. **Citation Practices:** Some fields cite more extensively (e.g., physics) than others (e.g., mathematics), skewing raw citation counts.\n",
      "3. **Field-Specific Impact Factors:** Journals in certain fields have higher impact factors, which can inflate citation metrics for researchers in those disciplines.\n",
      "4. **Publication Rates:** Fields with higher publication rates (e.g., computer science) may accumulate more citations simply due to volume, not necessarily quality.\n",
      "\n",
      "Efforts to normalize citation metrics often involve field-normalized indicators like the **Field-Weighted Citation Impact (FWCI)** or **Journal Normalized Citation Score**, but these approaches still face limitations in capturing the nuances of interdisciplinary research or emerging fields.\n",
      "\n",
      "If you'd like further details or a different perspective, my lady, please let me know!\n",
      "96\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.500\n",
      "Faithfulness score: 0\n",
      "Documents score: []\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1007/s11192-022-04367-w', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: The methodology presented in the article *Identifying and correcting invalid citations due to DOI errors in Crossref data* is evaluated in the dataset 10_random_citations_per_rule.csv.\n",
      "\n",
      "DOI: 10.5281/ZENODO.6188748\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.595\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.5874699, '10.1007/s11192-022-04367-w'), (0.34224167, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.1145/3681780.3697252\\n Title: 文変換と検索拡張生成 (RAG) による書誌分析の自動化: 影響力の高い都市研究のためのカスタマイズされた文献の特徴付けのための意味論的および文脈検索のパイロット研究 Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG): A Pilot Study in Semantic and Contextual Search for Customized Literature Characterization for High-Impact Urban Research\\nAbstract: 書誌学的分析は、都市科学、特にネイチャー ポートフォリオなどの影響力の大きいジャーナルにおける研究の傾向、範囲、影響を理解するために不可欠です。ただし、キーワード検索や基本的な NLP テクニックに依存する従来の方法では、記事のタイトルやキーワードに明示的に記載されていない貴重な洞察を発見できないことがよくあります。これらのアプローチでは、意味検索や文脈の理解を実行できないため、トピックの分類や研究の特徴付けにおける有効性が制限されます。このペーパーでは、生成 AI モデル、特にトランスフォーマーと検索拡張生成 (RAG) を活用して書誌学的分析を自動化および強化することで、これらの制限に対処します。私たちは、ベクトル データベース、センテンス トランスフォーマー、ガウス混合モデル (GMM)、検索エージェント、および大規模言語モデル (LLM) を統合する技術ワークフローを開発し、カスタマイズされたプロンプト テンプレートを使用した文脈検索、トピックのランキング、および研究の特徴付けを可能にしました。過去 10 年間に Nature Communications に掲載された都市科学関連の 223 件の論文を分析したパイロット研究では、影響力の高いジャーナルの論文の質、範囲、特徴に関する洞察力に富んだ要約統計を生成する際の私たちのアプローチの有効性が明らかになりました。この研究は、都市研究における書誌学的分析と知識検索を強化するための新しいパラダイムを導入し、AI エージェントを研究の評価と理解を進めるための強力なツールとして位置づけています。 Bibliometric analysis is essential for understanding research trends, scope, and impact in urban science, especially in high-impact journals, such Nature Portfolios. However, traditional methods, relying on keyword searches and basic NLP techniques, often fail to uncover valuable insights not explicitly stated in article titles or keywords. These approaches are unable to perform semantic searches and contextual understanding, limiting their effectiveness in classifying topics and characterizing studies. In this paper, we address these limitations by leveraging Generative AI models, specifically transformers and Retrieval-Augmented Generation (RAG), to automate and enhance bibliometric analysis. We developed a technical workflow that integrates a vector database, Sentence Transformers, a Gaussian Mixture Model (GMM), Retrieval Agent, and Large Language Models (LLMs) to enable contextual search, topic ranking, and characterization of research using customized prompt templates. A pilot study analyzing 223 urban science-related articles published in Nature Communications over the past decade highlights the effectiveness of our approach in generating insightful summary statistics on the quality, scope, and characteristics of papers in high-impact journals. This study introduces a new paradigm for enhancing bibliometric analysis and knowledge retrieval in urban research, positioning an AI agent as a powerful tool for advancing research evaluation and understanding.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', \"DOI: 10.48550/arXiv.2510.11195\\n Title: RAG-Pull: コード生成のための RAG システムに対する知覚できない攻撃 RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation\\nAbstract: 検索拡張生成 (RAG) は、LLM 応答の信頼性と信頼性を高め、モデルの再トレーニングの必要性を排除することで幻覚を軽減します。これは、LLM のコンテキストに外部データを追加することによって行われます。私たちは、隠れた UTF 文字をクエリまたは外部コード リポジトリに挿入し、検索を悪意のあるコードにリダイレクトして、モデルの安全性の調整を破る新しいクラスのブラック ボックス攻撃である RAG-Pull を開発しました。クエリとコードの摂動だけでは、検索が攻撃者が制御するスニペットにシフトする可能性がある一方、クエリとターゲットの摂動を組み合わせるとほぼ完璧な成功が得られることが観察されています。これらのスニペットが取得されると、リモート コード実行や SQL インジェクションなどの悪用可能な脆弱性が生じます。 RAG-Pull の最小限の摂動は、モデルの安全性の調整を変更し、安全でないコードへの優先順位を高める可能性があるため、LLM に対する新しい種類の攻撃を可能にします。 Retrieval-Augmented Generation (RAG) increases the reliability and trustworthiness of the LLM response and reduces hallucination by eliminating the need for model retraining. It does so by adding external data into the LLM's context. We develop a new class of black-box attack, RAG-Pull, that inserts hidden UTF characters into queries or external code repositories, redirecting retrieval toward malicious code, thereby breaking the models' safety alignment. We observe that query and code perturbations alone can shift retrieval toward attacker-controlled snippets, while combined query-and-target perturbations achieve near-perfect success. Once retrieved, these snippets introduce exploitable vulnerabilities such as remote code execution and SQL injection. RAG-Pull's minimal perturbations can alter the model's safety alignment and increase preference towards unsafe code, therefore opening up a new class of attacks on LLMs.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2406.13213', '10.1145/3681780.3697252', '10.1007/s44427-025-00006-3', '10.48550/arXiv.2510.11195']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a system that enhances language models by incorporating external knowledge sources.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16: RAG systems are composed of a retrieval and an LLM-based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213: RAG enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections.\n",
      "\n",
      "DOI: 10.1145/3681780.3697252: RAG is used to automate and enhance bibliometric analysis, addressing limitations of traditional methods in semantic searches and contextual understanding.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3: RAG systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources.\n",
      "\n",
      "DOI: 10.48550/arXiv.2510.11195: RAG increases the reliability and trustworthiness of the LLM response and reduces hallucination by eliminating the need for model retraining.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.917\n",
      "Recall: 0.917\n",
      "F1-Score: 0.917\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9552357, '10.18653/v1/2024.eacl-demo.16'), (0.6951311, '10.48550/arXiv.2406.13213'), (0.63906056, '10.1145/3681780.3697252'), (0.55823666, '10.1007/s44427-025-00006-3'), (0.46128976, '10.48550/arXiv.2510.11195')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1109/ADL.1998.670425', '10.1177/09610006241239080', '10.1145/2964909', '10.5860/crl.86.1.101']\n",
      "\u001b[93mSummary: The key challenges in metadata quality assessment include the lack of consensus on the exact definition and measurement of commonly used dimensions such as completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness. Additionally, there is a need for further investigation into less commonly assessed quality dimensions. Advancements include the development of automated quality assessment frameworks for open data portals, the application of qualitative and quantitative content analysis techniques, and the exploration of metadata quality issues across cultures.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - This study explores the dimensions of metadata quality assessment through a scoping review, identifying commonly measured dimensions and highlighting the need for further research into less commonly assessed aspects. It emphasizes the importance of metadata quality research and the improvement of assessment tool flexibility.\n",
      "\n",
      "DOI: 10.1109/ADL.1998.670425 - This research discusses the application of qualitative and quantitative content analysis techniques to assess metadata records, developing criteria and procedures for evaluating different types of metadata.\n",
      "\n",
      "DOI: 10.1145/2964909 - This work focuses on automated quality assessment of metadata across open data portals, developing a generic framework and defining quality metrics for efficient evaluation.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 - This study investigates metadata quality issues across cultures, identifying unique problems and categorizing them to understand the impact of metadata quality on individuals and communities.\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions or need additional information, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in metadata quality assessment?']:\n",
      "Precision: 0.906\n",
      "Recall: 0.906\n",
      "F1-Score: 0.906\n",
      "Accuracy: 0.906\n",
      "Balanced accuracy: 0.478\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.93546456, '10.1109/ADL.1998.670425'), (0.92905515, '10.1177/09610006241239080'), (0.85034436, '10.1145/2964909'), (0.20291664, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101', '10.1177/09610006241239080', '10.1145/2964909']\n",
      "\u001b[93mSummary: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101: Metadata serves discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems.\n",
      "\n",
      "DOI: 10.1177/09610006241239080: Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience.\n",
      "\n",
      "DOI: 10.1145/2964909: The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and potential solutions for improving metadata quality?']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.795\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.9813107, '10.48550/arXiv.2303.17661'), (0.42161134, '10.5860/crl.86.1.101'), (0.35064372, '10.1177/09610006241239080'), (0.23196201, '10.1145/2964909')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 14 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.1007/s44427-025-00006-3', '10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) systems face challenges in evaluating their effectiveness due to the need to assess both retrieval and generation components independently and in combination. However, advancements like RAGAs and Multi-Meta-RAG have been introduced to address these challenges, offering automated evaluation frameworks and improved performance for multi-hop queries.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3: This study evaluates open-source LLMs in RAG systems using a benchmark dataset derived from diploma theses abstracts, highlighting the strengths and weaknesses of different models and retrieval strategies.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16: Introduces RAGAs, a framework for reference-free evaluation of RAG pipelines, addressing the challenges of evaluating RAG architectures by introducing metrics that assess retrieval, generation, and overall quality without human annotations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213: Presents Multi-Meta-RAG, a method that improves RAG performance for multi-hop queries by using database filtering with LLM-extracted metadata, significantly enhancing results on the MultiHop-RAG benchmark.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need more details, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in Retrieval-Augmented Generation (RAG) systems?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.9996923, '10.1016/j.caeai.2025.100417'), (0.99849296, '10.1007/s44427-025-00006-3'), (0.99836427, '10.18653/v1/2024.eacl-demo.16'), (0.9962638, '10.48550/arXiv.2406.13213'), (0.9233162, '10.6109/jkiice.2023.27.12.1489')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 15 in loop: 2\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1109/ADL.1998.670425', '10.1162/qss_a_00286', '10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101']\n",
      "\u001b[93mSummary: two studies examined the abstract in metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286 - This study compared the amount of metadata and the completeness degree of research publications in new academic databases. A quantitative approach was used to select a random Crossref sample of more than 115,000 records, which was then searched in seven databases. Seven characteristics were analyzed, including the abstract, to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 - This study sought to understand how metadata quality, consistency, and completeness impact individuals and communities. A non-random sample of 427 records was reviewed, and 32 unique issues were identified and classified into 5 categories to better explain how metadata and communities interact to reflect cultural meanings.\n",
      "\n",
      "My lady, I hope this information is helpful. Is there anything else I can assist you with?\n",
      "96\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.927\n",
      "Recall: 0.927\n",
      "F1-Score: 0.927\n",
      "Accuracy: 0.927\n",
      "Balanced accuracy: 0.584\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.6092336, '10.1109/ADL.1998.670425'), (0.3526475, '10.1162/qss_a_00286'), (0.23633993, '10.48550/arXiv.2303.17661'), (0.111627996, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 0 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1108/JD-10-2022-0234\\n Title: すべての学術分野にわたる引用と参照の習慣の分析: 書誌の参照と引用の実践におけるアプローチと傾向 An analysis of citing and referencing habits across all scholarly disciplines: approaches and trends in bibliographic referencing and citing practices\\nAbstract: この研究で、著者らは、学術文献における引用および参照の誤りについて現在考えられる原因を特定し、スウィートランド氏が1989年の論文で提供したスナップショットから何かが変わったかどうかを比較したいと考えている。,著者らは、27の主題分野にわたる147のジャーナルに掲載された729件の論文から、参考要素、すなわち書誌的参照、言及、引用、およびそれぞれの本文中の参照ポインタを分析した。,分析の結果は、書誌的事項が指摘された。著者らの知る限り、この研究は、Sweetland (1989) 以来、文献における参照および引用の慣行における誤りを分析したものとしては、最近入手可能な最良のものである。 In this study, the authors want to identify current possible causes for citing and referencing errors in scholarly literature to compare if something changed from the snapshot provided by Sweetland in his 1989 paper.,The authors analysed reference elements, i.e. bibliographic references, mentions, quotations and respective in-text reference pointers, from 729 articles published in 147 journals across the 27 subject areas.,The outcomes of the analysis pointed out that bibliographic errors have been perpetuated for decades and that their possible causes have increased, despite the encouraged use of technological facilities, i.e. the reference managers.,As far as the authors know, the study is the best recent available analysis of errors in referencing and citing practices in the literature since Sweetland (1989).\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1108/JD-10-2022-0234']\n",
      "\u001b[93mSummary: This study examined citing and referencing habits across all scholarly disciplines.\n",
      "DOI: 10.1108/JD-10-2022-0234\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 0.958\n",
      "Recall: 0.958\n",
      "F1-Score: 0.958\n",
      "Accuracy: 0.958\n",
      "Balanced accuracy: 0.600\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.208018, '10.1108/JD-10-2022-0234')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 1 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mSummary: Crossref is a sustainable source of community-owned scholarly metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022 - Crossref is a major source of scholarly data for publishers, authors, librarians, funders and researchers. It contains over 106 million records and expands at an average rate of 11% a year. It includes 13 content types, such as journals, conference papers, data sets, reports, preprints, peer reviews and grants. The metadata includes abstracts and links to full text, funding and license information, citation links, and information about corrections, updates and retractions.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref is a source of open bibliographic metadata. There are six metadata elements: reference lists, abstracts, ORCIDs, author affiliations, funding information and license information. The availability of these metadata elements has improved over time.\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "96\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.94468933, '10.1162/qss_a_00022'), (0.28150436, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 2 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: OpenAlex の言語範囲の評価: メタデータの正確性と完全性の評価 Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate の Web of Science (WoS) と Elsevier の Scopus は、数十年にわたり書誌情報の主要な情報源でした。これらの非公開の独自データベースは高度に厳選されていますが、主に英語の出版物に偏っており、研究の普及における他の言語の使用が過小評価されています。 2022 年に設立された OpenAlex は、包括的で包括的なオープンソースの研究情報を提供することを約束しました。すでに学者や研究機関によって使用されていますが、そのメタデータの品質は現在評価されています。この論文は、WoS との比較や 6,836 件の記事サンプルの綿密な手動検証を通じて、言語に関連する OpenAlex のメタデータの完全性と正確性を評価することで、この文献に貢献します。結果は、OpenAlex が WoS よりもはるかにバランスの取れた言語範囲を示していることを示しています。ただし、言語メタデータは常に正確であるとは限らないため、OpenAlex は英語の位置を過大評価し、他の言語の位置を過小評価することになります。 OpenAlex を批判的に使用すると、学術出版に使用される言語の包括的で代表的な分析を提供できます。ただし、言語のメタデータの品質を確保するには、インフラストラクチャ レベルでのさらなる作業が必要です。 Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.1162/qss_a_00286']\n",
      "\u001b[93mSummary: I found two papers that evaluate the linguistic coverage or language-related metadata in scholarly databases.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633\n",
      "This paper evaluates the linguistic coverage of OpenAlex, a new open-source research information database, by assessing the completeness and accuracy of its language-related metadata through a comparison with Web of Science (WoS) and an in-depth manual validation of a sample of 6,836 articles.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286\n",
      "This paper compares the completeness degree of publication metadata in eight free-access scholarly databases, including language-related metadata, by analyzing a random Crossref sample of more than 115,000 records.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions or need more details, please let me know.\n",
      "96\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.595\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.9992792, '10.48550/arXiv.2409.10633'), (0.4369223, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 3 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00210\\n Title: オープン資金提供者メタデータの可用性と完全性: オランダ研究評議会が資金提供した出版物のケーススタディ he availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: 研究資金提供者は、資金提供した研究の成果に関する情報収集に多大な労力を費やします。資金提供者が資金提供に関連する出版物の成果を追跡できるようにするために、Crossref は 2013 年に FundRef を開始し、出版社が永続的な識別子を使用して資金調達情報を登録できるようにしました。ただし、資金提供された研究の結果であり、資金提供者のメタデータを含めるべき論文が何件あるかが不明であるため、資金提供者のメタデータの範囲を評価することは困難です。この論文では、特定の資金提供機関であるオランダ研究評議会 NWO による資金提供の結果であると研究者によって報告された 5,004 件の出版物を調査しました。これらの記事のうち、Crossref に資金提供情報が含まれているのは 67% のみで、一部の記事では NWO を資金提供者名として認識しているか、NWO にリンクされている資金提供者 ID が示されています (それぞれ 53% と 45%)。 Web of Science (WoS)、Scopus、および Dimensions はすべて、記事全文の資金調達明細書から追加の資金調達情報を推測できます。 Lens の資金提供情報は Crossref の資金提供情報とほぼ一致していますが、追加の資金提供情報の一部はおそらく PubMed から取得されています。独自のデータベースと比較して、Crossref の資金調達メタデータの網羅性と完全性においてパブリッシャー間の興味深い違いが観察され、資金調達に関するオープン メタデータの品質が向上する可能性が強調されています。 Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: COVID-19 研究への資金提供: オープン データ インフラストラクチャを使用した探索的分析からの洞察 Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: 資金提供機関が提供する資金の成果を分析するには、資金提供によって得られた出版物を追跡できることが不可欠です。私たちは、Crossref での資金調達データのオープンな利用可能性を調査し、新型コロナウイルス感染症 (COVID-19) に関連する研究を報告する出版物の資金調達データに焦点を当てています。また、Scopus と Web of Science という 2 つの独自の書誌データベースで利用可能な資金調達データとの比較も示します。私たちの分析では、Crossref の資金調達データの範囲が限られていることが明らかになりました。また、特に Scopus における資金調達データの品質に関連する問題も示しています。 Crossref での資金調達データのオープンな可用性を向上させるための推奨事項を提供します。 To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: 8 つのフリーアクセス学術データベースにおける出版物メタデータの完全性の程度 Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: この研究の主な目的は、新しい学術データベースのメタデータの量と研究出版物の完全性の程度を比較することです。定量的アプローチを使用して、115,000 レコードを超えるランダムな Crossref サンプルを選択し、7 つのデータベース (Dimensions、Google Scholar、Microsoft Academic、OpenAlex、Scilit、Semantic Sc\\u200b\\u200bholar、および The Lens) で検索しました。 7 つの特性 (要約、アクセス、書誌情報、文書タイプ、出版日、言語、識別子) が分析され、この情報を説明するフィールド、これらのフィールドの完全率、およびデータベース間の一致が観察されました。結果は、学術検索エンジン (Google Scholar、Microsoft Academic、Semantic Sc\\u200b\\u200bholar) が収集する情報が少なく、完全性が低いことを示しています。逆に、サードパーティのデータベース (Dimensions、OpenAlex、Scilit、The Lens) はメタデータの品質が高く、完全性が高くなります。私たちは、学術検索エンジンには Web を巡回して信頼できる記述データを取得する機能が欠けており、サードパーティのデータベースの主な問題は、さまざまなソースを統合することで得られる情報の損失であると結論付けています。 The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1162/qss_a_00210', '10.31222/osf.io/smxe5', '10.1162/qss_a_00212', '10.1162/qss_a_00286']\n",
      "\u001b[93mSummary: Three papers address funding metadata, its availability, or its analysis in scholarly databases.\n",
      "\n",
      "DOI: 10.1162/qss_a_00210 - This paper investigates the availability and completeness of open funder metadata, specifically focusing on publications funded by the Dutch Research Council (NWO). It highlights the challenges in assessing the coverage of funder metadata due to the unknown number of articles resulting from funded research. The study found that only 67% of articles funded by NWO contain funding information in Crossref, with variations in acknowledging NWO as the funder name or linking Funder IDs to NWO. The paper also compares funding metadata in Crossref with proprietary databases like Web of Science, Scopus, and Dimensions, revealing differences in coverage and completeness.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - This paper explores Crossref as a source of open bibliographic metadata, including funding information. It provides an overview of the availability of six metadata elements in Crossref, such as reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. The analysis shows that the availability of these metadata elements has improved over time, particularly for journal articles. However, it also highlights the need for publishers to make additional efforts to achieve full openness of bibliographic metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00212 - This paper focuses on funding data for COVID-19 research, specifically examining the open availability of funding data in Crossref and comparing it with proprietary bibliometric databases like Scopus and Web of Science. The analysis reveals limited coverage of funding data in Crossref and identifies issues related to the quality of funding data, particularly in Scopus. The paper offers recommendations for improving the open availability of funding data in Crossref.\n",
      "\n",
      "My lady, these papers provide insights into the availability, completeness, and analysis of funding metadata in scholarly databases, highlighting both challenges and opportunities for improvement.\n",
      "96\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.979\n",
      "Recall: 0.979\n",
      "F1-Score: 0.979\n",
      "Accuracy: 0.979\n",
      "Balanced accuracy: 0.870\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.98469365, '10.1162/qss_a_00210'), (0.6974029, '10.31222/osf.io/smxe5'), (0.6575384, '10.1162/qss_a_00212'), (0.6167627, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 4 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.48550/arXiv.2406.13213', '10.1007/s44427-025-00006-3', '10.6109/jkiice.2023.27.12.1489', '10.18653/v1/2024.eacl-demo.16']\n",
      "\u001b[93mI'm sorry, my lady, I cannot answer your question.\n",
      "96\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) when used in applications?']:\n",
      "Precision: 0.917\n",
      "Recall: 0.917\n",
      "F1-Score: 0.917\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 0\n",
      "Documents score: [(0.99998486, '10.1016/j.caeai.2025.100417'), (0.9986749, '10.48550/arXiv.2406.13213'), (0.9975656, '10.1007/s44427-025-00006-3'), (0.9943141, '10.6109/jkiice.2023.27.12.1489'), (0.9914225, '10.18653/v1/2024.eacl-demo.16')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 5 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: コミュニティが所有する学術メタデータの持続可能なソース Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: このペーパーでは、Crossref によって収集および利用可能にされた学術メタデータと、学術研究エコシステムにおけるその重要性について説明します。 1 億 600 万件を超えるレコードが含まれ、年間平均 11% の割合で拡大している Crossref のメタデータは、出版社、著者、図書館員、資金提供者、研究者にとって学術データの主要なソースの 1 つとなっています。メタデータ セットは 13 のコンテンツ タイプで構成されており、ジャーナルや会議論文などの従来のタイプだけでなく、データ セット、レポート、プレプリント、査読、助成金も含まれます。メタデータには、基本的な出版物のメタデータに限定されず、抄録と全文へのリンク、資金提供とライセンス情報、引用リンク、修正、更新、撤回などの情報も含まれる場合があります。この規模と幅広さにより、Crossref は、科学の成長と影響の測定、学術コミュニケーションの新しいトレンドの理解など、サイエントメトリクスの研究にとって貴重な情報源となっています。メタデータは、REST API や OAI-PMH などの多数の API を通じて利用できます。このペーパーでは、Crossref が提供するメタデータの種類と、それがどのように収集され、キュレーションされるかについて説明します。また、研究エコシステムにおける Crossref の役割と、引用データ提供の進化を含む、長年にわたるメタデータ キュレーションの傾向にも注目します。 Crossref のメタデータで使用された研究を要約し、将来のメタデータの品質と取得を向上させる計画について説明します。 This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: オープン書誌メタデータのソースとしての Crossref Crossref as a source of open bibliographic metadata\\nAbstract: Crossref における学術出版物の書誌メタデータのオープンな利用を促進するために、いくつかの取り組みが行われています。 Crossref の 6 つのメタデータ要素 (参考文献リスト、抄録、ORCID、著者の所属、資金提供情報、ライセンス情報) の利用可能性に関する最新の概要を示します。私たちの分析によると、少なくとも Crossref で最も一般的な出版物の種類である雑誌記事については、これらのメタデータ要素の可用性が時間の経過とともに向上しています。ただし、分析では、多くの出版社が書誌メタデータの完全なオープン性を実現するためにさらなる努力をする必要があることも示しています。 Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5']\n",
      "\u001b[93mSummary: Crossref is a community-owned source of scholarly metadata that is available to publishers, authors, librarians, funders and researchers.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022 - Crossref has over 106 million records and is expanding at an average rate of 11% a year. It has 13 content types including traditional types such as journals and conference papers, but also data sets, reports, preprints, peer reviews and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and information about corrections, updates and retractions.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5 - Crossref has six metadata elements: reference lists, abstracts, ORCIDs, author affiliations, funding information and license information. The availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realise full openness of bibliographic metadata.\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "96\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.99954873, '10.1162/qss_a_00022'), (0.7735587, '10.31222/osf.io/smxe5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 6 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.48550/arXiv.2401.16359\\n Title: Web of Science および Scopus と比較した OpenAlex の参照カバレッジ分析 Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: OpenAlex は、学術メタデータの有望なオープン ソースであり、Web of Science や Scopus などの確立された独自のソースの競合相手です。 OpenAlex はデータを自由かつオープンに提供するため、研究者はライセンスの壁なしにコミュニティ内で再現できる書誌学的研究を実行できます。ただし、OpenAlex は急速に進化するソースであり、そこに含まれるデータは拡大し、また急速に変化しているため、そのデータの信頼性に関する疑問が当然生じます。このレポートでは、各データベース内の参照範囲と選択されたメタデータを調査し、それらを相互に比較して、書誌計量学におけるこの未解決の質問に対処します。私たちの大規模調査では、3 つのデータベースすべてで共有される 1,680 万件の最近の出版物のクリーンなデータセットに限定した場合、OpenAlex は Web of Science と Scopus の両方に匹敵する平均的なソース参照番号と内部カバー率を持っていることを実証しました。さらに、OpenAlex、Web of Science、および Scopus のメタデータをジャーナルごとに分析し、OpenAlex と比較して、Web of Science と Scopus のソース参照数の分布に類似性があることがわかりました。また、OpenAlex がカバーする他のコア メタデータの比較では、ジャーナルごとに分類するとさまざまな結果が示され、Web of Science と Scopus の両方と比較した場合、論文あたりの ORCID 識別子がより多く、抄録が少なく、論文あたりのオープン アクセス ステータス インジケーターの数が同程度であることも示しています。 OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: OpenAlex、Web of Science、Scopus、Pubmed、Semantic Sc\\u200b\\u200bholar における出版物とドキュメント タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、OpenAlex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、PubMed の書誌データベースの出版物と文書の種類を比較および分析します。この結果は、類型が個々のデータベース プロバイダー間で大きく異なる可能性があることを示しています。さらに、書誌学的分析に関連する文書を特定するために必要な研究文書と非研究文書の区別は、データソースによって異なる場合があります。これは、出版物が各データベースで異なる方法で分類されているためです。 OpenAlex は、図書館や大学における書誌学的分析のための確立された独自のプロバイダーに代わる無料の代替手段としてますます重要になっているため、データベース間の比較に加えて、この研究の焦点は主に OpenAlex に含まれる出版物と文書タイプの範囲と分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n', 'DOI: 10.48550/ARXIV.2205.01833\\n Title: OpenAlex: 学術作品、著者、会場、機関、コンセプトの完全にオープンなインデックス OpenAlex: A fully-open index of scholarly works, authors, venues, institutions, and concepts\\nAbstract: OpenAlex は、廃止された Microsoft Academic Graph (MAG) を置き換えるために開始された、新しい完全にオープンな Scientific Knowledge Graph (SKG) です。 2 億 900 万件の作品 (雑誌記事、書籍など) のメタデータが含まれています。 2013M の明確な著者。 124,000 の会場 (ジャーナルやオンライン リポジトリなど、作品をホストする場所)。 109,000の機関。および 65,000 個のウィキデータの概念 (自動化された階層型マルチタグ分類子を介して作品にリンクされています)。データセットは、Web ベースの GUI、完全なデータ ダンプ、および大容量 REST API を介して完全かつ自由に利用できます。このリソースは現在鋭意開発中であり、将来の作業により、引用情報の精度と範囲、著者/機関の解析と重複排除が改善される予定です。 OpenAlex is a new, fully-open scientific knowledge graph (SKG), launched to replace the discontinued Microsoft Academic Graph (MAG). It contains metadata for 209M works (journal articles, books, etc); 2013M disambiguated authors; 124k venues (places that host works, such as journals and online repositories); 109k institutions; and 65k Wikidata concepts (linked to works via an automated hierarchical multi-tag classifier). The dataset is fully and freely available via a web-based GUI, a full data dump, and high-volume REST API. The resource is under active development and future work will improve accuracy and coverage of citation information and author/institution parsing and deduplication.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2508.18620', '10.48550/arXiv.2401.16359', '10.48550/arXiv.2406.15154', '10.48550/ARXIV.2205.01833']\n",
      "\u001b[93mSummary: OpenAlex is a new, open bibliographic database that has distinguished itself for its openness and extensive coverage.\n",
      "\n",
      "DOI: 10.48550/arXiv.2508.18620\n",
      "\n",
      "OpenAlex is a new, open bibliographic database that has distinguished itself for its openness and extensive coverage. It may reduce or eliminate barriers to accessing bibliometric data, but one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata.\n",
      "\n",
      "My lady, is there anything else I can help you with?\n",
      "96\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.99971753, '10.48550/arXiv.2404.17663'), (0.99565166, '10.48550/arXiv.2508.18620'), (0.9932811, '10.48550/arXiv.2401.16359'), (0.97090924, '10.48550/arXiv.2406.15154'), (0.8957053, '10.48550/ARXIV.2205.01833')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 7 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2508.18620\\n Title: OpenAlex と Web of Science の間の文書タイプ、言語、発行年、および著者数の相違を調査する Investigating Document Type, Language, Publication Year, and Author Count Discrepancies Between OpenAlex and Web of Science\\nAbstract: 書誌計量学は、研究または研究評価のどちらに使用される場合でも、研究成果と引用インデックスの大規模な学際的なデータベースに依存しています。 Web of Science (WoS) は、いくつかの新しい競合他社が現れるまで、30 年以上にわたってこの分野の主要なサポート インフラストラクチャでした。 2022 年に開始された書誌データベースである OpenAlex は、そのオープン性と広範な網羅性で際立っています。 OpenAlex は書誌データへのアクセスに対する障壁を軽減または排除する可能性がありますが、研究および研究評価への広範な採用を妨げる懸念の 1 つはメタデータの品質です。この調査は、文書の種類、発行年、言語、著者の数に焦点を当てて、OpenAlex と WoS のメタデータの品質を評価することを目的としています。この研究は、メタデータの不一致と誤った帰属に対処することで、書誌学的研究と評価の結果に影響を与える可能性のあるデータ品質の問題に対する認識を高めることを目指しています。 Bibliometrics, whether used for research or research evaluation, relies on large multidisciplinary databases of research outputs and citation indices. The Web of Science (WoS) was the main supporting infrastructure of the field for more than 30 years until several new competitors emerged. OpenAlex, a bibliographic database launched in 2022, has distinguished itself for its openness and extensive coverage. While OpenAlex may reduce or eliminate barriers to accessing bibliometric data, one of the concerns that hinders its broader adoption for research and research evaluation is the quality of its metadata. This study aims to assess metadata quality in OpenAlex and WoS, focusing on document type, publication year, language, and number of authors. By addressing discrepancies and misattributions in metadata, this research seeks to enhance awareness of data quality issues that could impact bibliometric research and evaluation outcomes.\\n', 'DOI: 10.1007/s11192-015-1765-5\\n Title: Web of Science と Scopus のジャーナルの報道: 比較分析 he journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: 書誌学的手法は、研究の評価など、さまざまな目的で複数の分野で使用されています。ほとんどの書誌学的分析には、トムソン・ロイターの Web of Science (WoS) とエルゼビアの Scopus というデータ ソースが共通しています。この調査の目的は、これら 2 つのデータベースのジャーナルの対象範囲を説明し、特定の分野、出版国、言語が過小評価されているかどうかを評価することです。これを行うために、WoS (13,605 ジャーナル) および Scopus (20,346 ジャーナル) のアクティブな学術ジャーナルの範囲を、ウルリッヒの広範な定期刊行物ディレクトリ (63,013 ジャーナル) と比較しました。結果は、WoS または Scopus を研究評価に使用すると、社会科学、芸術、人文科学に不利益をもたらす、自然科学と工学、生物医学研究に有利なバイアスを導入する可能性があることを示しています。同様に、英語のジャーナルが過大評価され、他の言語に損害を与えています。どちらのデータベースもこれらのバイアスを共有していますが、その範囲は大きく異なります。そのため、書誌情報分析の結果は使用するデータベースによって異なる場合があります。これらの結果は、比較研究評価の文脈において、特に異なる分野、機関、国、または言語を比較する場合には、WoS と Scopus を慎重に使用する必要があることを示唆しています。書誌学コミュニティは、分野固有の引用インデックスや全国的な引用インデックスなど、WoS や Scopus ではカバーされていない科学的成果を含む手法や指標を開発する努力を継続する必要があります。 Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: OpenAlex の書誌学的分析への適合性の分析 An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus と Web of Science は、これらの従来のデータベースが体系的に特定の分野や世界地域を過小評価しているにもかかわらず、科学研究の基盤となってきました。これに応えて、新しい包括的なデータベース、特に OpenAlex が登場しました。多くの研究が OpenAlex をデータ ソースとして使用し始めていますが、その限界を批判的に評価している研究はほとんどありません。 OpenAlex チームと協力して実施されたこの調査は、OpenAlex と Scopus をさまざまな側面から比較することで、このギャップに対処しています。この分析では、OpenAlex は Scopus のスーパーセットであり、一部の分析、特に国レベルでの信頼できる代替手段となり得ると結論付けています。それにもかかわらず、メタデータの精度と完全性の問題は、OpenAlex の制限を完全に理解し、それに対処するには追加の研究が必要であることを示しています。そうすることは、より制約されたデータベースではまったく不可能な分析も含め、幅広い分析にわたって自信を持って OpenAlex を使用するために必要です。 Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.1162/qss_a_00112\\n Title: 書誌データ ソースの大規模比較: Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: ここでは、Scopus、Web of Science、Dimensions、Crossref、Microsoft Academic の 5 つの学際的な書誌データ ソースの大規模な比較を示します。この比較では、これらのデータ ソースでカバーされている 2008 年から 2017 年の期間の科学文書が考慮されています。 Scopus は、他の各データ ソースとペアごとに比較されます。まず、文書の対象範囲におけるデータ ソース間の差異を分析します。たとえば、時間の経過による差異、文書タイプごとの差異、専門分野ごとの差異に焦点を当てます。次に、引用リンクの完全性と正確性の違いを研究します。分析に基づいて、さまざまなデータ ソースの長所と短所について説明します。私たちは、科学文献を包括的にカバーすることと、文献を選択するための柔軟なフィルターのセットを組み合わせることが重要であることを強調します。 We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: OpenAlex、Web of Science、Scopus、Pubmed、Semantic Sc\\u200b\\u200bholar における出版物とドキュメント タイプの分析 Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: この研究では、OpenAlex、Scopus、Web of Science、Semantic Sc\\u200b\\u200bholar、PubMed の書誌データベースの出版物と文書の種類を比較および分析します。この結果は、類型が個々のデータベース プロバイダー間で大きく異なる可能性があることを示しています。さらに、書誌学的分析に関連する文書を特定するために必要な研究文書と非研究文書の区別は、データソースによって異なる場合があります。これは、出版物が各データベースで異なる方法で分類されているためです。 OpenAlex は、図書館や大学における書誌学的分析のための確立された独自のプロバイダーに代わる無料の代替手段としてますます重要になっているため、データベース間の比較に加えて、この研究の焦点は主に OpenAlex に含まれる出版物と文書タイプの範囲と分析にあります。 This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2508.18620', '10.1007/s11192-015-1765-5', '10.48550/arXiv.2404.17663', '10.1162/qss_a_00112', '10.48550/arXiv.2406.15154']\n",
      "\u001b[93mSummary: I'm sorry, my lady, I was unable to find any information about the strengths and weaknesses of Web of Science (WoS) as a bibliometric database.\n",
      "\n",
      "However, I did find the following information about WoS:\n",
      "\n",
      "- WoS was the main supporting infrastructure of bibliometrics for more than 30 years until several new competitors emerged.\n",
      "- WoS is commonly used as a data source for bibliometric analyses.\n",
      "- The use of WoS for research evaluation may introduce biases that favour Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities.\n",
      "- English-language journals are overrepresented in WoS to the detriment of other languages.\n",
      "- WoS systematically underrepresents certain disciplines and world regions.\n",
      "- WoS is included in a large-scale comparison of multidisciplinary bibliographic data sources, which analyses differences in the coverage of documents and the completeness and accuracy of citation links.\n",
      "- WoS is compared with other bibliographic databases in terms of publication and document types, which demonstrates that typologies can differ considerably between individual database providers.\n",
      "\n",
      "DOI: N/A\n",
      "96\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 0\n",
      "Documents score: [(0.9980882, '10.48550/arXiv.2508.18620'), (0.99554896, '10.1007/s11192-015-1765-5'), (0.95565146, '10.48550/arXiv.2404.17663'), (0.6040486, '10.1162/qss_a_00112'), (0.3408142, '10.48550/arXiv.2406.15154')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 8 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n', \"DOI: 10.1145/3637528.3671470\\n Title: RAG ミーティング LLM に関する調査: 検索拡張された大規模言語モデルに向けて A Survey on RAG Meeting LLMs: Towards Retrieval-Augmented Large Language Models\\nAbstract: AI の最も高度な技術の 1 つである検索拡張生成 (RAG) は、信頼性の高い最新の外部知識を提供し、多数のタスクに大きな利便性をもたらします。特に AI 生成コンテンツ (AIGC) の時代では、追加の知識を提供する強力な検索能力により、RAG は既存の生成 AI による高品質の出力の生成を支援できます。最近、大規模言語モデル (LLM) は、言語の理解と生成において革新的な能力を実証しましたが、依然として幻覚や古い内部知識などの固有の制限に直面しています。最新の有用な補助情報を提供する RAG の強力な機能を考慮して、検索拡張大規模言語モデル (RA-LLM) は、モデルの内部知識だけに依存するのではなく、外部の信頼できる知識ベースを利用して、LLM で生成されるコンテンツの品質を強化するために登場しました。この調査では、RA-LLM に関する既存の研究研究を包括的にレビューし、3 つの主要な技術的観点をカバーします。さらに、より深い洞察を提供するために、現在の限界と将来の研究のいくつかの有望な方向性について議論します。 one of the most advanced techniques in AI, Retrieval-Augmented Generation (RAG) can offer reliable and up-to-date external knowledge, providing huge convenience for numerous tasks. Particularly in the era of AI-Generated Content (AIGC), the powerful capacity of retrieval in providing additional knowledge enables RAG to assist existing generative AI in producing high-quality outputs. Recently, Large Language Models (LLMs) have demonstrated revolutionary abilities in language understanding and generation, while still facing inherent limitations such as hallucinations and out-of-date internal knowledge. Given the powerful abilities of RAG in providing the latest and helpful auxiliary information, Retrieval-Augmented Large Language Models (RA-LLMs) have emerged to harness external and authoritative knowledge bases, rather than solely relying on the model's internal knowledge, to augment the quality of the generated content of LLMs. In this survey, we comprehensively review existing research studies in RA-LLMs, covering three primary technical perspectives: Furthermore, to deliver deeper insights, we discuss current limitations and several promising directions for future research.\\n\", 'DOI: 10.48550/arXiv.2502.15005\\n Title: 研究トピックに関する自然言語クエリを知識組織システムと結び付けるための Socratic RAG アプローチ A Socratic RAG Approach to Connect Natural Language Queries on Research Topics with Knowledge Organization Systems\\nAbstract: この論文では、研究トピックに関する自然言語クエリを正確で機械解釈可能な意味エンティティにマッピングする検索拡張生成 (RAG) エージェントを提案します。私たちのアプローチは、RAG とソクラテス対話を組み合わせて、研究トピックに対するユーザーの直感的な理解を確立された知識組織システム (KOS) に合わせます。提案されたアプローチは、「小さなセマンティクス」（ドメイン固有の KOS 構造）と「大きなセマンティクス」（広範な書誌情報リポジトリ）を効果的に橋渡しし、複雑な学術分類をよりアクセスしやすくします。このような薬剤は広範囲に使用される可能性があります。 CollabNext というサンプル アプリケーションを使って説明します。これは、人、組織、研究トピックを結び付ける個人中心のナレッジ グラフです。さらに、現在の科学システムでは歴史的に目に見えない存在となっていた人々の可視性を高めるために、アプリケーションの設計がどのように HBCU と新進の研究者に意図的に焦点を当てているかについて説明します。 In this paper, we propose a Retrieval Augmented Generation (RAG) agent that maps natural language queries about research topics to precise, machine-interpretable semantic entities. Our approach combines RAG with Socratic dialogue to align a user\\'s intuitive understanding of research topics with established Knowledge Organization Systems (KOSs). The proposed approach will effectively bridge \"little semantics\" (domain-specific KOS structures) with \"big semantics\" (broad bibliometric repositories), making complex academic taxonomies more accessible. Such agents have the potential for broad use. We illustrate with a sample application called CollabNext, which is a person-centric knowledge graph connecting people, organizations, and research topics. We further describe how the application design has an intentional focus on HBCUs and emerging researchers to raise visibility of people historically rendered invisible in the current science system.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489', '10.1145/3637528.3671470', '10.48550/arXiv.2502.15005', '10.18653/v1/2024.eacl-demo.16']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a technique that improves the quality of generated content in Large Language Models (LLMs) by retrieving relevant information from external knowledge sources.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - Multi-Meta-RAG is a new method that uses database filtering with LLM-extracted metadata to improve the selection of relevant documents from various sources, relevant to the question.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - M-RAG is a method that utilizes embeddings with metadata and generative models to generate automated responses, allowing the model to understand the order and context of the document through metadata.\n",
      "\n",
      "DOI: 10.1145/3637528.3671470 - Retrieval-Augmented Large Language Models (RA-LLMs) leverage external knowledge bases to enhance the quality of generated content, addressing limitations such as hallucinations and out-of-date internal knowledge.\n",
      "\n",
      "DOI: 10.48550/arXiv.2502.15005 - A Socratic RAG approach is proposed to map natural language queries to precise semantic entities, bridging the gap between domain-specific knowledge and broad bibliometric repositories.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16 - RAGAs is a framework for evaluating RAG pipelines, introducing a suite of metrics to assess the retrieval system's ability to identify relevant context passages, the LLM's ability to exploit such passages, and the quality of the generation itself.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.99971086, '10.48550/arXiv.2406.13213'), (0.99860066, '10.6109/jkiice.2023.27.12.1489'), (0.96741027, '10.1145/3637528.3671470'), (0.96518475, '10.48550/arXiv.2502.15005'), (0.92121595, '10.18653/v1/2024.eacl-demo.16')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 9 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: []\n",
      "length of reranked_documents: 0\n",
      "Retrieved DOIs: []\n",
      "\u001b[93mMy lady, normalizing citation metrics across scientific fields is a complex task due to several inherent challenges. Here’s a structured overview:\n",
      "\n",
      "**Summary:** The main challenges in normalizing citation metrics across scientific fields include differences in publication and citation practices, varying journal cultures, field-specific citation rates, and the impact of discipline size and age. These factors make it difficult to establish a universal standard for comparing citation metrics across disciplines.\n",
      "\n",
      "**DOI:** 10.1126/science.1177997  \n",
      "This article by Seglen (1997) highlights the variability in citation patterns across different scientific fields, emphasizing that citation counts are heavily influenced by field-specific practices, such as the average number of references per paper and the speed of publication. The study underscores the need for field-normalized metrics to ensure fair comparisons.\n",
      "\n",
      "**DOI:** 10.1002/asi.21353  \n",
      "Bornmann and Daniel (2008) discuss the challenges of normalizing citation metrics, particularly the choice of normalization methods (e.g., percentile ranks, z-scores) and the impact of database coverage. They argue that no single method can fully account for the diversity of citation behaviors across fields.\n",
      "\n",
      "**DOI:** 10.1073/pnas.1403879111  \n",
      "Waltman and van Eck (2013) explore the limitations of field-normalized citation metrics, such as the journal-based categorization used in the Leiden Ranking. They stress that inaccuracies in field classification and differences in citation cultures can distort normalized metrics.\n",
      "\n",
      "These challenges necessitate careful consideration of field-specific contexts when using citation metrics for evaluation purposes, my lady.\n",
      "96\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.500\n",
      "Faithfulness score: 0\n",
      "Documents score: []\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 10 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.1007/s11192-022-04367-w\\n Title: Crossref データの DOI エラーによる無効な引用を特定して修正する Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: この研究の目的は、Crossref で利用可能な公開書誌メタデータを分析することによって DOI の間違いのクラスを特定し、どの出版社がそのような間違いに責任を負っているのか、そしてこれらの間違った DOI のうちどれだけが自動プロセスによって修正できるのかを明らかにすることです。過去 2 年間に Crossref オープン DOI-to-DOI 引用 (COCI) の OpenCitations Index を処理する際に、OpenCitations によって収集された無効な引用 DOI のリストを使用することで、2021 年 1 月の Crossref ダンプ内のそのような無効な DOI への引用を取得しました。私たちは、これらの引用の有効性と、関連する引用データを Crossref にアップロードする責任のある出版社を追跡することで、これらの引用を処理しました。最後に、無効な DOI における事実誤認のパターンと、それらを捕捉して修正するために必要な正規表現を特定しました。この調査の結果は、少数の出版社だけが無効な引用の大部分に責任を負っていたり、影響を受けていたことを示しています。私たちは、過去の研究で提案された DOI 名エラーの分類を拡張し、以前のアプローチよりも無効な DOI のより多くの間違いを除去できる、より精巧な正規表現を定義しました。私たちの研究で収集されたデータは、定性的な観点から DOI ミスの考えられる理由を調査することを可能にし、出版社が無効な引用データの生成の根底にある問題を特定するのに役立ちます。また、私たちが提示する DOI クリーニング メカニズムを既存のプロセス (COCI など) に統合して、間違った DOI を自動的に修正して引用を追加することもできます。この研究はオープン サイエンスの原則に従って厳密に実行されたため、研究結果は完全に再現可能です。 This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.5281/ZENODO.6188748\\n Title: DOI 名のエラーのクラス: 評価データセット Classes of errors in DOI names: evaluation dataset\\nAbstract: このデータセットには、「Crossref データの DOI エラーによる無効な引用の特定と修正」という記事で紹介されている方法論の評価の結果が含まれています。 10_random_quotes_per_rule.csv という名前のファイルには、記事で説明されているプロセスによって取得された修正された引用からランダムに選択された 193 の引用が含まれています。これらは、evaluation.py というスクリプトを使用して抽出されており、GitHub リポジトリで確認できます。 This dataset contains the results of the evaluation of the methodology presented in the article Identifying and correcting invalid citations due to DOI errors in Crossref data. The file named 10_random_citations_per_rule.csv contains 193 randomly selected citations from the corrected citations obtained by the process described in the article. They were extracted using the script called evaluation.py, which can be viewed in the GitHub repository\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1007/s11192-022-04367-w', '10.5281/ZENODO.6188748']\n",
      "\u001b[93mSummary: DOI errors in Crossref data are identified and corrected using regular expressions.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w - This study analyses open bibliographic metadata in Crossref to identify DOI mistakes, highlighting responsible publishers and correcting errors through automatic processes. Invalid cited DOIs are retrieved and processed, with patterns of factual errors identified and regular expressions defined for correction. The research extends the taxonomy of DOI name errors and presents a DOI cleaning mechanism for integration into existing processes.\n",
      "\n",
      "DOI: 10.5281/ZENODO.6188748 - This dataset evaluates the methodology for identifying and correcting invalid citations due to DOI errors in Crossref data. It includes randomly selected corrected citations and is available for review on GitHub.\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "96\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.948\n",
      "Recall: 0.948\n",
      "F1-Score: 0.948\n",
      "Accuracy: 0.948\n",
      "Balanced accuracy: 0.595\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.5874699, '10.1007/s11192-022-04367-w'), (0.34224167, '10.5281/ZENODO.6188748')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 11 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.1145/3681780.3697252\\n Title: 文変換と検索拡張生成 (RAG) による書誌分析の自動化: 影響力の高い都市研究のためのカスタマイズされた文献の特徴付けのための意味論的および文脈検索のパイロット研究 Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG): A Pilot Study in Semantic and Contextual Search for Customized Literature Characterization for High-Impact Urban Research\\nAbstract: 書誌学的分析は、都市科学、特にネイチャー ポートフォリオなどの影響力の大きいジャーナルにおける研究の傾向、範囲、影響を理解するために不可欠です。ただし、キーワード検索や基本的な NLP テクニックに依存する従来の方法では、記事のタイトルやキーワードに明示的に記載されていない貴重な洞察を発見できないことがよくあります。これらのアプローチでは、意味検索や文脈の理解を実行できないため、トピックの分類や研究の特徴付けにおける有効性が制限されます。このペーパーでは、生成 AI モデル、特にトランスフォーマーと検索拡張生成 (RAG) を活用して書誌学的分析を自動化および強化することで、これらの制限に対処します。私たちは、ベクトル データベース、センテンス トランスフォーマー、ガウス混合モデル (GMM)、検索エージェント、および大規模言語モデル (LLM) を統合する技術ワークフローを開発し、カスタマイズされたプロンプト テンプレートを使用した文脈検索、トピックのランキング、および研究の特徴付けを可能にしました。過去 10 年間に Nature Communications に掲載された都市科学関連の 223 件の論文を分析したパイロット研究では、影響力の高いジャーナルの論文の質、範囲、特徴に関する洞察力に富んだ要約統計を生成する際の私たちのアプローチの有効性が明らかになりました。この研究は、都市研究における書誌学的分析と知識検索を強化するための新しいパラダイムを導入し、AI エージェントを研究の評価と理解を進めるための強力なツールとして位置づけています。 Bibliometric analysis is essential for understanding research trends, scope, and impact in urban science, especially in high-impact journals, such Nature Portfolios. However, traditional methods, relying on keyword searches and basic NLP techniques, often fail to uncover valuable insights not explicitly stated in article titles or keywords. These approaches are unable to perform semantic searches and contextual understanding, limiting their effectiveness in classifying topics and characterizing studies. In this paper, we address these limitations by leveraging Generative AI models, specifically transformers and Retrieval-Augmented Generation (RAG), to automate and enhance bibliometric analysis. We developed a technical workflow that integrates a vector database, Sentence Transformers, a Gaussian Mixture Model (GMM), Retrieval Agent, and Large Language Models (LLMs) to enable contextual search, topic ranking, and characterization of research using customized prompt templates. A pilot study analyzing 223 urban science-related articles published in Nature Communications over the past decade highlights the effectiveness of our approach in generating insightful summary statistics on the quality, scope, and characteristics of papers in high-impact journals. This study introduces a new paradigm for enhancing bibliometric analysis and knowledge retrieval in urban research, positioning an AI agent as a powerful tool for advancing research evaluation and understanding.\\n', 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', \"DOI: 10.48550/arXiv.2510.11195\\n Title: RAG-Pull: コード生成のための RAG システムに対する知覚できない攻撃 RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation\\nAbstract: 検索拡張生成 (RAG) は、LLM 応答の信頼性と信頼性を高め、モデルの再トレーニングの必要性を排除することで幻覚を軽減します。これは、LLM のコンテキストに外部データを追加することによって行われます。私たちは、隠れた UTF 文字をクエリまたは外部コード リポジトリに挿入し、検索を悪意のあるコードにリダイレクトして、モデルの安全性の調整を破る新しいクラスのブラック ボックス攻撃である RAG-Pull を開発しました。クエリとコードの摂動だけでは、検索が攻撃者が制御するスニペットにシフトする可能性がある一方、クエリとターゲットの摂動を組み合わせるとほぼ完璧な成功が得られることが観察されています。これらのスニペットが取得されると、リモート コード実行や SQL インジェクションなどの悪用可能な脆弱性が生じます。 RAG-Pull の最小限の摂動は、モデルの安全性の調整を変更し、安全でないコードへの優先順位を高める可能性があるため、LLM に対する新しい種類の攻撃を可能にします。 Retrieval-Augmented Generation (RAG) increases the reliability and trustworthiness of the LLM response and reduces hallucination by eliminating the need for model retraining. It does so by adding external data into the LLM's context. We develop a new class of black-box attack, RAG-Pull, that inserts hidden UTF characters into queries or external code repositories, redirecting retrieval toward malicious code, thereby breaking the models' safety alignment. We observe that query and code perturbations alone can shift retrieval toward attacker-controlled snippets, while combined query-and-target perturbations achieve near-perfect success. Once retrieved, these snippets introduce exploitable vulnerabilities such as remote code execution and SQL injection. RAG-Pull's minimal perturbations can alter the model's safety alignment and increase preference towards unsafe code, therefore opening up a new class of attacks on LLMs.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2406.13213', '10.1145/3681780.3697252', '10.1007/s44427-025-00006-3', '10.48550/arXiv.2510.11195']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a system that enhances language models by incorporating external knowledge sources.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16\n",
      "RAG systems are composed of a retrieval and an LLM-based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213\n",
      "RAG enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence.\n",
      "\n",
      "DOI: 10.1145/3681780.3697252\n",
      "RAG is used to automate and enhance bibliometric analysis, addressing limitations of traditional methods that rely on keyword searches and basic NLP techniques. A technical workflow integrating various components enables contextual search, topic ranking, and characterization of research.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3\n",
      "RAG systems are evaluated using the Ragas framework, focusing on retrieval effectiveness and answer quality. The study highlights the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\n",
      "\n",
      "DOI: 10.48550/arXiv.2510.11195\n",
      "RAG increases the reliability and trustworthiness of LLM responses and reduces hallucination by eliminating the need for model retraining. However, a new class of black-box attack, RAG-Pull, exploits vulnerabilities in RAG systems, redirecting retrieval toward malicious code.\n",
      "\n",
      "My lady, I hope this information is helpful. If you have any further questions, please let me know.\n",
      "96\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.917\n",
      "Recall: 0.917\n",
      "F1-Score: 0.917\n",
      "Accuracy: 0.917\n",
      "Balanced accuracy: 0.578\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9552357, '10.18653/v1/2024.eacl-demo.16'), (0.6951311, '10.48550/arXiv.2406.13213'), (0.63906056, '10.1145/3681780.3697252'), (0.55823666, '10.1007/s44427-025-00006-3'), (0.46128976, '10.48550/arXiv.2510.11195')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 12 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1109/ADL.1998.670425\\n Title: メタデータの品質の評価: 米国政府情報検索サービス (GILS) の評価から得られた結果と方法論上の考慮事項 Assessing metadata quality: findings and methodological considerations from an evaluation of the US Government Information Locator Service (GILS)\\nAbstract: メタデータ レコードを評価するための定性的および定量的なコンテンツ分析手法の適用について説明します。米国連邦政府機関による政府情報検索サービス (GILS) の実装に関する大規模な評価研究の一部として、このメタデータ評価では、メタデータの品質に関する探索的調査のための一連の基準と手順が開発されました。著者らは、記録コンテンツ分析とその他のいくつかの方法を使用して、GILS が政府機関が情報の配布と管理の責任を果たすのに役立っているかどうか、また GILS がユーザーの期待にどの程度応えているかを調査しました。記載された探索的分析に基づいて、著者らは、さまざまな種類のメタデータ (例: 記述的、トランザクション的など) を評価するには、さまざまな基準と手順が必要になる可能性があると結論付けています。 GILS の大規模な評価研究をサポートすることに加えて、メタデータ コンテンツのこの分析結果は、メタデータの品質の評価に関する対話の発展に貢献します。 Discusses the application of qualitative and quantitative content analysis techniques to assess metadata records. As a component of a larger evaluation study of US Federal agencies' implementation of the Government Information Locator Service (GILS), this metadata assessment developed a set of criteria and procedures for an exploratory investigation into metadata quality. The authors used record content analysis and several other methods to examine whether GILS is helping agencies fulfill information dissemination and management responsibilities and the extent to which GILS is meeting users' expectations. On the basis of the exploratory analysis described, the authors conclude that a range of criteria and procedures may be needed for evaluating different types of metadata (e.g. descriptive, transactional, etc.). In addition to supporting the larger evaluation study of GILS, the results of this analysis of metadata content contributes to a developing dialog about assessing the quality of metadata.\\n\", 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1109/ADL.1998.670425', '10.1177/09610006241239080', '10.1145/2964909', '10.5860/crl.86.1.101']\n",
      "\u001b[93mSummary: The key challenges in metadata quality assessment include defining and measuring quality dimensions, handling diverse metadata types, and addressing cultural and contextual influences. Advancements involve developing standardized frameworks, automating quality assessments, and integrating qualitative and quantitative methods.\n",
      "\n",
      "DOI: 10.1109/ADL.1998.670425 - This study discusses the application of qualitative and quantitative content analysis techniques to assess metadata records, emphasizing the need for diverse criteria and procedures for different metadata types.\n",
      "\n",
      "DOI: 10.1177/09610006241239080 - This scoping review identifies commonly used dimensions in metadata quality assessment, such as completeness, accuracy, and consistency, while highlighting the lack of consensus on their definitions and measurements.\n",
      "\n",
      "DOI: 10.1145/2964909 - This work presents a generic metadata quality assessment framework for open data portals, focusing on automated evaluation and standardized quality metrics.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101 - This study explores the impact of cultural and contextual factors on metadata quality, identifying unique issues and categorizing them to understand their influence on individuals and communities.\n",
      "\n",
      "My lady, I hope this summary meets your requirements. If you have any further questions or need additional information, please let me know.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in metadata quality assessment?']:\n",
      "Precision: 0.906\n",
      "Recall: 0.906\n",
      "F1-Score: 0.906\n",
      "Accuracy: 0.906\n",
      "Balanced accuracy: 0.478\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.93546456, '10.1109/ADL.1998.670425'), (0.92905515, '10.1177/09610006241239080'), (0.85034436, '10.1145/2964909'), (0.20291664, '10.5860/crl.86.1.101')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 13 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: 大学図書館の電子論文および学位論文のメタデータ品質向上 MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: メタデータの品質は、デジタル ライブラリ インターフェイスを通じてデジタル オブジェクトを発見するために非常に重要です。ただし、さまざまな理由により、デジタル オブジェクトのメタデータは、不完全、一貫性のない、不正確な値を示すことがよくあります。私たちは、電子論文および学位論文 (ETD) の 7 つの主要分野をケーススタディとして使用して、学術メタデータを自動的に検出、修正、正規化する方法を調査します。私たちは、これらの分野の品質を向上させるために、最先端の人工知能手法を活用するフレームワーク「MetaEnhance」を提案します。 MetaEnhance を評価するために、複数の基準を使用してサンプリングされたサブセットを組み合わせて、500 個の ETD を含むメタデータ品質評価ベンチマークをコンパイルしました。このベンチマークで MetaEnhance をテストしたところ、提案された手法が 7 つのフィールドのうち 5 つのフィールドで、エラー検出でほぼ完璧な F1 スコア、およびエラー修正で 0.85 ～ 1.00 の範囲の F1 スコアを達成したことがわかりました。 Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: 文化を超えたメタデータの品質問題の特定 Identifying Metadata Quality Issues Across Cultures\\nAbstract: メタデータは、コンテキスト情報、技術情報、および管理情報を標準形式で提供することにより、検出とアクセスに役立ちます。しかし、メタデータは、社会文化的表現、リソースの制約、標準化されたシステムの間で緊張が生じる場所でもあります。公式および非公式の介入は、品質の問題、アイデンティティを主張するための政治的行為、または可視性を最大化するための戦略的選択として解釈される場合があります。したがって、私たちはメタデータの品質、一貫性、完全性が個人やコミュニティにどのような影響を与えるかを理解しようと努めました。 427 レコードの非ランダム サンプルをレビューすることで、32 の固有の問題を特定し、それらを 5 つのカテゴリに分類して、文化的意味を意図的に反映する (またはしない) ためにメタデータとコミュニティがどのように相互作用するかをよりよく説明しました。 Metadata serve discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. We therefore sought to understand how metadata quality, consistency, and completeness impact individuals and communities. By reviewing a non-random sample of 427 records, we identified 32 unique issues and classified them into 5 categories to better explain how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.1177/09610006241239080\\n Title: メタデータ品質評価の側面を探る: スコーピングレビュー Exploring dimensions of metadata quality assessment: A scoping review\\nAbstract: メタデータの削除は、いくつかの重要な理由から最も重要です。メタデータは、データの取得と検索、データの編成、相互運用性、データの保存、全体的なユーザー エクスペリエンスなど、さまざまな側面で極めて重要な役割を果たします。このスコーピングレビューの目的は、メタデータ品質評価に関する既存の研究で最も一般的に測定されるメタデータ品質の側面を特定することです。この調査では、メタデータの品質評価に関する文献に最も貢献しているデータソースと国の種類、およびその結果を伝えるために使用される文書の種類も調査されています。この方法論には、メタデータの品質評価に関する 55 件の研究を定性的に評価するための PRISMA モデルの適用が含まれます。共起分析は、可視化ソフトウェア VOSviewer 1.6.18 バージョンを使用して、選択された論文のタイトルと要約に対して行われます。このレビューでは、完全性、正確さ、一貫性、アクセシビリティ、適合性、出所、適時性がメタデータの品質評価で一般的に使用される要素であることがわかりました。ただし、その正確な定義と測定については合意が得られておらず、あまり一般的に評価されていない品質側面についてはさらなる調査が必要であることが示されています。デジタル リポジトリとオープン ガバメント データが最も一般的に研究されているデータ ソースであり、米国が主要な貢献国であり、最も一般的に使用されている文書タイプは雑誌論文です。タイトルと要約の用語の共起に基づくクラスター分析により、「メタデータ品質評価」、「メタデータ品質次元」、および「メタデータ品質アプリケーション、フレームワーク、およびアプローチ」の 3 つの研究分野が顕著な研究分野であることがわかりました。この研究の独自性は、メタデータの品質に関する論文の厳格なスクリーニングを含む方法論にあります。これは、メタデータの品質に関する文献を定性的に統合する初めての試みです。この記事では、メタデータ品質調査の重要性と、より優れたメタデータ品質保証措置を促進するためにメタデータ品質評価ツールの柔軟性を向上させる必要性を強調しています。 essing metadata is of paramount importance for several critical reasons. Metadata plays a pivotal role in various aspects, including data retrieval and search, data organization, interoperability, data preservation, and the overall user experience. The purpose of this scoping review is to identify the most commonly measured dimensions of metadata quality in existing studies on metadata quality assessment. The study also investigates the types of data sources and countries contributing most to the literature on metadata quality assessment and the types of documents used to communicate their findings. The methodology involves the application of PRISMA model for qualitatively evaluating 55 studies on metadata quality assessment. The co-occurrence analysis is made on the title and abstract of selected articles using VOSviewer 1.6.18 version, visualization software. The review found that completeness, accuracy, consistency, accessibility, conformance, provenance, and timeliness are commonly used dimensions in metadata quality assessment. However, there is no consensus on their exact definition and measurement, indicating a need for further investigation into less commonly assessed quality dimensions. Digital repositories and open government data are the most commonly studied data sources, with the United States being the leading contributor and journal articles being the most commonly used document type. The cluster analysis based on co-occurrence of terms in title and abstract found three research areas, “Metadata Quality Assessment,” “Metadata Quality Dimensions,” and “Metadata Quality Applications, Frameworks, and Approaches” as prominent areas of research. The originality of the study lies in its methodology that involves rigorous screening of articles on metadata quality. It is a first attempt to qualitatively synthesize literature on metadata quality. The article emphasizes the importance of metadata quality research and the need to improve the flexibility of metadata quality assessment tools to facilitate better metadata quality assurance measures.\\n', 'DOI: 10.1145/2964909\\n Title: オープン データ ポータル全体のメタデータの自動品質評価 Automated Quality Assessment of Metadata across Open Data Portals\\nAbstract: オープン データ運動は、Web 上で公開データを利用する原動力となっています。政府や公的機関だけでなく民間部門からもオンラインで利用できるデータが増えており、主にいわゆるオープン データ ポータルで公開されています。ただし、公開されるリソースの数が増加するにつれて、データ ソースと対応するメタデータの品質に関して多くの懸念が生じ、リソースの検索可能性、発見可能性、および使いやすさが損なわれます。これらの問題の深刻さをより完全に把握するために、現在の作業は、さまざまなオープン データ ポータル向けの汎用メタデータ品質評価フレームワークを開発することを目的としています。私たちは、広く使用されている 3 つのポータル ソフトウェア フレームワーク (CKAN、Socrata、OpenDataSoft) の特定のメタデータを標準化されたデータ カタログ語彙メタデータ スキーマにマッピングすることにより、データ ポータルをポータル ソフトウェア フレームワークから独立して扱います。その後、自動的かつ効率的な方法で評価できるいくつかの品質指標を定義します。最後に、110 万のデータセットを含む 260 以上のオープン データ ポータルのセットを監視した結果を報告します。これには、データの取得可能性や特定の品質指標の分析など、一般的な品質問題の議論が含まれます。 The Open Data movement has become a driver for publicly available data on the Web. More and more data—from governments and public institutions but also from the private sector—are made available online and are mainly published in so-called Open Data portals. However, with the increasing number of published resources, there is a number of concerns with regards to the quality of the data sources and the corresponding metadata, which compromise the searchability, discoverability, and usability of resources. In order to get a more complete picture of the severity of these issues, the present work aims at developing a generic metadata quality assessment framework for various Open Data portals: We treat data portals independently from the portal software frameworks by mapping the specific metadata of three widely used portal software frameworks (CKAN, Socrata, OpenDataSoft) to the standardized Data Catalog Vocabulary metadata schema. We subsequently define several quality metrics, which can be evaluated automatically and in an efficient manner. Finally, we report findings based on monitoring a set of over 260 Open Data portals with 1.1M datasets. This includes the discussion of general quality issues, for example, the retrievability of data, and the analysis of our specific quality metrics.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.48550/arXiv.2303.17661', '10.5860/crl.86.1.101', '10.1177/09610006241239080', '10.1145/2964909']\n",
      "\u001b[93mSummary: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661\n",
      "\n",
      "My lady, I have found some information on metadata quality. Would you like me to elaborate on any of the points?\n",
      "96\n",
      "For query: ['What are the key challenges and potential solutions for improving metadata quality?']:\n",
      "Precision: 0.969\n",
      "Recall: 0.969\n",
      "F1-Score: 0.969\n",
      "Accuracy: 0.969\n",
      "Balanced accuracy: 0.795\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.9813107, '10.48550/arXiv.2303.17661'), (0.42161134, '10.5860/crl.86.1.101'), (0.35064372, '10.1177/09610006241239080'), (0.23196201, '10.1145/2964909')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 14 in loop: 1\n",
      "Length of documents: 96\n",
      "reranked_documents: [\"DOI: 10.1016/j.caeai.2025.100417\\n Title: 教育用途のための検索拡張生成: 体系的な調査 Retrieval-augmented generation for educational application: A systematic survey\\nAbstract: 大規模言語モデル (LLM) の進歩により、AI 主導の教育が変革され、さまざまな学習および教育領域にわたって革新的なアプリケーションが可能になりました。しかし、LLM は依然として、幻覚や静的な内部知識など、教育現場での信頼性を妨げるいくつかの課題に直面しています。検索拡張生成 (RAG) は、外部の知識ベースから関連情報を取得し、それを LLM の生成プロセスに組み込むことで、LLM を強化します。このアプローチにより、事実の正確性が向上し、動的な知識の更新が可能になるため、LLM は教育用途に特に適しています。この論文では、RAG を教育シナリオに統合する既存の研究を包括的にレビューします。まず RAG の定義とワークフローを明確にし、RAG のインデックス作成メカニズムに従って、さまざまな種類の取得機能と生成最適化手法を紹介します。この研究の主な焦点として、対話型学習システム、教育コンテンツの生成と評価、教育エコシステムでの大規模な展開をカバーする、教育における RAG の実践的な応用を調査します。この文書では、包括的なレビューに基づいて、幻覚の軽減、取得した知識の完全性と適時性の確保、計算コストの削減、RAG ベースの教育アプリケーションのマルチモーダル サポートの強化など、既存の課題と将来の方向性について説明します。 dvancements in large language models (LLMs) have transformed AI-driven education, enabling innovative applications across various learning and teaching domains. However, LLMs still face several challenges, including hallucination and static internal knowledge, which hinder their reliability in educational settings. Retrieval-Augmented Generation (RAG) enhances LLMs by retrieving relevant information from an external knowledge base and incorporating it into the LLM's generation process. This approach improves factual accuracy and enables dynamic knowledge updates, making LLMs particularly suitable for educational applications. In this paper, we comprehensively review existing research that integrates RAG into educational scenarios. We first clarify the definition and workflow of RAG, and following the indexing mechanism of RAG, we introduce different types of retrievers and generation optimization methods. As the main focus of this work, we explore the practical applications of RAG in education, covering interactive learning systems, generation and assessment of educational content, and large-scale deployment in educational ecosystems. Based on our comprehensive review, this paper discusses existing challenges and future directions, including mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\\n\", 'DOI: 10.1007/s44427-025-00006-3\\n Title: RAG システムにおけるオープンソース LLM の評価: Ragas を使用した卒業論文要約のベンチマーク Evaluating Open-Source LLMs in RAG Systems: A Benchmark on Diploma Theses Abstracts Using Ragas\\nAbstract: 検索拡張生成 (RAG) システムは、外部の知識ソースを組み込むことで言語モデルを強化できる機能で大きな注目を集めています。ただし、検索コンポーネントと生成コンポーネントの両方を個別に、または組み合わせて評価する必要があるため、これらのシステムの有効性を評価することは依然として課題です。この研究は、卒業論文の要約から得られたデータセットを使用して、RAG システム内のオープンソースの大規模言語モデル (LLM) の包括的な評価を提供することを目的としています。パフォーマンスを系統的に評価するために、推論、事実ベース、要約タイプの質問に分類された、参考回答を含む 122 の質問で構成されるベンチマーク データセットを生成しました。 Elasticsearch を取得者として使用し、いくつかのオープンソース LLM をジェネレーターとして使用し、検索の有効性と回答の品質に焦点を当てて、Ragas (Retrieval Augmented Generation Assessment) フレームワークを使用してシステムを評価しました。私たちの調査結果は、さまざまなモデルと検索戦略の長所と短所を浮き彫りにし、学術的および構造化された知識タスクの RAG 実装を最適化するための洞察を提供します。 Retrieval-Augmented Generation (RAG) systems have gained significant attention for their ability to enhance language models by incorporating external knowledge sources. However, evaluating the effectiveness of these systems remains a challenge, as both the retrieval and generation components must be assessed independently and in combination. This study aims to provide a comprehensive evaluation of open-source large language models (LLMs) within a RAG system, using a dataset derived from diploma theses abstracts. To systematically assess performance, we generated a benchmark dataset consisting of 122 questions with reference answers, categorized into reasoning, fact-based, and summary-type questions. Using Elasticsearch as the retriever and several open-source LLMs as generators, we evaluated the system using the Ragas (Retrieval Augmented Generation Assessment) framework, focusing on retrieval effectiveness and answer quality. Our findings highlight the strengths and weaknesses of different models and retrieval strategies, offering insights into optimizing RAG implementations for academic and structured knowledge tasks.\\n', 'DOI: 10.18653/v1/2024.eacl-demo.16\\n Title: RAGA: 検索拡張生成の自動評価 RAGAs: Automated Evaluation of Retrieval Augmented Generation\\nAbstract: 取得拡張生成 (RAG) パイプラインをリファレンスフリーで評価するためのフレームワークである RAGA (取得拡張生成評価) を紹介します。 RAG システムは、検索モジュールと LLM ベースの生成モジュールで構成されます。これらは、参照テキスト データベースからの知識を LLM に提供し、LLM がユーザーとテキスト データベースの間の自然言語層として機能できるようにし、幻覚のリスクを軽減します。 RAG アーキテクチャの評価は、いくつかの側面を考慮する必要があるため、困難です。それは、関連する焦点を絞ったコンテキストのパッセージを識別する検索システムの能力、そのようなパッセージを忠実に利用する LLM の能力、生成自体の品質です。 RAGA では、人間によるグラウンド トゥルースのアノテーションに依存せずに、これらのさまざまな次元を評価できる一連のメトリクスを導入します。私たちは、このようなフレームワークが RAG アーキテクチャの評価サイクルの高速化に大きく貢献できると考えています。これは、LLM の急速な導入を考えると特に重要です。 We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module. They provide LLMs with knowledge from a reference textual database, enabling them to act as a natural language layer between a user and textual databases, thus reducing the risk of hallucinations. Evaluating RAG architectures is challenging due to several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages faithfully, and the quality of the generation itself. With RAGAs, we introduce a suite of metrics that can evaluate these different dimensions without relying on ground truth human annotations. We posit that such a framework can contribute crucially to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: LLM 抽出メタデータによるデータベース フィルタリングを使用したマルチホップ クエリの RAG の改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索拡張生成 (RAG) により、外部の知識ソースから関連情報を取得できるようになり、大規模言語モデル (LLM) がこれまでに見たことのない文書コレクションに対するクエリに応答できるようになります。ただし、従来の RAG アプリケーションは、裏付けとなる証拠の複数の要素を取得して推論する必要があるマルチホップの質問に答える際にパフォーマンスが低いことが実証されています。 Multi-Meta-RAG と呼ばれる新しい方法を導入します。これは、LLM で抽出されたメタデータによるデータベース フィルタリングを使用して、質問に関連するさまざまなソースから関連ドキュメントの RAG 選択を改善します。データベース フィルタリングは特定のドメインおよび形式からの一連の質問に固有ですが、Multi-Meta-RAG により MultiHop-RAG ベンチマークの結果が大幅に向上することがわかりました。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: メタデータ検索拡張生成によるオープンドメインの質問応答の強化 M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: この論文では、オープンドメインの質問応答 (ODQA) システムを使用して 1 つ以上のドキュメントを効果的に検索して質問に答え、そのパフォーマンスを比較できるメタデータ検索拡張生成 (M-RAG) 手法を提案します。これを行うために、メタデータを含む埋め込みと、GPT-3.5-turbo-16K や GPT-4 などの生成モデルを利用して、自動応答を生成します。この論文の方法により、生成モデル (GPT-3.5、GPT-4) はメタデータを通じて文書の順序とコンテキストを理解し、それに答えることができます。また、文書の出典や原文要件を追加する迅速なエンジニアリングにより、質問と回答（QA）の出典表示機能を有効にし、回答の精度を高めることができます。実験の結果、本稿の手法は、同じ外部推論ODQAシステムと比較して最大46%の性能向上を示し、従来のRAG手法と比較しても6%の性能向上を示した。 In this paper, we propose a Metadata Retrieval-Augmented Generation (M-RAG) method that can effectively search and answer questions with an open-domain Question Answering (ODQA) system for one or more documents and compare their performance. To do this, it utilizes embeddings with metadata and generative models such as GPT-3.5-turbo-16K and GPT-4 to generate automated responses. Through the method of this paper, the generative model (GPT-3.5, GPT-4) can understand the order and context of the document through metadata and answer it. In addition, through prompt engineering that adds the source of the document and the original text requirements, the source indication function of the question and answer (QA) can be activated, which can increase the accuracy of the answer. As a result of the experiment, the method in this paper showed a performance improvement of up to 46% compared to the same external inference ODQA system, and also showed a 6% performance improvement over the conventional RAG method.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1016/j.caeai.2025.100417', '10.1007/s44427-025-00006-3', '10.18653/v1/2024.eacl-demo.16', '10.48550/arXiv.2406.13213', '10.6109/jkiice.2023.27.12.1489']\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) systems face challenges in evaluating their effectiveness, particularly in assessing both retrieval and generation components independently and in combination. Advancements include the development of frameworks like RAGAs and Ragas for automated and comprehensive evaluation, and methods like Multi-Meta-RAG and M-RAG to improve performance in multi-hop queries and open-domain question answering, respectively.\n",
      "\n",
      "DOI: 10.1016/j.caeai.2025.100417 - This paper discusses the integration of RAG into educational scenarios, highlighting challenges such as mitigating hallucinations, ensuring the completeness and timeliness of retrieved knowledge, reducing computational costs, and enhancing multimodal support for RAG-based educational applications.\n",
      "\n",
      "DOI: 10.1007/s44427-025-00006-3 - The study evaluates open-source LLMs in RAG systems using a benchmark dataset derived from diploma theses abstracts, focusing on retrieval effectiveness and answer quality, and provides insights into optimizing RAG implementations for academic tasks.\n",
      "\n",
      "DOI: 10.18653/v1/2024.eacl-demo.16 - Introduces RAGAs, a framework for reference-free evaluation of RAG pipelines, addressing challenges in evaluating the retrieval system's ability, LLM's utilization of retrieved passages, and generation quality without relying on human annotations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - Presents Multi-Meta-RAG, a method that improves RAG performance in multi-hop queries by using database filtering with LLM-extracted metadata, significantly enhancing results on the MultiHop-RAG benchmark.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489 - Proposes M-RAG, a method for enhancing open-domain question answering by utilizing metadata embeddings and generative models, achieving up to 46% performance improvement compared to external inference ODQA systems and 6% over conventional RAG methods.\n",
      "\n",
      "My lady, these are the key challenges and advancements in Retrieval-Augmented Generation (RAG) systems based on the provided documents.\n",
      "96\n",
      "For query: ['What are the key challenges and advancements in Retrieval-Augmented Generation (RAG) systems?']:\n",
      "Precision: 0.938\n",
      "Recall: 0.938\n",
      "F1-Score: 0.938\n",
      "Accuracy: 0.938\n",
      "Balanced accuracy: 0.684\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.9996923, '10.1016/j.caeai.2025.100417'), (0.99849296, '10.1007/s44427-025-00006-3'), (0.99836427, '10.18653/v1/2024.eacl-demo.16'), (0.9962638, '10.48550/arXiv.2406.13213'), (0.9233162, '10.6109/jkiice.2023.27.12.1489')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The time has come!\n",
      "\u001b[96mWorking on row: 15 in loop: 1\n",
      "\u001b[95m!!!!! All Done!!!!!\n"
     ]
    }
   ],
   "source": [
    "#run the test from here\n",
    "\n",
    "\n",
    "#***** Begin chat session *****\n",
    "# set directory path\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_jats\"\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_multi_lang_ja\"\n",
    "\n",
    "# read documents and dois from the directory path\n",
    "documents_with_doi = read_documents_with_doi(directory_path)\n",
    "documents = [doc[0].split('\\n')[1:] for doc in documents_with_doi]\n",
    "print(f\"Length of documents: {len(documents)}\")\n",
    "print(f\"Length of corpus: {len(documents_with_doi)}\")\n",
    "\n",
    "# Countdown function\n",
    "def countdown(seconds:int)->None:\n",
    "    # Loop until seconds is 0\n",
    "    while seconds > 0:\n",
    "        print(Fore.LIGHTMAGENTA_EX + f\"{seconds}\", end='      \\r')  # Print current countdown value\n",
    "        time.sleep(1)  # Wait for 1 second\n",
    "        seconds -= 1  # Decrease seconds by 1\n",
    "    print(\"The time has come!\")  # Countdown finished message\n",
    "\n",
    "def evaluate_retrieval(retrieved_dois, ground_truth, response, query:str,reranked_DOIs_with_score_end)->Dict:\n",
    "    corpus_doi_list = []\n",
    "    #corpus_list is a global variable in rag_pipeline()\n",
    "    for each in range(len(documents_with_doi)):\n",
    "        #a = documents_with_doi[each].get('doi',\"\")\n",
    "        a = documents_with_doi[each].split(\"\\n\")[0].lstrip(\"DOI: \")\n",
    "        corpus_doi_list.append(a)\n",
    "    print(len(corpus_doi_list))\n",
    "\n",
    "    def compare_lists(list1, list2, list3):\n",
    "        for val in list1:\n",
    "            if val in list2:\n",
    "                list3.append(1)\n",
    "            else:\n",
    "                list3.append(0)\n",
    "\n",
    "    #set y_true so that len(y_true)==len(corpus_doi_list)\n",
    "    y_true = []\n",
    "    compare_lists(corpus_doi_list,ground_truth,y_true)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = []\n",
    "    compare_lists(corpus_doi_list,retrieved_dois,y_pred)\n",
    "\n",
    "\n",
    "    # calculate metrics - could also use sklearn.metrics functions such as precision_score, but this is easier to read\n",
    "    precision = precision_score(y_true, y_pred, average=\"micro\")\n",
    "    recall = recall_score(y_true, y_pred,average=\"micro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    faithfulness_score = 0\n",
    "    for each in retrieved_dois:\n",
    "        if each in response.message.content[0].text:\n",
    "            faithfulness_score+=1\n",
    "        else:\n",
    "            faithfulness_score+=0\n",
    "\n",
    "        \n",
    "    return {\n",
    "        'Query':f\"{query}\",\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\":accuracy,\n",
    "        \"Balanced accuracy\":balanced_accuracy,\n",
    "        \"Faithfulness score\":faithfulness_score,\n",
    "        \"Documents score\":str(reranked_DOIs_with_score_end),\n",
    "        \"Response\":response.message.content[0].text\n",
    "    }\n",
    "\n",
    "def print_results(retrieved_dois, ground_truth, response, query:str, reranked_DOIs_with_score_end)->Dict:\n",
    "    \"\"\"\n",
    "    Prints a nicely ordered set of results from evalaute_retrieval()\n",
    "    \"\"\"\n",
    "\n",
    "    results = evaluate_retrieval(retrieved_dois, ground_truth, response, query, reranked_DOIs_with_score_end)\n",
    "    print(f\"For query: {results['Query']}:\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"F1-Score: {results['F1-Score']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    print(f\"Balanced accuracy: {results['Balanced accuracy']:.3f}\")\n",
    "    print(f\"Faithfulness score: {results['Faithfulness score']}\")\n",
    "    print(f\"Documents score: {results['Documents score']}\")\n",
    "    return results\n",
    "\n",
    "#print_results()\n",
    "\n",
    "def cohere_test_loop(query:str,ground_truth:List[str]):\n",
    "\n",
    "    # set top_k\n",
    "    top_k = 5\n",
    "    #set threshold \n",
    "    threshold = 0.10\n",
    "\n",
    "    response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,query,top_k,threshold)\n",
    "    \n",
    "    # Extract DOIs from retrieved documents\n",
    "    retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "    print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "    # Display the response\n",
    "    print(Fore.LIGHTYELLOW_EX + f\"{response.message.content[0].text}\")\n",
    "\n",
    "    new_result = print_results(retrieved_dois, ground_truth, response, query, reranked_DOIs_with_score_end)\n",
    "    # add the new result to the df\n",
    "    results_df.loc[len(results_df)] = new_result\n",
    "\n",
    "    #save the queries and responses to separate dataframe to be manually annontated\n",
    "    answer_relevance_df = results_df[['Query','Response']].copy(deep=True)\n",
    "\n",
    "    # save out answer_relevance_df\n",
    "    filename=\"analysis/Round2/results/english_multi_lang_answer_relevance_results.xlsx\"\n",
    "    answer_relevance_df.to_excel(filename)\n",
    "\n",
    "    filename = \"analysis/Round2/results/english_multi_lang_analysis_results.xlsx\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    results_df.to_excel(filename)\n",
    "\n",
    "    # rate limit functions\n",
    "    seconds = 10\n",
    "    print(Fore.LIGHTRED_EX + f\"Waiting for {seconds} seconds...\")\n",
    "    countdown(seconds)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "#golden_set_df_test['Response\\nDense'] = golden_set_df_test.apply(lambda x: test_loop(x.query,x.ground_truth), axis=1)\n",
    "golden_set_df_query = golden_set_df['query'].to_list()\n",
    "golden_set_df_ground_truth = golden_set_df['ground_truth'].to_list()\n",
    "\n",
    "loop_length = 5\n",
    "while loop_length:\n",
    "    for i in range(len(golden_set_df_query)):\n",
    "        \n",
    "        cohere_test_loop([golden_set_df_query[i]],golden_set_df_ground_truth[i])\n",
    "        print(Fore.LIGHTCYAN_EX + f\"Working on row: {i} in loop: {loop_length}\")\n",
    "    loop_length = loop_length-1\n",
    "\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"!!!!! All Done!!!!!\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
