{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere API and SciBERT for RAG\n",
    "This notebook uses a Cohere API for generating responses to text. A query input is required from the user. \n",
    "SciBERT is used for embeddings in a dense vector array both the text and the query. \n",
    "A DOI is supplied with the text as both an identifier and locator. \n",
    "\n",
    "- [ ] set up venv\n",
    "- [ ] install transformers torch cohere in command line\n",
    "\n",
    "### todo\n",
    "- [ ] create script that compiles data/documents.txt with DOI || text for all documents\n",
    "- [ ] store vectorized documents in a db\n",
    "    - https://huggingface.co/learn/cookbook/rag_with_hugging_face_gemma_mongodb\n",
    "\n",
    "### options\n",
    "- Batch Processing:\n",
    "    If large number of texts, process them in batches to avoid memory issues.\n",
    "    Example: Use a loop or torch.utils.data.DataLoader.\n",
    "\n",
    "- Change model size: smaller models require less processing\n",
    "\n",
    "- fine tune model on corpus\n",
    "\n",
    "- look into pooling strategies\n",
    "\n",
    "- Tokenizer\n",
    "    - put cleaning process distincly prior to the tokenizer, using the default values as much as possible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import cohere\n",
    "from cohere import Client\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.Client(\"i4WfLKa1zNNKsPU3n4ZEVuzpaTCBwztx6p6hebpO\")\n",
    "\n",
    "# Load SciBERT model and tokenizer\n",
    "\"\"\"\n",
    "Autotokenizer documentation can be found here: https://huggingface.co/docs/transformers/v4.50.0/en/model_doc/auto#transformers.AutoTokenizer\n",
    "\n",
    "Model documentation can be found here: https://huggingface.co/allenai/scibert_scivocab_uncased\n",
    "Citation for SciBERT:\n",
    "@inproceedings{beltagy-etal-2019-scibert,\n",
    "    title = \"SciBERT: A Pretrained Language Model for Scientific Text\",\n",
    "    author = \"Beltagy, Iz  and Lo, Kyle  and Cohan, Arman\",\n",
    "    booktitle = \"EMNLP\",\n",
    "    year = \"2019\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://www.aclweb.org/anthology/D19-1371\"\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Initialize tokenizer with custom parameters\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"allenai/scibert_scivocab_uncased\",\n",
    "    max_len=512,\n",
    "    use_fast=True,  # Use the fast tokenizer\n",
    "    do_lower_case=False,  # Preserve case\n",
    "    add_prefix_space=False,  # No prefix space\n",
    "    never_split=[\"[DOC]\", \"[REF]\"],  # Tokens to never split\n",
    "    additional_special_tokens=[\"<doi>\", \"</doi>\"]  # Add custom special tokens\n",
    ")\n",
    "\n",
    "# This is the SciBERT model that is used to embed the text and query.\n",
    "# other models: 'allenai-specter', \n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents:\n",
      "Score: 0.7746, DOI: https://doi.org/10.1162/qss_a_00286, Document: ABSTRACT  The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\n",
      "Score: 0.7364, DOI: https://doi.org/10.48550/arXiv.2401.16359, Document: Abstract  OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our largescale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\n",
      "\n",
      "Generated Response:\n",
      " The abstracts referenced in the provided DOIs study a variety of cat breeds, including 'Siamese' and 'mixed'. \n",
      "\n",
      "Would you like me to extract any other information from these abstracts? \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basic RAG with Cohere model\n",
    "Document source: data/documents.txt where the DOI with resolver is separated from the abstract by ||. One record per line. \n",
    "Saved as UTF-8\n",
    "\n",
    "Returns:  answers based on query from input()\n",
    "\"\"\"\n",
    "\n",
    "# Function to generate embeddings using SciBERT\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    converts raw text to numerical representations using a pretrained model, in this case, SciBERT.\n",
    "    Currently this is applied to both the document text and the query. \n",
    "    May want a different version or decorator for the query as they are generally much shorter and more sparse.\n",
    "\n",
    "    Input: text from tokenizer step above as a list of strings\n",
    "    Output: np.array\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512, # returns PyTorch tensors which are compatible with model\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True # return the attention mask - need to learn more\n",
    "        )\n",
    "    # this passes the tokenized inputs through the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # applies mean pooling to get a fixed size embedding\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Function to read documents and their DOIs from a file\n",
    "def read_documents_with_doi(file_path: str) -> List[Dict[str, str]]:\n",
    "    documents_with_doi = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\"||\")  # Assuming DOI and document are separated by \"||\"\n",
    "            if len(parts) == 2:\n",
    "                doi, document = parts\n",
    "                documents_with_doi.append({\"doi\": doi.strip(), \"text\": document.strip()})\n",
    "    return documents_with_doi\n",
    "\n",
    "# Path to the file containing documents and DOIs\n",
    "file_path = \"data/documents.txt\"  # Replace with your file path\n",
    "\n",
    "# Read documents and DOIs from the file\n",
    "documents_with_doi = read_documents_with_doi(file_path)\n",
    "\n",
    "# Extract document texts and DOIs\n",
    "documents = [doc[\"text\"] for doc in documents_with_doi]\n",
    "dois = [doc[\"doi\"] for doc in documents_with_doi]\n",
    "\n",
    "# Example query\n",
    "query = input(\" What is your query: \")\n",
    "\n",
    "# Generate document embeddings\n",
    "document_embeddings = generate_embeddings(documents)\n",
    "# print(document_embeddings.shape) # to see the output shape of the array\n",
    "\n",
    "# Generate query embedding\n",
    "query_embedding = generate_embeddings([query])[0] # generates np.array for the query text\n",
    "\n",
    "# Function to retrieve top-k documents using cosine similarity\n",
    "def retrieve_documents(query_embedding: np.ndarray, document_embeddings: List[np.ndarray], top_k: int = 2) -> List[Tuple[float, Dict[str, str]]]:\n",
    "    similarities = []\n",
    "    for doc_emb in document_embeddings:\n",
    "        # cosine similarity\n",
    "        similarity = np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb)) \n",
    "        similarities.append(similarity)\n",
    "    # ranking\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    return [(similarities[i], documents_with_doi[i]) for i in top_indices]\n",
    "\n",
    "# Retrieve top documents\n",
    "top_documents = retrieve_documents(query_embedding, document_embeddings)\n",
    "print(\"Retrieved Documents:\")\n",
    "for score, doc in top_documents:\n",
    "    print(f\"Score: {score:.4f}, DOI: {doc['doi']}, Document: {doc['text']}\")\n",
    "\n",
    "# Prepare context for Cohere's Command model (include DOI) - need to add in cited by here\n",
    "context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for _, doc in top_documents])\n",
    "# need to learn how to improve this\n",
    "prompt = f\"Query: {query}\\nContext: {context}\\nAnswer: Include the DOI of the referenced document in your response.\"\n",
    "\n",
    "# Generate response using Cohere's Command model\n",
    "response = co.generate(\n",
    "  model=\"command\", # there are other models to consider within command\n",
    "  prompt=prompt,\n",
    "  max_tokens=150, # allowable length of response\n",
    "  temperature=0.5 # lower for less creativity, more for more creativity\n",
    ")\n",
    "\n",
    "# Print the generated response\n",
    "print(\"\\nGenerated Response:\")\n",
    "print(response.generations[0].text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V2: implementing chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      " I'm not sure how I can help you without any additional information, but if you provide a specific query or discussion point, I'll do my best to assist you! Also, DOI (Document Object Identifier) is a unique identifier assigned to a publication. It's a great way to cite papers and refer to them unequivocally, as it always leads to the exact same document. You can find the DOI on the document's title page, and it's been given for both of the documents you mentioned above. Let me know if I can help you with anything! \n",
      "Generated Response:\n",
      " Based on the DOI you provided, it appears you are asking about the 2017 study that analyzed 115,000 records across seven databases. These databases were Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens. \n",
      "\n",
      "In this study, the authors evaluated the completeness of metadata across these databases and compared the results regarding the quality of information contained within. \n",
      "\n",
      "Please let me know if you would like me to retrieve any other information from this source for you. \n",
      "Generated Response:\n",
      " https://doi.org/10.48550/arXiv.2401.16359\n",
      "Generated Response:\n",
      " The DOI you provided links to a journal article that compares the metadata coverage of various academic databases. It appears to conclude that third-party databases like OpenAlex have more metadata quality and a higher completeness rate than academic search engines like Google Scholar. \n",
      "\n",
      "Is there anything specific you would like to know or understand from the context of this DOI?  I can extract and summarize some of the key points and findings from the article for you if you'd like. \n",
      "Generated Response:\n",
      " The study referred to in the provided context encompassed two primary objectives: \n",
      "\n",
      "1. Analyzing and comparing the amount of metadata and completeness of publications across seven distinct academic databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens)\n",
      "2. Identifying and classifying DOI mistakes found within a Crossref sample, as well as highlighting the publishers responsible for the majority of these errors\n",
      "\n",
      "The study employed a quantitative approach, and the results indicated that third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) exhibited higher metadata quality and completeness rates in comparison to academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar). Regarding the analysis of DOI errors, the\n",
      "Generated Response:\n",
      " The DOI you provided points to a study that compares the amount of metadata and the completeness of research publications in different academic databases and search engines. \n",
      "\n",
      "Is there anything specific you would like to know or inquire about this study?  I can answer any questions you have about the metadata and completeness comparisons performed in this study if you provide more details regarding your query. \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 120\u001b[39m\n",
      "\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m query.lower() == \u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43mrag_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 97\u001b[39m, in \u001b[36mrag_pipeline\u001b[39m\u001b[34m(query)\u001b[39m\n",
      "\u001b[32m     94\u001b[39m prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mQuery: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mContext: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAnswer: Include the DOI of the referenced document in your response.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m     96\u001b[39m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n",
      "\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m response = \u001b[43mco\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcommand\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\n",
      "\u001b[32m    102\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m].text\n",
      "\u001b[32m    104\u001b[39m \u001b[38;5;66;03m# Update chat history\u001b[39;00m\n",
      "\u001b[32m    105\u001b[39m update_chat_history(query, retrieved_docs, response)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/cohere/base_client.py:1568\u001b[39m, in \u001b[36mBaseCohere.generate\u001b[39m\u001b[34m(self, prompt, model, num_generations, max_tokens, truncate, temperature, seed, preset, end_sequences, stop_sequences, k, p, frequency_penalty, presence_penalty, return_likelihoods, raw_prompting, request_options)\u001b[39m\n",
      "\u001b[32m   1433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate\u001b[39m(\n",
      "\u001b[32m   1434\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n",
      "\u001b[32m   1435\u001b[39m     *,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m   1452\u001b[39m     request_options: typing.Optional[RequestOptions] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "\u001b[32m   1453\u001b[39m ) -> Generation:\n",
      "\u001b[32m   1454\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n",
      "\u001b[32m   1455\u001b[39m \u001b[33;03m    <Warning>\u001b[39;00m\n",
      "\u001b[32m   1456\u001b[39m \u001b[33;03m    This API is marked as \"Legacy\" and is no longer maintained. Follow the [migration guide](https://docs.cohere.com/docs/migrating-from-cogenerate-to-cochat) to start using the Chat API.\u001b[39;00m\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m   1566\u001b[39m \u001b[33;03m    )\u001b[39;00m\n",
      "\u001b[32m   1567\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1568\u001b[39m     _response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client_wrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhttpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m   1569\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mv1/generate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1570\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPOST\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1571\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[32m   1572\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1573\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1574\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnum_generations\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_generations\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1575\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1576\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncate\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1577\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1578\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1579\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpreset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1580\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mend_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_sequences\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1581\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop_sequences\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1582\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mk\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1583\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mp\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1584\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1585\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1586\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreturn_likelihoods\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_likelihoods\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1587\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mraw_prompting\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_prompting\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1588\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1589\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1590\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[32m   1591\u001b[39m \u001b[43m            \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent-type\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapplication/json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1592\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1593\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1594\u001b[39m \u001b[43m        \u001b[49m\u001b[43momit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mOMIT\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m   1595\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m   1597\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m200\u001b[39m <= _response.status_code < \u001b[32m300\u001b[39m:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/cohere/core/http_client.py:198\u001b[39m, in \u001b[36mHttpClient.request\u001b[39m\u001b[34m(self, path, method, base_url, params, json, data, content, files, headers, request_options, retries, omit)\u001b[39m\n",
      "\u001b[32m    190\u001b[39m timeout = (\n",
      "\u001b[32m    191\u001b[39m     request_options.get(\u001b[33m\"\u001b[39m\u001b[33mtimeout_in_seconds\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m request_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m request_options.get(\u001b[33m\"\u001b[39m\u001b[33mtimeout_in_seconds\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    193\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.base_timeout()\n",
      "\u001b[32m    194\u001b[39m )\n",
      "\u001b[32m    196\u001b[39m json_body, data_body = get_request_body(json=json, data=data, request_options=request_options, omit=omit)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhttpx_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m.\u001b[49m\u001b[43murljoin\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_url\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjsonable_encoder\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    203\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[32m    204\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    205\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    206\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madditional_headers\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    207\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[32m    208\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    210\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencode_query\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    211\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjsonable_encoder\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    212\u001b[39m \u001b[43m            \u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    213\u001b[39m \u001b[43m                \u001b[49m\u001b[43mremove_omit_from_dict\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    214\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m{\u001b[49m\n",
      "\u001b[32m    215\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    216\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    217\u001b[39m \u001b[43m                            \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43madditional_query_parameters\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[32m    218\u001b[39m \u001b[43m                            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequest_options\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n",
      "\u001b[32m    219\u001b[39m \u001b[43m                            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[32m    220\u001b[39m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    221\u001b[39m \u001b[43m                    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    222\u001b[39m \u001b[43m                    \u001b[49m\u001b[43momit\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    223\u001b[39m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    224\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    225\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_body\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_file_dict_to_httpx_tuples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_omit_from_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremove_none_from_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiles\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43momit\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n",
      "\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    236\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    238\u001b[39m max_retries: \u001b[38;5;28mint\u001b[39m = request_options.get(\u001b[33m\"\u001b[39m\u001b[33mmax_retries\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m request_options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[32m    239\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _should_retry(response=response):\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n",
      "\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n",
      "\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n",
      "\u001b[32m    813\u001b[39m     method=method,\n",
      "\u001b[32m    814\u001b[39m     url=url,\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n",
      "\u001b[32m    824\u001b[39m )\n",
      "\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n",
      "\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n",
      "\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n",
      "\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n",
      "\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n",
      "\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n",
      "\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[32m    977\u001b[39m     hook(request)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n",
      "\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[32m   1011\u001b[39m     )\n",
      "\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n",
      "\u001b[32m   1018\u001b[39m response.request = request\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    237\u001b[39m req = httpcore.Request(\n",
      "\u001b[32m    238\u001b[39m     method=request.method,\n",
      "\u001b[32m    239\u001b[39m     url=httpcore.URL(\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n",
      "\u001b[32m    248\u001b[39m )\n",
      "\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n",
      "\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n",
      "\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n",
      "\u001b[32m    255\u001b[39m     status_code=resp.status,\n",
      "\u001b[32m    256\u001b[39m     headers=resp.headers,\n",
      "\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n",
      "\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n",
      "\u001b[32m    259\u001b[39m )\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n",
      "\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n",
      "\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n",
      "\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n",
      "\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n",
      "\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n",
      "\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n",
      "\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n",
      "\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n",
      "\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n",
      "\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n",
      "\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n",
      "\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n",
      "\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n",
      "\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n",
      "\u001b[32m    100\u001b[39m     (\n",
      "\u001b[32m    101\u001b[39m         http_version,\n",
      "\u001b[32m    102\u001b[39m         status,\n",
      "\u001b[32m    103\u001b[39m         reason_phrase,\n",
      "\u001b[32m    104\u001b[39m         headers,\n",
      "\u001b[32m    105\u001b[39m         trailing_data,\n",
      "\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    107\u001b[39m     trace.return_value = (\n",
      "\u001b[32m    108\u001b[39m         http_version,\n",
      "\u001b[32m    109\u001b[39m         status,\n",
      "\u001b[32m    110\u001b[39m         reason_phrase,\n",
      "\u001b[32m    111\u001b[39m         headers,\n",
      "\u001b[32m    112\u001b[39m     )\n",
      "\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n",
      "\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n",
      "\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n",
      "\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n",
      "\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n",
      "\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n",
      "\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n",
      "\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n",
      "\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n",
      "\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n",
      "\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n",
      "\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n",
      "\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/PhD/Part_3/part_3_cohere/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n",
      "\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n",
      "\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n",
      "\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n",
      "\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n",
      "\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n",
      "\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n",
      "\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n",
      "\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n",
      "\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n",
      "\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Load SciBERT model and tokenizer \n",
    "\"\"\"\n",
    "REMOVE THIS ONCE RUNNING TO GO BACK TO THE CHANGED TOKENIZER AND MODEL ABOVE\n",
    "\"\"\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "model = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "# Initialize chat history\n",
    "chat_history = []\n",
    "\n",
    "# Function to generate embeddings using SciBERT\n",
    "def generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True\n",
    "    )\n",
    "    outputs = model(**inputs)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()\n",
    "    return embeddings\n",
    "\n",
    "# Function to read documents and their DOIs from a file\n",
    "def read_documents_with_doi(file_path: str) -> List[Dict[str, str]]:\n",
    "    documents_with_doi = []\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split(\"||\")\n",
    "            if len(parts) == 2:\n",
    "                doi, document = parts\n",
    "                documents_with_doi.append({\"doi\": doi.strip(), \"text\": document.strip()})\n",
    "    return documents_with_doi\n",
    "\n",
    "# Path to the file containing documents and DOIs\n",
    "file_path = \"data/documents.txt\"\n",
    "\n",
    "# Read documents and DOIs from the file\n",
    "documents_with_doi = read_documents_with_doi(file_path)\n",
    "documents = [doc[\"text\"] for doc in documents_with_doi]\n",
    "\n",
    "\n",
    "# Function to update chat history\n",
    "def update_chat_history(query, retrieved_docs, response):\n",
    "    global chat_histor # declare this as global variable available outside this function\n",
    "    chat_history.append({\n",
    "        \"query\": query,\n",
    "        \"retrieved_docs\": [doc[\"text\"] for doc in retrieved_docs],  # Store only the text of retrieved documents\n",
    "        \"response\": response\n",
    "    })\n",
    "\n",
    "# Function to incorporate history into the next query\n",
    "def get_context_with_history(query) -> str:\n",
    "    global chat_history # also declare here since chat_history is being modified\n",
    "    if not chat_history:\n",
    "        return query\n",
    "    \n",
    "    history_str = \"\\n\".join([\n",
    "        f\"User: {entry['query']}\\n\"\n",
    "        f\"Context: {'; '.join(entry['retrieved_docs'])}\\n\"\n",
    "        f\"Response: {entry['response']}\"\n",
    "        for entry in chat_history\n",
    "    ])\n",
    "    full_context = f\"Chat History:\\n{history_str}\\n\\nCurrent Query: {query}\"\n",
    "    return full_context\n",
    "\n",
    "# Function to truncate chat history\n",
    "def truncate_chat_history(max_length=3):\n",
    "    global chat_history # modifies it so it also must be global\n",
    "    if len(chat_history) > max_length:\n",
    "        chat_history = chat_history[-max_length:]\n",
    "\n",
    "# Function to retrieve top-k documents using cosine similarity\n",
    "def retrieve_documents(query: str, top_k: int = 2) -> List[Dict[str, str]]:\n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    document_embeddings = generate_embeddings(documents)\n",
    "    similarities = [\n",
    "        np.dot(query_embedding, doc_emb) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_emb))\n",
    "        for doc_emb in document_embeddings\n",
    "    ]\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    return [documents_with_doi[i] for i in top_indices]\n",
    "\n",
    "# RAG pipeline function\n",
    "def rag_pipeline(query):\n",
    "    # Incorporate chat history\n",
    "    full_context = get_context_with_history(query)\n",
    "    \n",
    "    # Retrieve documents\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "    \n",
    "    # Prepare context for Cohere's Command model\n",
    "    context = \"\\n\".join([f\"DOI: {doc['doi']}, Text: {doc['text']}\" for doc in retrieved_docs])\n",
    "    prompt = f\"Query: {query}\\nContext: {context}\\nAnswer: Include the DOI of the referenced document in your response.\"\n",
    "    \n",
    "    # Generate response\n",
    "    response = co.generate(\n",
    "        model=\"command\",\n",
    "        prompt=prompt,\n",
    "        max_tokens=150,\n",
    "        temperature=0.5\n",
    "    ).generations[0].text\n",
    "    \n",
    "    # Update chat history\n",
    "    update_chat_history(query, retrieved_docs, response)\n",
    "    \n",
    "    # Truncate history if necessary\n",
    "    truncate_chat_history()\n",
    "    \n",
    "    # Print the response\n",
    "    print(\"Generated Response:\")\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "# Main loop for user interaction\n",
    "while True:\n",
    "    query = input(\"What is your query (or type 'exit' to quit): \")\n",
    "    if query.lower() == \"exit\":\n",
    "        break\n",
    "    rag_pipeline(query)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
