DOI: 10.48550/arXiv.2510.11195
Title: RAG-Pull: コード生成のための RAG システムに対する知覚できない攻撃 RAG-Pull: Imperceptible Attacks on RAG Systems for Code Generation
Abstract: 検索拡張生成 (RAG) は、LLM 応答の信頼性と信頼性を高め、モデルの再トレーニングの必要性を排除することで幻覚を軽減します。これは、LLM のコンテキストに外部データを追加することによって行われます。私たちは、隠れた UTF 文字をクエリまたは外部コード リポジトリに挿入し、検索を悪意のあるコードにリダイレクトして、モデルの安全性の調整を破る新しいクラスのブラック ボックス攻撃である RAG-Pull を開発しました。クエリとコードの摂動だけでは、検索が攻撃者が制御するスニペットにシフトする可能性がある一方、クエリとターゲットの摂動を組み合わせるとほぼ完璧な成功が得られることが観察されています。これらのスニペットが取得されると、リモート コード実行や SQL インジェクションなどの悪用可能な脆弱性が生じます。 RAG-Pull の最小限の摂動は、モデルの安全性の調整を変更し、安全でないコードへの優先順位を高める可能性があるため、LLM に対する新しい種類の攻撃を可能にします。 Retrieval-Augmented Generation (RAG) increases the reliability and trustworthiness of the LLM response and reduces hallucination by eliminating the need for model retraining. It does so by adding external data into the LLM's context. We develop a new class of black-box attack, RAG-Pull, that inserts hidden UTF characters into queries or external code repositories, redirecting retrieval toward malicious code, thereby breaking the models' safety alignment. We observe that query and code perturbations alone can shift retrieval toward attacker-controlled snippets, while combined query-and-target perturbations achieve near-perfect success. Once retrieved, these snippets introduce exploitable vulnerabilities such as remote code execution and SQL injection. RAG-Pull's minimal perturbations can alter the model's safety alignment and increase preference towards unsafe code, therefore opening up a new class of attacks on LLMs.