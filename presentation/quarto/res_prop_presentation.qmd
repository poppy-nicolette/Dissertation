---
title: "Can RAG use bibliometric metadata?"
subtitle: "Research Proposal, PhD Program in Information Science"
author: "Poppy Riddle"
format: 
  revealjs:
    theme: [default, www/custom.scss]
    progress: true
    slide-number: true
    preview-links: auto
    #css: www/styles.css
    embed-resources: false
editor: visual
backgroundcolor: "#660099"
background-transition: fade
project:
  execute-dir: project
---

# hello there!

<!--# https://htmlcolorcodes.com/color-chart/web-safe-color-chart/ -->

<!--# more helpful stuff: https://emilhvitfeldt.com/blog -->

<!--# plugins should you want them: https://github.com/hakimel/reveal.js/wiki/Plugins,-Tools-and-Hardware -->

::: notes
Speaker notes go here. Thank goodness, I need me some notes.
:::

## Presentation overview

-   Introduction
-   Part 1: Random sample
-   Part 2: Comparing sources
-   Part 3: RAG in action
-   Concluding thoughts and questions

# Introduction

## Background {.scrollable}
::: notes
Speaker notes go here. Thank goodness, I need me some notes.
:::

abstracts as summaries, though limited in depth

many abstracts, might provide a more holistic answer

systematic review well known process

abstracts may be available when the paper is [paywalled]{.yellow}

summer of 2024 experimented using abstracts as a source of RAG applications

cite work by Zain and Gowri, published proceedings,

3 areas should be explored:

1.  metadata characterization
2.  metadata source
3.  evaluate characteristic impact through case study

### Gaps addressed

-   evidence for bibliometric metadata directly as a document source in RAG

-   Lack of characterization of title and abstract metadata

-   Metadata source: compares two sources for effects of aggregation and transformation

## Framework

![](images/framework.svg){fig-align="center"}

::: notes
past studies have examined coverage, completeness, accuracy, but not content.
:::

# Part 1: Characterize metadata {.scrollable}

## Characterize metadata from Crossref {.scrollable}

What are the characteristics of textual metadata from a registration agency, such as Crossref?

![](images/part_1.svg){fig-align="center"}

## Example: {.scrollable}

Abstract metadata from <https://api.crossref.org/works/10.1103/physrevd.110.036015>

![](images/abstract.png){fig-align="center"}

Or, another example from <https://api.crossref.org/works/10.33024/hjk.v15i1.3791>:

![](images/abstract2.png){fig-align="center"}

Or one that contains geometric shapes, from <https://api.crossref.org/works/10.1108/eemcs-04-2022-0108>

![](images/abstract3.png){fig-align="center"}

## 

[Past studies have examined metadata for presence and accuracy, but have not examined content.]{.yellow .absolute left="20%" top="30%" right="20%"}

# Part 2: Compare data sources {.scrollable}

## Compare the same metadata from Crossref and OpenAlex {.scrollable}

![](images/part_2.svg){width="587"}

##  {.scrollable}

::::: columns
::: {.column width="50%"}
-   Same DOIs from Part 1

-   Shared corpus

-   Compare for changes

    -   Missing

    -   Incorrect

    -   Wrong element

    -   Different

    -   Representation
:::

::: {.column width="50%"}
-   Subset analysis

-   Code into patterns

-   Analyze full dataset with patterns
:::
:::::

This should tell us what has changed, by how much, and provide some insight into which data source, Crossref or OpenAlex, might be better for using in a RAG application.

# Part 3: Evaluate some characteristics in action

## What happens when we put some of this metadata in a RAG application? {.scrollable}

![](images/part_3_framework.svg){fig-align="center"}

##  {.scrollable}

::::: columns
::: {.column width="40%"}
Three datasets of 45 works containing the DOI, the title, and the abstract
:::

::: {.column width="60%"}
A clean dataset

With JATS tags prepending and appending the title and abstract.

A translated version in Japanese prepending the English
:::
:::::

-   RAG pipeline with embedding, reranking, and generation models

-   Analyze results from the retriever:

    -   precision

    -   recall

    -   F1

    -   accuracy

    -   balanced accuracy

    -   document scores

-   Analyze results from the generator model:

    -   faithfulness

    -   answer relevance

# Summary

## Purpose:

[[Bibliometric metadata]{.yellow} can be used as a [source of external knowledge]{.yellow} in RAG, however, it is unknown what kind of errors, where these are found, and how they may impact RAG outcomes.]{.absolute left="20%" top="30%" right="20%"}

## Benefits

[This study should help us understand how [characteristic]{.yellow} from Part 1 and how a [data source]{.yellow} from Part 2, may [impact retrieval and generation]{.yellow} in a RAG application.]{.absolute left="20%" top="30%" right="20%"}

##  

[Additionally, it also provides a handy set of [patterns]{.yellow} that can be used to [identify]{.yellow} such [problems]{.yellow} before using the metadata, or for the purpose of cleaning.]{.absolute left="20%" top="30%" right="20%"}

##  

[It [characterizes metadata]{.yellow} from Crossref and OpenAlex in a way that has not been done before.]{.absolute left="20%" top="30%" right="20%"}

##  

[Introduces the [Crossref XML API]{.yellow} as part of analyzing metadata completeness and accuracy.]{.absolute left="20%" top="30%" right="20%"}

##  

[It contributes to research on the [effect of perturbations]{.yellow} in embedding and generation models.]{.absolute left="20%" top="30%" right="20%"}

##  

[Raised my own skills in [Python]{.yellow}, knowledge of how [search and retrieval]{.yellow} is changing, how [LLMs]{.yellow} works and their limitations.]{.absolute left="20%" top="30%" right="20%"}

# Thank you!

And now, its time for questions.

# Sneak peek
Who wants to see some results?

## Ok, here's what actually happened:

1.  Part 1 was coded more than once to get consistency.

2.  Part 2 was easy, yet important to [confirm prior research]{.yellow}.

3.  Licensing information is a real [train wreck]{.magenta} and would require more time and expertise.

4.  I created two RAG applications:

    -   One uses [closed models]{.yellow} from Cohere - [I got a research grant!]{.blue}

    -   One uses [open models]{.yellow} for embedding: SciBERT and Specter.

5.  I reran the original test with different configurations of open and closed models:
    with [multilingual capability]{.yellow}, 
    with [only English]{.yellow}, and
    compared [word embedding]{.yellow} to [sentence embedding]{.yellow}.

## Results

Part 1 summary results

Part 2 summary results

Part 3 summary results