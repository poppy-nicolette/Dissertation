DOI: 10.48550/arXiv.2402.11782
Title: 言語モデルはどのような証拠を説得しますか？ What Evidence Do Language Models Find Convincing?
Abstract: 検索された言語モデルは、「がんに関連するアスパルテーム」などの主観的で論争的で矛盾するクエリをますます任されています。これらのあいまいなクエリを解決するには、さまざまなウェブサイトを検索し、「もしあれば、この証拠のうち、説得力があると思いますか？」と考える必要があります。この作業では、LLMSがこの質問にどのように答えるかを研究します。特に、競合するqaを構築します。これは、異なる事実（定量的結果など）、引数スタイル（たとえば、権限への訴え）、および回答（はいまたはいいえ）を含む一連の実世界の証拠文書と、物議を醸す質問をペアにするデータセットです。このデータセットを使用して、感度と反事実的分析を実行して、どのテキストがLLM予測に最も影響するかを調査します。全体として、現在のモデルは、Webサイトとクエリへの関連性に大きく依存していることがわかりますが、テキストに科学的参照が含まれているか、ニュートラルなトーンで書かれているかなど、人間が重要であると判断する文体的な特徴をほとんど無視しています。まとめると、これらの結果は、Rag Corpusの品質の重要性（たとえば、誤った情報をフィルタリングする必要性）、さらにはLLMが人間の判断とより適合するように訓練されている方法の変化を強調しています。 Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as "is aspartame linked to cancer". To resolve these ambiguous queries, one must search through a large range of websites and consider "which, if any, of this evidence do I find convincing?". In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone. Taken together, these results highlight the importance of RAG corpus quality (e.g., the need to filter misinformation), and possibly even a shift in how LLMs are trained to better align with human judgements.
