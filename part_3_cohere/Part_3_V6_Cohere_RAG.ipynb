{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere RAG with dense retriever and ReRank model\n",
    "- references: https://docs.cohere.com/v2/docs/rag-complete-example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import cohere\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import sys\n",
    "from colorama import Fore, Style, Back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mall is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "# load secret from local .env file\n",
    "def get_key():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    _key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "    #verify if it worked\n",
    "    if _key is not None:\n",
    "        print(Fore.GREEN + \"all is good, beautiful!\")\n",
    "        return _key\n",
    "    else:\n",
    "        print(Fore.LIGHTRED_EX + \"API Key is missing\")\n",
    "\n",
    "# initilize client\n",
    "co = cohere.ClientV2(get_key())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/s11192-022-04367-w\\n Title: Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: Abstract We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n']\n",
      "length of reranked_documents: 2\n",
      "Summary: Two studies examined citations.\n",
      "\n",
      "DOI: 10.1007/s11192-022-04367-w\n",
      "Identifying and correcting invalid citations due to DOI errors in Crossref data\n",
      "\n",
      "DOI: 10.1162/qss_a_00112\n",
      "Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "\n",
      "Is there anything else I can help you with, my lady?\n",
      "\u001b[96m------\n",
      "Reranked documents:\n",
      "DOI: 10.1007/s11192-022-04367-w\n",
      " Title: Identifying and correcting invalid citations due to DOI errors in Crossref data\n",
      "Abstract: This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\n",
      "\n",
      "DOI: 10.1162/qss_a_00112\n",
      " Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n",
      "Abstract: Abstract We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\n",
      "\n",
      "\u001b[93m\n",
      "CITATIONS:\n",
      "source text: Two studies examined citations.,\n",
      "source: DOI: 10.1007/s11192-022-04367-w\n",
      "------\n",
      "source text: 10.1007/s11192-022-04367-w,\n",
      "source: DOI: 10.1007/s11192-022-04367-w\n",
      "------\n",
      "source text: Identifying and correcting invalid citations due to DOI errors in Crossref data,\n",
      "source: DOI: 10.1007/s11192-022-04367-w\n",
      "------\n",
      "source text: 10.1162/qss_a_00112,\n",
      "source: DOI: 10.1162/qss_a_00112\n",
      "------\n",
      "source text: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic,\n",
      "source: DOI: 10.1162/qss_a_00112\n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# load documents\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0]\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append(f\"{doi} {text}\\n\")\n",
    "    return documents_with_doi\n",
    "\n",
    "# initialize empty search query\n",
    "search_queries = []\n",
    "# Embed the documents\n",
    "def document_embed(documents:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the documents from a list provided from read_documents_with_doi()\n",
    "    \"\"\"\n",
    "    doc_emb = co.embed(\n",
    "        model=\"embed-v4.0\",\n",
    "        input_type=\"search_document\",\n",
    "        texts=[doc for doc in documents],\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return doc_emb\n",
    "\n",
    "# Embed the search query\n",
    "def query_embed(search_queries:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the query from a list provided in search_queries variable\n",
    "    \"\"\"\n",
    "    query_emb = co.embed(\n",
    "        model=\"embed-v4.0\",\n",
    "        input_type=\"search_query\",\n",
    "        texts=search_queries,\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return query_emb\n",
    "\n",
    "# retrieve top_k and compute similarity using dot product\n",
    "def retrieve_top_k(top_k, query_embedded, documents_embedded, documents)->List[str]:\n",
    "    \"\"\"\n",
    "    returns the top_k documents based on dot product similarity\n",
    "    \"\"\"\n",
    "\n",
    "    scores = np.dot(query_embedded, np.transpose(documents_embedded))[0]#ordered list!\n",
    "    # takes top scores, and returns sorted list and returns indices sliced by top_k\n",
    "    max_idx = np.argsort(-scores)[:top_k]\n",
    "    # returns documents by index\n",
    "    retrieved_docs = [documents[item] for item in max_idx]\n",
    "    # returns a list of documents\n",
    "    return retrieved_docs\n",
    "\n",
    "def rerank_documents(retrieved_documents,search_queries,threshold,top_k)->List[str]:\n",
    "    \"\"\"\n",
    "    takes retrieved_documents as input along with search_queries and runs them through the \n",
    "    rerank model from cohere for semantic similarity. \n",
    "\n",
    "    top_n = top_k\n",
    "    Limits those returned by a threshold score. this is to reduce those that are irrelevant.\n",
    "    \"\"\"\n",
    "    # Rerank the documents\n",
    "    results = co.rerank(\n",
    "        model=\"rerank-v3.5\",\n",
    "        query=search_queries[0],\n",
    "        documents=[doc for doc in retrieved_documents],\n",
    "        top_n=top_k,\n",
    "        max_tokens_per_doc=4096,# defaults to 4096\n",
    "    )\n",
    "\n",
    "    # Display the reranking results\n",
    "    #for idx, result in enumerate(results.results):\n",
    "    #    print(f\"Rank: {idx+1}\")\n",
    "    #    print(f\"Score: {result.relevance_score}\")\n",
    "    #    print(f\"Document: {retrieved_documents[result.index]}\\n\")\n",
    "\n",
    "    #returns only those over threshold\n",
    "    reranked_docs = [\n",
    "        retrieved_documents[result.index] for result in results.results if result.relevance_score >=threshold\n",
    "    ]\n",
    "    reranked_with_score = [(result.relevance_score, retrieved_documents[result.index].split(\"\\n\")[0].strip(\"DOI: \")) for result in results.results if result.relevance_score >=threshold]\n",
    "\n",
    "    print(f\"reranked_documents: {reranked_docs}\")\n",
    "    print(f\"length of reranked_documents: {len(reranked_docs)}\")\n",
    "\n",
    "    return reranked_docs, reranked_with_score\n",
    "\n",
    "def cohere_rag_pipeline(directory_path,search_queries,top_k,threshold):\n",
    "\n",
    "    # retrieve documents from directory\n",
    "    documents = read_documents_with_doi(directory_path)\n",
    "    print(f\"Length of documents: {len(documents)}\")\n",
    "    # embed the documents\n",
    "    documents_embedded = document_embed(documents)\n",
    "\n",
    "    #embed the query:\n",
    "    query_embedded = query_embed(search_queries)\n",
    "\n",
    "    # retrieve the top_k documents\n",
    "    retrieved_documents = retrieve_top_k(top_k, query_embedded, documents_embedded, documents)\n",
    "\n",
    "    # rerank the documents using the Rerank model from Cohere\n",
    "    reranked_documents, reranked_DOIs_with_score = rerank_documents(retrieved_documents,search_queries,threshold,top_k)\n",
    "    # set system instructions\n",
    "    instructions = \"\"\"\n",
    "                    You are an academic research assistant.\n",
    "                    You must include the DOI in your response.\n",
    "                    If there is no content provided, ask for a different question.\n",
    "                    Please structure your response like this:\n",
    "                    Summary: summary statement here. \n",
    "                    DOI: summary of the text associated with this DOI.\n",
    "                    Address me as, 'my lady'.\n",
    "                    \"\"\"\n",
    "    # create messages to model\n",
    "    messages = [{\"role\":\"user\",\n",
    "                \"content\": search_queries[0]},\n",
    "                {\"role\":\"system\",\n",
    "                \"content\":instructions}]\n",
    "\n",
    "    # Generate the response\n",
    "    resp = co.chat(\n",
    "        model=\"command-a-03-2025\",\n",
    "        messages=messages,\n",
    "        documents=reranked_documents,\n",
    "    )\n",
    "\n",
    "    return resp, reranked_documents, reranked_DOIs_with_score\n",
    "\n",
    "# ****** Pipeline ********\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.1\n",
    "\n",
    "response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "# Display the response\n",
    "print(response.message.content[0].text)\n",
    "print(Fore.LIGHTCYAN_EX + f\"------\\nReranked documents:\")\n",
    "for doc in reranked_documents_end:\n",
    "    print(doc)\n",
    "\n",
    "# Display the citations and source documents\n",
    "if response.message.citations:\n",
    "    print(Fore.LIGHTYELLOW_EX + \"\\nCITATIONS:\")\n",
    "    for citation in response.message.citations:\n",
    "        print(f\"source text: {citation.text},\\nsource: {citation.sources[0].document.get('content').split(\"\\n\")[0]}\\n------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.6519982, '10.1162/qss_a_00286'),\n",
       " (0.3312825, '10.48550/arXiv.2303.17661'),\n",
       " (0.32875618, '10.31222/osf.io/smxe5'),\n",
       " (0.19046825, '10.31274/b8136f97.ccc3dae4'),\n",
       " (0.10833058, '10.1002/leap.1411')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reranked_DOIs_with_score_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# Analysis\n",
    "Precision, recall, accuracy, F1 scores and faithfulness\n",
    "## Precision, recall, F1 score\n",
    "### references\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "- https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "import openpyxl\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Documents score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query, Precision, Recall, F1-Score, Accuracy, Balanced accuracy, Faithfulness score, Documents score, Response]\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial dataframe to capture results from each query and results\n",
    "#ONLY DO THIS AT THE BEGINNING OF THE ANALYSIS PROCEDURE, OTHERWISE, IT WILL ERASE THE PREVIOUS RESULTS!!\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Query','Precision','Recall','F1-Score','Accuracy', 'Balanced accuracy', 'Faithfulness score', 'Documents score', 'Response'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved DOIs: ['10.1007/s11192-022-04367-w', '10.1162/qss_a_00112']\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "\"\"\"\n",
    "change this to read in an excel sheet of queries and ground_truth dois.\n",
    "Then it should be isolated as a function.\n",
    "Run the function to iterature through the list.\n",
    "\"\"\"\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# initiates the variable\n",
    "ground_truth = []\n",
    "\n",
    "def evaluate_retrieval(retrieved_dois, ground_truth):\n",
    "    corpus_doi_list = []\n",
    "    #corpus_list is a global variable in rag_pipeline()\n",
    "    for each in range(len(documents_with_doi)):\n",
    "        a = documents_with_doi[each].split(\"\\n\")[0].strip(\"DOI: \")\n",
    "        corpus_doi_list.append(a)\n",
    "    print(len(corpus_doi_list))\n",
    "\n",
    "    def compare_lists(list1, list2, list3):\n",
    "        for val in list1:\n",
    "            if val in list2:\n",
    "                list3.append(1)\n",
    "            else:\n",
    "                list3.append(0)\n",
    "\n",
    "    #set y_true so that len(y_true)==len(corpus_doi_list)\n",
    "    y_true = []\n",
    "    compare_lists(corpus_doi_list,ground_truth,y_true)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = []\n",
    "    compare_lists(corpus_doi_list,retrieved_dois,y_pred)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # calculate metrics - could also use sklearn.metrics functions such as precision_score, but this is easier to read\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    faithfulness_score = 0\n",
    "    for each in retrieved_dois:\n",
    "        if each in response.message.content[0].text:\n",
    "            faithfulness_score+=1\n",
    "        else:\n",
    "            faithfulness_score+=0\n",
    "\n",
    "    return {\n",
    "        'Query':f\"{search_queries[0]}\",\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\":accuracy,\n",
    "        \"Balanced accuracy\":balanced_accuracy,\n",
    "        \"Faithfulness score\":faithfulness_score,\n",
    "        \"Documents score\":str(reranked_DOIs_with_score_end),#converted to string because pandas was only taking the first tuple not the entire list - use ast.literal_eval() to unpack later.\n",
    "        \"Response\":response.message.content[0].text\n",
    "    }\n",
    "\n",
    "def print_results()->Dict:\n",
    "    \"\"\"\n",
    "    Prints a nicely ordered set of results from evalaute_retrieval()\n",
    "    \"\"\"\n",
    "    results = evaluate_retrieval(retrieved_dois, ground_truth)\n",
    "    print(f\"For query: {results['Query']}:\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"F1-Score: {results['F1-Score']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    print(f\"Balanced accuracy: {results['Balanced accuracy']:.3f}\")\n",
    "    print(f\"Faithfulness score: {results['Faithfulness score']}\")\n",
    "    print(Fore.LIGHTMAGENTA_EX + f\"Documents scores: {results['Documents score']}\")\n",
    "    return results\n",
    "\n",
    "#for debugging\n",
    "\n",
    "#print_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the test from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved DOIs: ['10.1007/978-3-031-88708-6_3', '10.48550/arXiv.2312.10997', '10.48550/arXiv.2505.18247', '10.48550/arXiv.2406.13213', '10.48550/arXiv.2404.13948']\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/978-3-031-88708-6_3\\n Title: 関連性はレトリバーからジェネレーターにぼろきれに伝播されますか？ Is Relevance Propagated from Retriever to Generator in RAG?\\nAbstract: 検索拡張生成（RAG）は、通常、コレクションから取得された一連のドキュメントの形で、プロンプトの大規模な言語モデル（LLM）への一連のドキュメントの形で、質問の回答などの下流タスクのパフォーマンスを潜在的に改善するためのフレームワークです。一連のトップランクのドキュメントの関連性を最大化するという標準検索タスクの目的とは異なり、RAGシステムの目的は、ドキュメントのユーティリティがLLMプロンプトの追加コンテキスト情報の一部としてそれを含めることがダウンストリームタスクを改善するかどうかを示します。既存の研究では、知識集約型の言語タスク（KILT）のRAGコンテキストの関連性の役割を調査します。対照的に、私たちの仕事では、関連性は、情報を求めるタスクのクエリとドキュメントの間の局所的な重複の関連性に対応しています。具体的には、IRテストコレクションを利用して、局所的に関連するドキュメントで構成されるRAGコンテキストが下流のパフォーマンスの改善につながるかどうかを経験的に調査します。私たちの実験は、次の発見につながります。（a）関連性と有用性の間には小さな正の相関があります。 （b）この相関は、コンテキストサイズの増加とともに減少します（k-shotのkの値が高い）。 （c）より効果的な検索モデルは、一般に、下流のラグパフォーマンスの向上につながります。 Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\\n', \"DOI: 10.48550/arXiv.2312.10997\\n Title: 大規模な言語モデルの検索された生成：調査 Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: 大規模な言語モデル（LLMS）は印象的な能力を紹介しますが、幻覚、時代遅れの知識、非透明な、追跡不可能な推論プロセスなどの課題に遭遇します。検索された生成（RAG）は、外部データベースから知識を組み込むことにより、有望なソリューションとして浮上しています。これにより、特に知識集約型タスクの生成の正確性と信頼性が向上し、ドメイン固有の情報の継続的な知識の更新と統合が可能になります。 RAGは、外部データベースの広大で動的なリポジトリとLLMSの本質的な知識を相乗的に統合します。この包括的なレビューペーパーでは、素朴なぼろきれ、高度なぼろ、モジュラーラグを含むRAGパラダイムの進行に関する詳細な調査を提供します。検索、生成、増強技術を含む、RAGフレームワークの三者基盤を細心の注意を払って精査します。この論文は、これらの各重要なコンポーネントに組み込まれた最先端のテクノロジーを強調し、RAGシステムの進歩を深く理解することを提供します。さらに、このペーパーでは、最新の評価フレームワークとベンチマークを紹介します。最後に、この記事は現在直面している課題を描き、研究開発の将来の道を指摘しています。 Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", \"DOI: 10.48550/arXiv.2505.18247\\n Title: メタゲンブレンドラグ：特殊なドメインの質問を解決するためのゼロショット精度のロックを解除する MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: 検索された生成（RAG）は、ドメイン固有のエンタープライズデータセットとの闘いであり、しばしばファイアウォールの背後に隔離され、トレーニング前にLLMSによって見えない複雑で特殊な用語が豊富です。医学、ネットワーキング、または法律などのドメイン間のセマンティックな変動は、Ragのコンテキストの精度を妨げますが、微調整ソリューションはコストがかかり、遅く、新しいデータが出現するにつれて一般化が欠けています。微調整せずにレトリーバーでゼロショット精度を達成することは、依然として重要な課題です。メタデータの生成パイプラインとハイブリッドクエリインデックスを介してセマンティックレトリバーを強化する新しいエンタープライズ検索アプローチである「メタゲンブレンドラグ」を紹介します。重要な概念、トピック、頭字語を活用することにより、メタデータが豊富なセマンティックインデックスを作成し、ハイブリッドクエリをブーストし、微調整せずに堅牢でスケーラブルなパフォーマンスを提供します。 Biomedical PubMedqaデータセットでは、Metagenブレンドラグは82％の回収精度と77％のRAG精度を達成し、以前のゼロショットラグベンチマークをすべて上回り、そのデータセットの微調整されたモデルに匹敵し、SquadやNQのようなデータセットでも優れています。このアプローチは、特殊なドメイン全体で比類のない一般化を備えたセマンティックレトリバーを構築するための新しいアプローチを使用して、エンタープライズ検索を再定義します。 Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\", 'DOI: 10.48550/arXiv.2406.13213\\n Title: マルチメタラグ：LLM抽出メタデータを使用したデータベースフィルタリングを使用したマルチホップクエリのRAGの改善 Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: 検索された生成（RAG）により、外部の知識ソースから関連情報の取得が可能になり、大規模な言語モデル（LLM）が以前に見えなかったドキュメントコレクションのクエリに答えることができます。ただし、従来のRAGアプリケーションは、マルチホップの質問への回答においてパフォーマンスが低いことが実証されました。 LLM抽出メタデータを使用したデータベースフィルタリングを使用して、質問に関連するさまざまなソースからの関連ドキュメントのRAG選択を改善するMulti-Meta-Ragと呼ばれる新しい方法を導入します。データベースフィルタリングは、特定のドメインと形式からの一連の質問に固有のものですが、Multi-Meta-RagがMultihop-Ragベンチマークの結果を大幅に改善することがわかりました。このコードは、https：//github.com/mxpoliakov/multi-meta-ragで入手できます。 The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', \"DOI: 10.48550/arXiv.2404.13948\\n Title: ぼろきれの背中を壊したYPO ypos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\\nAbstract: 最近の大規模な言語モデル（LLMS）の堅牢性は、さまざまなドメインや現実世界のアプリケーションで適用性が拡大するにつれてますます重要になっています。検索された生成（RAG）は、LLMの限界に対処するための有望なソリューションですが、RAGの堅牢性に関する既存の研究は、しばしば、RAGコンポーネント間の相互接続された関係またはマイナーなテキストエラーなどの実際のデータベースで一般的な潜在的な脅威を見落としています。この作業では、RAGの堅牢性を評価する際に、2つの未掘りの側面を調査します。1）低レベルの摂動を通じて騒々しい文書に対する脆弱性と2）Ragの堅牢性の全体的な評価。さらに、これらの側面をターゲットにする新しい攻撃法であるRag（\\\\ textit {garag}）に対する遺伝的攻撃を紹介します。具体的には、GARAGは各コンポーネント内の脆弱性を明らかにし、騒々しいドキュメントに対してシステム全体の機能をテストするように設計されています。 \\\\ textIT {garag}を標準のQAデータセットに適用し、多様なレトリバーとLLMを組み込んで、ragの堅牢性を検証します。実験結果は、Garagが一貫して高い攻撃の成功率を達成することを示しています。また、各コンポーネントのパフォーマンスとその相乗効果を大幅に破壊し、現実世界のぼろきれシステムを混乱させる際にマイナーなテキストの不正確さがもたらす実質的なリスクを強調しています。 The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\\\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "\u001b[93mSummary: Retrieval-Augmented Generation (RAG) is a framework that integrates external knowledge into Large Language Models (LLMs) to enhance their performance on downstream tasks, such as question answering. Unlike standard retrieval tasks, RAG focuses on maximising the utility of retrieved documents rather than just their relevance. RAG has been explored in various contexts, including knowledge-intensive tasks, domain-specific challenges, and multi-hop queries. It also faces challenges like robustness to noisy documents and semantic variability across domains.\n",
      "\n",
      "DOI: 10.1007/978-3-031-88708-6_3 - This paper investigates the role of relevance in RAG for knowledge-intensive language tasks (KILT), finding a small positive correlation between relevance and utility, which decreases with larger context sizes.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997 - This survey examines the progression of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG, highlighting its role in enhancing accuracy and credibility in knowledge-intensive tasks.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247 - Introduces MetaGen Blended RAG, a method to improve RAG's performance in specialized domains by enhancing semantic retrievers with metadata generation, achieving high zero-shot precision.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213 - Presents Multi-Meta-RAG, a technique to improve RAG for multi-hop queries using database filtering with LLM-extracted metadata, significantly improving benchmark results.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.13948 - Explores the robustness of RAG, introducing GARAG, a genetic attack method to test vulnerabilities in RAG systems, particularly against noisy documents.\n",
      "\n",
      "My lady, if you have further questions or need more details, please let me know.\n",
      "45\n",
      "For query: tell me about how RAG works.:\n",
      "Precision: 0.800\n",
      "Recall: 0.800\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.887\n",
      "Faithfulness score: 5\n",
      "\u001b[95mDocuments scores: [(0.6628188, '10.1007/978-3-031-88708-6_3'), (0.58408195, '10.48550/arXiv.2312.10997'), (0.5106191, '10.48550/arXiv.2505.18247'), (0.44985238, '10.48550/arXiv.2406.13213'), (0.34508374, '10.48550/arXiv.2404.13948')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Documents score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>tell me about how RAG works.</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.4625</td>\n",
       "      <td>0</td>\n",
       "      <td>[(0.6628188, '10.1007/978-3-031-88708-6_3'), (...</td>\n",
       "      <td>Summary: Retrieval-Augmented Generation (RAG) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>tell me about how RAG works.</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>5</td>\n",
       "      <td>[(0.6628188, '10.1007/978-3-031-88708-6_3'), (...</td>\n",
       "      <td>Summary: Retrieval-Augmented Generation (RAG) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>tell me about how RAG works.</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>5</td>\n",
       "      <td>[(0.6628188, '10.1007/978-3-031-88708-6_3'), (...</td>\n",
       "      <td>Summary: Retrieval-Augmented Generation (RAG) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>tell me about how RAG works.</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>5</td>\n",
       "      <td>[(0.6628188, '10.1007/978-3-031-88708-6_3'), (...</td>\n",
       "      <td>Summary: Retrieval-Augmented Generation (RAG) ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>tell me about how RAG works.</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>5</td>\n",
       "      <td>[(0.6628188, '10.1007/978-3-031-88708-6_3'), (...</td>\n",
       "      <td>Summary: Retrieval-Augmented Generation (RAG) ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Query  ...                                           Response\n",
       "65  tell me about how RAG works.  ...  Summary: Retrieval-Augmented Generation (RAG) ...\n",
       "66  tell me about how RAG works.  ...  Summary: Retrieval-Augmented Generation (RAG) ...\n",
       "67  tell me about how RAG works.  ...  Summary: Retrieval-Augmented Generation (RAG) ...\n",
       "68  tell me about how RAG works.  ...  Summary: Retrieval-Augmented Generation (RAG) ...\n",
       "69  tell me about how RAG works.  ...  Summary: Retrieval-Augmented Generation (RAG) ...\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the test from here\n",
    "\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# Ground truth relevant documents (DOIs) for each query\n",
    "ground_truth = [\"10.1007/978-3-031-88708-6_3\",\"10.1609/aaai.v38i16.29728\",\"10.48550/arXiv.2312.10997\",\"10.48550/arXiv.2406.13213\",\"10.48550/arXiv.2505.18247\"]\n",
    "\n",
    "#***** Begin chat session *****\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_multi_lang\"\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.10\n",
    "\n",
    "response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "# Display the response\n",
    "print(Fore.LIGHTYELLOW_EX + f\"{response.message.content[0].text}\")\n",
    "\n",
    "new_result = print_results()\n",
    "# add the new result to the df\n",
    "results_df.loc[len(results_df)] = new_result\n",
    "\n",
    "#save the queries and responses to separate dataframe to be manually annontated\n",
    "answer_relevance_df = results_df[['Query','Response']].copy(deep=True)\n",
    "\n",
    "# save out answer_relevance_df\n",
    "filename=\"analysis/dense_answer_relevance_results.xlsx\"\n",
    "answer_relevance_df.to_excel(filename)\n",
    "\n",
    "filename = \"analysis/dense_analysis_results.xlsx\"\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "results_df.to_excel(filename)\n",
    "results_df.tail(5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
