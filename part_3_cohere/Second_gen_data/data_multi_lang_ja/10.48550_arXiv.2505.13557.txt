DOI: 10.48550/arXiv.2505.13557
Title: AMAQA: RAG システム用のメタデータベースの QA データセット AMAQA: A Metadata-based QA Dataset for RAG Systems
Abstract: 検索拡張生成 (RAG) システムは、質問応答 (QA) タスクで広く使用されていますが、現在のベンチマークにはメタデータの統合が不足しており、テキスト データと外部情報の両方を必要とするシナリオでの評価が妨げられています。これに対処するために、テキストとメタデータを組み合わせたタスクを評価するように設計された新しいオープンアクセス QA データセットである AMAQA を紹介します。メタデータの統合は、サイバーセキュリティやインテリジェンスなど、関連情報へのタイムリーなアクセスが重要な、大量のデータの迅速な分析が必要な分野で特に重要です。 AMAQA には、26 のパブリック Telegram グループから収集された約 110 万件の英語メッセージが含まれており、タイムスタンプ、トピック、感情の調子、毒性指標などのメタデータが充実しており、特定の基準に基づいてドキュメントをフィルタリングすることで、正確で文脈に応じたクエリを実行できます。また、450 の高品質 QA ペアも含まれており、メタデータ主導の QA および RAG システムの研究を進めるための貴重なリソースとなります。私たちの知る限り、AMAQA は、メッセージで扱われるトピックなどのメタデータとラベルを組み込んだ最初のシングルホップ QA ベンチマークです。私たちはベンチマークで広範なテストを実施し、将来の研究のための新しい基準を確立します。メタデータを活用すると精度が 0.12 から 0.61 に向上し、構造化コンテキストの価値が強調されることがわかりました。これに基づいて、提供されたコンテキストを反復処理し、ノイズの多いドキュメントで強化することで LLM 入力を洗練するためのいくつかの戦略を検討し、最良のベースラインよりもさらに 3 ポイントの向上を達成し、単純なメタデータ フィルタリングよりも 14 ポイントの改善を達成しました。 Retrieval-augmented generation (RAG) systems are widely used in question-answering (QA) tasks, but current benchmarks lack metadata integration, hindering evaluation in scenarios requiring both textual data and external information. To address this, we present AMAQA, a new open-access QA dataset designed to evaluate tasks combining text and metadata. The integration of metadata is especially important in fields that require rapid analysis of large volumes of data, such as cybersecurity and intelligence, where timely access to relevant information is critical. AMAQA includes about 1.1 million English messages collected from 26 public Telegram groups, enriched with metadata such as timestamps, topics, emotional tones, and toxicity indicators, which enable precise and contextualized queries by filtering documents based on specific criteria. It also includes 450 high-quality QA pairs, making it a valuable resource for advancing research on metadata-driven QA and RAG systems. To the best of our knowledge, AMAQA is the first single-hop QA benchmark to incorporate metadata and labels such as topics covered in the messages. We conduct extensive tests on the benchmark, establishing a new standard for future research. We show that leveraging metadata boosts accuracy from 0.12 to 0.61, highlighting the value of structured context. Building on this, we explore several strategies to refine the LLM input by iterating over provided context and enriching it with noisy documents, achieving a further 3-point gain over the best baseline and a 14-point improvement over simple metadata filtering. 