{"id":0,"text":"DOI: 10.48550/arXiv.2406.15154\n Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\n Abstract: This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\n"}
{"id":1,"text":"DOI: 10.48550/arXiv.2402.01788\n Title: LitLLM: A Toolkit for Scientific Literature Review\n Abstract: Conducting literature reviews for scientific papers is essential for understanding research, its limitations, and building on existing work. It is a tedious task which makes an automatic literature review generator appealing. Unfortunately, many existing works that generate such reviews using Large Language Models (LLMs) have significant limitations. They tend to hallucinate-generate non-actual information-and ignore the latest research they have not been trained on. To address these limitations, we propose a toolkit that operates on Retrieval Augmented Generation (RAG) principles, specialized prompting and instructing techniques with the help of LLMs. Our system first initiates a web search to retrieve relevant papers by summarizing user-provided abstracts into keywords using an off-the-shelf LLM. Authors can enhance the search by supplementing it with relevant papers or keywords, contributing to a tailored retrieval process. Second, the system re-ranks the retrieved papers based on the user-provided abstract. Finally, the related work section is generated based on the re-ranked results and the abstract. There is a substantial reduction in time and effort for literature review compared to traditional methods, establishing our toolkit as an efficient alternative. Our open-source toolkit is accessible at https://github.com/shubhamagarwal92/LitLLM and Huggingface space (https://huggingface.co/spaces/shubhamagarwal92/LitLLM) with the video demo at https://youtu.be/E2ggOZBAFw0.\n"}
{"id":2,"text":"DOI: 10.1590/SciELOPreprints.11205\n Title: On the Open Road to Universal Indexing: OpenAlex and OpenJournal Systems\n Abstract: This study examines OpenAlex’s indexing of journals using Open Journal Systems (JUOJS), reflecting two open source software initiatives supporting inclusive scholarly participation. By analyzing a dataset of 47,625 active JUOJS, we reveal that 71% of these journals have at least one article indexed in OpenAlex. Our findings underscore the central role of Crossref DOIs in achieving indexing, with 97% of the journals using Crossref DOIs included in OpenAlex. However, this technical dependency reflects broader structural inequities, as resource-limited journals, particularly those from low-income countries (47% of JUOJS) and non-English language journals (55%-64% of JUOJS), remain underrepresented. Our work highlights the theoretical implications of scholarly infrastructure dependencies and their role in perpetuating systemic disparities in global knowledge visibility. We argue that even inclusive bibliographic databases like OpenAlex must actively address financial, infrastructural, and linguistic barriers to foster equitable indexing on a global scale. By conceptualizing the relationship between indexing mechanisms, persistent identifiers, and structural inequities, this study provides a critical lens for rethinking the dynamics of universal indexing and its realization in a global, multilingual scholarly ecosystem.\n"}
{"id":3,"text":"DOI: 10.48550/arXiv.2410.04231\n Title: Metadata-based Data Exploration with Retrieval-Augmented Generation for Large Language Models\n Abstract: Developing the capacity to effectively search for requisite datasets is an urgent requirement to assist data users in identifying relevant datasets considering the very limited available metadata. For this challenge, the utilization of third-party data is emerging as a valuable source for improvement. Our research introduces a new architecture for data exploration which employs a form of Retrieval-Augmented Generation (RAG) to enhance metadata-based data discovery. The system integrates large language models (LLMs) with external vector databases to identify semantic relationships among diverse types of datasets. The proposed framework offers a new method for evaluating semantic similarity among heterogeneous data sources and for improving data exploration. Our study includes experimental results on four critical tasks: 1) recommending similar datasets, 2) suggesting combinable datasets, 3) estimating tags, and 4) predicting variables. Our results demonstrate that RAG can enhance the selection of relevant datasets, particularly from different categories, when compared to conventional metadata approaches. However, performance varied across tasks and models, which confirms the significance of selecting appropriate techniques based on specific use cases. The findings suggest that this approach holds promise for addressing challenges in data exploration and discovery, although further refinement is necessary for estimation tasks.\n"}
{"id":4,"text":"DOI: 10.48550/arXiv.2404.17663\n Title: An analysis of the suitability of OpenAlex for bibliometric analyses\n Abstract: Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\n"}
{"id":5,"text":"DOI: 10.1093/jamia/ocae129\n Title: RefAI: a GPT-powered retrieval-augmented generative tool for biomedical literature recommendation and summarization\n Abstract: Abstract Objectives Precise literature recommendation and summarization are crucial for biomedical professionals. While the latest iteration of generative pretrained transformer (GPT) incorporates 2 distinct modes—real-time search and pretrained model utilization—it encounters challenges in dealing with these tasks. Specifically, the real-time search can pinpoint some relevant articles but occasionally provides fabricated papers, whereas the pretrained model excels in generating well-structured summaries but struggles to cite specific sources. In response, this study introduces RefAI, an innovative retrieval-augmented generative tool designed to synergize the strengths of large language models (LLMs) while overcoming their limitations. Materials and Methods RefAI utilized PubMed for systematic literature retrieval, employed a novel multivariable algorithm for article recommendation, and leveraged GPT-4 turbo for summarization. Ten queries under 2 prevalent topics (“cancer immunotherapy and target therapy” and “LLMs in medicine”) were chosen as use cases and 3 established counterparts (ChatGPT-4, ScholarAI, and Gemini) as our baselines. The evaluation was conducted by 10 domain experts through standard statistical analyses for performance comparison. Results The overall performance of RefAI surpassed that of the baselines across 5 evaluated dimensions—relevance and quality for literature recommendation, accuracy, comprehensiveness, and reference integration for summarization, with the majority exhibiting statistically significant improvements (P-values &amp;lt;.05). Discussion RefAI demonstrated substantial improvements in literature recommendation and summarization over existing tools, addressing issues like fabricated papers, metadata inaccuracies, restricted recommendations, and poor reference integration. Conclusion By augmenting LLM with external resources and a novel ranking algorithm, RefAI is uniquely capable of recommending high-quality literature and generating well-structured summaries, holding the potential to meet the critical needs of biomedical professionals in navigating and synthesizing vast amounts of scientific literature.\n"}
{"id":6,"text":"DOI: 10.1007/s11192-023-04923-y\n Title: Missing institutions in OpenAlex: possible reasons, implications, and solutions\n Abstract: The advent of open science calls for open data platforms with high data quality. As a fully open catalog of the global research system launched in January 2022, OpenAlex features two main advantages of easy data accessibility and broad data coverage, which has been widely used in quantitative science studies. Remarkably, OpenAlex is adopted as an important data source for Leiden university ranking. However, there is a severe data quality problem of missing institutions in journal article metadata in OpenAlex. This study investigates the possible reasons for the problem and its consequences and solutions by defining three types of institutional information—full institutional information (FII), partially missing institutional information (PMII) and completely missing institutional information (CMII). Our results show that the problem of missing institutions occurs in more than 60% of the journal articles in OpenAlex. The problem is particularly widespread in metadata from the early years and in the social sciences and humanities. Using sub-samples of the data, we further explore the possible reasons for the problem, the risk it might represent for distorted results, and possible solutions to the problem of missing institutions. The aim is to raise the importance of data quality improvements in open resources, and thus to support the responsible use of open resources in quantitative science studies and also in broader contexts.\n"}
{"id":7,"text":"DOI: 10.48550/arXiv.2401.16359\n Title: Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\n Abstract: OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\n"}
{"id":8,"text":"DOI: 10.1007/s11192-015-1765-5\n Title: The journal coverage of Web of Science and Scopus: a comparative analysis\n Abstract: Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\n"}
{"id":9,"text":"DOI: 10.1162/qss_a_00022\n Title: Crossref: The sustainable source of community-owned scholarly metadata\n Abstract: This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\n"}
{"id":10,"text":"DOI: 10.48550/arXiv.2404.01985\n Title: The open access coverage of OpenAlex, Scopus and Web of Science\n Abstract: Diamond open access (OA) journals offer a publishing model that is free for both authors and readers, but their lack of indexing in major bibliographic databases presents challenges in assessing the uptake of these journals. Furthermore, OA characteristics such as publication language and country of publication have often been used to support the argument that OA journals are more diverse and aim to serve a local community, but there is a current lack of empirical evidence related to the geographical and linguistic characteristics of OA journals. Using OpenAlex and the Directory of Open Access Journals as a benchmark, this paper investigates the coverage of diamond and gold through authorship and journal coverage in the Web of Science and Scopus by field, country, and language. Results show their lower coverage in WoS and Scopus, and the local scope of diamond OA. The share of English-only journals is considerably higher among gold journals. High-income countries have the highest share of authorship in every domain and type of journal, except for diamond journals in the social sciences and humanities. Understanding the current landscape of diamond OA indexing can aid the scholarly communications network with advancing policy and practices towards more inclusive OA models.\n"}
{"id":11,"text":"DOI: 10.3145/epi.2023.mar.09\n Title: Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\n Abstract: With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\n"}
{"id":12,"text":"DOI: 10.1162/qss_a_00286\n Title: Completeness degree of publication metadata in eight free-access scholarly databases\n Abstract: Abstract The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\n"}
{"id":13,"text":"DOI: 10.5860/crl.86.1.101\n Title: Identifying Metadata Quality Issues Across Cultures\n Abstract: Metadata are crucial for discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. In this context, we sought to understand how metadata quality, consistency, and completeness impact individuals and communities. Reviewing a sample of records, we identified and classified issues stemming from how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\n"}
{"id":14,"text":"DOI: 10.1007/s11192-022-04367-w\n Title: Identifying and correcting invalid citations due to DOI errors in Crossref data\n Abstract: This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\n"}
{"id":15,"text":"DOI: 10.1002/leap.1411\n Title: Over‐promotion and caution in abstracts of preprints during the <scp>COVID</scp>‐19 crisis\n Abstract: Abstract The abstract is known to be a promotional genre where researchers tend to exaggerate the benefit of their research and use a promotional discourse to catch the reader's attention. The COVID‐19 pandemic has prompted intensive research and has changed traditional publishing with the massive adoption of preprints by researchers. Our aim is to investigate whether the crisis and the ensuing scientific and economic competition have changed the lexical content of abstracts. We propose a comparative study of abstracts associated with preprints issued in response to the pandemic relative to abstracts produced during the closest pre‐pandemic period. We show that with the increase (on average and in percentage) of positive words (especially effective ) and the slight decrease of negative words, there is a strong increase in hedge words (the most frequent of which are the modal verbs can and may ). Hedge words counterbalance the excessive use of positive words and thus invite the readers, who go probably beyond the ‘usual’ audience, to be cautious with the obtained results. The abstracts of preprints urgently produced in response to the COVID‐19 crisis stand between uncertainty and over‐promotion, illustrating the balance that authors have to achieve between promoting their results and appealing for caution.\n"}
{"id":16,"text":"DOI: 10.48550/arXiv.2409.10633\n Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\n Abstract: Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\n"}
{"id":17,"text":"DOI: 10.31274/b8136f97.ccc3dae4\n Title: Comparing Funder Metadata in OpenAlex and Dimensions\n Abstract: I often need to look at metadata for scholarly articles that acknowledge a particular funding agency or group of  agencies. I wanted to compare the results from OpenAlex, a free and open scholarly database that provides  metadata under a CC0 license, and Dimensions, a paid database we have a site license to here at Iowa State  University (note: Dimensions also offers a free version, but with limited features that do not include a Funder  filter).  Funder metadata tends to get into scholarly databases in two ways. It can be provided by publishers when  registering a work for a DOI, or it can be inferred from analyzing an article’s Acknowledgements section.  Unfortunately, the funder metadata field is an optional value at DOI registration, so it is often left blank.  OpenAlex uses Crossref as a major building block in its dataset, with the general consequence that if Crossref  doesn’t have it, OpenAlex won’t see it. By contrast, Dimensions analyzes an article’s Acknowledgements  section using natural language processing to infer funders and enrich the metadata coming from Crossref. It  also has agreements with publishers to obtain additional funding information."}
{"id":18,"text":"DOI: 10.48550/arXiv.2407.17023\n Title: DYNAMICQA: Tracing Internal Knowledge Conflicts in Language Models\n Abstract: Knowledge-intensive language understanding tasks require Language Models (LMs) to integrate relevant context, mitigating their inherent weaknesses, such as incomplete or outdated knowledge. However, conflicting knowledge can be present in the LM's parameters, termed intra-memory conflict, which can affect a model's propensity to accept contextual knowledge. To study the effect of intra-memory conflict on an LM's ability to accept relevant context, we utilize two knowledge conflict measures and a novel dataset containing inherently conflicting data, DynamicQA. This dataset includes facts with a temporal dynamic nature where facts can change over time and disputable dynamic facts, which can change depending on the viewpoint. DynamicQA is the first to include real-world knowledge conflicts and provide context to study the link between the different types of knowledge conflicts. We also evaluate several measures on their ability to reflect the presence of intra-memory conflict: semantic entropy and a novel coherent persuasion score. With our extensive experiments, we verify that LMs exhibit a greater degree of intra-memory conflict with dynamic facts compared to facts that have a single truth value. Furthermore, we reveal that facts with intra-memory conflict are harder to update with context, suggesting that retrieval-augmented generation will struggle with the most commonly adapted facts.\n"}
{"id":19,"text":"DOI: 10.1145/3626772.3657848\n Title: Large Language Models and Future of Information Retrieval: Opportunities and Challenges\n Abstract: Recent years have seen great success of large language models (LLMs) in performing many natural language processing tasks with impressive performance, including tasks that directly serve users such as question answering and text summarization. They open up unprecedented opportunities for transforming information retrieval (IR) research and applications. However, concerns such as halluciation undermine their trustworthiness, limiting their actual utility when deployed in real-world applications, especially high-stake applications where trust is vital. How can we both exploit the strengths of LLMs and mitigate any risk caused by their weaknesses when applying LLMs to IR? What are the best opportunities for us to apply LLMs to IR? What are the major challenges that we will need to address in the future to fully exploit such opportunities? Given the anticipated growth of LLMs, what will future information retrieval systems look like? Will LLMs eventually replace an IR system? In this perspective paper, we examine these questions and provide provisional answers to them. We argue that LLMs will not be able to replace search engines, and future LLMs would need to learn how to use a search engine so that they can interact with a search engine on behalf of users. We conclude with a set of promising future research directions in applying LLMs to IR.\n"}
{"id":20,"text":"DOI: 10.48550/arXiv.2502.03627\n Title: Sorting the Babble in Babel: Assessing the Performance of Language Detection Algorithms on the OpenAlex Database\n Abstract: This project aims to compare various language classification procedures, procedures combining various Python language detection algorithms and metadata-based corpora extracted from manually-annotated articles sampled from the OpenAlex database. Following an analysis of precision and recall performance for each algorithm, corpus, and language as well as of processing speeds recorded for each algorithm and corpus type, overall procedure performance at the database level was simulated using probabilistic confusion matrices for each algorithm, corpus, and language as well as a probabilistic model of relative article language frequencies for the whole OpenAlex database. Results show that procedure performance strongly depends on the importance given to each of the measures implemented: for contexts where precision is preferred, using the LangID algorithm on the greedy corpus gives the best results; however, for all cases where recall is considered at least slightly more important than precision or as soon as processing times are given any kind of consideration, the procedure combining the FastSpell algorithm and the Titles corpus outperforms all other alternatives. Given the lack of truly multilingual, large-scale bibliographic databases, it is hoped that these results help confirm and foster the unparalleled potential of the OpenAlex database for cross-linguistic, bibliometric-based research and analysis.\n"}
{"id":21,"text":"DOI: 10.1162/qss_a_00112\n Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\n Abstract: Abstract We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\n"}
{"id":22,"text":"DOI: 10.48550/arXiv.2505.18247\n Title: MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\n Abstract: Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\n"}
{"id":23,"text":"DOI: 10.48550/arXiv.2303.17661\n Title: MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\n Abstract: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\n"}
{"id":24,"text":"DOI: 10.1109/ACCESS.2024.3395449\n Title: Harnessing the Power of Metadata for Enhanced Question Retrieval in Community Question Answering\n Abstract: Community Question Answering CQA forums such as Yahoo! Answers and Stack Overflow have become popular. The main goal of a CQA is to provide the most suitable answer in the shortest possible time. Since there is a reach archive of answered questions, similar question retrieval has received much attention intending to answer questions immediately after asking. One of the main challenges in this task is the lexical gap between questions, which refers to the discrepancies between the terminologies used by users asking questions. In this paper, we use metadata and two transformer-based techniques to improve the translation-based language model as a traditional technique addressing the lexical gap in retrieval systems. To overcome the lexical gap problem, additional context and information about the questions can help. Metadata is a rich source of information that refers to supplementary data associated with each question. Subject, category, and answer are metadata used in this article. To leverage these metadata, two transformer-based methods are employed. First, to utilize category information, we build category-specific dictionaries to obtain more accurate translation probabilities. A BERT model predicts the categories of the questions. Second, to utilize answer information, we propose a question expansion technique. Expansion is done by a transformer-based model using a retrieval-augmented generation (RAG) model to generate answers and expand new questions with corresponding answers. Finally, candidate questions are ranked according to their similarity to the expanded new question. Our proposed method achieves 51.47 in terms of MAP, outperforming all state-of-the-art approaches in question retrieval."}
{"id":25,"text":"DOI: 10.1609/aaai.v38i16.29728\n Title: Benchmarking Large Language Models in Retrieval-Augmented Generation\n Abstract: Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\n"}
{"id":26,"text":"DOI: 10.48550/arXiv.2406.13213\n Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n Abstract: The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\n"}
{"id":27,"text":"DOI: 10.6109/jkiice.2023.27.12.1489\n Title: M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\n Abstract: This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method"}
{"id":28,"text":"DOI: 10.7710/2162-3309.2110\n Title: Write up! A Study of Copyright Information on Library-Published Journals\n Abstract: INTRODUCTION Libraries have a mission to educate users about copyright, and library publishing staff are often involved in that work. This article investigates a concrete point of intersection between the two areas - copyright statements on library-published journals. METHODS Journals published by members of the Library Publishing Coalition were examined for open access status, type and placement of copyright information, copyright ownership, and open licensing. RESULTS Journals in the sample were overwhelmingly (93%) open access. 80% presented copyright information of some kind, but only 30% of those included it at both the journal and the article level. Open licensing was present in 38% of the journals, and the most common ownership scenario was the author retaining copyright while granting a nonexclusive license to the journal or publisher. 9% of the sample journals included two or more conflicting rights statements. DISCUSSION 76% of the journals did not consistently provide accurate, easily-accessible rights information, and numerous problems were found with the use of open licensing, including conflicting licenses, incomplete licenses, and licenses not appearing at the article level. CONCLUSION Recommendations include presenting full copyright and licensing information at both the journal and the article level, careful use of open licenses, and publicly-available author agreements.External Data or Supplements:Schlosser, Melanie, 2016, \"Data from: Write Up! A Study of Copyright Information on Library-Published Journals\", http://dx.doi.org/10.7910/DVN/R36SVZ, Harvard Dataverse.\n"}
{"id":29,"text":"DOI: 10.5281/ZENODO.13960973\n Title: Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\n Abstract: The DORA declaration and CoARA call for the use of bibliometric indicators based on open data. However, established scholarly metadata datasets are closed, and the quality of open datasets has not yet been thoroughly examined. In this paper, I present a method to detect errors in a dataset using missing data patterns. As an example, the method is applied to the affiliation metadata of publications associated with ETH Zurich. This allows me to identify a series of incorrectly affiliated papers. The method introduced in this paper is not specifically designed for affiliation data and can also be used to detect errors in other types of data. It could lead to corrections which will hopefully benefit providers as well as users of data.\n"}
{"id":30,"text":"DOI: 10.1007/s11192-020-03632-0\n Title: Characteristics of scientific articles on COVID-19 published during the initial 3 months of the pandemic\n Abstract: The COVID-19 pandemic has been characterized by an unprecedented amount of published scientific articles. The aim of this study is to assess the type of articles published during the first 3 months of the COVID-19 pandemic and to compare them with articles published during 2009 H1N1 swine influenza pandemic. Two operators independently extracted and assessed all articles on COVID-19 and on H1N1 swine influenza that had an abstract and were indexed in PubMed during the first 3 months of these pandemics. Of the 2482 articles retrieved on COVID-19, 1165 were included. Over half of them were secondary articles (590, 50.6%). Common primary articles were: human medical research (340, 59.1%), in silico studies (182, 31.7%) and in vitro studies (26, 4.5%). Of the human medical research, the vast majority were observational studies and cases series, followed by single case reports and one randomized controlled trial. Secondary articles were mainly reviews, viewpoints and editorials (373, 63.2%). Limitations were reported in 42 out of 1165 abstracts (3.6%), with 10 abstracts reporting actual methodological limitations. In a similar timeframe, there were 223 articles published on the H1N1 pandemic in 2009. During the COVID-19 pandemic there was a higher prevalence of reviews and guidance articles and a lower prevalence of in vitro and animal research studies compared with the H1N1 pandemic. In conclusions, compared to the H1N1 pandemic, the majority of early publications on COVID-19 does not provide new information, possibly diluting the original data published on this disease and consequently slowing down the development of a valid knowledge base on this disease. Also, only a negligible number of published articles reports limitations in the abstracts, hindering a rapid interpretation of their shortcomings. Researchers, peer reviewers, and editors should take action to flatten the curve of secondary articles.\n"}
{"id":31,"text":"DOI: 10.1371/journal.pbio.1002542\n Title: Citation Metrics: A Primer on How (Not) to Normalize\n Abstract: Citation metrics are increasingly used to appraise published research. One challenge is whether and how to normalize these metrics to account for differences across scientific fields, age (year of publication), type of document, database coverage, and other factors. We discuss the pros and cons for normalizations using different approaches. Additional challenges emerge when citation metrics need to be combined across multiple papers to appraise the corpus of scientists, institutions, journals, or countries, as well as when trying to attribute credit in multiauthored papers. Different citation metrics may offer complementary insights, but one should carefully consider the assumptions that underlie their calculation."}
{"id":32,"text":"DOI: 10.1162/qss_a_00212\n Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\n Abstract: Abstract To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\n"}
{"id":33,"text":"DOI: 10.48550/arXiv.2312.10997\n Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n Abstract: Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\n"}
{"id":34,"text":"DOI: 10.18653/v1/D19-1371\n Title: SciBERT: A Pretrained Language Model for Scientific Text\n Abstract: Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et. al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.\n"}
{"id":35,"text":"DOI: 10.31222/osf.io/smxe5\n Title: Crossref as a source of open bibliographic metadata\n Abstract: Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\n"}
{"id":36,"text":"DOI: 10.1162/qss_a_00210\n Title: The availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\n Abstract: Abstract Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\n"}
{"id":37,"text":"DOI: 10.1002/asi.24171\n Title: PaperPoles: Facilitating adaptive visual exploration of scientific publications by citation links\n Abstract: Finding relevant publications is a common task. Typically, a researcher browses through a list of publications and traces additional relevant publications. When relevant publications are identified, the list may be expanded by the citation links of the relevant publications. The information needs of researchers may change as they go through such iterative processes. The exploration process quickly becomes cumbersome as the list expands. Most existing academic search systems tend to be limited in terms of the extent to which searchers can adapt their search as they proceed. In this article, we introduce an adaptive visual exploration system named PaperPoles to support exploration of scientific publications in a context‐aware environment. Searchers can express their information needs by intuitively formulating positive and negative queries. The search results are grouped and displayed in a cluster view, which shows aspects and relevance patterns of the results to support navigation and exploration. We conducted an experiment to compare PaperPoles with a list‐based interface in performing two academic search tasks with different complexity. The results show that PaperPoles can improve the accuracy of searching for the simple and complex tasks. It can also reduce the completion time of searching and improve exploration effectiveness in the complex task. PaperPoles demonstrates a potentially effective workflow for adaptive visual search of complex information.\n"}
{"id":38,"text":"DOI: 10.48550/arXiv.2404.13948\n Title: Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\n Abstract: The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\n"}
{"id":39,"text":"DOI: 10.1007/978-3-031-88708-6_3\n Title: Is Relevance Propagated from Retriever to Generator in RAG?\n Abstract: Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\n"}
{"id":40,"text":"DOI: 10.48550/arXiv.2402.11782\n Title: What Evidence Do Language Models Find Convincing?\n Abstract: Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as \"is aspartame linked to cancer\". To resolve these ambiguous queries, one must search through a large range of websites and consider \"which, if any, of this evidence do I find convincing?\". In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone. Taken together, these results highlight the importance of RAG corpus quality (e.g., the need to filter misinformation), and possibly even a shift in how LLMs are trained to better align with human judgements.\n"}
{"id":41,"text":"DOI: 10.48550/arXiv.2109.05052\n Title: Entity-Based Knowledge Conflicts in Question Answering\n Abstract: Knowledge-dependent tasks typically use two sources of knowledge: parametric, learned at training time, and contextual, given as a passage at inference time. To understand how models use these sources together, we formalize the problem of knowledge conflicts, where the contextual information contradicts the learned information. Analyzing the behaviour of popular models, we measure their over-reliance on memorized information (the cause of hallucinations), and uncover important factors that exacerbate this behaviour. Lastly, we propose a simple method to mitigate over-reliance on parametric knowledge, which minimizes hallucination, and improves out-of-distribution generalization by 4%-7%. Our findings demonstrate the importance for practitioners to evaluate model tendency to hallucinate rather than read, and show that our mitigation strategy encourages generalization to evolving information (i.e., time-dependent queries). To encourage these practices, we have released our framework for generating knowledge conflicts.\n"}
{"id":42,"text":"DOI: 10.1007/978-3-031-63759-9_27\n Title: S3LLM: Large-Scale Scientific Software Understanding with LLMs Using Source, Metadata, and Document\n Abstract: The understanding of large-scale scientific software is a significant challenge due to its diverse codebase, extensive code length, and target computing architectures. The emergence of generative AI, specifically large language models (LLMs), provides novel pathways for understanding such complex scientific codes. This paper presents S3LLM, an LLM-based framework designed to enable the examination of source code, code metadata, and summarized information in conjunction with textual technical reports in an interactive, conversational manner through a user-friendly interface. S3LLM leverages open-source LLaMA-2 models to enhance code analysis through the automatic transformation of natural language queries into domain-specific language (DSL) queries. In addition, S3LLM is equipped to handle diverse metadata types, including DOT, SQL, and customized formats. Furthermore, S3LLM incorporates retrieval-augmented generation (RAG) and LangChain technologies to directly query extensive documents. S3LLM demonstrates the potential of using locally deployed open-source LLMs for the rapid understanding of large-scale scientific computing software, eliminating the need for extensive coding expertise and thereby making the process more efficient and effective. S3LLM is available at https://github.com/ResponsibleAILab/s3llm.\n"}
{"id":43,"text":"DOI: 10.1162/qss_a_00349\n Title: Representing the disciplinary structure of physics: a comparative evaluation of graph and text embedding methods\n Abstract: ABSTRACT Recent advances in machine learning offer new ways to represent and study scholarly works and the space of knowledge. Graph and text embeddings provide a convenient vector representation of scholarly works based on citations and text. Yet, it is unclear whether their representations are consistent or provide different views of the structure of science. Here, we compare graph and text embedding by testing their ability to capture the hierarchical structure of the Physics and Astronomy Classification Scheme (PACS) of papers published by the American Physical Society (APS). We also provide a qualitative comparison of the overall structure of the graph and text embeddings for reference. We find that neural network-based methods outperform traditional methods, and graph embedding methods node2vec and residual2vec are better than other methods at capturing the PACS structure. Our results call for further investigations into how different contexts of scientific papers are captured by different methods, and how we can combine and leverage such information in an interpretable manner.\n"}
{"id":44,"text":"DOI: 10.1145/3543873.3587655\n Title: Decoding Prompt Syntax: Analysing its Impact on Knowledge Retrieval in Large Language Models\n Abstract: Large Language Models (LLMs), with their advanced architectures and training on massive language datasets, contain unexplored knowledge. One method to infer this knowledge is through the use of cloze-style prompts. Typically, these prompts are manually designed because the phrasing of these prompts impacts the knowledge retrieval performance, even if the LLM encodes the desired information. In this paper, we study the impact of prompt syntax on the knowledge retrieval capacity of LLMs. We use a template-based approach to paraphrase simple prompts into prompts with a more complex grammatical structure. We then analyse the LLM performance for these structurally different but semantically equivalent prompts. Our study reveals that simple prompts work better than complex forms of sentences. The performance across the syntactical variations for simple relations (1:1) remains best, with a marginal decrease across different typologies. These results reinforce that simple prompt structures are more effective for knowledge retrieval in LLMs and motivate future research into the impact of prompt syntax on various tasks.\n"}
