{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere RAG with dense retriever and ReRank model\n",
    "- references: https://docs.cohere.com/v2/docs/rag-complete-example\n",
    "- [ ] check performance with SciFact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import cohere\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import sys\n",
    "from colorama import Fore, Style, Back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mall is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "# load secret from local .env file\n",
    "def get_key():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    _key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "    #verify if it worked\n",
    "    if _key is not None:\n",
    "        print(Fore.GREEN + \"all is good, beautiful!\")\n",
    "        return _key\n",
    "    else:\n",
    "        print(Fore.LIGHTRED_EX + \"API Key is missing\")\n",
    "\n",
    "# initilize client\n",
    "co = cohere.ClientV2(get_key())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0]\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append(f\"{doi} {text}\\n\")\n",
    "    return documents_with_doi\n",
    "\n",
    "# initialize empty search query\n",
    "search_queries = []\n",
    "# Embed the documents\n",
    "def document_embed(documents:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the documents from a list provided from read_documents_with_doi()\n",
    "    \"\"\"\n",
    "    doc_emb = co.embed(\n",
    "        #model=\"embed-v4.0\",\n",
    "        model=\"embed-english-v3.0\",\n",
    "        input_type=\"search_document\",\n",
    "        texts=[doc for doc in documents],\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return doc_emb\n",
    "\n",
    "# Embed the search query\n",
    "def query_embed(search_queries:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the query from a list provided in search_queries variable\n",
    "    \"\"\"\n",
    "    query_emb = co.embed(\n",
    "        #model=\"embed-v4.0\",\n",
    "        model=\"embed-english-v3.0\",\n",
    "        input_type=\"search_query\",\n",
    "        texts=search_queries,\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return query_emb\n",
    "\n",
    "# retrieve top_k and compute similarity using dot product\n",
    "def retrieve_top_k(top_k, query_embedded, documents_embedded, documents)->List[str]:\n",
    "    \"\"\"\n",
    "    returns the top_k documents based on dot product similarity\n",
    "    \"\"\"\n",
    "\n",
    "    scores = np.dot(query_embedded, np.transpose(documents_embedded))[0]#ordered list!\n",
    "    # takes top scores, and returns sorted list and returns indices sliced by top_k\n",
    "    max_idx = np.argsort(-scores)[:top_k]\n",
    "    # returns documents by index\n",
    "    retrieved_docs = [documents[item] for item in max_idx]\n",
    "    # returns a list of documents\n",
    "    return retrieved_docs\n",
    "\n",
    "def rerank_documents(retrieved_documents,search_queries,threshold,top_k)->List[str]:\n",
    "    \"\"\"\n",
    "    takes retrieved_documents as input along with search_queries and runs them through the \n",
    "    rerank model from cohere for semantic similarity. \n",
    "\n",
    "    top_n = top_k\n",
    "    Limits those returned by a threshold score. this is to reduce those that are irrelevant.\n",
    "    \"\"\"\n",
    "    # Rerank the documents\n",
    "    results = co.rerank(\n",
    "        model=\"rerank-v3.5\",\n",
    "        #model=\"rerank-english-v3.0\",\n",
    "        query=search_queries[0],\n",
    "        documents=[doc for doc in retrieved_documents],\n",
    "        top_n=top_k,\n",
    "        max_tokens_per_doc=4096,# defaults to 4096\n",
    "    )\n",
    "\n",
    "    # Display the reranking results\n",
    "    #for idx, result in enumerate(results.results):\n",
    "    #    print(f\"Rank: {idx+1}\")\n",
    "    #    print(f\"Score: {result.relevance_score}\")\n",
    "    #    print(f\"Document: {retrieved_documents[result.index]}\\n\")\n",
    "\n",
    "    #returns only those over threshold\n",
    "    reranked_docs = [\n",
    "        retrieved_documents[result.index] for result in results.results if result.relevance_score >=threshold\n",
    "    ]\n",
    "    reranked_with_score = [(result.relevance_score, retrieved_documents[result.index].split(\"\\n\")[0].strip(\"DOI: \")) for result in results.results if result.relevance_score >=threshold]\n",
    "\n",
    "    print(f\"reranked_documents: {reranked_docs}\")\n",
    "    print(f\"length of reranked_documents: {len(reranked_docs)}\")\n",
    "\n",
    "    return reranked_docs, reranked_with_score\n",
    "\n",
    "def cohere_rag_pipeline(directory_path,search_queries,top_k,threshold):\n",
    "\n",
    "    # retrieve documents from directory\n",
    "    documents = read_documents_with_doi(directory_path)\n",
    "    print(f\"Length of documents: {len(documents)}\")\n",
    "    # embed the documents\n",
    "    documents_embedded = document_embed(documents)\n",
    "\n",
    "    #embed the query:\n",
    "    query_embedded = query_embed(search_queries)\n",
    "\n",
    "    # retrieve the top_k documents\n",
    "    retrieved_documents = retrieve_top_k(top_k, query_embedded, documents_embedded, documents)\n",
    "\n",
    "    # rerank the documents using the Rerank model from Cohere\n",
    "    reranked_documents, reranked_DOIs_with_score = rerank_documents(retrieved_documents,search_queries,threshold,top_k)\n",
    "    # set system instructions\n",
    "    instructions = \"\"\"\n",
    "                    You are an academic research assistant.\n",
    "                    You must include the DOI in your response.\n",
    "                    If there is no content provided, ask for a different question.\n",
    "                    Please structure your response like this:\n",
    "                    Summary: summary statement here. \n",
    "                    DOI: summary of the text associated with this DOI.\n",
    "                    Address me as, 'my lady'.\n",
    "                    \"\"\"\n",
    "    # create messages to model\n",
    "    messages = [{\"role\":\"user\",\n",
    "                \"content\": search_queries[0]},\n",
    "                {\"role\":\"system\",\n",
    "                \"content\":instructions}]\n",
    "\n",
    "    # Generate the response\n",
    "    resp = co.chat(\n",
    "        #model=\"command-a-03-2025\",\n",
    "        model=\"command-r-08-2024\",\n",
    "        #model=\"command-r7b-12-2024\", #https://docs.cohere.com/docs/command-r7b\n",
    "        messages=messages,\n",
    "        documents=reranked_documents,\n",
    "    )\n",
    "\n",
    "    return resp, reranked_documents, reranked_DOIs_with_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#  run here to test functions avove\n",
    "# ****** Pipeline ********\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.1\n",
    "\n",
    "response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "# Display the response\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"{response.message.content[0].text}\")\n",
    "print(Fore.LIGHTCYAN_EX + f\"------\\nReranked documents:\")\n",
    "for doc in reranked_documents_end:\n",
    "    print(doc)\n",
    "\n",
    "# Display the citations and source documents\n",
    "if response.message.citations:\n",
    "    print(Fore.LIGHTYELLOW_EX + \"\\nCITATIONS:\")\n",
    "    for citation in response.message.citations:\n",
    "        print(f\"source text: {citation.text},\\nsource: {citation.sources[0].document.get('content').split(\"\\n\")[0]}\\n------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# Analysis\n",
    "Precision, recall, accuracy, F1 scores and faithfulness\n",
    "## Precision, recall, F1 score\n",
    "### references\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "- https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from colorama import Fore, Back, Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Documents score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query, Precision, Recall, F1-Score, Accuracy, Balanced accuracy, Faithfulness score, Documents score, Response]\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial dataframe to capture results from each query and results\n",
    "#ONLY DO THIS AT THE BEGINNING OF THE ANALYSIS PROCEDURE, OTHERWISE, IT WILL ERASE THE PREVIOUS RESULTS!!\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Query','Precision','Recall','F1-Score','Accuracy', 'Balanced accuracy', 'Faithfulness score', 'Documents score', 'Response'])\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set up functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "import time\n",
    "from colorama import Fore, Back, Style\n",
    "\"\"\"\n",
    "change this to read in an excel sheet of queries and ground_truth dois.\n",
    "Then it should be isolated as a function.\n",
    "Run the function to iterature through the list.\n",
    "\"\"\"\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "#retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "#print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# initiates the variable\n",
    "ground_truth = []\n",
    "\n",
    "\n",
    "def evaluate_retrieval(retrieved_dois, ground_truth):\n",
    "    corpus_doi_list = []\n",
    "    #corpus_list is a global variable in rag_pipeline()\n",
    "    for each in range(len(documents_with_doi)):\n",
    "        a = documents_with_doi[each].split(\"\\n\")[0].strip(\"DOI: \")\n",
    "        corpus_doi_list.append(a)\n",
    "    print(len(corpus_doi_list))\n",
    "\n",
    "    def compare_lists(list1, list2, list3):\n",
    "        for val in list1:\n",
    "            if val in list2:\n",
    "                list3.append(1)\n",
    "            else:\n",
    "                list3.append(0)\n",
    "\n",
    "    #set y_true so that len(y_true)==len(corpus_doi_list)\n",
    "    y_true = []\n",
    "    compare_lists(corpus_doi_list,ground_truth,y_true)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = []\n",
    "    compare_lists(corpus_doi_list,retrieved_dois,y_pred)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # calculate metrics - could also use sklearn.metrics functions such as precision_score, but this is easier to read\n",
    "    precision = precision_score(y_true, y_pred, average='binary')\n",
    "    recall = recall_score(y_true, y_pred, average='binary')\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    faithfulness_score = 0\n",
    "    for each in retrieved_dois:\n",
    "        if each in response.message.content[0].text:\n",
    "            faithfulness_score+=1\n",
    "        else:\n",
    "            faithfulness_score+=0\n",
    "\n",
    "    return {\n",
    "        'Query':f\"{search_queries[0]}\",\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\":accuracy,\n",
    "        \"Balanced accuracy\":balanced_accuracy,\n",
    "        \"Faithfulness score\":faithfulness_score,\n",
    "        \"Documents score\":str(reranked_DOIs_with_score_end),#converted to string because pandas was only taking the first tuple not the entire list - use ast.literal_eval() to unpack later.\n",
    "        \"Response\":response.message.content[0].text\n",
    "    }\n",
    "\n",
    "def print_results()->Dict:\n",
    "    \"\"\"\n",
    "    Prints a nicely ordered set of results from evalaute_retrieval()\n",
    "    \"\"\"\n",
    "    results = evaluate_retrieval(retrieved_dois, ground_truth)\n",
    "    print(f\"For query: {results['Query']}:\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"F1-Score: {results['F1-Score']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    print(f\"Balanced accuracy: {results['Balanced accuracy']:.3f}\")\n",
    "    print(f\"Faithfulness score: {results['Faithfulness score']}\")\n",
    "    print(f\"Documents scores: {results['Documents score']}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "#for debugging\n",
    "\n",
    "#print_results()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the test from here *OLD*\n",
    "this is the one-at-a-time version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#run the test from here\n",
    "\n",
    "# Ground truth relevant documents (DOIs) for each query\n",
    "ground_truth = [\"10.1002/leap.1411\",\"10.1007/s11192-020-03632-0\",\"10.1162/qss_a_00286\",\"10.1162/qss_a_00022\",\"10.31222/osf.io/smxe5\"]\n",
    "\n",
    "#***** Begin chat session *****\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_jats\"\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_multi_lang\"\n",
    "\n",
    "documents_with_doi = read_documents_with_doi(directory_path)\n",
    "documents = [doc[0].split('\\n')[1:] for doc in documents_with_doi]\n",
    "print(f\"Length of documents: {len(documents)}\")\n",
    "print(f\"Length of corpus: {len(documents_with_doi)}\")\n",
    "\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.10\n",
    "\n",
    "response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "\n",
    "# Extract DOIs from retrieved documents\n",
    "retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "# Display the response\n",
    "print(Fore.LIGHTYELLOW_EX + f\"{response.message.content[0].text}\")\n",
    "\n",
    "new_result = print_results()\n",
    "# add the new result to the df\n",
    "results_df.loc[len(results_df)] = new_result\n",
    "\n",
    "#save the queries and responses to separate dataframe to be manually annontated\n",
    "answer_relevance_df = results_df[['Query','Response']].copy(deep=True)\n",
    "\n",
    "# save out answer_relevance_df\n",
    "filename=\"analysis/dense_answer_relevance_results.xlsx\"\n",
    "answer_relevance_df.to_excel(filename)\n",
    "\n",
    "filename = \"analysis/dense_analysis_results.xlsx\"\n",
    "os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "results_df.to_excel(filename)\n",
    "results_df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## automated version \n",
    "Currently Works!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Documents score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query, Precision, Recall, F1-Score, Accuracy, Balanced accuracy, Faithfulness score, Documents score, Response]\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial dataframe to capture results from each query and results\n",
    "#ONLY DO THIS AT THE BEGINNING OF THE ANALYSIS PROCEDURE, OTHERWISE, IT WILL ERASE THE PREVIOUS RESULTS!!\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Query','Precision','Recall','F1-Score','Accuracy', 'Balanced accuracy', 'Faithfulness score', 'Documents score', 'Response'])\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>n_ground_truth</th>\n",
       "      <th>expected_response</th>\n",
       "      <th>Refernces:</th>\n",
       "      <th>Response\\nBM25</th>\n",
       "      <th>Response\\nDense</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>which studies examined the abstract in metadata?</td>\n",
       "      <td>[\"10.1002/leap.1411\",\"10.1007/s11192-020-03632...</td>\n",
       "      <td>5</td>\n",
       "      <td>Lexical content changes in abstracts during th...</td>\n",
       "      <td>DOI: 10.1002/leap.1411\\nInvestigates lexical c...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>which studies examined citations?</td>\n",
       "      <td>[\"10.1007/s11192-022-04367-w\",\"10.1371/journal...</td>\n",
       "      <td>5</td>\n",
       "      <td>Identifying and correcting invalid citations d...</td>\n",
       "      <td>DOI: 10.1007/s11192-022-04367-w\\nFocuses on id...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tell me about OpenAlex.</td>\n",
       "      <td>[\"10.3145/epi.2023.mar.09\",\"10.1590/SciELOPrep...</td>\n",
       "      <td>7</td>\n",
       "      <td>OpenAlex is presented as a promising open-sour...</td>\n",
       "      <td>DOI: 10.3145/epi.2023.mar.09\\nCompares OpenAle...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tell me about Crossref.</td>\n",
       "      <td>[\"10.1162/qss_a_00212\",\"10.31274/b8136f97.ccc3...</td>\n",
       "      <td>9</td>\n",
       "      <td>Crossref is portrayed as a major source of sch...</td>\n",
       "      <td>DOI: 10.1162/qss_a_00212 - Examines Crossref’s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Which papers evaluate the linguistic coverage ...</td>\n",
       "      <td>[\"10.1007/s11192-015-1765-5\",\"10.48550/arXiv.2...</td>\n",
       "      <td>5</td>\n",
       "      <td>Biases in Traditional Databases: WoS and Scopu...</td>\n",
       "      <td>DOI: 10.1007/s11192-015-1765-5 - Compares the ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Which papers address funding metadata, its ava...</td>\n",
       "      <td>[\"10.1162/qss_a_00210\",\"10.1162/qss_a_00212\",\"...</td>\n",
       "      <td>5</td>\n",
       "      <td>Assessing Availability: Highlighting gaps in f...</td>\n",
       "      <td>DOI: 10.1162/qss_a_00210 - Examines the availa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Which papers discuss the use of Retrieval-Augm...</td>\n",
       "      <td>[\"10.1007/978-3-031-88708-6_3\",\"10.1609/aaai.v...</td>\n",
       "      <td>5</td>\n",
       "      <td>Evaluation and Benchmarking: Diagnosing RAG’s ...</td>\n",
       "      <td>DOI: 10.1007/978-3-031-88708-6_3 \\nInvestigate...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is Crossref’s role in the scholarly resea...</td>\n",
       "      <td>[\"10.1162/qss_a_00022\",\"10.1162/qss_a_00210\",\"...</td>\n",
       "      <td>5</td>\n",
       "      <td>Crossref plays a central role in the scholarly...</td>\n",
       "      <td>DOI: 10.1162/qss_a_00022\\n Describes Crossref ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What are the key features and limitations of O...</td>\n",
       "      <td>[\"10.3145/epi.2023.mar.09\",\"10.1590/SciELOPrep...</td>\n",
       "      <td>5</td>\n",
       "      <td>OpenAlex is highlighted as a promising open-so...</td>\n",
       "      <td>DOI: 10.3145/epi.2023.mar.09\\nKey Features: Pr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the strengths and weaknesses of Web o...</td>\n",
       "      <td>[\"10.1007/s11192-015-1765-5\",\"10.1162/qss_a_00...</td>\n",
       "      <td>5</td>\n",
       "      <td>Web of Science (WoS) is recognized for its str...</td>\n",
       "      <td>DOI: 10.1007/s11192-015-1765-5\\nStrengths: Com...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>How is RAG used to improve question answering ...</td>\n",
       "      <td>[\"10.1007/978-3-031-88708-6_3\",\"10.1109/ACCESS...</td>\n",
       "      <td>5</td>\n",
       "      <td>RAG is used to improve question answering and ...</td>\n",
       "      <td>DOI: 10.1007/978-3-031-88708-6_3\\nApplication:...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What are the main challenges in normalizing ci...</td>\n",
       "      <td>[\"10.1007/s11192-015-1765-5\",\"10.1162/qss_a_00...</td>\n",
       "      <td>5</td>\n",
       "      <td>The main challenges in normalizing citation me...</td>\n",
       "      <td>DOI: 10.1007/s11192-015-1765-5\\nChallenge: Bia...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What methods are used to detect and correct er...</td>\n",
       "      <td>[\"10.5281/ZENODO.13960973\",\"10.1007/s11192-022...</td>\n",
       "      <td>5</td>\n",
       "      <td>Methods used to detect and correct errors in b...</td>\n",
       "      <td>DOI: 10.5281/ZENODO.13960973\\nMethod: Uses mis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>tell me about how RAG works.</td>\n",
       "      <td>[\"10.1007/978-3-031-88708-6_3\",\"10.1609/aaai.v...</td>\n",
       "      <td>5</td>\n",
       "      <td>These papers explain RAG as a framework that:\\...</td>\n",
       "      <td>DOI: 10.1007/978-3-031-88708-6_3\\nInvestigates...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                query  ... Response\\nDense\n",
       "0    which studies examined the abstract in metadata?  ...             NaN\n",
       "1                   which studies examined citations?  ...             NaN\n",
       "2                             Tell me about OpenAlex.  ...             NaN\n",
       "3                             Tell me about Crossref.  ...             NaN\n",
       "4   Which papers evaluate the linguistic coverage ...  ...             NaN\n",
       "5   Which papers address funding metadata, its ava...  ...             NaN\n",
       "6   Which papers discuss the use of Retrieval-Augm...  ...             NaN\n",
       "7   What is Crossref’s role in the scholarly resea...  ...             NaN\n",
       "8   What are the key features and limitations of O...  ...             NaN\n",
       "9   What are the strengths and weaknesses of Web o...  ...             NaN\n",
       "10  How is RAG used to improve question answering ...  ...             NaN\n",
       "11  What are the main challenges in normalizing ci...  ...             NaN\n",
       "12  What methods are used to detect and correct er...  ...             NaN\n",
       "13                       tell me about how RAG works.  ...             NaN\n",
       "\n",
       "[14 rows x 7 columns]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "golden_set_df = pd.read_excel(\"golden_set.xlsx\")\n",
    "#golden_set_df_test = golden_set_df.head(3)\n",
    "golden_set_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of documents: 45\n",
      "Length of corpus: 45\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00286\\n Title: Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: Abstract The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.5860/crl.86.1.101\\n Title: Identifying Metadata Quality Issues Across Cultures\\nAbstract: Metadata are crucial for discovery and access by providing contextual, technical, and administrative information in a standard form. Yet metadata are also sites of tension between sociocultural representations, resource constraints, and standardized systems. Formal and informal interventions may be interpreted as quality issues, political acts to assert identity, or strategic choices to maximize visibility. In this context, we sought to understand how metadata quality, consistency, and completeness impact individuals and communities. Reviewing a sample of records, we identified and classified issues stemming from how metadata and communities press up against each other to intentionally reflect (or not) cultural meanings.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: Crossref as a source of open bibliographic metadata\\nAbstract: Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.3145/epi.2023.mar.09\\n Title: Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\\nAbstract: With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00286', '10.5860/crl.86.1.101', '10.48550/arXiv.2303.17661', '10.31222/osf.io/smxe5', '10.3145/epi.2023.mar.09']\n",
      "\u001b[93mMy lady, I have found several studies that examined the abstract in metadata.\n",
      "\n",
      "Summary: Several studies have examined the completeness, quality, and availability of metadata, including the abstract, in various contexts.\n",
      "\n",
      "DOI: 10.1162/qss_a_00286\n",
      "This study compared the amount of metadata and the completeness degree of research publications in new academic databases. The abstract was one of seven characteristics analysed to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases.\n",
      "\n",
      "DOI: 10.5860/crl.86.1.101\n",
      "This study sought to understand how metadata quality, consistency, and completeness impact individuals and communities. The abstract was identified as one of the six metadata elements in Crossref, alongside reference lists, ORCIDs, author affiliations, funding information, and license information.\n",
      "\n",
      "DOI: 10.48550/arXiv.2303.17661\n",
      "This study investigated methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations as a case study. The abstract was not explicitly mentioned, but the study focused on improving the quality of metadata fields.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5\n",
      "This study provided an overview of the availability of six metadata elements in Crossref, including the abstract. The analysis showed that the availability of these elements has improved over time for journal articles, the most common publication type in Crossref.\n",
      "\n",
      "DOI: 10.3145/epi.2023.mar.09\n",
      "This study compared the metadata with relevance to bibliometric analyses of Microsoft Academic Graph and OpenAlex. While the abstract was not specifically mentioned, the study examined the transfer of bibliographic data, including publication year, volume, page numbers, and DOI, from MAG to OpenAlex.\n",
      "45\n",
      "For query: ['which studies examined the abstract in metadata?']:\n",
      "Precision: 0.867\n",
      "Recall: 0.867\n",
      "F1-Score: 0.867\n",
      "Accuracy: 0.867\n",
      "Balanced accuracy: 0.663\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.6519982, '10.1162/qss_a_00286'), (0.38505143, '10.5860/crl.86.1.101'), (0.33157462, '10.48550/arXiv.2303.17661'), (0.32875618, '10.31222/osf.io/smxe5'), (0.18396018, '10.3145/epi.2023.mar.09')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The timeout has elapsed!\n",
      "\u001b[96mWorking on row: 0\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/s11192-022-04367-w\\n Title: Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: Abstract We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n']\n",
      "length of reranked_documents: 2\n",
      "Retrieved DOIs: ['10.1007/s11192-022-04367-w', '10.1162/qss_a_00112']\n",
      "\u001b[93mMy lady, I have found two studies that examined citations.\n",
      "\n",
      "## Summary: Two studies compared bibliographic data sources and identified and corrected invalid citations due to DOI errors.\n",
      "\n",
      "## DOI: 10.1007/s11192-022-04367-w\n",
      "This study aimed to identify classes of DOI mistakes by analysing open bibliographic metadata available in Crossref. The researchers highlighted which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. The outcomes showed that only a few publishers were responsible for and/or affected by the majority of invalid citations.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00112\n",
      "This study presented a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considered scientific documents from the period 2008–2017 covered by these data sources. The researchers analysed differences between the data sources in the coverage of documents, focusing on differences over time, per document type, and per discipline. They also studied differences in the completeness and accuracy of citation links.\n",
      "45\n",
      "For query: ['which studies examined citations?']:\n",
      "Precision: 0.933\n",
      "Recall: 0.933\n",
      "F1-Score: 0.933\n",
      "Accuracy: 0.933\n",
      "Balanced accuracy: 0.700\n",
      "Faithfulness score: 2\n",
      "Documents score: [(0.22735819, '10.1007/s11192-022-04367-w'), (0.13381115, '10.1162/qss_a_00112')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The timeout has elapsed!\n",
      "\u001b[96mWorking on row: 1\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.48550/arXiv.2401.16359\\n Title: Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', \"DOI: 10.48550/arXiv.2409.10633\\n Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.1007/s11192-023-04923-y\\n Title: Missing institutions in OpenAlex: possible reasons, implications, and solutions\\nAbstract: The advent of open science calls for open data platforms with high data quality. As a fully open catalog of the global research system launched in January 2022, OpenAlex features two main advantages of easy data accessibility and broad data coverage, which has been widely used in quantitative science studies. Remarkably, OpenAlex is adopted as an important data source for Leiden university ranking. However, there is a severe data quality problem of missing institutions in journal article metadata in OpenAlex. This study investigates the possible reasons for the problem and its consequences and solutions by defining three types of institutional information—full institutional information (FII), partially missing institutional information (PMII) and completely missing institutional information (CMII). Our results show that the problem of missing institutions occurs in more than 60% of the journal articles in OpenAlex. The problem is particularly widespread in metadata from the early years and in the social sciences and humanities. Using sub-samples of the data, we further explore the possible reasons for the problem, the risk it might represent for distorted results, and possible solutions to the problem of missing institutions. The aim is to raise the importance of data quality improvements in open resources, and thus to support the responsible use of open resources in quantitative science studies and also in broader contexts.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.1590/SciELOPreprints.11205\\n Title: On the Open Road to Universal Indexing: OpenAlex and OpenJournal Systems\\nAbstract: This study examines OpenAlex’s indexing of journals using Open Journal Systems (JUOJS), reflecting two open source software initiatives supporting inclusive scholarly participation. By analyzing a dataset of 47,625 active JUOJS, we reveal that 71% of these journals have at least one article indexed in OpenAlex. Our findings underscore the central role of Crossref DOIs in achieving indexing, with 97% of the journals using Crossref DOIs included in OpenAlex. However, this technical dependency reflects broader structural inequities, as resource-limited journals, particularly those from low-income countries (47% of JUOJS) and non-English language journals (55%-64% of JUOJS), remain underrepresented. Our work highlights the theoretical implications of scholarly infrastructure dependencies and their role in perpetuating systemic disparities in global knowledge visibility. We argue that even inclusive bibliographic databases like OpenAlex must actively address financial, infrastructural, and linguistic barriers to foster equitable indexing on a global scale. By conceptualizing the relationship between indexing mechanisms, persistent identifiers, and structural inequities, this study provides a critical lens for rethinking the dynamics of universal indexing and its realization in a global, multilingual scholarly ecosystem.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2401.16359', '10.48550/arXiv.2409.10633', '10.1007/s11192-023-04923-y', '10.48550/arXiv.2404.17663', '10.1590/SciELOPreprints.11205']\n",
      "\u001b[93mMy lady, OpenAlex is a promising open-source of scholarly metadata, offering an alternative to established proprietary sources like the Web of Science and Scopus. Launched in 2022, it provides easy access to a broad range of data, which has been widely used in quantitative science studies.\n",
      "\n",
      "However, there are some concerns about the quality of its data. One study found that over 60% of journal articles in OpenAlex have missing institutional information, particularly in the social sciences and humanities. Another study compared OpenAlex's metadata to that of the Web of Science and found that while it offers more balanced linguistic coverage, the language metadata is not always accurate.\n",
      "\n",
      "Despite these limitations, OpenAlex is considered a superset of Scopus and can be a reliable alternative for certain analyses, especially at the country level. Its indexing of journals using Open Journal Systems has revealed that 71% of active journals have at least one article indexed in OpenAlex, with Crossref DOIs playing a central role.\n",
      "\n",
      "DOI: 10.48550/arXiv.2401.16359\n",
      "DOI: 10.48550/arXiv.2409.10633\n",
      "DOI: 10.1007/s11192-023-04923-y\n",
      "DOI: 10.48550/arXiv.2404.17663\n",
      "DOI: 10.1590/SciELOPreprints.11205\n",
      "45\n",
      "For query: ['Tell me about OpenAlex.']:\n",
      "Precision: 0.956\n",
      "Recall: 0.956\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.857\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.79732054, '10.48550/arXiv.2401.16359'), (0.73505294, '10.48550/arXiv.2409.10633'), (0.69697046, '10.1007/s11192-023-04923-y'), (0.6894943, '10.48550/arXiv.2404.17663'), (0.63452125, '10.1590/SciELOPreprints.11205')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The timeout has elapsed!\n",
      "\u001b[96mWorking on row: 2\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: Crossref as a source of open bibliographic metadata\\nAbstract: Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1162/qss_a_00210\\n Title: The availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: Abstract Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: Abstract We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5', '10.1162/qss_a_00210', '10.1162/qss_a_00112', '10.1007/s11192-022-04367-w']\n",
      "\u001b[93mMy lady, Crossref is a sustainable source of community-owned scholarly metadata. It contains over 106 million records and is expanding at an average rate of 11% a year, making it a major source of scholarly data for publishers, authors, librarians, funders, and researchers.\n",
      "\n",
      "The metadata set includes 13 content types, such as journals, conference papers, data sets, reports, preprints, peer reviews, and grants. It also includes abstracts, links to full text, funding and license information, citation links, and information about corrections, updates, and retractions.\n",
      "\n",
      "Crossref's metadata is available through APIs, including REST API and OAI-PMH. It is a valuable source for research in scientometrics, helping to measure the growth and impact of science and understand new trends in scholarly communications.\n",
      "\n",
      "Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. An analysis of six metadata elements—reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information—shows improved availability over time, particularly for journal articles. However, many publishers need to make additional efforts to achieve full openness of bibliographic metadata.\n",
      "\n",
      "Crossref initiated FundRef in 2013 to help funders track publication output associated with their funding. A case study of publications funded by the Dutch Research Council found that only 67% of articles contain funding information in Crossref, with a subset acknowledging the funder name and/or Funder IDs linked to the Dutch Research Council.\n",
      "\n",
      "A large-scale comparison of bibliographic data sources, including Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic, analysed differences in coverage of documents, completeness and accuracy of citation links, and the importance of combining comprehensive coverage of scientific literature with flexible filters for literature selection.\n",
      "\n",
      "Research has also been conducted to identify and correct invalid citations due to DOI errors in Crossref data. By analysing open bibliographic metadata, the study aimed to highlight responsible publishers and the potential for automatic correction of incorrect DOIs. The outcomes showed that only a few publishers were responsible for or affected by the majority of invalid citations, and a DOI cleaning mechanism was proposed to integrate into existing processes to automatically correct wrong DOIs.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "DOI: 10.31222/osf.io/smxe5\n",
      "DOI: 10.1162/qss_a_00210\n",
      "DOI: 10.1162/qss_a_00112\n",
      "DOI: 10.1007/s11192-022-04367-w\n",
      "45\n",
      "For query: ['Tell me about Crossref.']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.778\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.7476348, '10.1162/qss_a_00022'), (0.52173984, '10.31222/osf.io/smxe5'), (0.33646238, '10.1162/qss_a_00210'), (0.21965577, '10.1162/qss_a_00112'), (0.21606153, '10.1007/s11192-022-04367-w')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The timeout has elapsed!\n",
      "\u001b[96mWorking on row: 3\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2409.10633\\n Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.1162/qss_a_00286\\n Title: Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: Abstract The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n', 'DOI: 10.48550/arXiv.2502.03627\\n Title: Sorting the Babble in Babel: Assessing the Performance of Language Detection Algorithms on the OpenAlex Database\\nAbstract: This project aims to compare various language classification procedures, procedures combining various Python language detection algorithms and metadata-based corpora extracted from manually-annotated articles sampled from the OpenAlex database. Following an analysis of precision and recall performance for each algorithm, corpus, and language as well as of processing speeds recorded for each algorithm and corpus type, overall procedure performance at the database level was simulated using probabilistic confusion matrices for each algorithm, corpus, and language as well as a probabilistic model of relative article language frequencies for the whole OpenAlex database. Results show that procedure performance strongly depends on the importance given to each of the measures implemented: for contexts where precision is preferred, using the LangID algorithm on the greedy corpus gives the best results; however, for all cases where recall is considered at least slightly more important than precision or as soon as processing times are given any kind of consideration, the procedure combining the FastSpell algorithm and the Titles corpus outperforms all other alternatives. Given the lack of truly multilingual, large-scale bibliographic databases, it is hoped that these results help confirm and foster the unparalleled potential of the OpenAlex database for cross-linguistic, bibliometric-based research and analysis.\\n', 'DOI: 10.1007/s11192-015-1765-5\\n Title: The journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.48550/arXiv.2409.10633', '10.1162/qss_a_00286', '10.48550/arXiv.2502.03627', '10.1007/s11192-015-1765-5']\n",
      "\u001b[93mMy lady, here are the papers that evaluate the linguistic coverage or language-related metadata in scholarly databases:\n",
      "\n",
      "## Summary: Papers that assess the linguistic coverage and metadata accuracy of scholarly databases.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2409.10633\n",
      "Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness.\n",
      "This paper compares the metadata accuracy and completeness of OpenAlex with Clarivate's Web of Science (WoS), finding that OpenAlex has a more balanced linguistic coverage but overestimates the place of English.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00286\n",
      "Title: Completeness degree of publication metadata in eight free-access scholarly databases.\n",
      "This study compares the completeness of metadata in new academic databases, finding that academic search engines gather less information, while third-party databases have higher metadata quality.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2502.03627\n",
      "Title: Sorting the Babble in Babel: Assessing the Performance of Language Detection Algorithms on the OpenAlex Database.\n",
      "The project aims to compare language classification procedures on the OpenAlex database, highlighting its potential for cross-linguistic, bibliometric-based research.\n",
      "\n",
      "## DOI: 10.1007/s11192-015-1765-5\n",
      "Title: The journal coverage of Web of Science and Scopus: a comparative analysis.\n",
      "This research compares the journal coverage of WoS and Scopus with Ulrich's extensive periodical directory, finding biases towards Natural Sciences, Engineering, and English-language journals.\n",
      "45\n",
      "For query: ['Which papers evaluate the linguistic coverage or language-related metadata in scholarly databases?']:\n",
      "Precision: 0.933\n",
      "Recall: 0.933\n",
      "F1-Score: 0.933\n",
      "Accuracy: 0.933\n",
      "Balanced accuracy: 0.787\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.8991304, '10.48550/arXiv.2409.10633'), (0.27390003, '10.1162/qss_a_00286'), (0.14425759, '10.48550/arXiv.2502.03627'), (0.11645239, '10.1007/s11192-015-1765-5')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The timeout has elapsed!\n",
      "\u001b[96mWorking on row: 4\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.31222/osf.io/smxe5\\n Title: Crossref as a source of open bibliographic metadata\\nAbstract: Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1162/qss_a_00210\\n Title: The availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: Abstract Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: Abstract To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n', 'DOI: 10.1162/qss_a_00022\\n Title: Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.1162/qss_a_00286\\n Title: Completeness degree of publication metadata in eight free-access scholarly databases\\nAbstract: Abstract The main objective of this study is to compare the amount of metadata and the completeness degree of research publications in new academic databases. Using a quantitative approach, we selected a random Crossref sample of more than 115,000 records, which was then searched in seven databases (Dimensions, Google Scholar, Microsoft Academic, OpenAlex, Scilit, Semantic Scholar, and The Lens). Seven characteristics were analyzed (abstract, access, bibliographic info, document type, publication date, language, and identifiers), to observe fields that describe this information, the completeness rate of these fields, and the agreement among databases. The results show that academic search engines (Google Scholar, Microsoft Academic, and Semantic Scholar) gather less information and have a low degree of completeness. Conversely, third-party databases (Dimensions, OpenAlex, Scilit, and The Lens) have more metadata quality and a higher completeness rate. We conclude that academic search engines lack the ability to retrieve reliable descriptive data by crawling the web, and the main problem of third-party databases is the loss of information derived from integrating different sources.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.31222/osf.io/smxe5', '10.1162/qss_a_00210', '10.1162/qss_a_00212', '10.1162/qss_a_00022', '10.1162/qss_a_00286']\n",
      "\u001b[93mMy lady, here are some papers that address funding metadata, its availability, or its analysis in scholarly databases:\n",
      "\n",
      "## Summary: Papers discussing the availability and completeness of funding metadata in scholarly databases.\n",
      "\n",
      "## DOI: 10.31222/osf.io/smxe5\n",
      "This paper presents an overview of the availability of six metadata elements in Crossref, including funding information. The analysis shows that the availability of these metadata elements has improved over time, but many publishers need to make additional efforts to achieve full openness of bibliographic metadata.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00210\n",
      "This paper examines the availability and completeness of open funder metadata for publications funded by the Dutch Research Council. It highlights the challenges in assessing coverage due to the unknown number of articles resulting from funded research. The study found that only 67% of the analysed articles contained funding information in Crossref, with variations in the acknowledgement of the funder name and Funder IDs.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00212\n",
      "Focusing on funding data for COVID-19-related publications, this paper explores the open availability of funding data in Crossref and compares it with proprietary databases like Scopus and Web of Science. The analysis reveals limited coverage and quality issues with funding data, particularly in Scopus. Recommendations are provided to enhance the open availability of funding data in Crossref.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00022\n",
      "This paper describes the scholarly metadata collected and made available by Crossref, including funding information. It highlights the importance of Crossref's metadata in the research ecosystem, containing over 106 million records and covering various content types. The paper discusses the metadata's availability, its role in research, and plans for improving metadata quality and retrieval.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00286\n",
      "This study compares the amount and completeness of metadata in new academic databases. By analysing a random sample of Crossref records in seven databases, it assesses the completeness rate and agreement among databases for various metadata fields. The results indicate that academic search engines have lower metadata quality and completeness compared to third-party databases.\n",
      "45\n",
      "For query: ['Which papers address funding metadata, its availability, or its analysis in scholarly databases?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.5602773, '10.31222/osf.io/smxe5'), (0.53181785, '10.1162/qss_a_00210'), (0.5172057, '10.1162/qss_a_00212'), (0.4797236, '10.1162/qss_a_00022'), (0.3355473, '10.1162/qss_a_00286')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The timeout has elapsed!\n",
      "\u001b[96mWorking on row: 5\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2312.10997\\n Title: Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', 'DOI: 10.1609/aaai.v38i16.29728\\n Title: Benchmarking Large Language Models in Retrieval-Augmented Generation\\nAbstract: Retrieval-Augmented Generation (RAG) is a promising approach for mitigating the hallucination of large language models (LLMs). However, existing research lacks rigorous evaluation of the impact of retrieval-augmented generation on different large language models, which make it challenging to identify the potential bottlenecks in the capabilities of RAG for different LLMs. In this paper, we systematically investigate the impact of Retrieval-Augmented Generation on large language models. We analyze the performance of different large language models in 4 fundamental abilities required for RAG, including noise robustness, negative rejection, information integration, and counterfactual robustness. To this end, we establish Retrieval-Augmented Generation Benchmark (RGB), a new corpus for RAG evaluation in both English and Chinese. RGB divides the instances within the benchmark into 4 separate testbeds based on the aforementioned fundamental abilities required to resolve the case. Then we evaluate 6 representative LLMs on RGB to diagnose the challenges of current LLMs when applying RAG. Evaluation reveals that while LLMs exhibit a certain degree of noise robustness, they still struggle significantly in terms of negative rejection, information integration, and dealing with false information. The aforementioned assessment outcomes indicate that there is still a considerable journey ahead to effectively apply RAG to LLMs.\\n', 'DOI: 10.1007/978-3-031-88708-6_3\\n Title: Is Relevance Propagated from Retriever to Generator in RAG?\\nAbstract: Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\\n', \"DOI: 10.48550/arXiv.2404.13948\\n Title: Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\\nAbstract: The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\\\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2312.10997', '10.48550/arXiv.2406.13213', '10.1609/aaai.v38i16.29728', '10.1007/978-3-031-88708-6_3', '10.48550/arXiv.2404.13948']\n",
      "\u001b[93mMy lady, here are some papers discussing the use of Retrieval-Augmented Generation (RAG) in large language models or related applications:\n",
      "\n",
      "## Summary:\n",
      "RAG is a promising approach to enhancing the capabilities of large language models (LLMs) by incorporating external knowledge.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2312.10997\n",
      "This paper offers a comprehensive survey of RAG paradigms, examining the progression of RAG frameworks and their critical components. It highlights the benefits of RAG, such as improved accuracy and credibility, particularly for knowledge-intensive tasks.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2406.13213\n",
      "The paper introduces Multi-Meta-RAG, a method that improves RAG for multi-hop queries using database filtering with LLM-extracted metadata. It demonstrates the effectiveness of Multi-Meta-RAG in selecting relevant documents from various sources.\n",
      "\n",
      "## DOI: 10.1609/aaai.v38i16.29728\n",
      "Here, the paper investigates the impact of RAG on LLMs, analyzing their performance in fundamental abilities required for RAG. It establishes the Retrieval-Augmented Generation Benchmark (RGB) to evaluate and diagnose the challenges of applying RAG to LLMs.\n",
      "\n",
      "## DOI: 10.1007/978-3-031-88708-6_3\n",
      "This paper explores the role of relevance in RAG, specifically examining whether a RAG context comprised of topically relevant documents leads to improved downstream performance. The findings suggest a small positive correlation between relevance and utility.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.13948\n",
      "The paper investigates the robustness of RAG, introducing a novel attack method called GARAG to reveal vulnerabilities within RAG components. The experimental results highlight the risk of minor textual inaccuracies in disrupting RAG systems.\n",
      "45\n",
      "For query: ['Which papers discuss the use of Retrieval-Augmented Generation (RAG) in large language models or related applications?']:\n",
      "Precision: 0.956\n",
      "Recall: 0.956\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.887\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.910998, '10.48550/arXiv.2312.10997'), (0.8565368, '10.48550/arXiv.2406.13213'), (0.8536326, '10.1609/aaai.v38i16.29728'), (0.7763013, '10.1007/978-3-031-88708-6_3'), (0.7300017, '10.48550/arXiv.2404.13948')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The timeout has elapsed!\n",
      "\u001b[96mWorking on row: 6\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1162/qss_a_00022\\n Title: Crossref: The sustainable source of community-owned scholarly metadata\\nAbstract: This paper describes the scholarly metadata collected and made available by Crossref, as well as its importance in the scholarly research ecosystem. Containing over 106 million records and expanding at an average rate of 11% a year, Crossref’s metadata has become one of the major sources of scholarly data for publishers, authors, librarians, funders, and researchers. The metadata set consists of 13 content types, including not only traditional types, such as journals and conference papers, but also data sets, reports, preprints, peer reviews, and grants. The metadata is not limited to basic publication metadata, but can also include abstracts and links to full text, funding and license information, citation links, and the information about corrections, updates, retractions, etc. This scale and breadth make Crossref a valuable source for research in scientometrics, including measuring the growth and impact of science and understanding new trends in scholarly communications. The metadata is available through a number of APIs, including REST API and OAI-PMH. In this paper, we describe the kind of metadata that Crossref provides and how it is collected and curated. We also look at Crossref’s role in the research ecosystem and trends in metadata curation over the years, including the evolution of its citation data provision. We summarize the research used in Crossref’s metadata and describe plans that will improve metadata quality and retrieval in the future.\\n', 'DOI: 10.31222/osf.io/smxe5\\n Title: Crossref as a source of open bibliographic metadata\\nAbstract: Several initiatives have been taken to promote the open availability of bibliographic metadata of scholarly publications in Crossref. We present an up-to-date overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. Our analysis shows that the availability of these metadata elements has improved over time, at least for journal articles, the most common publication type in Crossref. However, the analysis also shows that many publishers need to make additional efforts to realize full openness of bibliographic metadata.\\n', 'DOI: 10.1162/qss_a_00210\\n Title: The availability and completeness of open funder metadata: Case study for publications funded by the Dutch Research Council\\nAbstract: Abstract Research funders spend considerable efforts collecting information on the outcomes of the research they fund. To help funders track publication output associated with their funding, Crossref initiated FundRef in 2013, enabling publishers to register funding information using persistent identifiers. However, it is hard to assess the coverage of funder metadata because it is unknown how many articles are the result of funded research and should therefore include funder metadata. In this paper we looked at 5,004 publications reported by researchers to be the result of funding by a specific funding agency: the Dutch Research Council NWO. Only 67% of these articles contain funding information in Crossref, with a subset acknowledging NWO as funder name and/or Funder IDs linked to NWO (53% and 45%, respectively). Web of Science (WoS), Scopus, and Dimensions are all able to infer additional funding information from funding statements in the full text of the articles. Funding information in Lens largely corresponds to that in Crossref, with some additional funding information likely taken from PubMed. We observe interesting differences between publishers in the coverage and completeness of funding metadata in Crossref compared to proprietary databases, highlighting the potential to increase the quality of open metadata on funding.\\n', 'DOI: 10.1162/qss_a_00212\\n Title: Funding COVID-19 research: Insights from an exploratory analysis using open data infrastructures\\nAbstract: Abstract To analyze the outcomes of the funding they provide, it is essential for funding agencies to be able to trace the publications resulting from their funding. We study the open availability of funding data in Crossref, focusing on funding data for publications that report research related to COVID-19. We also present a comparison with the funding data available in two proprietary bibliometric databases: Scopus and Web of Science. Our analysis reveals limited coverage of funding data in Crossref. It also shows problems related to the quality of funding data, especially in Scopus. We offer recommendations for improving the open availability of funding data in Crossref.\\n']\n",
      "length of reranked_documents: 4\n",
      "Retrieved DOIs: ['10.1162/qss_a_00022', '10.31222/osf.io/smxe5', '10.1162/qss_a_00210', '10.1162/qss_a_00212']\n",
      "\u001b[93mSummary: Crossref is a valuable source of scholarly metadata, containing over 106 million records and expanding at an average rate of 11% a year.\n",
      "\n",
      "DOI: 10.1162/qss_a_00022\n",
      "This paper describes the scholarly metadata collected and made available by Crossref, highlighting its importance in the research ecosystem. The metadata set includes 13 content types, such as journals, conference papers, data sets, reports, preprints, peer reviews, and grants. It provides abstracts, links to full text, funding and license information, citation links, and information about corrections, updates, and retractions. This comprehensive metadata is accessible through APIs, including REST API and OAI-PMH.\n",
      "\n",
      "DOI: 10.31222/osf.io/smxe5\n",
      "This source presents an overview of the availability of six metadata elements in Crossref: reference lists, abstracts, ORCIDs, author affiliations, funding information, and license information. It discusses the improved availability of these elements over time, particularly for journal articles, but also highlights the need for publishers to make further efforts to achieve full openness of bibliographic metadata.\n",
      "\n",
      "DOI: 10.1162/qss_a_00210\n",
      "This paper focuses on the availability and completeness of open funder metadata, specifically for publications funded by the Dutch Research Council. It introduces FundRef, initiated by Crossref in 2013, which allows publishers to register funding information using persistent identifiers. The analysis reveals that only 67% of the articles contain funding information in Crossref, with variations in coverage and completeness compared to proprietary databases.\n",
      "\n",
      "DOI: 10.1162/qss_a_00212\n",
      "This study examines the open availability of funding data in Crossref, particularly for COVID-19-related research. It compares the funding data with that of two proprietary bibliometric databases, Scopus and Web of Science. The analysis shows limited coverage and quality issues with funding data in Crossref, especially in Scopus. The paper offers recommendations to enhance the open availability of funding data.\n",
      "My lady, Crossref plays a crucial role in the scholarly research ecosystem by providing a vast and expanding collection of metadata, offering valuable insights and resources for researchers, authors, librarians, and funders.\n",
      "45\n",
      "For query: ['What is Crossref’s role in the scholarly research ecosystem?']:\n",
      "Precision: 0.978\n",
      "Recall: 0.978\n",
      "F1-Score: 0.978\n",
      "Accuracy: 0.978\n",
      "Balanced accuracy: 0.900\n",
      "Faithfulness score: 4\n",
      "Documents score: [(0.95696133, '10.1162/qss_a_00022'), (0.46774563, '10.31222/osf.io/smxe5'), (0.22025895, '10.1162/qss_a_00210'), (0.19019738, '10.1162/qss_a_00212')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The timeout has elapsed!\n",
      "\u001b[96mWorking on row: 7\n",
      "Length of documents: 45\n",
      "reranked_documents: [\"DOI: 10.48550/arXiv.2404.17663\\n Title: An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2401.16359\\n Title: Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n', \"DOI: 10.48550/arXiv.2409.10633\\n Title: Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness\\nAbstract: Clarivate's Web of Science (WoS) and Elsevier's Scopus have been for decades the main sources of bibliometric information. Although highly curated, these closed, proprietary databases are largely biased towards English-language publications, underestimating the use of other languages in research dissemination. Launched in 2022, OpenAlex promised comprehensive, inclusive, and open-source research information. While already in use by scholars and research institutions, the quality of its metadata is currently being assessed. This paper contributes to this literature by assessing the completeness and accuracy of OpenAlex's metadata related to language, through a comparison with WoS, as well as an in-depth manual validation of a sample of 6,836 articles. Results show that OpenAlex exhibits a far more balanced linguistic coverage than WoS. However, language metadata is not always accurate, which leads OpenAlex to overestimate the place of English while underestimating that of other languages. If used critically, OpenAlex can provide comprehensive and representative analyses of languages used for scholarly publishing. However, more work is needed at infrastructural level to ensure the quality of metadata on language.\\n\", 'DOI: 10.3145/epi.2023.mar.09\\n Title: Which of the metadata with relevance for bibliometrics are the same and which are different when switching from Microsoft Academic Graph to OpenAlex?\\nAbstract: With the announcement of the retirement of Microsoft Academic Graph (MAG), the non-profit organization OurResearch announced that they would provide a similar resource under the name OpenAlex. Thus, we compare the metadata with relevance to bibliometric analyses of the latest MAG snapshot with an early OpenAlex snapshot. Practically all works from MAG were transferred to OpenAlex preserving their bibliographic data publication year, volume, first and last page, DOI as well as the number of references that are important ingredients of citation analysis. More than 90% of the MAG documents have equivalent document types in OpenAlex. Of the remaining ones, especially reclassifications to the OpenAlex document types journal-article and book-chapter seem to be correct and amount to more than 7%, so that the document type specifications have improved significantly from MAG to OpenAlex. As another item of bibliometric relevant metadata, we looked at the paper-based subject classification in MAG and in OpenAlex. We found significantly more documents with a subject classification assignment in OpenAlex than in MAG. On the first and second level, the classification structure is nearly identical. We present data on the subject reclassifications on both levels in tabular and graphical form. The assessment of the consequences of the abundant subject reclassifications on field-normalized bibliometric evaluations is not in the scope of the present paper. Apart from this open question, OpenAlex seems to be overall at least as suited for bibliometric analyses as MAG for publication years before 2021 or maybe even better because of the broader coverage of document type assignments.\\n', 'DOI: 10.48550/arXiv.2406.15154\\n Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2401.16359', '10.48550/arXiv.2409.10633', '10.3145/epi.2023.mar.09', '10.48550/arXiv.2406.15154']\n",
      "\u001b[93mSummary: OpenAlex is a promising open-source database for bibliometric analyses, offering a reliable alternative to established proprietary sources like Scopus and the Web of Science. However, its rapidly evolving nature and potential issues with metadata accuracy and completeness require further research to fully understand and address its limitations.\n",
      "\n",
      "DOI: 10.48550/arXiv.2404.17663\n",
      "This study compares OpenAlex with Scopus across various dimensions. It concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for certain analyses, especially at the country level. However, concerns about metadata accuracy and completeness persist, necessitating additional research to confidently expand its usage.\n",
      "\n",
      "DOI: 10.48550/arXiv.2401.16359\n",
      "This report compares OpenAlex's reference coverage and selected metadata with those of the Web of Science and Scopus. When restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex demonstrates average source reference numbers and internal coverage rates comparable to its competitors. The analysis also reveals mixed results for other core metadata, capturing more ORCID identifiers, fewer abstracts, and a similar number of Open Access status indicators per article.\n",
      "\n",
      "DOI: 10.48550/arXiv.2409.10633\n",
      "This paper assesses the completeness and accuracy of OpenAlex's metadata related to language. It finds that OpenAlex offers a more balanced linguistic coverage than the Web of Science (WoS), but language metadata accuracy remains an issue, leading to an overestimation of English and an underestimation of other languages. Critical usage of OpenAlex can provide comprehensive and representative language analyses, but infrastructural improvements are needed to enhance metadata quality.\n",
      "\n",
      "DOI: 10.3145/epi.2023.mar.09\n",
      "This study compares the metadata of OpenAlex with that of the Microsoft Academic Graph (MAG). It finds that OpenAlex preserves most of MAG's bibliographic data, including publication year, volume, page range, DOI, and reference counts. More than 90% of MAG documents have equivalent document types in OpenAlex, with improvements in document type specifications. OpenAlex also has more documents with subject classification assignments, especially on the first and second levels, making it at least as suitable as MAG for bibliometric analyses, and potentially even better due to its broader coverage.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.15154\n",
      "This study compares publication and document types across various bibliographic databases, including OpenAlex. It highlights the variability in typologies and the distinction between research and non-research texts across different database providers. OpenAlex is gaining importance as a free alternative to established proprietary sources for bibliometric analyses in libraries and universities.\n",
      "\n",
      "My lady, I have presented the key features and limitations of OpenAlex as a bibliometric database, as per your request.\n",
      "45\n",
      "For query: ['What are the key features and limitations of OpenAlex as a bibliometric database?']:\n",
      "Precision: 0.956\n",
      "Recall: 0.956\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.887\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.8364614, '10.48550/arXiv.2404.17663'), (0.54695654, '10.48550/arXiv.2401.16359'), (0.4784076, '10.48550/arXiv.2409.10633'), (0.47533798, '10.3145/epi.2023.mar.09'), (0.47241634, '10.48550/arXiv.2406.15154')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The timeout has elapsed!\n",
      "\u001b[96mWorking on row: 8\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/s11192-015-1765-5\\n Title: The journal coverage of Web of Science and Scopus: a comparative analysis\\nAbstract: Bibliometric methods are used in multiple fields for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some field, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different fields, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientific output that are not covered in WoS or Scopus, such as field-specific and national citation indexes.\\n', 'DOI: 10.1162/qss_a_00112\\n Title: Large-scale comparison of bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic\\nAbstract: Abstract We present a large-scale comparison of five multidisciplinary bibliographic data sources: Scopus, Web of Science, Dimensions, Crossref, and Microsoft Academic. The comparison considers scientific documents from the period 2008–2017 covered by these data sources. Scopus is compared in a pairwise manner with each of the other data sources. We first analyze differences between the data sources in the coverage of documents, focusing for instance on differences over time, differences per document type, and differences per discipline. We then study differences in the completeness and accuracy of citation links. Based on our analysis, we discuss the strengths and weaknesses of the different data sources. We emphasize the importance of combining a comprehensive coverage of the scientific literature with a flexible set of filters for making selections of the literature.\\n', \"DOI: 10.48550/arXiv.2404.17663\\n Title: An analysis of the suitability of OpenAlex for bibliometric analyses\\nAbstract: Scopus and the Web of Science have been the foundation for research in the science of science even though these traditional databases systematically underrepresent certain disciplines and world regions. In response, new inclusive databases, notably OpenAlex, have emerged. While many studies have begun using OpenAlex as a data source, few critically assess its limitations. This study, conducted in collaboration with the OpenAlex team, addresses this gap by comparing OpenAlex to Scopus across a number of dimensions. The analysis concludes that OpenAlex is a superset of Scopus and can be a reliable alternative for some analyses, particularly at the country level. Despite this, issues of metadata accuracy and completeness show that additional research is needed to fully comprehend and address OpenAlex's limitations. Doing so will be necessary to confidently use OpenAlex across a wider set of analyses, including those that are not at all possible with more constrained databases.\\n\", 'DOI: 10.48550/arXiv.2406.15154\\n Title: Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar\\nAbstract: This study compares and analyses publication and document types in the following bibliographic databases: OpenAlex, Scopus, Web of Science, Semantic Scholar and PubMed. The results demonstrate that typologies can differ considerably between individual database providers. Moreover, the distinction between research and non-research texts, which is required to identify relevant documents for bibliometric analysis, can vary depending on the data source because publications are classified differently in the respective databases. The focus of this study, in addition to the cross-database comparison, is primarily on the coverage and analysis of the publication and document types contained in OpenAlex, as OpenAlex is becoming increasingly important as a free alternative to established proprietary providers for bibliometric analyses at libraries and universities.\\n', 'DOI: 10.48550/arXiv.2401.16359\\n Title: Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus\\nAbstract: OpenAlex is a promising open source of scholarly metadata, and competitor to established proprietary sources, such as the Web of Science and Scopus. As OpenAlex provides its data freely and openly, it permits researchers to perform bibliometric studies that can be reproduced in the community without licensing barriers. However, as OpenAlex is a rapidly evolving source and the data contained within is expanding and also quickly changing, the question naturally arises as to the trustworthiness of its data. In this report, we will study the reference coverage and selected metadata within each database and compare them with each other to help address this open question in bibliometrics. In our large-scale study, we demonstrate that, when restricted to a cleaned dataset of 16.8 million recent publications shared by all three databases, OpenAlex has average source reference numbers and internal coverage rates comparable to both Web of Science and Scopus. We further analyse the metadata in OpenAlex, the Web of Science and Scopus by journal, finding a similarity in the distribution of source reference counts in the Web of Science and Scopus as compared to OpenAlex. We also demonstrate that the comparison of other core metadata covered by OpenAlex shows mixed results when broken down by journal, capturing more ORCID identifiers, fewer abstracts and a similar number of Open Access status indicators per article when compared to both the Web of Science and Scopus.\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1007/s11192-015-1765-5', '10.1162/qss_a_00112', '10.48550/arXiv.2404.17663', '10.48550/arXiv.2406.15154', '10.48550/arXiv.2401.16359']\n",
      "\u001b[93mMy lady, here is a summary of the strengths and weaknesses of Web of Science (WoS) as a bibliometric database:\n",
      "\n",
      "## Summary:\n",
      "WoS is a widely used bibliometric database, but it has some limitations and biases that researchers should be aware of when conducting research evaluation.\n",
      "\n",
      "## DOI: 10.1007/s11192-015-1765-5\n",
      "This study compares the journal coverage of WoS and Scopus, finding that both databases share biases that favour Natural Sciences, Engineering, and Biomedical Research over Social Sciences, Arts, and Humanities. English-language journals are also overrepresented. The results suggest that WoS and Scopus should be used with caution when comparing different fields, institutions, countries, or languages.\n",
      "\n",
      "## DOI: 10.1162/qss_a_00112\n",
      "This large-scale comparison of bibliographic data sources emphasises the importance of combining comprehensive coverage of scientific literature with flexible filters for selecting literature. WoS is one of the data sources analysed, but the specific strengths and weaknesses mentioned are not clear from this abstract.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.17663\n",
      "This study compares OpenAlex with Scopus and WoS, concluding that OpenAlex is a reliable alternative for some analyses, particularly at the country level. However, issues of metadata accuracy and completeness mean that further research is needed to fully understand and address OpenAlex's limitations.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2406.15154\n",
      "This study compares publication and document types in various bibliographic databases, including WoS. It highlights that typologies can differ between database providers, and the distinction between research and non-research texts can vary, impacting the identification of relevant documents for bibliometric analysis.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2401.16359\n",
      "This report analyses the reference coverage and metadata of OpenAlex compared to WoS and Scopus. It finds that OpenAlex has average source reference numbers and internal coverage rates comparable to WoS and Scopus when restricted to a cleaned dataset of recent publications. However, the comparison of core metadata shows mixed results, with OpenAlex capturing more ORCID identifiers but fewer abstracts.\n",
      "45\n",
      "For query: ['What are the strengths and weaknesses of Web of Science (WoS) as a bibliometric database?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.47022632, '10.1007/s11192-015-1765-5'), (0.36141315, '10.1162/qss_a_00112'), (0.15583521, '10.48550/arXiv.2404.17663'), (0.14520046, '10.48550/arXiv.2406.15154'), (0.13098438, '10.48550/arXiv.2401.16359')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The timeout has elapsed!\n",
      "\u001b[96mWorking on row: 9\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/978-3-031-88708-6_3\\n Title: Is Relevance Propagated from Retriever to Generator in RAG?\\nAbstract: Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\\n', 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n', 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', \"DOI: 10.48550/arXiv.2505.18247\\n Title: MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\\nAbstract: Retrieval-Augmented Generation (RAG) struggles with domain-specific enterprise datasets, often isolated behind firewalls and rich in complex, specialized terminology unseen by LLMs during pre-training. Semantic variability across domains like medicine, networking, or law hampers RAG's context precision, while fine-tuning solutions are costly, slow, and lack generalization as new data emerges. Achieving zero-shot precision with retrievers without fine-tuning still remains a key challenge. We introduce 'MetaGen Blended RAG', a novel enterprise search approach that enhances semantic retrievers through a metadata generation pipeline and hybrid query indexes using dense and sparse vectors. By leveraging key concepts, topics, and acronyms, our method creates metadata-enriched semantic indexes and boosted hybrid queries, delivering robust, scalable performance without fine-tuning. On the biomedical PubMedQA dataset, MetaGen Blended RAG achieves 82% retrieval accuracy and 77% RAG accuracy, surpassing all prior zero-shot RAG benchmarks and even rivaling fine-tuned models on that dataset, while also excelling on datasets like SQuAD and NQ. This approach redefines enterprise search using a new approach to building semantic retrievers with unmatched generalization across specialized domains.\\n\", \"DOI: 10.48550/arXiv.2312.10997\\n Title: Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\"]\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1007/978-3-031-88708-6_3', '10.6109/jkiice.2023.27.12.1489', '10.48550/arXiv.2406.13213', '10.48550/arXiv.2505.18247', '10.48550/arXiv.2312.10997']\n",
      "\u001b[93mMy lady, RAG (Retrieval Augmented Generation) is a framework that incorporates external knowledge, typically in the form of a set of documents retrieved from a collection, as part of a prompt to a large language model (LLM). This approach aims to enhance the performance of downstream tasks, such as question answering and information retrieval.\n",
      "\n",
      "Here's how RAG improves these systems:\n",
      "\n",
      "Summary: RAG enhances the accuracy and credibility of generation by merging LLMs' knowledge with external databases, particularly for knowledge-intensive tasks.\n",
      "\n",
      "DOI: 10.48550/arXiv.2312.10997\n",
      "Title: Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "Abstract: This paper highlights how RAG addresses challenges faced by LLMs, such as hallucination and outdated knowledge, by incorporating external knowledge. It provides a comprehensive review of RAG paradigms and their components, offering insights into the advancements and challenges in RAG systems.\n",
      "\n",
      "DOI: 10.1007/978-3-031-88708-6_3\n",
      "Title: Is Relevance Propagated from Retriever to Generator in RAG?\n",
      "Abstract: This study investigates the role of relevance in RAG contexts for knowledge-intensive language tasks. It empirically examines whether topically relevant documents lead to improved downstream performance, finding a small positive correlation between relevance and utility.\n",
      "\n",
      "DOI: 10.48550/arXiv.2505.18247\n",
      "Title: MetaGen Blended RAG: Unlocking Zero-Shot Precision for Specialized Domain Question-Answering\n",
      "Abstract: MetaGen Blended RAG is introduced to enhance RAG's performance on domain-specific datasets. It achieves impressive retrieval and RAG accuracy on various datasets, showcasing its ability to handle specialized domains without fine-tuning.\n",
      "\n",
      "DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "Title: M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\n",
      "Abstract: M-RAG utilizes metadata and generative models to improve open-domain question answering. Experimental results demonstrate significant performance improvements compared to existing RAG methods.\n",
      "\n",
      "DOI: 10.48550/arXiv.2406.13213\n",
      "Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\n",
      "Abstract: Multi-Meta-RAG is proposed to address the limitations of traditional RAG in answering multi-hop questions. It uses database filtering and LLM-extracted metadata to improve the selection of relevant documents, leading to better results on the MultiHop-RAG benchmark.\n",
      "45\n",
      "For query: ['How is RAG used to improve question answering or information retrieval systems?']:\n",
      "Precision: 0.956\n",
      "Recall: 0.956\n",
      "F1-Score: 0.956\n",
      "Accuracy: 0.956\n",
      "Balanced accuracy: 0.887\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.8190992, '10.1007/978-3-031-88708-6_3'), (0.76826453, '10.6109/jkiice.2023.27.12.1489'), (0.7166357, '10.48550/arXiv.2406.13213'), (0.5843666, '10.48550/arXiv.2505.18247'), (0.3127692, '10.48550/arXiv.2312.10997')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The timeout has elapsed!\n",
      "\u001b[96mWorking on row: 10\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1371/journal.pbio.1002542\\n Title: Citation Metrics: A Primer on How (Not) to Normalize\\nAbstract: Citation metrics are increasingly used to appraise published research. One challenge is whether and how to normalize these metrics to account for differences across scientific fields, age (year of publication), type of document, database coverage, and other factors. We discuss the pros and cons for normalizations using different approaches. Additional challenges emerge when citation metrics need to be combined across multiple papers to appraise the corpus of scientists, institutions, journals, or countries, as well as when trying to attribute credit in multiauthored papers. Different citation metrics may offer complementary insights, but one should carefully consider the assumptions that underlie their calculation.\\n']\n",
      "length of reranked_documents: 1\n",
      "Retrieved DOIs: ['10.1371/journal.pbio.1002542']\n",
      "\u001b[93mMy lady, the main challenges in normalizing citation metrics across scientific fields include:\n",
      "\n",
      "- **Differences across scientific fields**: Citation metrics need to account for variations in research practices, publication rates, and citation patterns between different disciplines.\n",
      "- **Age (year of publication)**: Normalization must consider the age of publications, as older papers may have accumulated more citations over time, potentially skewing the metrics.\n",
      "- **Type of document**: The type of document, such as research articles, reviews, or book chapters, can influence citation patterns and should be factored into normalization.\n",
      "- **Database coverage**: Incomplete or inconsistent database coverage can affect the accuracy of citation metrics, especially when combining data from multiple sources.\n",
      "- **Combining metrics across multiple papers**: When assessing the corpus of scientists, institutions, journals, or countries, challenges arise in attributing credit in multiauthored papers and combining different citation metrics.\n",
      "\n",
      "DOI: 10.1371/journal.pbio.1002542\n",
      "This text discusses the challenges and considerations in normalizing citation metrics, highlighting the need to carefully evaluate the assumptions underlying their calculation.\n",
      "45\n",
      "For query: ['What are the main challenges in normalizing citation metrics across scientific fields?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.600\n",
      "Faithfulness score: 1\n",
      "Documents score: [(0.93694025, '10.1371/journal.pbio.1002542')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The timeout has elapsed!\n",
      "\u001b[96mWorking on row: 11\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.5281/ZENODO.13960973\\n Title: Using missing data patterns to detect incorrectly assigned articles in bibliographic datasets\\nAbstract: The DORA declaration and CoARA call for the use of bibliometric indicators based on open data. However, established scholarly metadata datasets are closed, and the quality of open datasets has not yet been thoroughly examined. In this paper, I present a method to detect errors in a dataset using missing data patterns. As an example, the method is applied to the affiliation metadata of publications associated with ETH Zurich. This allows me to identify a series of incorrectly affiliated papers. The method introduced in this paper is not specifically designed for affiliation data and can also be used to detect errors in other types of data. It could lead to corrections which will hopefully benefit providers as well as users of data.\\n', 'DOI: 10.48550/arXiv.2303.17661\\n Title: MetaEnhance: Metadata Quality Improvement for Electronic Theses and Dissertations of University Libraries\\nAbstract: Metadata quality is crucial for digital objects to be discovered through digital library interfaces. However, due to various reasons, the metadata of digital objects often exhibits incomplete, inconsistent, and incorrect values. We investigate methods to automatically detect, correct, and canonicalize scholarly metadata, using seven key fields of electronic theses and dissertations (ETDs) as a case study. We propose MetaEnhance, a framework that utilizes state-of-the-art artificial intelligence methods to improve the quality of these fields. To evaluate MetaEnhance, we compiled a metadata quality evaluation benchmark containing 500 ETDs, by combining subsets sampled using multiple criteria. We tested MetaEnhance on this benchmark and found that the proposed methods achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\\n', 'DOI: 10.1007/s11192-022-04367-w\\n Title: Identifying and correcting invalid citations due to DOI errors in Crossref data\\nAbstract: This work aims to identify classes of DOI mistakes by analysing the open bibliographic metadata available in Crossref, highlighting which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes. By using a list of invalid cited DOIs gathered by OpenCitations while processing the OpenCitations Index of Crossref open DOI-to-DOI citations (COCI) in the past two years, we retrieved the citations in the January 2021 Crossref dump to such invalid DOIs. We processed these citations by keeping track of their validity and the publishers responsible for uploading the related citation data in Crossref. Finally, we identified patterns of factual errors in the invalid DOIs and the regular expressions needed to catch and correct them. The outcomes of this research show that only a few publishers were responsible for and/or affected by the majority of invalid citations. We extended the taxonomy of DOI name errors proposed in past studies and defined more elaborated regular expressions that can clean a higher number of mistakes in invalid DOIs than prior approaches. The data gathered in our study can enable investigating possible reasons for DOI mistakes from a qualitative point of view, helping publishers identify the problems underlying their production of invalid citation data. Also, the DOI cleaning mechanism we present could be integrated into the existing process (e.g. in COCI) to add citations by automatically correcting a wrong DOI. This study was run strictly following Open Science principles, and, as such, our research outcomes are fully reproducible.\\n']\n",
      "length of reranked_documents: 3\n",
      "Retrieved DOIs: ['10.5281/ZENODO.13960973', '10.48550/arXiv.2303.17661', '10.1007/s11192-022-04367-w']\n",
      "\u001b[93mMy lady, here are some methods used to detect and correct errors in bibliographic datasets:\n",
      "\n",
      "## Summary:\n",
      "Artificial intelligence methods can be used to detect, correct, and improve the quality of metadata in electronic theses and dissertations.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2303.17661\n",
      "The paper proposes MetaEnhance, a framework that utilises state-of-the-art artificial intelligence methods to improve the quality of metadata in electronic theses and dissertations. The framework achieved nearly perfect F1-scores in detecting errors and F1-scores in correcting errors ranging from 0.85 to 1.00 for five of seven fields.\n",
      "\n",
      "## DOI: 10.5281/ZENODO.13960973\n",
      "This paper presents a method to detect errors in a dataset using missing data patterns. The method is applied to the affiliation metadata of publications associated with ETH Zurich, allowing the identification of incorrectly affiliated papers.\n",
      "\n",
      "## DOI: 10.1007/s11192-022-04367-w\n",
      "This work aims to identify and correct invalid citations due to DOI errors in Crossref data. By analysing open bibliographic metadata available in Crossref, the study highlights which publishers were responsible for such mistakes and how many of these incorrect DOIs could be corrected through automatic processes.\n",
      "45\n",
      "For query: ['What methods are used to detect and correct errors in bibliographic datasets?']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.688\n",
      "Faithfulness score: 3\n",
      "Documents score: [(0.73846215, '10.5281/ZENODO.13960973'), (0.31897336, '10.48550/arXiv.2303.17661'), (0.25660416, '10.1007/s11192-022-04367-w')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The timeout has elapsed!\n",
      "\u001b[96mWorking on row: 12\n",
      "Length of documents: 45\n",
      "reranked_documents: ['DOI: 10.1007/978-3-031-88708-6_3\\n Title: Is Relevance Propagated from Retriever to Generator in RAG?\\nAbstract: Retrieval Augmented Generation (RAG) is a framework for incorporating external knowledge, usually in the form of a set of documents retrieved from a collection, as a part of a prompt to a large language model (LLM) to potentially improve the performance of a downstream task, such as question answering. Different from a standard retrieval task’s objective of maximising the relevance of a set of top-ranked documents, a RAG system’s objective is rather to maximise their total utility, where the utility of a document indicates whether including it as a part of the additional contextual information in an LLM prompt improves a downstream task. Existing studies investigate the role of the relevance of a RAG context for knowledge-intensive language tasks (KILT), where relevance essentially takes the form of answer containment. In contrast, in our work, relevance corresponds to that of topical overlap between a query and a document for an information seeking task. Specifically, we make use of an IR test collection to empirically investigate whether a RAG context comprised of topically relevant documents leads to improved downstream performance. Our experiments lead to the following findings: (a) there is a small positive correlation between relevance and utility; (b) this correlation decreases with increasing context sizes (higher values of k in k-shot); and (c) a more effective retrieval model generally leads to better downstream RAG performance.\\n', \"DOI: 10.48550/arXiv.2312.10997\\n Title: Retrieval-Augmented Generation for Large Language Models: A Survey\\nAbstract: Large Language Models (LLMs) showcase impressive capabilities but encounter challenges like hallucination, outdated knowledge, and non-transparent, untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases. This enhances the accuracy and credibility of the generation, particularly for knowledge-intensive tasks, and allows for continuous knowledge updates and integration of domain-specific information. RAG synergistically merges LLMs' intrinsic knowledge with the vast, dynamic repositories of external databases. This comprehensive review paper offers a detailed examination of the progression of RAG paradigms, encompassing the Naive RAG, the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the tripartite foundation of RAG frameworks, which includes the retrieval, the generation and the augmentation techniques. The paper highlights the state-of-the-art technologies embedded in each of these critical components, providing a profound understanding of the advancements in RAG systems. Furthermore, this paper introduces up-to-date evaluation framework and benchmark. At the end, this article delineates the challenges currently faced and points out prospective avenues for research and development.\\n\", 'DOI: 10.48550/arXiv.2406.13213\\n Title: Multi-Meta-RAG: Improving RAG for Multi-Hop Queries using Database Filtering with LLM-Extracted Metadata\\nAbstract: The retrieval-augmented generation (RAG) enables retrieval of relevant information from an external knowledge source and allows large language models (LLMs) to answer queries over previously unseen document collections. However, it was demonstrated that traditional RAG applications perform poorly in answering multi-hop questions, which require retrieving and reasoning over multiple elements of supporting evidence. We introduce a new method called Multi-Meta-RAG, which uses database filtering with LLM-extracted metadata to improve the RAG selection of the relevant documents from various sources, relevant to the question. While database filtering is specific to a set of questions from a particular domain and format, we found out that Multi-Meta-RAG greatly improves the results on the MultiHop-RAG benchmark. The code is available at https://github.com/mxpoliakov/Multi-Meta-RAG.\\n', \"DOI: 10.48550/arXiv.2404.13948\\n Title: Typos that Broke the RAG's Back: Genetic Attack on RAG Pipeline by Simulating Documents in the Wild via Low-level Perturbations\\nAbstract: The robustness of recent Large Language Models (LLMs) has become increasingly crucial as their applicability expands across various domains and real-world applications. Retrieval-Augmented Generation (RAG) is a promising solution for addressing the limitations of LLMs, yet existing studies on the robustness of RAG often overlook the interconnected relationships between RAG components or the potential threats prevalent in real-world databases, such as minor textual errors. In this work, we investigate two underexplored aspects when assessing the robustness of RAG: 1) vulnerability to noisy documents through low-level perturbations and 2) a holistic evaluation of RAG robustness. Furthermore, we introduce a novel attack method, the Genetic Attack on RAG (\\\\textit{GARAG}), which targets these aspects. Specifically, GARAG is designed to reveal vulnerabilities within each component and test the overall system functionality against noisy documents. We validate RAG robustness by applying our \\\\textit{GARAG} to standard QA datasets, incorporating diverse retrievers and LLMs. The experimental results show that GARAG consistently achieves high attack success rates. Also, it significantly devastates the performance of each component and their synergy, highlighting the substantial risk that minor textual inaccuracies pose in disrupting RAG systems in the real world.\\n\", 'DOI: 10.6109/jkiice.2023.27.12.1489\\n Title: M-RAG: Enhancing Open-domain Question Answering with Metadata Retrieval-Augmented Generation\\nAbstract: This paper proposes a method called Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain Question Answering (ODQA) systems for one or more documents and compares its performance. To achieve this, it utilizes embeddings that include metadata and employs generative models such as gpt-3.5-turbo-16k and gpt-4 for automated answer generation. Through this approach, the generative models (gpt-3.5, gpt-4) are able to understand the order and context of query documents through metadata. Additionally, by incorporating source information and original text requirements through prompt engineering, it activates source attribution capabilities in question-answering (QA), thereby enhancing answer accuracy. As a result of this paper, information that LLM does not have can be retrieved from external sources and an appropriate response can be found.. Experimental results show that this method exhibited up to a 46% performance improvement compared to the same external inference ODQA system and a 6% improvement over the existing RAG method\\n']\n",
      "length of reranked_documents: 5\n",
      "Retrieved DOIs: ['10.1007/978-3-031-88708-6_3', '10.48550/arXiv.2312.10997', '10.48550/arXiv.2406.13213', '10.48550/arXiv.2404.13948', '10.6109/jkiice.2023.27.12.1489']\n",
      "\u001b[93m## Summary: Retrieval-Augmented Generation (RAG) is a framework that enhances the capabilities of large language models (LLMs) by incorporating external knowledge.\n",
      "\n",
      "## DOI: 10.1007/978-3-031-88708-6_3\n",
      "RAG aims to improve the performance of LLMs in downstream tasks, such as question answering, by including relevant external documents in the prompt. The objective is to maximise the utility of these documents, where utility indicates their value in enhancing the LLM's performance. Studies investigate the role of relevance in RAG contexts for knowledge-intensive tasks, with relevance defined as answer containment or topical overlap.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2312.10997\n",
      "This review paper examines the progression of RAG paradigms, including Naive RAG, Advanced RAG, and Modular RAG. It highlights how RAG synergistically combines the intrinsic knowledge of LLMs with external databases, enhancing accuracy and credibility, particularly for knowledge-intensive tasks. The paper also introduces an evaluation framework and benchmark for RAG systems.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2406.13213\n",
      "Traditional RAG applications struggle with multi-hop questions, which require retrieving and reasoning over multiple pieces of evidence. The paper introduces Multi-Meta-RAG, a method that uses database filtering with LLM-extracted metadata to improve the selection of relevant documents. Multi-Meta-RAG significantly improves results on the MultiHop-RAG benchmark.\n",
      "\n",
      "## DOI: 10.48550/arXiv.2404.13948\n",
      "This work investigates the robustness of RAG systems, particularly their vulnerability to noisy documents with minor textual errors. It introduces a novel attack method, Genetic Attack on RAG (GARAG), to reveal vulnerabilities and test system functionality against such documents. Experimental results highlight the risk of minor textual inaccuracies in disrupting RAG systems.\n",
      "\n",
      "## DOI: 10.6109/jkiice.2023.27.12.1489\n",
      "The paper proposes Metadata Retrieval-Augmented Generation (M-RAG) for effective search in open-domain question answering (ODQA) systems. M-RAG utilises embeddings with metadata and generative models like gpt-3.5-turbo-16k and gpt-4. Experimental results show significant performance improvements compared to existing RAG and ODQA methods.\n",
      "My lady, I hope this overview of RAG and its various applications is to your liking.\n",
      "45\n",
      "For query: ['tell me about how RAG works.']:\n",
      "Precision: 0.911\n",
      "Recall: 0.911\n",
      "F1-Score: 0.911\n",
      "Accuracy: 0.911\n",
      "Balanced accuracy: 0.775\n",
      "Faithfulness score: 5\n",
      "Documents score: [(0.6350646, '10.1007/978-3-031-88708-6_3'), (0.5265625, '10.48550/arXiv.2312.10997'), (0.4896749, '10.48550/arXiv.2406.13213'), (0.32794857, '10.48550/arXiv.2404.13948'), (0.32209986, '10.6109/jkiice.2023.27.12.1489')]\n",
      "\u001b[91mWaiting for 10 seconds...\n",
      "The timeout has elapsed!\n",
      "\u001b[96mWorking on row: 13\n",
      "\u001b[95m!!!!! All Done!!!!!\n"
     ]
    }
   ],
   "source": [
    "#run the test from here\n",
    "\n",
    "\n",
    "#***** Begin chat session *****\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_jats\"\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_multi_lang\"\n",
    "\n",
    "# read documents and dois from the directory path\n",
    "documents_with_doi = read_documents_with_doi(directory_path)\n",
    "documents = [doc[0].split('\\n')[1:] for doc in documents_with_doi]\n",
    "print(f\"Length of documents: {len(documents)}\")\n",
    "print(f\"Length of corpus: {len(documents_with_doi)}\")\n",
    "\n",
    "# Countdown function\n",
    "def countdown(seconds:int)->None:\n",
    "    # Loop until seconds is 0\n",
    "    while seconds > 0:\n",
    "        print(Fore.LIGHTMAGENTA_EX + f\"{seconds}\", end='      \\r')  # Print current countdown value\n",
    "        time.sleep(1)  # Wait for 1 second\n",
    "        seconds -= 1  # Decrease seconds by 1\n",
    "    print(\"The timeout has elapsed!\")  # Countdown finished message\n",
    "\n",
    "def evaluate_retrieval(retrieved_dois, ground_truth, response, query:str,reranked_DOIs_with_score_end)->Dict:\n",
    "    corpus_doi_list = []\n",
    "    #corpus_list is a global variable in rag_pipeline()\n",
    "    for each in range(len(documents_with_doi)):\n",
    "        #a = documents_with_doi[each].get('doi',\"\")\n",
    "        a = documents_with_doi[each].split(\"\\n\")[0].lstrip(\"DOI: \")\n",
    "        corpus_doi_list.append(a)\n",
    "    print(len(corpus_doi_list))\n",
    "\n",
    "    def compare_lists(list1, list2, list3):\n",
    "        for val in list1:\n",
    "            if val in list2:\n",
    "                list3.append(1)\n",
    "            else:\n",
    "                list3.append(0)\n",
    "\n",
    "    #set y_true so that len(y_true)==len(corpus_doi_list)\n",
    "    y_true = []\n",
    "    compare_lists(corpus_doi_list,ground_truth,y_true)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = []\n",
    "    compare_lists(corpus_doi_list,retrieved_dois,y_pred)\n",
    "\n",
    "\n",
    "    # calculate metrics - could also use sklearn.metrics functions such as precision_score, but this is easier to read\n",
    "    precision = precision_score(y_true, y_pred, average=\"micro\")\n",
    "    recall = recall_score(y_true, y_pred,average=\"micro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    faithfulness_score = 0\n",
    "    for each in retrieved_dois:\n",
    "        if each in response.message.content[0].text:\n",
    "            faithfulness_score+=1\n",
    "        else:\n",
    "            faithfulness_score+=0\n",
    "\n",
    "        \n",
    "    return {\n",
    "        'Query':f\"{query}\",\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\":accuracy,\n",
    "        \"Balanced accuracy\":balanced_accuracy,\n",
    "        \"Faithfulness score\":faithfulness_score,\n",
    "        \"Documents score\":str(reranked_DOIs_with_score_end),\n",
    "        \"Response\":response.message.content[0].text\n",
    "    }\n",
    "\n",
    "def print_results(retrieved_dois, ground_truth, response, query:str, reranked_DOIs_with_score_end)->Dict:\n",
    "    \"\"\"\n",
    "    Prints a nicely ordered set of results from evalaute_retrieval()\n",
    "    \"\"\"\n",
    "\n",
    "    results = evaluate_retrieval(retrieved_dois, ground_truth, response, query, reranked_DOIs_with_score_end)\n",
    "    print(f\"For query: {results['Query']}:\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"F1-Score: {results['F1-Score']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    print(f\"Balanced accuracy: {results['Balanced accuracy']:.3f}\")\n",
    "    print(f\"Faithfulness score: {results['Faithfulness score']}\")\n",
    "    print(f\"Documents score: {results['Documents score']}\")\n",
    "    return results\n",
    "\n",
    "#print_results()\n",
    "\n",
    "def cohere_test_loop(query:str,ground_truth:List[str]):\n",
    "\n",
    "    # set top_k\n",
    "    top_k = 5\n",
    "    #set threshold \n",
    "    threshold = 0.10\n",
    "\n",
    "    response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,query,top_k,threshold)\n",
    "    \n",
    "    # Extract DOIs from retrieved documents\n",
    "    retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "    print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "    # Display the response\n",
    "    print(Fore.LIGHTYELLOW_EX + f\"{response.message.content[0].text}\")\n",
    "\n",
    "    new_result = print_results(retrieved_dois, ground_truth, response, query, reranked_DOIs_with_score_end)\n",
    "    # add the new result to the df\n",
    "    results_df.loc[len(results_df)] = new_result\n",
    "\n",
    "    #save the queries and responses to separate dataframe to be manually annontated\n",
    "    answer_relevance_df = results_df[['Query','Response']].copy(deep=True)\n",
    "\n",
    "    # save out answer_relevance_df\n",
    "    filename=\"analysis/dense_answer_relevance_results.xlsx\"\n",
    "    answer_relevance_df.to_excel(filename)\n",
    "\n",
    "    filename = \"analysis/dense_analysis_results.xlsx\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    results_df.to_excel(filename)\n",
    "\n",
    "    # rate limit functions\n",
    "    seconds = 10\n",
    "    print(Fore.LIGHTRED_EX + f\"Waiting for {seconds} seconds...\")\n",
    "    countdown(seconds)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "#golden_set_df_test['Response\\nDense'] = golden_set_df_test.apply(lambda x: test_loop(x.query,x.ground_truth), axis=1)\n",
    "golden_set_df_query = golden_set_df['query'].to_list()\n",
    "golden_set_df_ground_truth = golden_set_df['ground_truth'].to_list()\n",
    "\n",
    "loop_length = 5\n",
    "while loop_length:\n",
    "    for i in range(len(golden_set_df_query)):\n",
    "        \n",
    "        cohere_test_loop([golden_set_df_query[i]],golden_set_df_ground_truth[i])\n",
    "        print(Fore.LIGHTCYAN_EX + f\"Working on row: {i}\")\n",
    "    loop_length = loop_length-1\n",
    "\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"!!!!! All Done!!!!!\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
