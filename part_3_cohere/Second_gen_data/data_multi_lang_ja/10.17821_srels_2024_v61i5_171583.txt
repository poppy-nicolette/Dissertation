DOI: 10.17821/srels/2024/v61i5/171583
Title: 図書館における質問回答ベースの検索システムの設計: オープンソースの検索拡張生成 (RAG) パイプラインのアプリケーション Designing Question-Answer Based Search System in Libraries: Application of Open Source Retrieval Augmented Generation (RAG) Pipeline
Abstract: この研究の主な目的は、プロトタイプを準備し、図書館が検索拡張生成 (RAG) フレームワークを通じてオープンソース ソフトウェア ツールと大規模言語モデル (LLM) を使用して低コストの会話型検索システムを開発できることを実証することです。 LLM は幻覚を起こし、時代遅れで文脈を理解していない応答を返すことがよくあります。ただし、この実験は、LLM が一連の関連文書で強化された場合、文脈に応じた適切な応答を提供できることを示しています。回答を生成する前に関連ドキュメントで LLM を拡張することは、検索拡張生成として知られています。この方法論には、LangChain などのツール、ChromaDB などのベクトル データベース、Llama3 (700 億のパラメーター ベースのモデル) などのオープンソース LLM を使用して RAG パイプラインを作成することが含まれていました。開発されたプロトタイプには、収集、処理され、パイプラインに取り込まれたチャンドラヤーン 3 ミッションに関する 250 以上の関連文書のデータセットが含まれています。最後に、研究では標準的な LLM と RAG 拡張を備えた LLM からの応答を比較しました。主な調査結果から、標準的な LLM (RAG なし) は、チャンドラヤーン 3 に関連するクエリに対して自信を持って不正確で幻覚のような応答を生成するのに対し、RAG を使用する LLM は、応答を生成する前に関連文書のセットが提供されると、一貫して正確で有益な、文脈に沿った応答を提供することが明らかになりました。この調査では、オープンソースの RAG ベースのシステムは、情報検索を強化し、図書館を動的な情報サービスに変えるための費用対効果の高いソリューションを図書館に提供すると結論付けています。 This study primarily aims to prepare a prototype and demonstrate that libraries can develop a low-cost conversational search system using open-source software tools and Large Language Models (LLMs) through a Retrieval-Augmented Generation (RAG) framework. LLMs often hallucinate and provide outdated and non-contextualized responses. However, this experiment shows that LLMs can deliver contextualized, relevant responses when augmented with a set of relevant documents. Augmenting LLMs with relevant documents before generating answers is known as retrieval-augmented generation. The methodology involved creating a RAG pipeline using tools like LangChain, vector databases like ChromaDB, and open-source LLMs like Llama3 (a 70-billion parameter-based model). The prototype developed includes a dataset of 250+ relevant documents on the Chandrayaan-3 mission that was collected, processed, and ingested into the pipeline. Finally, the study compared responses from standard LLMs and LLMs with RAG augmentation. Key findings revealed that standard LLMs (without RAG) produced confidently incorrect, hallucinated responses against queries related to Chandrayaan-3, while LLMs with RAG consistently provided accurate, informative, and contextualized answers when supplied with a set of relevant documents before generating the response. The study concluded that open-source RAG-based systems offer a cost-effective solution for libraries to enhance information retrieval and transform libraries into dynamic information services.