DOI: 10.48550/arXiv.1607.04606
Title: サブワード情報による単語ベクトルの強化 Enriching Word Vectors with Subword Information
Abstract: ラベルのない大規模なコーパスでトレーニングされた連続単語表現は、多くの自然言語処理タスクに役立ちます。このような表現を学習する一般的なモデルは、各単語に個別のベクトルを割り当てることにより、単語の形態を無視します。これは、特に語彙が多く、珍しい単語が多い言語の場合に制限となります。この論文では、各単語が文字 N グラムのバッグとして表現されるスキップグラム モデルに基づいた新しいアプローチを提案します。ベクトル表現は各文字 n グラムに関連付けられます。単語はこれらの表現の合計として表現されます。私たちの方法は高速なので、大規模なコーパスでモデルを迅速にトレーニングでき、トレーニング データに現れなかった単語の単語表現を計算できます。私たちは、単語の類似性と類推タスクの両方について、9 つの異なる言語で単語の表現を評価します。最近提案された形態素単語表現と比較することにより、私たちのベクトルがこれらのタスクで最先端のパフォーマンスを達成することを示します。 Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.