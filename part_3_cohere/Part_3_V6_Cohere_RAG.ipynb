{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohere RAG with dense retriever and ReRank model\n",
    "- references: https://docs.cohere.com/v2/docs/rag-complete-example\n",
    "- [ ] check performance with SciFact\n",
    "<br>\n",
    "“This work was supported by compute credits from a Cohere Labs Research Grant, these grants are designed to support academic partners conducting research with the goal of releasing scientific artifacts and data for good projects.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import cohere\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import sys\n",
    "from colorama import Fore, Style, Back\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mall is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "# load secret from local .env file\n",
    "def get_key():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    _key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "    #verify if it worked\n",
    "    if _key is not None:\n",
    "        print(Fore.GREEN + \"all is good, beautiful!\")\n",
    "        return _key\n",
    "    else:\n",
    "        print(Fore.LIGHTRED_EX + \"API Key is missing\")\n",
    "\n",
    "# initilize client\n",
    "co = cohere.ClientV2(get_key())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# load documents\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0]\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append(f\"{doi} {text}\\n\")\n",
    "    return documents_with_doi\n",
    "\n",
    "# initialize empty search query\n",
    "search_queries = []\n",
    "# Embed the documents\n",
    "def document_embed(documents:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the documents from a list provided from read_documents_with_doi()\n",
    "    \"\"\"\n",
    "    doc_emb = co.embed(\n",
    "        #model=\"embed-v4.0\",\n",
    "        model=\"embed-english-v3.0\",\n",
    "        input_type=\"search_document\",\n",
    "        texts=[doc for doc in documents],\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return doc_emb\n",
    "\n",
    "# Embed the search query\n",
    "def query_embed(search_queries:List[str])->List[float]:\n",
    "    \"\"\"\n",
    "    Embeds the query from a list provided in search_queries variable\n",
    "    \"\"\"\n",
    "    query_emb = co.embed(\n",
    "        #model=\"embed-v4.0\",\n",
    "        model=\"embed-english-v3.0\",\n",
    "        input_type=\"search_query\",\n",
    "        texts=search_queries,\n",
    "        embedding_types=[\"float\"],\n",
    "        ).embeddings.float\n",
    "    return query_emb\n",
    "\n",
    "# retrieve top_k and compute similarity using dot product\n",
    "def retrieve_top_k(top_k, query_embedded, documents_embedded, documents)->List[str]:\n",
    "    \"\"\"\n",
    "    returns the top_k documents based on dot product similarity\n",
    "    \"\"\"\n",
    "\n",
    "    scores = np.dot(query_embedded, np.transpose(documents_embedded))[0]#ordered list!\n",
    "    # takes top scores, and returns sorted list and returns indices sliced by top_k\n",
    "    max_idx = np.argsort(-scores)[:top_k]\n",
    "    # returns documents by index\n",
    "    retrieved_docs = [documents[item] for item in max_idx]\n",
    "    # returns a list of documents\n",
    "    return retrieved_docs\n",
    "\n",
    "def rerank_documents(retrieved_documents,search_queries,threshold,top_k)->List[str]:\n",
    "    \"\"\"\n",
    "    takes retrieved_documents as input along with search_queries and runs them through the \n",
    "    rerank model from cohere for semantic similarity. \n",
    "\n",
    "    top_n = top_k\n",
    "    Limits those returned by a threshold score. this is to reduce those that are irrelevant.\n",
    "    \"\"\"\n",
    "    # Rerank the documents\n",
    "    results = co.rerank(\n",
    "        #model=\"rerank-v3.5\",\n",
    "        model=\"rerank-english-v3.0\",\n",
    "        query=search_queries[0],\n",
    "        documents=[doc for doc in retrieved_documents],\n",
    "        top_n=top_k,\n",
    "        max_tokens_per_doc=4096,# defaults to 4096\n",
    "    )\n",
    "\n",
    "    # Display the reranking results\n",
    "    #for idx, result in enumerate(results.results):\n",
    "    #    print(f\"Rank: {idx+1}\")\n",
    "    #    print(f\"Score: {result.relevance_score}\")\n",
    "    #    print(f\"Document: {retrieved_documents[result.index]}\\n\")\n",
    "\n",
    "    #returns only those over threshold\n",
    "    reranked_docs = [\n",
    "        retrieved_documents[result.index] for result in results.results if result.relevance_score >=threshold\n",
    "    ]\n",
    "    reranked_with_score = [(result.relevance_score, retrieved_documents[result.index].split(\"\\n\")[0].strip(\"DOI: \")) for result in results.results if result.relevance_score >=threshold]\n",
    "\n",
    "    print(f\"reranked_documents: {reranked_docs}\")\n",
    "    print(f\"length of reranked_documents: {len(reranked_docs)}\")\n",
    "\n",
    "    return reranked_docs, reranked_with_score\n",
    "\n",
    "def cohere_rag_pipeline(directory_path,search_queries,top_k,threshold):\n",
    "\n",
    "    # retrieve documents from directory\n",
    "    documents = read_documents_with_doi(directory_path)\n",
    "    print(f\"Length of documents: {len(documents)}\")\n",
    "    # embed the documents\n",
    "    documents_embedded = document_embed(documents)\n",
    "\n",
    "    #embed the query:\n",
    "    query_embedded = query_embed(search_queries)\n",
    "\n",
    "    # retrieve the top_k documents\n",
    "    retrieved_documents = retrieve_top_k(top_k, query_embedded, documents_embedded, documents)\n",
    "\n",
    "    # rerank the documents using the Rerank model from Cohere\n",
    "    reranked_documents, reranked_DOIs_with_score = rerank_documents(retrieved_documents,search_queries,threshold,top_k)\n",
    "    # set system instructions\n",
    "    instructions = \"\"\"\n",
    "                    You are an academic research assistant.\n",
    "                    You must include the DOI in your response.\n",
    "                    If there is no content provided, ask for a different question.\n",
    "                    Please structure your response like this:\n",
    "                    Summary: summary statement here. \n",
    "                    DOI: summary of the text associated with this DOI.\n",
    "                    Address me as, 'my lady'.\n",
    "                    \"\"\"\n",
    "    # create messages to model\n",
    "    messages = [{\"role\":\"user\",\n",
    "                \"content\": search_queries[0]},\n",
    "                {\"role\":\"system\",\n",
    "                \"content\":instructions}]\n",
    "\n",
    "    # Generate the response\n",
    "    resp = co.chat(\n",
    "        #model=\"command-a-03-2025\",\n",
    "        model=\"command-r-08-2024\",\n",
    "        #model=\"command-r7b-12-2024\", #https://docs.cohere.com/docs/command-r7b\n",
    "        messages=messages,\n",
    "        documents=reranked_documents,\n",
    "    )\n",
    "\n",
    "    return resp, reranked_documents, reranked_DOIs_with_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debugging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#  run here to test functions avove\n",
    "# ****** Pipeline ********\n",
    "# set directory path\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "# initialize search_queries \n",
    "search_queries = [input(\"what is your query?\")]#could be a list of multiple queries\n",
    "# set top_k\n",
    "top_k = 5\n",
    "#set threshold \n",
    "threshold = 0.1\n",
    "\n",
    "response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,search_queries,top_k,threshold)\n",
    "# Display the response\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"{response.message.content[0].text}\")\n",
    "print(Fore.LIGHTCYAN_EX + f\"------\\nReranked documents:\")\n",
    "for doc in reranked_documents_end:\n",
    "    print(doc)\n",
    "\n",
    "# Display the citations and source documents\n",
    "if response.message.citations:\n",
    "    print(Fore.LIGHTYELLOW_EX + \"\\nCITATIONS:\")\n",
    "    for citation in response.message.citations:\n",
    "        print(f\"source text: {citation.text},\\nsource: {citation.sources[0].document.get('content').split(\"\\n\")[0]}\\n------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# Analysis\n",
    "Precision, recall, accuracy, F1 scores and faithfulness\n",
    "## Precision, recall, F1 score\n",
    "### references\n",
    "- https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html\n",
    "- https://vitalflux.com/accuracy-precision-recall-f1-score-python-example/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, balanced_accuracy_score\n",
    "import openpyxl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from colorama import Fore, Back, Style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## automated version \n",
    "Currently Works!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1-Score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Balanced accuracy</th>\n",
       "      <th>Faithfulness score</th>\n",
       "      <th>Documents score</th>\n",
       "      <th>Response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Query, Precision, Recall, F1-Score, Accuracy, Balanced accuracy, Faithfulness score, Documents score, Response]\n",
       "Index: []"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#initial dataframe to capture results from each query and results\n",
    "#ONLY DO THIS AT THE BEGINNING OF THE ANALYSIS PROCEDURE, OTHERWISE, IT WILL ERASE THE PREVIOUS RESULTS!!\n",
    "\n",
    "results_df = pd.DataFrame(columns=['Query','Precision','Recall','F1-Score','Accuracy', 'Balanced accuracy', 'Faithfulness score', 'Documents score', 'Response'])\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m Golder set loaded!\n"
     ]
    }
   ],
   "source": [
    "golden_set_df = pd.read_excel(\"golden_set.xlsx\")\n",
    "#golden_set_df_test = golden_set_df.head(3)\n",
    "#golden_set_df\n",
    "print(Fore.LIGHTGREEN_EX + f\" Golder set loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "#run the test from here\n",
    "\n",
    "\n",
    "#***** Begin chat session *****\n",
    "# set directory path\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "#directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_jats\"\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data_multi_lang\"\n",
    "\n",
    "# read documents and dois from the directory path\n",
    "documents_with_doi = read_documents_with_doi(directory_path)\n",
    "documents = [doc[0].split('\\n')[1:] for doc in documents_with_doi]\n",
    "print(f\"Length of documents: {len(documents)}\")\n",
    "print(f\"Length of corpus: {len(documents_with_doi)}\")\n",
    "\n",
    "# Countdown function\n",
    "def countdown(seconds:int)->None:\n",
    "    # Loop until seconds is 0\n",
    "    while seconds > 0:\n",
    "        print(Fore.LIGHTMAGENTA_EX + f\"{seconds}\", end='      \\r')  # Print current countdown value\n",
    "        time.sleep(1)  # Wait for 1 second\n",
    "        seconds -= 1  # Decrease seconds by 1\n",
    "    print(\"The time has come!\")  # Countdown finished message\n",
    "\n",
    "def evaluate_retrieval(retrieved_dois, ground_truth, response, query:str,reranked_DOIs_with_score_end)->Dict:\n",
    "    corpus_doi_list = []\n",
    "    #corpus_list is a global variable in rag_pipeline()\n",
    "    for each in range(len(documents_with_doi)):\n",
    "        #a = documents_with_doi[each].get('doi',\"\")\n",
    "        a = documents_with_doi[each].split(\"\\n\")[0].lstrip(\"DOI: \")\n",
    "        corpus_doi_list.append(a)\n",
    "    print(len(corpus_doi_list))\n",
    "\n",
    "    def compare_lists(list1, list2, list3):\n",
    "        for val in list1:\n",
    "            if val in list2:\n",
    "                list3.append(1)\n",
    "            else:\n",
    "                list3.append(0)\n",
    "\n",
    "    #set y_true so that len(y_true)==len(corpus_doi_list)\n",
    "    y_true = []\n",
    "    compare_lists(corpus_doi_list,ground_truth,y_true)\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = []\n",
    "    compare_lists(corpus_doi_list,retrieved_dois,y_pred)\n",
    "\n",
    "\n",
    "    # calculate metrics - could also use sklearn.metrics functions such as precision_score, but this is easier to read\n",
    "    precision = precision_score(y_true, y_pred, average=\"micro\")\n",
    "    recall = recall_score(y_true, y_pred,average=\"micro\")\n",
    "    f1 = f1_score(y_true, y_pred, average=\"micro\")\n",
    "    accuracy = accuracy_score(y_true, y_pred, normalize=True)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    faithfulness_score = 0\n",
    "    for each in retrieved_dois:\n",
    "        if each in response.message.content[0].text:\n",
    "            faithfulness_score+=1\n",
    "        else:\n",
    "            faithfulness_score+=0\n",
    "\n",
    "        \n",
    "    return {\n",
    "        'Query':f\"{query}\",\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1-Score\": f1,\n",
    "        \"Accuracy\":accuracy,\n",
    "        \"Balanced accuracy\":balanced_accuracy,\n",
    "        \"Faithfulness score\":faithfulness_score,\n",
    "        \"Documents score\":str(reranked_DOIs_with_score_end),\n",
    "        \"Response\":response.message.content[0].text\n",
    "    }\n",
    "\n",
    "def print_results(retrieved_dois, ground_truth, response, query:str, reranked_DOIs_with_score_end)->Dict:\n",
    "    \"\"\"\n",
    "    Prints a nicely ordered set of results from evalaute_retrieval()\n",
    "    \"\"\"\n",
    "\n",
    "    results = evaluate_retrieval(retrieved_dois, ground_truth, response, query, reranked_DOIs_with_score_end)\n",
    "    print(f\"For query: {results['Query']}:\")\n",
    "    print(f\"Precision: {results['Precision']:.3f}\")\n",
    "    print(f\"Recall: {results['Recall']:.3f}\")\n",
    "    print(f\"F1-Score: {results['F1-Score']:.3f}\")\n",
    "    print(f\"Accuracy: {results['Accuracy']:.3f}\")\n",
    "    print(f\"Balanced accuracy: {results['Balanced accuracy']:.3f}\")\n",
    "    print(f\"Faithfulness score: {results['Faithfulness score']}\")\n",
    "    print(f\"Documents score: {results['Documents score']}\")\n",
    "    return results\n",
    "\n",
    "#print_results()\n",
    "\n",
    "def cohere_test_loop(query:str,ground_truth:List[str]):\n",
    "\n",
    "    # set top_k\n",
    "    top_k = 5\n",
    "    #set threshold \n",
    "    threshold = 0.10\n",
    "\n",
    "    response, reranked_documents_end, reranked_DOIs_with_score_end = cohere_rag_pipeline(directory_path,query,top_k,threshold)\n",
    "    \n",
    "    # Extract DOIs from retrieved documents\n",
    "    retrieved_dois = [doc.split(\"\\n\")[0].strip(\"DOI: \") for doc in reranked_documents_end]\n",
    "    print(\"Retrieved DOIs:\", retrieved_dois)\n",
    "\n",
    "    # Display the response\n",
    "    print(Fore.LIGHTYELLOW_EX + f\"{response.message.content[0].text}\")\n",
    "\n",
    "    new_result = print_results(retrieved_dois, ground_truth, response, query, reranked_DOIs_with_score_end)\n",
    "    # add the new result to the df\n",
    "    results_df.loc[len(results_df)] = new_result\n",
    "\n",
    "    #save the queries and responses to separate dataframe to be manually annontated\n",
    "    answer_relevance_df = results_df[['Query','Response']].copy(deep=True)\n",
    "\n",
    "    # save out answer_relevance_df\n",
    "    filename=\"analysis/dense_answer_relevance_results.xlsx\"\n",
    "    answer_relevance_df.to_excel(filename)\n",
    "\n",
    "    filename = \"analysis/dense_analysis_results.xlsx\"\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    results_df.to_excel(filename)\n",
    "\n",
    "    # rate limit functions\n",
    "    seconds = 10\n",
    "    print(Fore.LIGHTRED_EX + f\"Waiting for {seconds} seconds...\")\n",
    "    countdown(seconds)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "#golden_set_df_test['Response\\nDense'] = golden_set_df_test.apply(lambda x: test_loop(x.query,x.ground_truth), axis=1)\n",
    "golden_set_df_query = golden_set_df['query'].to_list()\n",
    "golden_set_df_ground_truth = golden_set_df['ground_truth'].to_list()\n",
    "\n",
    "loop_length = 5\n",
    "while loop_length:\n",
    "    for i in range(len(golden_set_df_query)):\n",
    "        \n",
    "        cohere_test_loop([golden_set_df_query[i]],golden_set_df_ground_truth[i])\n",
    "        print(Fore.LIGHTCYAN_EX + f\"Working on row: {i}\")\n",
    "    loop_length = loop_length-1\n",
    "\n",
    "print(Fore.LIGHTMAGENTA_EX + f\"!!!!! All Done!!!!!\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
