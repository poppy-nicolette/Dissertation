{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create questions\n",
    "This notebook creates questions using LLM-assist from Cohere models. <br>\n",
    "This follows the method outlined in Teixera de Lima et al., (2025), Wang et al., (2024), and Bruni et al., (2025)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mall is good, beautiful!\n"
     ]
    }
   ],
   "source": [
    "import cohere\n",
    "from typing import List, Dict, Tuple\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "import time # for timing functions\n",
    "import sys\n",
    "from colorama import Fore, Style, Back\n",
    "import pandas as pd\n",
    "\n",
    "# load secret from local .env file\n",
    "def get_key():\n",
    "    #load secret .env file\n",
    "    load_dotenv()\n",
    "\n",
    "    #store credentials\n",
    "    _key = os.getenv('COHERE_API_KEY')\n",
    "\n",
    "    #verify if it worked\n",
    "    if _key is not None:\n",
    "        print(Fore.GREEN + \"all is good, beautiful!\")\n",
    "        return _key\n",
    "    else:\n",
    "        print(Fore.LIGHTRED_EX + \"API Key is missing\")\n",
    "\n",
    "# Initialize Cohere client\n",
    "co = cohere.ClientV2(get_key(),client_name=\"little_steve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up functions\n",
    "\n",
    "def embed_documents(documents: List[str]) -> List[List[float]]:\n",
    "    \"\"\"Generate embeddings for a list of documents.\"\"\"\n",
    "    response = co.embed(texts=documents, model=\"embed-v4.0\",input_type=\"search_query\").embeddings\n",
    "    return response\n",
    "\n",
    "def generate_questions(documents: List[str]) -> List[str]:\n",
    "    \"\"\"Generate questions using the chat endpoint in the V2 API.\"\"\"\n",
    "    instruction = \"\"\"\n",
    "    Develop questions for an undergraduate exam based on the given documents. \n",
    "    Each simple, single fact question must not be a compound question using a conjunction.\n",
    "    Generate a simple, single fact, question that requires reasoning across at least 5 of the following documents:\n",
    "    \"\"\"\n",
    "    prompt = f\"{instruction} \\n\".join(documents)\n",
    "    response = co.chat(\n",
    "        messages=[{\"role\":\"user\",\"content\":prompt}],#V2 requires an object\n",
    "        model=\"command-a-03-2025\",\n",
    "        temperature=0.2, # default is 3\n",
    "        max_tokens=500,\n",
    "    )\n",
    "    return [response.message.content[0].text]  # Adjust if you want multiple questions\n",
    "\n",
    "def find_true_positives(question: str, documents: List[str]) -> List[str]:\n",
    "    \"\"\"Identify the top 5 documents relevant to the question using the rerank endpoint.\"\"\"\n",
    "    response = co.rerank(\n",
    "        query=question,\n",
    "        documents=documents,\n",
    "        model=\"rerank-english-v3.0\",\n",
    "        top_n=5,\n",
    "    )\n",
    "    return [doc for doc in response.results]\n",
    "\n",
    "#read documents as .txt files in data director\n",
    "def read_documents_with_doi(directory_path: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Reads documents and their DOIs from individual files in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): Path to the directory containing the document files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, str]]: A list of dictionaries, each containing 'doi' and 'text' keys.\n",
    "    \"\"\"\n",
    "\n",
    "    documents_with_doi = []\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                lines = file.readlines()\n",
    "                if len(lines) >= 1:\n",
    "                    doi = lines[0]\n",
    "                    text = \"\".join(lines[1:]).strip()\n",
    "                    documents_with_doi.append(f\"{doi} {text}\\n\")\n",
    "    return documents_with_doi\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    # Step 2: Generate questions\n",
    "    questions = generate_questions(documents)\n",
    "    \n",
    "    # Step 3: Find true positives for each question\n",
    "    for question in questions:\n",
    "        true_positives = find_true_positives(question, documents)\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"True Positives by DOI: {[documents[doc.index].split(\"\\n\")[0].strip(\"DOI: \") for doc in true_positives]}\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# go ahead and embed so that we don't have to do this each time\n",
    "directory_path = \"/Users/poppyriddle/Documents/PhD/Research_proposal/Part_3/part_3_cohere/data\"\n",
    "documents = read_documents_with_doi(directory_path)\n",
    "#embeddings = embed_documents(documents)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: **Question:**  \n",
      "What are the key challenges and opportunities associated with the use of OpenAlex as a bibliometric data source, and how does it compare to traditional databases like Web of Science and Scopus in terms of metadata accuracy, coverage, and inclusivity?  \n",
      "\n",
      "**Explanation:**  \n",
      "This question requires reasoning across at least five documents:  \n",
      "1. **DOI: 10.48550/arXiv.2401.16359** (Reference Coverage Analysis of OpenAlex compared to Web of Science and Scopus)  \n",
      "2. **DOI: 10.48550/arXiv.2404.17663** (An analysis of the suitability of OpenAlex for bibliometric analyses)  \n",
      "3. **DOI: 10.48550/arXiv.2406.15154** (Analysis of the Publication and Document Types in OpenAlex, Web of Science, Scopus, Pubmed and Semantic Scholar)  \n",
      "4. **DOI: 10.48550/arXiv.2409.10633** (Evaluating the Linguistic Coverage of OpenAlex: An Assessment of Metadata Accuracy and Completeness)  \n",
      "5. **DOI: 10.1590/SciELOPreprints.11205** (On the Open Road to Universal Indexing: OpenAlex and OpenJournal Systems)  \n",
      "\n",
      "The question integrates insights from these documents to address OpenAlex's challenges (e.g., metadata accuracy, structural inequities), opportunities (e.g., inclusivity, open access), and its comparative performance against traditional databases.\n",
      "True Positives by DOI: ['10.48550/arXiv.2404.17663', '10.48550/arXiv.2409.10633', '10.48550/arXiv.2401.16359', '10.48550/arXiv.2406.15154', '10.1590/SciELOPreprints.11205']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    " - need to run loop for x number questions\n",
    " - collect questions, true positives, and answer in pandas df\n",
    " - save to excel file called golden_set.xlsx\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "# randomize order of documents\n",
    "random.shuffle(documents)\n",
    "embeddings = embed_documents(documents)\n",
    "# now, generate a question\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
